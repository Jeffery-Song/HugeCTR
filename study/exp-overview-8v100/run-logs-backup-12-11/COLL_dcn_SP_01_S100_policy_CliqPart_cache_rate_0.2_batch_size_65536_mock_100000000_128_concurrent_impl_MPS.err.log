2022-12-12 05:30:27.914144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.921117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.934727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.943147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.951846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.958423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.964106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:27.977171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.035857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.040614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.051429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.053085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.062940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.066664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.069319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.070523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.071495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.072771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.073605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.075019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.075818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.077380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.078059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.079747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.080026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.082270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.082466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.084562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.086063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.087477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.088985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.090682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.093038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.094694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.098039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.100664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.102193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.103274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.104308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.106442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.112653: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.115148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.116371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.117548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.118769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.120008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.121098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.122230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.123563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.123661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.125966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.126588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.127866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.128717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.128756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.129655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.131221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.131297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.132422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.134062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.134204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.134832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.136781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.136964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.137595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.139637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.139804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.140414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.143096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.143149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.143574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.146325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.146402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.146442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.146586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.150099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.150102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.150227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.153494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.153831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.155008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.156564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.157065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.158453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.159405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.169938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.170709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.171087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.171426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.171952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.173845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.175623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.176016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.176754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.177264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.178072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.195082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.198578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.212895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.213052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.214123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.214685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.215467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.217402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.217444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.218349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.219099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.220608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.221153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.221372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.222495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.222633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.224601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.226989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.228198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.228241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.229091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.229137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.229946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.232539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.232918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.233380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.233388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.233502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.234234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.237946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.238135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.238149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.238216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.238746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.242370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.242461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.242561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.243239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.245808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.246006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.246058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.246506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.249100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.249187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.249323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.249985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.252805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.253201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.253359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.253690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.257129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.257811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.257956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.258266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.260519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.261296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.261351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.261525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.264131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.264855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.265104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.267215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.267850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.267895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.269293: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.270062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.270474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.270810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.271370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.273062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.273882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.274386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.275005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.278592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.279424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.279889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.279955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.280292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.282059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.282960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.283731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.284438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.284482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.284875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.286744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.286917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.287645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.288581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.289276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.289329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.289753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.292194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.292424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.293004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.293860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.294598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.295216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.297042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.297687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.298541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.299758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.301157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.302301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.304158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.304322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.305005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.306057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.306754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.307470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.309647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.309704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.310377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.311210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.312845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.314514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.314573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.315055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.315924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.316749: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.317153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.318860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.319464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.320074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.322330: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.323717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.324433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.325025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.325037: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.327287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.327522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.327550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.328568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.331361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.331540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.331663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.332472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.332838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.335313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.335496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.335526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.335920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.336906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.337674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.342443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.342500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.342599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.353464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.355606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.358487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.358597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.358730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.360967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.363597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.363820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.366252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.398094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.398247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.401158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.431379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.432871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.434691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.438616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.440332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.442170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.446446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.447434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.448571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.453222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.453949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.456397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.460535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.461365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.463716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.465827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.467547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.473026: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.483450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.484827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.486333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.489260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.489948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.491059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.496169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.498736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.504416: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.515059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.535410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.536880: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:30:28.546892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.601298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.604524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:28.611899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.540850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.541464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.542163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.542648: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.542711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:30:29.560827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.561909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.564160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.566249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.568751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.570014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:30:29.616968: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.617165: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.660484: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:30:29.786095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.787211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.788762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.790552: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.790626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:30:29.808903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.810383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.812358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.815325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.816409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.817551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:30:29.844580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.845202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.845916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.846590: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.846653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:30:29.864912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.865549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.866070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.866591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.866732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.867910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.868286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.868929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.869327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:30:29.869754: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.869816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:30:29.874807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.875413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.875948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.876409: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.876460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:30:29.887604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.888230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.888742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.889306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.889823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.890289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:30:29.894651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.895284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.895808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.896370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.896952: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.897146: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.897452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.897975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:30:29.898942: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:30:29.930290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.930930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.931470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.931946: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.932006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:30:29.936393: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.936589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.938413: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 05:30:29.944200: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.944372: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.946097: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:30:29.950097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.950729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.951928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.952509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.953052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.953526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:30:29.955726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.959172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.960026: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.960193: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.960284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.960339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.961250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.961326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.961983: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:30:29.962153: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.962212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:30:29.962225: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:30:29.962275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:30:29.979962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.980629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.980986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.981142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.982122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.982195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.983067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.983201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.984002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.984067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:30:29.984533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:30:29.985012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:30:29.999611: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:29.999794: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:30.001830: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:30:30.029172: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:30.029373: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:30.029419: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:30.029576: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:30:30.031195: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:30:30.031369: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][05:30:31.296][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.296][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.297][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.297][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.297][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.298][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.299][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:30:31.299][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 97it [00:01, 82.96it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.62s/it]warmup run: 194it [00:01, 179.81it/s]warmup run: 97it [00:01, 81.04it/s]warmup run: 101it [00:01, 83.61it/s]warmup run: 101it [00:01, 82.92it/s]warmup run: 100it [00:01, 82.09it/s]warmup run: 96it [00:01, 80.32it/s]warmup run: 100it [00:01, 83.43it/s]warmup run: 89it [00:01, 71.99it/s]warmup run: 291it [00:01, 286.15it/s]warmup run: 192it [00:01, 173.92it/s]warmup run: 203it [00:01, 182.75it/s]warmup run: 200it [00:01, 178.64it/s]warmup run: 201it [00:01, 179.35it/s]warmup run: 194it [00:01, 176.53it/s]warmup run: 202it [00:01, 183.31it/s]warmup run: 172it [00:01, 150.69it/s]warmup run: 388it [00:01, 396.55it/s]warmup run: 287it [00:01, 276.47it/s]warmup run: 305it [00:01, 292.49it/s]warmup run: 300it [00:01, 285.02it/s]warmup run: 298it [00:01, 283.07it/s]warmup run: 291it [00:01, 281.60it/s]warmup run: 304it [00:01, 293.65it/s]warmup run: 272it [00:01, 260.21it/s]warmup run: 484it [00:02, 502.14it/s]warmup run: 382it [00:01, 383.18it/s]warmup run: 405it [00:01, 404.20it/s]warmup run: 399it [00:01, 397.09it/s]warmup run: 400it [00:01, 397.11it/s]warmup run: 388it [00:01, 391.10it/s]warmup run: 406it [00:01, 408.70it/s]warmup run: 371it [00:02, 373.44it/s]warmup run: 581it [00:02, 599.67it/s]warmup run: 477it [00:02, 487.63it/s]warmup run: 506it [00:02, 514.70it/s]warmup run: 500it [00:02, 509.00it/s]warmup run: 491it [00:02, 488.76it/s]warmup run: 486it [00:02, 499.82it/s]warmup run: 499it [00:02, 498.91it/s]warmup run: 470it [00:02, 485.40it/s]warmup run: 674it [00:02, 671.46it/s]warmup run: 573it [00:02, 586.25it/s]warmup run: 604it [00:02, 610.62it/s]warmup run: 603it [00:02, 616.86it/s]warmup run: 593it [00:02, 597.96it/s]warmup run: 578it [00:02, 578.97it/s]warmup run: 595it [00:02, 593.27it/s]warmup run: 571it [00:02, 592.89it/s]warmup run: 771it [00:02, 744.18it/s]warmup run: 670it [00:02, 673.87it/s]warmup run: 706it [00:02, 703.87it/s]warmup run: 706it [00:02, 711.16it/s]warmup run: 694it [00:02, 690.71it/s]warmup run: 694it [00:02, 682.82it/s]warmup run: 668it [00:02, 646.99it/s]warmup run: 673it [00:02, 689.77it/s]warmup run: 870it [00:02, 806.19it/s]warmup run: 767it [00:02, 745.45it/s]warmup run: 804it [00:02, 771.57it/s]warmup run: 808it [00:02, 785.46it/s]warmup run: 795it [00:02, 768.05it/s]warmup run: 793it [00:02, 756.40it/s]warmup run: 767it [00:02, 730.26it/s]warmup run: 774it [00:02, 768.08it/s]warmup run: 970it [00:02, 856.75it/s]warmup run: 864it [00:02, 803.26it/s]warmup run: 911it [00:02, 847.50it/s]warmup run: 902it [00:02, 820.00it/s]warmup run: 896it [00:02, 828.80it/s]warmup run: 892it [00:02, 814.91it/s]warmup run: 867it [00:02, 798.45it/s]warmup run: 874it [00:02, 826.08it/s]warmup run: 1070it [00:02, 895.26it/s]warmup run: 963it [00:02, 851.92it/s]warmup run: 1015it [00:02, 897.41it/s]warmup run: 997it [00:02, 877.13it/s]warmup run: 999it [00:02, 843.52it/s]warmup run: 994it [00:02, 867.84it/s]warmup run: 968it [00:02, 854.33it/s]warmup run: 973it [00:02, 869.45it/s]warmup run: 1168it [00:02, 919.17it/s]warmup run: 1063it [00:02, 891.89it/s]warmup run: 1118it [00:02, 933.82it/s]warmup run: 1099it [00:02, 915.17it/s]warmup run: 1094it [00:02, 859.26it/s]warmup run: 1070it [00:02, 899.84it/s]warmup run: 1097it [00:02, 911.21it/s]warmup run: 1072it [00:02, 899.13it/s]warmup run: 1266it [00:02, 934.29it/s]warmup run: 1164it [00:02, 924.08it/s]warmup run: 1220it [00:02, 954.79it/s]warmup run: 1201it [00:02, 942.67it/s]warmup run: 1195it [00:02, 899.66it/s]warmup run: 1200it [00:02, 943.59it/s]warmup run: 1174it [00:02, 937.05it/s]warmup run: 1171it [00:02, 923.29it/s]warmup run: 1365it [00:02, 949.01it/s]warmup run: 1265it [00:02, 947.23it/s]warmup run: 1302it [00:02, 959.53it/s]warmup run: 1322it [00:02, 963.45it/s]warmup run: 1294it [00:02, 923.49it/s]warmup run: 1276it [00:02, 960.36it/s]warmup run: 1301it [00:02, 961.54it/s]warmup run: 1270it [00:02, 939.44it/s]warmup run: 1464it [00:03, 959.33it/s]warmup run: 1368it [00:02, 969.58it/s]warmup run: 1404it [00:03, 975.90it/s]warmup run: 1423it [00:03, 972.64it/s]warmup run: 1394it [00:02, 944.73it/s]warmup run: 1404it [00:02, 981.45it/s]warmup run: 1379it [00:02, 978.58it/s]warmup run: 1369it [00:03, 952.03it/s]warmup run: 1565it [00:03, 972.17it/s]warmup run: 1470it [00:03, 983.34it/s]warmup run: 1527it [00:03, 990.56it/s]warmup run: 1505it [00:03, 964.79it/s]warmup run: 1494it [00:03, 960.28it/s]warmup run: 1481it [00:03, 989.98it/s]warmup run: 1509it [00:03, 998.71it/s]warmup run: 1469it [00:03, 963.42it/s]warmup run: 1667it [00:03, 985.88it/s]warmup run: 1571it [00:03, 989.77it/s]warmup run: 1630it [00:03, 999.52it/s]warmup run: 1594it [00:03, 969.98it/s]warmup run: 1613it [00:03, 1010.79it/s]warmup run: 1584it [00:03, 999.23it/s]warmup run: 1604it [00:03, 878.70it/s]warmup run: 1569it [00:03, 972.21it/s]warmup run: 1767it [00:03, 988.59it/s]warmup run: 1672it [00:03, 978.86it/s]warmup run: 1732it [00:03, 1004.16it/s]warmup run: 1696it [00:03, 981.90it/s]warmup run: 1717it [00:03, 1016.67it/s]warmup run: 1686it [00:03, 1001.58it/s]warmup run: 1700it [00:03, 900.48it/s]warmup run: 1668it [00:03, 972.55it/s]warmup run: 1868it [00:03, 992.60it/s]warmup run: 1834it [00:03, 1008.53it/s]warmup run: 1771it [00:03, 957.19it/s]warmup run: 1799it [00:03, 994.48it/s]warmup run: 1820it [00:03, 1017.21it/s]warmup run: 1788it [00:03, 1002.42it/s]warmup run: 1799it [00:03, 922.86it/s]warmup run: 1767it [00:03, 974.97it/s]warmup run: 1968it [00:03, 994.19it/s]warmup run: 1936it [00:03, 1010.69it/s]warmup run: 1901it [00:03, 1002.00it/s]warmup run: 1868it [00:03, 953.05it/s]warmup run: 1923it [00:03, 1016.32it/s]warmup run: 1889it [00:03, 1004.12it/s]warmup run: 1897it [00:03, 938.38it/s]warmup run: 1868it [00:03, 982.71it/s]warmup run: 2081it [00:03, 1034.12it/s]warmup run: 2044it [00:03, 1031.11it/s]warmup run: 2004it [00:03, 1009.64it/s]warmup run: 1964it [00:03, 949.84it/s]warmup run: 2030it [00:03, 1031.18it/s]warmup run: 1991it [00:03, 1008.06it/s]warmup run: 1996it [00:03, 952.02it/s]warmup run: 1968it [00:03, 987.22it/s]warmup run: 2201it [00:03, 1081.57it/s]warmup run: 2166it [00:03, 1084.92it/s]warmup run: 2124it [00:03, 1064.78it/s]warmup run: 2075it [00:03, 994.86it/s]warmup run: 2151it [00:03, 1084.36it/s]warmup run: 2108it [00:03, 1053.92it/s]warmup run: 2115it [00:03, 1021.10it/s]warmup run: 2082it [00:03, 1030.12it/s]warmup run: 2321it [00:03, 1115.29it/s]warmup run: 2288it [00:03, 1122.49it/s]warmup run: 2244it [00:03, 1104.10it/s]warmup run: 2196it [00:03, 1057.35it/s]warmup run: 2273it [00:03, 1122.11it/s]warmup run: 2229it [00:03, 1099.39it/s]warmup run: 2236it [00:03, 1076.39it/s]warmup run: 2202it [00:03, 1080.06it/s]warmup run: 2441it [00:03, 1138.90it/s]warmup run: 2410it [00:03, 1149.09it/s]warmup run: 2364it [00:03, 1132.53it/s]warmup run: 2317it [00:03, 1101.28it/s]warmup run: 2395it [00:03, 1149.24it/s]warmup run: 2350it [00:03, 1131.11it/s]warmup run: 2358it [00:03, 1116.21it/s]warmup run: 2322it [00:03, 1115.64it/s]warmup run: 2561it [00:04, 1156.44it/s]warmup run: 2531it [00:04, 1166.48it/s]warmup run: 2485it [00:04, 1152.80it/s]warmup run: 2438it [00:03, 1133.31it/s]warmup run: 2471it [00:03, 1153.19it/s]warmup run: 2517it [00:03, 1168.01it/s]warmup run: 2479it [00:04, 1144.00it/s]warmup run: 2443it [00:04, 1141.08it/s]warmup run: 2682it [00:04, 1169.55it/s]warmup run: 2653it [00:04, 1179.84it/s]warmup run: 2605it [00:04, 1166.47it/s]warmup run: 2559it [00:04, 1155.26it/s]warmup run: 2592it [00:04, 1169.34it/s]warmup run: 2639it [00:04, 1181.41it/s]warmup run: 2600it [00:04, 1162.77it/s]warmup run: 2564it [00:04, 1159.38it/s]warmup run: 2801it [00:04, 1173.55it/s]warmup run: 2774it [00:04, 1186.28it/s]warmup run: 2725it [00:04, 1175.94it/s]warmup run: 2679it [00:04, 1168.40it/s]warmup run: 2713it [00:04, 1181.40it/s]warmup run: 2759it [00:04, 1185.47it/s]warmup run: 2721it [00:04, 1176.35it/s]warmup run: 2685it [00:04, 1173.27it/s]warmup run: 2921it [00:04, 1178.63it/s]warmup run: 2897it [00:04, 1198.15it/s]warmup run: 2844it [00:04, 1178.61it/s]warmup run: 2798it [00:04, 1172.81it/s]warmup run: 3000it [00:04, 680.01it/s] warmup run: 2832it [00:04, 1183.48it/s]warmup run: 2880it [00:04, 1190.21it/s]warmup run: 2841it [00:04, 1181.54it/s]warmup run: 2805it [00:04, 1178.38it/s]warmup run: 3000it [00:04, 681.54it/s] warmup run: 2964it [00:04, 1184.23it/s]warmup run: 2918it [00:04, 1179.76it/s]warmup run: 2953it [00:04, 1188.91it/s]warmup run: 3000it [00:04, 682.79it/s] warmup run: 2962it [00:04, 1189.05it/s]warmup run: 3000it [00:04, 675.51it/s] warmup run: 2926it [00:04, 1187.16it/s]warmup run: 3000it [00:04, 677.43it/s] warmup run: 3000it [00:04, 668.32it/s] warmup run: 3000it [00:04, 671.79it/s] warmup run: 3000it [00:04, 665.75it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1656.55it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.00it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.75it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1605.47it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1650.95it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.50it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.08it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.89it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1647.27it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1665.15it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.75it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1621.35it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.64it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1667.29it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1659.80it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1608.87it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1642.82it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1668.04it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1626.66it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1666.00it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1640.05it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1620.30it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1652.91it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1595.87it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1667.25it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1665.20it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1624.14it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1619.31it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1637.79it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1650.24it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1634.46it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1590.52it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1665.69it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1662.57it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1636.52it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1616.56it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1620.55it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1647.53it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1605.23it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1588.19it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1664.90it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1634.43it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1613.67it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1616.63it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1654.07it/s]warmup should be done:  33%|      | 997/3000 [00:00<00:01, 1644.60it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1607.47it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1584.80it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1661.00it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1630.27it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1609.13it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1610.17it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1640.05it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1645.34it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1605.31it/s]warmup should be done:  38%|      | 1131/3000 [00:00<00:01, 1580.20it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:01, 1660.80it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1630.90it/s]warmup should be done:  43%|     | 1298/3000 [00:00<00:01, 1607.98it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1643.03it/s]warmup should be done:  43%|     | 1303/3000 [00:00<00:01, 1610.20it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1643.11it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1606.77it/s]warmup should be done:  43%|     | 1290/3000 [00:00<00:01, 1579.53it/s]warmup should be done:  50%|     | 1504/3000 [00:00<00:00, 1660.64it/s]warmup should be done:  49%|     | 1459/3000 [00:00<00:00, 1607.07it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1631.05it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1648.92it/s]warmup should be done:  49%|     | 1465/3000 [00:00<00:00, 1611.09it/s]warmup should be done:  50%|     | 1499/3000 [00:00<00:00, 1641.68it/s]warmup should be done:  49%|     | 1471/3000 [00:00<00:00, 1608.01it/s]warmup should be done:  48%|     | 1448/3000 [00:00<00:00, 1578.29it/s]warmup should be done:  56%|    | 1671/3000 [00:01<00:00, 1659.99it/s]warmup should be done:  54%|    | 1620/3000 [00:01<00:00, 1605.74it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1630.79it/s]warmup should be done:  55%|    | 1661/3000 [00:01<00:00, 1652.50it/s]warmup should be done:  54%|    | 1627/3000 [00:01<00:00, 1611.71it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1608.75it/s]warmup should be done:  54%|    | 1606/3000 [00:01<00:00, 1576.94it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1615.30it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1658.75it/s]warmup should be done:  59%|    | 1783/3000 [00:01<00:00, 1611.50it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1630.07it/s]warmup should be done:  61%|    | 1827/3000 [00:01<00:00, 1654.46it/s]warmup should be done:  60%|    | 1789/3000 [00:01<00:00, 1611.74it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1609.43it/s]warmup should be done:  59%|    | 1768/3000 [00:01<00:00, 1588.72it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1606.86it/s]warmup should be done:  67%|   | 2004/3000 [00:01<00:00, 1659.92it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1615.65it/s]warmup should be done:  66%|   | 1993/3000 [00:01<00:00, 1655.17it/s]warmup should be done:  65%|   | 1951/3000 [00:01<00:00, 1611.81it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1610.03it/s]warmup should be done:  66%|   | 1971/3000 [00:01<00:00, 1601.72it/s]warmup should be done:  64%|   | 1930/3000 [00:01<00:00, 1597.01it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1613.32it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1659.65it/s]warmup should be done:  70%|   | 2109/3000 [00:01<00:00, 1617.95it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1648.90it/s]warmup should be done:  70%|   | 2113/3000 [00:01<00:00, 1610.22it/s]warmup should be done:  71%|   | 2119/3000 [00:01<00:00, 1609.54it/s]warmup should be done:  70%|   | 2093/3000 [00:01<00:00, 1604.32it/s]warmup should be done:  72%|  | 2152/3000 [00:01<00:00, 1617.12it/s]warmup should be done:  71%|   | 2132/3000 [00:01<00:00, 1583.55it/s]warmup should be done:  78%|  | 2336/3000 [00:01<00:00, 1655.09it/s]warmup should be done:  76%|  | 2273/3000 [00:01<00:00, 1623.32it/s]warmup should be done:  77%|  | 2324/3000 [00:01<00:00, 1644.93it/s]warmup should be done:  76%|  | 2275/3000 [00:01<00:00, 1608.41it/s]warmup should be done:  76%|  | 2281/3000 [00:01<00:00, 1611.99it/s]warmup should be done:  75%|  | 2256/3000 [00:01<00:00, 1609.67it/s]warmup should be done:  77%|  | 2317/3000 [00:01<00:00, 1624.33it/s]warmup should be done:  76%|  | 2295/3000 [00:01<00:00, 1595.87it/s]warmup should be done:  81%| | 2438/3000 [00:01<00:00, 1630.71it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1652.31it/s]warmup should be done:  81%|  | 2436/3000 [00:01<00:00, 1605.51it/s]warmup should be done:  83%| | 2489/3000 [00:01<00:00, 1639.08it/s]warmup should be done:  81%| | 2443/3000 [00:01<00:00, 1613.02it/s]warmup should be done:  81%|  | 2418/3000 [00:01<00:00, 1610.95it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1626.96it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1599.55it/s]warmup should be done:  87%| | 2604/3000 [00:01<00:00, 1638.27it/s]warmup should be done:  89%| | 2668/3000 [00:01<00:00, 1654.56it/s]warmup should be done:  87%| | 2597/3000 [00:01<00:00, 1605.26it/s]warmup should be done:  88%| | 2653/3000 [00:01<00:00, 1637.22it/s]warmup should be done:  87%| | 2607/3000 [00:01<00:00, 1620.94it/s]warmup should be done:  86%| | 2581/3000 [00:01<00:00, 1616.07it/s]warmup should be done:  88%| | 2644/3000 [00:01<00:00, 1627.58it/s]warmup should be done:  87%| | 2619/3000 [00:01<00:00, 1607.36it/s]warmup should be done:  92%|| 2770/3000 [00:01<00:00, 1644.65it/s]warmup should be done:  94%|| 2835/3000 [00:01<00:00, 1657.01it/s]warmup should be done:  92%|| 2758/3000 [00:01<00:00, 1605.47it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1637.47it/s]warmup should be done:  92%|| 2771/3000 [00:01<00:00, 1625.44it/s]warmup should be done:  91%|| 2744/3000 [00:01<00:00, 1620.21it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1629.49it/s]warmup should be done:  93%|| 2782/3000 [00:01<00:00, 1612.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1659.80it/s]warmup should be done:  98%|| 2938/3000 [00:01<00:00, 1653.15it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1614.27it/s]warmup should be done:  99%|| 2983/3000 [00:01<00:00, 1642.55it/s]warmup should be done:  98%|| 2937/3000 [00:01<00:00, 1633.42it/s]warmup should be done:  97%|| 2909/3000 [00:01<00:00, 1627.05it/s]warmup should be done:  99%|| 2974/3000 [00:01<00:00, 1637.55it/s]warmup should be done:  98%|| 2944/3000 [00:01<00:00, 1611.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.00it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1619.57it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1618.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1613.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1603.98it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.60it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.28it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.47it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1676.80it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.31it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.47it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1653.42it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1692.87it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1668.50it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1688.07it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.32it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1694.48it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1673.02it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1682.35it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1652.30it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1627.11it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1691.98it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1685.09it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1669.24it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1675.75it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1654.82it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1684.02it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1697.70it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1639.26it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1696.39it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1668.01it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1659.21it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1678.38it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1681.67it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1684.15it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1703.91it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1656.59it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1700.34it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1671.77it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1662.33it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1683.59it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1682.04it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1686.40it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1707.11it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1663.05it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1701.25it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1671.76it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1661.90it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1684.31it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1683.45it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1685.84it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1705.42it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1663.64it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1672.74it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1686.63it/s]warmup should be done:  40%|      | 1192/3000 [00:00<00:01, 1698.84it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1704.83it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1662.95it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1681.22it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1682.12it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1671.58it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1677.52it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1690.34it/s]warmup should be done:  46%|     | 1365/3000 [00:00<00:00, 1706.38it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1683.28it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:00, 1666.21it/s]warmup should be done:  46%|     | 1370/3000 [00:00<00:00, 1709.75it/s]warmup should be done:  45%|     | 1354/3000 [00:00<00:00, 1684.89it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1683.54it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1678.43it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1690.19it/s]warmup should be done:  51%|     | 1519/3000 [00:00<00:00, 1684.44it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1669.00it/s]warmup should be done:  51%|    | 1542/3000 [00:00<00:00, 1710.16it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1685.24it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1682.60it/s]warmup should be done:  51%|     | 1536/3000 [00:00<00:00, 1696.86it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1684.21it/s]warmup should be done:  56%|    | 1693/3000 [00:01<00:00, 1689.86it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1677.14it/s]warmup should be done:  56%|    | 1692/3000 [00:01<00:00, 1684.12it/s]warmup should be done:  56%|    | 1678/3000 [00:01<00:00, 1682.12it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1692.91it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1650.37it/s]warmup should be done:  57%|    | 1714/3000 [00:01<00:00, 1685.69it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1690.12it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1684.50it/s]warmup should be done:  62%|   | 1863/3000 [00:01<00:00, 1690.37it/s]warmup should be done:  62%|   | 1861/3000 [00:01<00:00, 1684.46it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1679.49it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1692.78it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1650.02it/s]warmup should be done:  63%|   | 1885/3000 [00:01<00:00, 1692.09it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1697.18it/s]warmup should be done:  68%|   | 2026/3000 [00:01<00:00, 1683.03it/s]warmup should be done:  68%|   | 2030/3000 [00:01<00:00, 1681.87it/s]warmup should be done:  68%|   | 2033/3000 [00:01<00:00, 1686.45it/s]warmup should be done:  68%|   | 2046/3000 [00:01<00:00, 1692.30it/s]warmup should be done:  67%|   | 2015/3000 [00:01<00:00, 1670.68it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1655.98it/s]warmup should be done:  69%|   | 2057/3000 [00:01<00:00, 1698.52it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1700.93it/s]warmup should be done:  73%|  | 2195/3000 [00:01<00:00, 1682.82it/s]warmup should be done:  73%|  | 2199/3000 [00:01<00:00, 1682.17it/s]warmup should be done:  73%|  | 2202/3000 [00:01<00:00, 1684.65it/s]warmup should be done:  74%|  | 2216/3000 [00:01<00:00, 1689.75it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1659.73it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1663.09it/s]warmup should be done:  74%|  | 2228/3000 [00:01<00:00, 1700.07it/s]warmup should be done:  79%|  | 2360/3000 [00:01<00:00, 1705.12it/s]warmup should be done:  79%|  | 2364/3000 [00:01<00:00, 1684.65it/s]warmup should be done:  79%|  | 2368/3000 [00:01<00:00, 1683.96it/s]warmup should be done:  79%|  | 2371/3000 [00:01<00:00, 1685.28it/s]warmup should be done:  80%|  | 2385/3000 [00:01<00:00, 1689.13it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1663.00it/s]warmup should be done:  78%|  | 2350/3000 [00:01<00:00, 1664.82it/s]warmup should be done:  80%|  | 2399/3000 [00:01<00:00, 1700.25it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1706.84it/s]warmup should be done:  84%| | 2534/3000 [00:01<00:00, 1688.28it/s]warmup should be done:  85%| | 2538/3000 [00:01<00:00, 1687.77it/s]warmup should be done:  85%| | 2555/3000 [00:01<00:00, 1689.61it/s]warmup should be done:  85%| | 2540/3000 [00:01<00:00, 1676.14it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1667.56it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1663.67it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1700.71it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1709.01it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1690.29it/s]warmup should be done:  90%| | 2708/3000 [00:01<00:00, 1689.87it/s]warmup should be done:  91%| | 2724/3000 [00:01<00:00, 1688.04it/s]warmup should be done:  90%| | 2708/3000 [00:01<00:00, 1675.78it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1669.86it/s]warmup should be done:  91%|| 2741/3000 [00:01<00:00, 1695.55it/s]warmup should be done:  89%| | 2684/3000 [00:01<00:00, 1640.49it/s]warmup should be done:  96%|| 2876/3000 [00:01<00:00, 1709.88it/s]warmup should be done:  96%|| 2874/3000 [00:01<00:00, 1689.97it/s]warmup should be done:  96%|| 2877/3000 [00:01<00:00, 1689.40it/s]warmup should be done:  96%|| 2876/3000 [00:01<00:00, 1674.71it/s]warmup should be done:  96%|| 2893/3000 [00:01<00:00, 1683.39it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1666.96it/s]warmup should be done:  97%|| 2911/3000 [00:01<00:00, 1687.65it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1640.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1696.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.92it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1685.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1684.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.75it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1666.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.14it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d404bcd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d404b9e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d401852b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d401771f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d404ba730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d401751c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d401760d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5d40177190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 05:32:00.187220: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f587282f570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:00.187278: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:00.195268: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:00.423466: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f586b031510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:00.423542: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:00.431653: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:01.306468: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f586b02d610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:01.306532: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:01.308953: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f58728342e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:01.309003: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:01.310738: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f587282bad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:01.310781: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:01.316155: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:01.317999: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:01.318034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:01.412063: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f586af93090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:01.412123: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:01.420149: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:01.443445: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f586b02ce50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:01.443508: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:01.451843: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:01.478637: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f586a8376f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:32:01.478701: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:32:01.489027: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:32:07.547033: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:07.767596: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:08.171841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:08.200730: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:08.218724: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:08.280583: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:08.556608: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:32:08.557412: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:32:58.710][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:32:58.710][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:58.710][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:32:58.710][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:58.716][ERROR][RK0][main]: coll ps creation done
[HCTR][05:32:58.716][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][05:32:58.720][ERROR][RK0][main]: coll ps creation done
[HCTR][05:32:58.720][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][05:32:59.075][ERROR][RK0][tid #140018014222080]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:32:59.075][ERROR][RK0][tid #140018014222080]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:59.080][ERROR][RK0][tid #140018014222080]: coll ps creation done
[HCTR][05:32:59.080][ERROR][RK0][tid #140018014222080]: replica 6 waits for coll ps creation barrier
[HCTR][05:32:59.081][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:32:59.081][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:59.084][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:32:59.084][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:59.086][ERROR][RK0][main]: coll ps creation done
[HCTR][05:32:59.086][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][05:32:59.093][ERROR][RK0][main]: coll ps creation done
[HCTR][05:32:59.093][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][05:32:59.139][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:32:59.139][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:59.144][ERROR][RK0][main]: coll ps creation done
[HCTR][05:32:59.144][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][05:32:59.161][ERROR][RK0][tid #140018014222080]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:32:59.162][ERROR][RK0][tid #140018014222080]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:59.169][ERROR][RK0][tid #140018014222080]: coll ps creation done
[HCTR][05:32:59.169][ERROR][RK0][tid #140018014222080]: replica 2 waits for coll ps creation barrier
[HCTR][05:32:59.200][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:32:59.200][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:32:59.207][ERROR][RK0][main]: coll ps creation done
[HCTR][05:32:59.208][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:32:59.208][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][05:33:00.040][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][05:33:00.085][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][tid #140018014222080]: replica 6 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][tid #140018014222080]: replica 2 calling init per replica
[HCTR][05:33:00.085][ERROR][RK0][main]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][tid #140018014222080]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][tid #140018014222080]: Calling build_v2
[HCTR][05:33:00.085][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][tid #140018014222080]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][tid #140018014222080]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:33:00.085][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 05:33:00[[2022-12-12 05:33:002022-12-12 05:33:00.2022-12-12 05:33:002022-12-12 05:33:002022-12-12 05:33:00.2022-12-12 05:33:002022-12-12 05:33:00. 85639... 85640.. 85640:  85640 85640 85641:  85644 85644: E: : : E: : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccEEE EE :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] :::136::136using concurrent impl MPS136136136] 136136] 
] ] ] using concurrent impl MPS] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS





[2022-12-12 05:33:00. 89953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:33:00. 89993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196[] 2022-12-12 05:33:00assigning 8 to cpu.
 90003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 05:33:002022-12-12 05:33:00.. 90049 90055: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::2022-12-12 05:33:00178196[.] ] 2022-12-12 05:33:00 90084v100x8, slow pcieassigning 8 to cpu.: 

 90095E[: [ 2022-12-12 05:33:00E2022-12-12 05:33:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. [.[: 90139/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:33:00 901512022-12-12 05:33:00212: :.: .] E178 90178E 90188build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 ] : [ : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcieE2022-12-12 05:33:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:
 .:[ 178[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 902351962022-12-12 05:33:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 05:33:002022-12-12 05:33:00:: ] .:v100x8, slow pcie..212Eassigning 8 to cpu 90294178
 90278 90285[]  
: ] : : 2022-12-12 05:33:00build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcieEE.
: 
 [  90395178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:33:00[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] ::.2022-12-12 05:33:002022-12-12 05:33:00:Ev100x8, slow pcie196178 90443..213 
] ] :  90464 90465] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuv100x8, slow pcie[E: : remote time is 8.68421:

2022-12-12 05:33:00 EE
196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[  ]  90552:2022-12-12 05:33:002022-12-12 05:33:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[: 212..::
2022-12-12 05:33:00E]  90600 90619196213. build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : ] ]  90633/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
EEassigning 8 to cpuremote time is 8.68421: :  

[[E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:33:002022-12-12 05:33:00 [] ::../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:33:00assigning 8 to cpu196214 90726 90727:.
] ] [: : 212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 90749assigning 8 to cpucpu time is 97.05882022-12-12 05:33:00EE
: 

.  E[ 90795[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 05:33:00: 2022-12-12 05:33:00::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E[.212213: 90851 2022-12-12 05:33:00 90860] ] 214: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421] E: 90893E

cpu time is 97.0588 212:  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 05:33:002022-12-12 05:33:00build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :212..
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213]  90978 90980:] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : 212remote time is 8.68421[
EE] 
2022-12-12 05:33:00  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 05:33:00 91046:2022-12-12 05:33:00:.: 214[.213 91076E] 2022-12-12 05:33:00 91080] :  cpu time is 97.0588.: remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 91112E
 ::  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] 2022-12-12 05:33:00 :213remote time is 8.68421./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214] 
 91202:] remote time is 8.68421: 213cpu time is 97.0588
[E] [
2022-12-12 05:33:00 remote time is 8.684212022-12-12 05:33:00./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
. 91267: 91285: [214: E2022-12-12 05:33:00] E .cpu time is 97.0588 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 91321
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: :214E214]  ] cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588
:
214] cpu time is 97.0588
[2022-12-12 05:34:19.277586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:34:19.317621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 05:34:19.429471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:34:19.429535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:34:19.500657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:34:19.500698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:34:19.501223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:34:19.501271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.502208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.503004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.515850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 05:34:19.515911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 05:34:19.516118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 05:34:19.516174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 05:34:19.[5162732022-12-12 05:34:19: .E516285 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 05:34:19E:. 202516319/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] : :E4 solved202 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2 solved:[
18152022-12-12 05:34:19] .Building Coll Cache with ... num gpu device is 8[516373
2022-12-12 05:34:19: .E516387 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE[: 2022-12-12 05:34:19205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.] :516414worker 0 thread 4 initing device 4205: 
] Eworker 0 thread 2 initing device 2 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.516580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:34:19.516624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.516775: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 05:34:19:.202516798] : 6 solvedE
 [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 05:34:19[2022-12-12 05:34:19:.2022-12-12 05:34:19.202516842.516843] : 516850: 5 solvedE: E
 E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:2022-12-12 05:34:191815:1815.] 205] 516904Building Coll Cache with ... num gpu device is 8] Building Coll Cache with ... num gpu device is 8: 
worker 0 thread 6 initing device 6
E
[ 2022-12-12 05:34:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:516965[205: 2022-12-12 05:34:19] E.worker 0 thread 5 initing device 5 516980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.517349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:34:19.517389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 05:34:191980.] 517400eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:34:19.517452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.517582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 05:34:19.517675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 05:34:19.518294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:34:19.518368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.520618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.521077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.521132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.521716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.521807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.521859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.522847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.525144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.525465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.525516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.526117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.526173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.526213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.527242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:34:19.583940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 05:34:19.589572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:34:19.589696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:34:19.590512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.591164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:19.592263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:19.592325: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[[[[[[2022-12-12 05:34:192022-12-12 05:34:192022-12-12 05:34:192022-12-12 05:34:192022-12-12 05:34:192022-12-12 05:34:19......611613611613611613611613611613611613: : : : : : EEEEEE      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::198019801980198019801980] ] ] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes





[2022-12-12 05:34:19.617123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 05:34:19.618972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 05:34:192022-12-12 05:34:19..619052619072: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 05:34:19.619120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:34:19.619171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:34:19.619207[: 2022-12-12 05:34:19E. 619202/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 5
[2022-12-12 05:34:19.619295: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:34:19:.638619294] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 05:34:192022-12-12 05:34:19..619384619372: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 05:34:19.619471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:34:19.621187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.621684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.622215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.622839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.623362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.623896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.625764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:19.625807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:19.625846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:19.625890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:19.625990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:34:192022-12-12 05:34:19..626033626036: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 5eager alloc mem 611.00 KB

[2022-12-12 05:34:19.626167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:34:19.626802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 05:34:19.626824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 05:34:19
.626851: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB[
2022-12-12 05:34:19.626877: [W2022-12-12 05:34:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc626885:: 43E]  WORKER[0] alloc host memory 76.29 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 05:34:19.626933: E[ 2022-12-12 05:34:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:626946638: ] Weager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:34:19.626991: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:34:19.627053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:19.627099: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:34:19.627159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:19.627205: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:34:19.631977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:34:19.632786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:19.633855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:19.633901: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:34:19.644049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:34:19.644699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:34:19.644746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[2022-12-12 05:34:192022-12-12 05:34:19..678267678268: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 05:34:19.678377: [E2022-12-12 05:34:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu678391:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-12 05:34:19.678746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 05:34:192022-12-12 05:34:19..678926678926: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-12 05:34:192022-12-12 05:34:19..678992678993: [: E2022-12-12 05:34:19E .[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6790052022-12-12 05:34:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: .:1980E6790151980]  : ] eager alloc mem 9.54 GB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager alloc mem 9.54 GB
: 
638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 25855638
] eager release cuda mem 25855
[2022-12-12 05:34:19.[6791172022-12-12 05:34:19: .E679123 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 9.54 GB1980
] eager alloc mem 9.54 GB
[2022-12-12 05:34:19.679360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:34:19.679402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:34:19.679454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:34:19.680063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:34:19.680104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:34:19.684605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:34:19.685222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:34:19.685265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[2022-12-12 05:34:23.[2022-12-12 05:34:23294122[.2022-12-12 05:34:23[: 294129[2022-12-12 05:34:23.E[2022-12-12 05:34:23: .2022-12-12 05:34:232022-12-12 05:34:23294122 .E2022-12-12 05:34:23294122..: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu294122 .: 294122294122E:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu294131E: :  1926E::  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  1926E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :Device 3 init p2p of link 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926
:Device 0 init p2p of link 3/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926::] 1926
:] 19261926Device 2 init p2p of link 1] 1926Device 4 init p2p of link 5] ] 
Device 6 init p2p of link 0] 
Device 5 init p2p of link 6Device 7 init p2p of link 4
Device 1 init p2p of link 7


[2022-12-12 05:34:23.294651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 05:34:23
.294672: E[ 2022-12-12 05:34:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[.:2022-12-12 05:34:232946881980.[: ] 2946952022-12-12 05:34:23[E[eager alloc mem 611.00 KB: .2022-12-12 05:34:23 2022-12-12 05:34:23
E294705./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. : 294714[:294714/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 2022-12-12 05:34:231980: : E.] E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 294746eager alloc mem 611.00 KB ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980:E:
] 1980 1980eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
eager alloc mem 611.00 KB:eager alloc mem 611.00 KB
1980
] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.295592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.295650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 05:34:23eager release cuda mem 625663.
295668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 05:34:23638.] 295700eager release cuda mem 625663: 
E[ 2022-12-12 05:34:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:295734[638: 2022-12-12 05:34:23] E.eager release cuda mem 625663 295760[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 2022-12-12 05:34:23:[E.6382022-12-12 05:34:23 295791] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager release cuda mem 625663295810:E
: 638 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 05:34:23.309164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 05:34:23.309313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.309596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 05:34:23.309745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.310225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.310692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.317249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 05:34:23.317414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.317651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 05:34:23.317816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.318325[: 2022-12-12 05:34:23E. 318321/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 4 init p2p of link 7
[2022-12-12 05:34:23.318510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.318553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 7 init p2p of link 12022-12-12 05:34:23
.318589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.318689: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:34:23:.1926318719] : Device 2 init p2p of link 3E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.318861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.319383: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:34:23:.1926319413] : Device 5 init p2p of link 4E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.319508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.319557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.319795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.320502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.322838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 05:34:23.322957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 05:34:231980.] 322970eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 05:34:23.323112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.323896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.324025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.330647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 05:34:23.330773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.331343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 05:34:23.331465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.331712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.332403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.338425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 05:34:23.338563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.339356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.339397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 05:34:23.339532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.339830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 05:34:23.339953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.340063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 05:34:23.340185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.340316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.340848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.341090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.345273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 05:34:23.345396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.346325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.347824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 05:34:23.347937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.348862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.351067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 05:34:23.351187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.351408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 05:34:23.351543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.352111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.352474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.353566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 05:34:23.353683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.353950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 05:34:23.354064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.354600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.354979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.365335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 05:34:23.365454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.365677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 05:34:23.365793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:34:23.366233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.366570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:34:23.367640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.368396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.368598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85219 secs 
[2022-12-12 05:34:23.369547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85257 secs 
[2022-12-12 05:34:23.371527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.371949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.374012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85705 secs 
[2022-12-12 05:34:23.374175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85673 secs 
[2022-12-12 05:34:23.374709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.375579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 05:34:23] .eager release cuda mem 80400000375625
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85824 secs 
[2022-12-12 05:34:23.377551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.377845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:34:23.377899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85955 secs 
[2022-12-12 05:34:23.381776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.88052 secs 
[2022-12-12 05:34:23.381939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86532 secs 
[2022-12-12 05:34:23.382478: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.86 GB
[2022-12-12 05:34:24.915058: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.12 GB
[2022-12-12 05:34:24.916792: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.12 GB
[2022-12-12 05:34:24.919260: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.12 GB
[2022-12-12 05:34:26.281269: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.39 GB
[2022-12-12 05:34:26.281963: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.39 GB
[2022-12-12 05:34:26.282690: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.39 GB
[2022-12-12 05:34:27.489471: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.60 GB
[2022-12-12 05:34:27.489605: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.60 GB
[2022-12-12 05:34:27.490069: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.60 GB
[2022-12-12 05:34:28.614707: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.82 GB
[2022-12-12 05:34:28.615331: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.82 GB
[2022-12-12 05:34:28.616880: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.82 GB
[2022-12-12 05:34:29.939280: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.28 GB
[2022-12-12 05:34:29.940496: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.28 GB
[2022-12-12 05:34:29.942364: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.28 GB
[2022-12-12 05:34:31.596882: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.47 GB
[2022-12-12 05:34:31.623278: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.47 GB
[HCTR][05:34:32.880][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][tid #140018014222080]: replica 2 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][tid #140018014222080]: replica 6 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][05:34:32.880][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][05:34:32.881][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][tid #140018014222080]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][tid #140018014222080]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:34:32.881][ERROR][RK0][main]: init per replica done
[HCTR][05:34:32.881][ERROR][RK0][main]: init per replica done
[HCTR][05:34:32.881][ERROR][RK0][tid #140018014222080]: init per replica done
[HCTR][05:34:32.881][ERROR][RK0][main]: init per replica done
[HCTR][05:34:32.881][ERROR][RK0][main]: init per replica done
[HCTR][05:34:32.881][ERROR][RK0][main]: init per replica done
[HCTR][05:34:32.881][ERROR][RK0][tid #140018014222080]: init per replica done
[HCTR][05:34:32.884][ERROR][RK0][main]: init per replica done
[HCTR][05:34:32.919][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3ad4238400
[HCTR][05:34:32.919][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3ad4558400
[HCTR][05:34:32.919][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3ad4b98400
[HCTR][05:34:32.919][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3ad4eb8400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 5 allocated 3276800 at 0x7f39ac238400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 5 allocated 6553600 at 0x7f39ac558400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 5 allocated 3276800 at 0x7f39acb98400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 5 allocated 6553600 at 0x7f39aceb8400
[HCTR][05:34:32.919][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f3ad6238400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 1 allocated 3276800 at 0x7f3a70238400
[HCTR][05:34:32.919][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f3ad6558400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 1 allocated 6553600 at 0x7f3a70558400
[HCTR][05:34:32.919][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f3ad6b98400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 1 allocated 3276800 at 0x7f3a70b98400
[HCTR][05:34:32.919][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f3ad6eb8400
[HCTR][05:34:32.919][ERROR][RK0][tid #140018005829376]: 1 allocated 6553600 at 0x7f3a70eb8400
[HCTR][05:34:32.920][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f3ad6238400
[HCTR][05:34:32.920][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f3ad6558400
[HCTR][05:34:32.920][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f3ad6b98400
[HCTR][05:34:32.920][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f3ad6eb8400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 3 allocated 3276800 at 0x7f39b4238400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 3 allocated 6553600 at 0x7f39b4558400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 6 allocated 3276800 at 0x7f3ad6238400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 3 allocated 3276800 at 0x7f39b4b98400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 3 allocated 6553600 at 0x7f39b4eb8400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 6 allocated 6553600 at 0x7f3ad6558400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 6 allocated 3276800 at 0x7f3ad6b98400
[HCTR][05:34:32.920][ERROR][RK0][tid #140018014222080]: 6 allocated 6553600 at 0x7f3ad6eb8400
[HCTR][05:34:32.922][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f3aa8320000
[HCTR][05:34:32.922][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f3aa8640000
[HCTR][05:34:32.922][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f3aa8c80000
[HCTR][05:34:32.922][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f3aa8fa0000
