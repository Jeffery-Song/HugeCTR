2022-12-12 06:12:54.285146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.294842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.299052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.303957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.315633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.321603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.335433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.343102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.391754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.395531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.399181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.399985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.400353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.401589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.401888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.403258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.403301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.405021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.405071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.406595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.406790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.408090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.408585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.409484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.410247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.410918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.411981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.412544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.414045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.414987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.416023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.416989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.418716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.419815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.420736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.421747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.422769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.423757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.424687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.425697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.430929: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.433177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.434869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.435721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.436331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.437248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.437964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.438868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.439709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.440493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.441356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.441472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.442072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.443448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.443679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.444530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.445692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.446317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.447494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.447920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.450571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.451243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.451519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.454389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.454759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.455073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.456760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.457200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.457837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.458347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.459640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.460322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.461161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.461555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.461977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.463123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.463711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.464479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.464620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.465532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.466434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.467063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.467750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.467883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.468857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.469654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.470375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.470692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.471203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.472195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.473462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.473510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.474239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.474855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.489849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.491837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.492188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.492870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.493585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.494547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.495081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.495992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.497484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.517204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.530390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.530935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.532091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.532180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.532208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.534181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.535048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.535667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.535714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.535798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.538308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.538329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.539039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.539768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.539854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.539896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.542770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.542889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.543582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.544182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.544317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.544639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.546660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.546761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.548088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.548691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.548828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.548912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.550863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.550958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.552143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.552917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.553113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.553153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.555277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.555378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.556157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.556939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.557033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.557159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.559311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.560039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.560092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.560680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.560845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.560930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.563171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.563823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.563870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.564421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.564618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.564659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.566978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.567586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.567663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.568343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.568442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.568722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.571167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.571510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.571676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.572241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.572289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.572632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.575030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.575555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.575789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.576309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.576400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.576600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.577721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.579028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.579769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.579872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.580380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.580426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.581028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.582575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.583596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.584845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.584890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.585213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.585472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.585835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.588134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.589271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.590097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.590197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.590773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.590913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.591315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.592760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.593531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.594708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.595775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.596135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.596487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.596736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.597375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.598178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.598819: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.599620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.600152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.600401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.600516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.601311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.603383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.603493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.604144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.604405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.604478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.605722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.607551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.607612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.608175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.608239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.608269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.609669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.609729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.611536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.611714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.612096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.612406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.612516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.614556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.614563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.616406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.616621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.616690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.616710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.616971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.618792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.619004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.621091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.621320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.621584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.621675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.621820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.623616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.625596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.625886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.626181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.626372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.628161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.629761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.629886: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.630103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.631943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.633248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.633366: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.633580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.633849: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.635035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.636534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.638945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.640123: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.640412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.641528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.641847: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.642599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.643806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.644416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.644481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.645784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.647300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.647955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.648077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.650408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.650444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.651393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.652027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.652110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.666631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.667039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.668783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.701957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.702582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.703566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.707492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.741773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.751671: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:12:54.762358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.767259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:54.772117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.839183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.839811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.840754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.841316: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:55.841386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:12:55.859318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.860161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.860676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.861242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.861761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:55.863215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:12:55.906972: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:55.907200: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:55.955857: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:12:56.038576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.039400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.039935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.040409: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.040469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:12:56.057739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.058400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.058909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.059515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.060064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.060539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:12:56.116154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.116784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.117300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.117772: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.117832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:12:56.119141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.119744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.120263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.120735: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.120785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:12:56.121278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.121901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.122433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.122897: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.122953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:12:56.135007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.135212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.136116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.136477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.136795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.137450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.137779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.138007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.138377: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.138435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:12:56.139034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.139384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.139884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:12:56.140163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.140738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.140967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.141518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.142092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.142381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:12:56.142795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.143390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.143994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.144734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:12:56.149410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.149816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.150166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.150822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.151024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.151759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.151845: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.151903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:12:56.152488: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:12:56.152535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:12:56.153946: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.154104: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.155785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.156129: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:12:56.156437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.156949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.157523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.158029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.158499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:12:56.169299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.169575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.170260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.170512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.171159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.171451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.172185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.172351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.173068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.173291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:12:56.173950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:12:56.174150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:12:56.184994: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.185205: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.187241: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:12:56.187598: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.187770: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.188756: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:12:56.202009: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.202202: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.204107: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:12:56.217541: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.217740: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.218325: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.218476: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.219670: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:12:56.220312: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:12:56.229308: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.229505: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:12:56.231321: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:12:57.503][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.60s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 100it [00:01, 85.35it/s]warmup run: 91it [00:01, 74.46it/s]warmup run: 101it [00:01, 86.17it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 97it [00:01, 83.92it/s]warmup run: 95it [00:01, 82.09it/s]warmup run: 198it [00:01, 182.72it/s]warmup run: 187it [00:01, 167.28it/s]warmup run: 200it [00:01, 184.54it/s]warmup run: 97it [00:01, 84.21it/s]warmup run: 101it [00:01, 88.78it/s]warmup run: 100it [00:01, 87.23it/s]warmup run: 198it [00:01, 185.89it/s]warmup run: 294it [00:01, 287.31it/s]warmup run: 189it [00:01, 176.57it/s]warmup run: 285it [00:01, 273.15it/s]warmup run: 299it [00:01, 292.71it/s]warmup run: 195it [00:01, 183.21it/s]warmup run: 198it [00:01, 187.30it/s]warmup run: 200it [00:01, 188.60it/s]warmup run: 290it [00:01, 285.58it/s]warmup run: 284it [00:01, 281.58it/s]warmup run: 393it [00:01, 400.13it/s]warmup run: 385it [00:01, 386.88it/s]warmup run: 395it [00:01, 399.73it/s]warmup run: 294it [00:01, 293.21it/s]warmup run: 300it [00:01, 301.63it/s]warmup run: 297it [00:01, 295.40it/s]warmup run: 390it [00:01, 401.50it/s]warmup run: 381it [00:01, 393.30it/s]warmup run: 492it [00:02, 509.55it/s]warmup run: 486it [00:02, 500.39it/s]warmup run: 495it [00:02, 511.50it/s]warmup run: 394it [00:01, 408.20it/s]warmup run: 400it [00:01, 415.82it/s]warmup run: 395it [00:01, 406.77it/s]warmup run: 489it [00:02, 512.15it/s]warmup run: 478it [00:02, 501.91it/s]warmup run: 591it [00:02, 610.67it/s]warmup run: 588it [00:02, 607.65it/s]warmup run: 595it [00:02, 614.38it/s]warmup run: 496it [00:01, 522.70it/s]warmup run: 497it [00:01, 520.62it/s]warmup run: 491it [00:01, 511.16it/s]warmup run: 588it [00:02, 612.63it/s]warmup run: 575it [00:02, 601.08it/s]warmup run: 691it [00:02, 699.63it/s]warmup run: 690it [00:02, 700.89it/s]warmup run: 695it [00:02, 703.01it/s]warmup run: 597it [00:02, 625.07it/s]warmup run: 596it [00:02, 619.96it/s]warmup run: 589it [00:02, 609.69it/s]warmup run: 687it [00:02, 698.88it/s]warmup run: 674it [00:02, 690.17it/s]warmup run: 792it [00:02, 774.67it/s]warmup run: 791it [00:02, 775.03it/s]warmup run: 794it [00:02, 773.56it/s]warmup run: 697it [00:02, 711.95it/s]warmup run: 696it [00:02, 707.18it/s]warmup run: 687it [00:02, 694.34it/s]warmup run: 785it [00:02, 768.34it/s]warmup run: 772it [00:02, 761.23it/s]warmup run: 890it [00:02, 826.14it/s]warmup run: 892it [00:02, 834.30it/s]warmup run: 894it [00:02, 831.22it/s]warmup run: 797it [00:02, 782.27it/s]warmup run: 794it [00:02, 774.22it/s]warmup run: 785it [00:02, 763.82it/s]warmup run: 883it [00:02, 821.98it/s]warmup run: 869it [00:02, 814.32it/s]warmup run: 988it [00:02, 867.08it/s]warmup run: 993it [00:02, 880.65it/s]warmup run: 993it [00:02, 872.91it/s]warmup run: 896it [00:02, 835.36it/s]warmup run: 884it [00:02, 823.03it/s]warmup run: 891it [00:02, 822.93it/s]warmup run: 982it [00:02, 865.54it/s]warmup run: 965it [00:02, 852.16it/s]warmup run: 1086it [00:02, 893.12it/s]warmup run: 1093it [00:02, 913.32it/s]warmup run: 1093it [00:02, 906.93it/s]warmup run: 995it [00:02, 877.28it/s]warmup run: 982it [00:02, 865.30it/s]warmup run: 988it [00:02, 859.22it/s]warmup run: 1081it [00:02, 898.07it/s]warmup run: 1061it [00:02, 873.10it/s]warmup run: 1183it [00:02, 910.05it/s]warmup run: 1194it [00:02, 938.21it/s]warmup run: 1192it [00:02, 928.70it/s]warmup run: 1096it [00:02, 911.95it/s]warmup run: 1081it [00:02, 898.71it/s]warmup run: 1085it [00:02, 889.57it/s]warmup run: 1179it [00:02, 920.49it/s]warmup run: 1281it [00:02, 928.31it/s]warmup run: 1156it [00:02, 889.11it/s]warmup run: 1295it [00:02, 957.29it/s]warmup run: 1291it [00:02, 943.93it/s]warmup run: 1195it [00:02, 932.12it/s]warmup run: 1181it [00:02, 926.77it/s]warmup run: 1182it [00:02, 909.38it/s]warmup run: 1277it [00:02, 937.31it/s]warmup run: 1378it [00:02, 938.37it/s]warmup run: 1254it [00:02, 912.91it/s]warmup run: 1395it [00:03, 958.77it/s]warmup run: 1390it [00:02, 953.15it/s]warmup run: 1295it [00:02, 950.68it/s]warmup run: 1281it [00:02, 947.26it/s]warmup run: 1279it [00:02, 915.56it/s]warmup run: 1377it [00:02, 953.00it/s]warmup run: 1478it [00:03, 955.18it/s]warmup run: 1353it [00:02, 933.44it/s]warmup run: 1495it [00:03, 968.51it/s]warmup run: 1489it [00:03, 962.88it/s]warmup run: 1396it [00:02, 967.32it/s]warmup run: 1382it [00:02, 963.72it/s]warmup run: 1375it [00:02, 926.32it/s]warmup run: 1478it [00:03, 968.06it/s]warmup run: 1453it [00:03, 952.56it/s]warmup run: 1576it [00:03, 955.39it/s]warmup run: 1596it [00:03, 979.18it/s]warmup run: 1590it [00:03, 975.63it/s]warmup run: 1497it [00:03, 979.53it/s]warmup run: 1483it [00:02, 975.90it/s]warmup run: 1471it [00:02, 931.81it/s]warmup run: 1558it [00:03, 980.63it/s]warmup run: 1676it [00:03, 966.35it/s]warmup run: 1697it [00:03, 987.76it/s]warmup run: 1577it [00:03, 910.70it/s]warmup run: 1693it [00:03, 990.30it/s]warmup run: 1598it [00:03, 986.44it/s]warmup run: 1587it [00:03, 992.96it/s]warmup run: 1567it [00:03, 939.07it/s]warmup run: 1663it [00:03, 1000.01it/s]warmup run: 1775it [00:03, 971.30it/s]warmup run: 1797it [00:03, 991.26it/s]warmup run: 1680it [00:03, 941.89it/s]warmup run: 1796it [00:03, 1000.35it/s]warmup run: 1701it [00:03, 996.69it/s]warmup run: 1691it [00:03, 1005.59it/s]warmup run: 1665it [00:03, 948.99it/s]warmup run: 1767it [00:03, 1010.15it/s]warmup run: 1873it [00:03, 969.98it/s]warmup run: 1897it [00:03, 989.77it/s]warmup run: 1780it [00:03, 958.35it/s]warmup run: 1900it [00:03, 1010.28it/s]warmup run: 1803it [00:03, 1001.89it/s]warmup run: 1795it [00:03, 1014.86it/s]warmup run: 1763it [00:03, 956.75it/s]warmup run: 1871it [00:03, 1018.71it/s]warmup run: 1971it [00:03, 969.39it/s]warmup run: 1997it [00:03, 978.30it/s]warmup run: 1881it [00:03, 971.01it/s]warmup run: 2003it [00:03, 1013.84it/s]warmup run: 1904it [00:03, 1002.24it/s]warmup run: 1899it [00:03, 1021.12it/s]warmup run: 1862it [00:03, 965.58it/s]warmup run: 1974it [00:03, 1019.61it/s]warmup run: 2086it [00:03, 1021.94it/s]warmup run: 2114it [00:03, 1034.34it/s]warmup run: 1982it [00:03, 982.37it/s]warmup run: 2123it [00:03, 1067.98it/s]warmup run: 2007it [00:03, 1008.04it/s]warmup run: 2002it [00:03, 1016.51it/s]warmup run: 1960it [00:03, 968.65it/s]warmup run: 2088it [00:03, 1055.13it/s]warmup run: 2207it [00:03, 1076.49it/s]warmup run: 2234it [00:03, 1082.76it/s]warmup run: 2096it [00:03, 1027.82it/s]warmup run: 2243it [00:03, 1107.22it/s]warmup run: 2127it [00:03, 1062.78it/s]warmup run: 2123it [00:03, 1073.81it/s]warmup run: 2072it [00:03, 1013.67it/s]warmup run: 2208it [00:03, 1096.95it/s]warmup run: 2328it [00:03, 1114.86it/s]warmup run: 2355it [00:03, 1119.43it/s]warmup run: 2215it [00:03, 1073.19it/s]warmup run: 2364it [00:03, 1135.41it/s]warmup run: 2247it [00:03, 1101.86it/s]warmup run: 2244it [00:03, 1113.38it/s]warmup run: 2194it [00:03, 1073.02it/s]warmup run: 2328it [00:03, 1125.90it/s]warmup run: 2448it [00:03, 1139.55it/s]warmup run: 2476it [00:04, 1144.98it/s]warmup run: 2332it [00:03, 1101.43it/s]warmup run: 2484it [00:03, 1154.17it/s]warmup run: 2367it [00:03, 1129.23it/s]warmup run: 2366it [00:03, 1142.90it/s]warmup run: 2316it [00:03, 1114.55it/s]warmup run: 2448it [00:03, 1147.00it/s]warmup run: 2569it [00:04, 1157.96it/s]warmup run: 2597it [00:04, 1162.85it/s]warmup run: 2450it [00:03, 1124.05it/s]warmup run: 2604it [00:04, 1167.19it/s]warmup run: 2487it [00:03, 1148.22it/s]warmup run: 2487it [00:03, 1162.80it/s]warmup run: 2438it [00:03, 1143.89it/s]warmup run: 2568it [00:04, 1162.54it/s]warmup run: 2690it [00:04, 1171.66it/s]warmup run: 2717it [00:04, 1173.22it/s]warmup run: 2568it [00:04, 1140.65it/s]warmup run: 2723it [00:04, 1172.94it/s]warmup run: 2606it [00:04, 1160.25it/s]warmup run: 2609it [00:04, 1177.51it/s]warmup run: 2559it [00:03, 1161.51it/s]warmup run: 2687it [00:04, 1170.21it/s]warmup run: 2810it [00:04, 1177.22it/s]warmup run: 2839it [00:04, 1184.80it/s]warmup run: 2685it [00:04, 1149.10it/s]warmup run: 2843it [00:04, 1180.33it/s]warmup run: 2725it [00:04, 1166.99it/s]warmup run: 2729it [00:04, 1183.55it/s]warmup run: 2679it [00:04, 1170.61it/s]warmup run: 2807it [00:04, 1176.53it/s]warmup run: 2930it [00:04, 1183.64it/s]warmup run: 2961it [00:04, 1193.21it/s]warmup run: 2801it [00:04, 1145.08it/s]warmup run: 2963it [00:04, 1184.81it/s]warmup run: 2845it [00:04, 1175.26it/s]warmup run: 2850it [00:04, 1190.23it/s]warmup run: 3000it [00:04, 672.95it/s] warmup run: 2798it [00:04, 1175.73it/s]warmup run: 3000it [00:04, 685.27it/s] warmup run: 3000it [00:04, 679.65it/s] warmup run: 2926it [00:04, 1178.46it/s]warmup run: 2918it [00:04, 1151.79it/s]warmup run: 2964it [00:04, 1178.77it/s]warmup run: 2970it [00:04, 1193.02it/s]warmup run: 2916it [00:04, 1170.25it/s]warmup run: 3000it [00:04, 683.23it/s] warmup run: 3000it [00:04, 692.03it/s] warmup run: 3000it [00:04, 689.88it/s] warmup run: 3000it [00:04, 679.58it/s] warmup run: 3000it [00:04, 684.80it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.10it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1618.61it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1592.63it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.47it/s]warmup should be done:   4%|         | 111/3000 [00:00<00:02, 1109.09it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.14it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1621.59it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.14it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1653.91it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1622.06it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.53it/s]warmup should be done:   9%|         | 260/3000 [00:00<00:02, 1329.77it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1592.63it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1643.22it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.32it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1626.78it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1651.98it/s]warmup should be done:  14%|        | 423/3000 [00:00<00:01, 1464.80it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1621.82it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1641.29it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1627.62it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1643.33it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1621.89it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1586.50it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1652.15it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1629.81it/s]warmup should be done:  20%|        | 586/3000 [00:00<00:01, 1526.34it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1620.87it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1641.74it/s]warmup should be done:  21%|       | 639/3000 [00:00<00:01, 1583.55it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1619.90it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1597.34it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1628.13it/s]warmup should be done:  25%|       | 750/3000 [00:00<00:01, 1565.57it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1650.24it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1641.01it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1619.59it/s]warmup should be done:  27%|       | 798/3000 [00:00<00:01, 1582.01it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1612.47it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1588.69it/s]warmup should be done:  30%|       | 913/3000 [00:00<00:01, 1584.52it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1615.48it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1644.34it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1617.58it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1633.40it/s]warmup should be done:  32%|      | 957/3000 [00:00<00:01, 1578.61it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1601.60it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1600.65it/s]warmup should be done:  36%|      | 1075/3000 [00:00<00:01, 1593.03it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1616.13it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1644.33it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1614.99it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1638.93it/s]warmup should be done:  37%|      | 1115/3000 [00:00<00:01, 1565.70it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1595.59it/s]warmup should be done:  38%|      | 1144/3000 [00:00<00:01, 1605.35it/s]warmup should be done:  41%|      | 1237/3000 [00:00<00:01, 1600.89it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1615.57it/s]warmup should be done:  44%|     | 1325/3000 [00:00<00:01, 1643.97it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1642.40it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1606.61it/s]warmup should be done:  42%|     | 1272/3000 [00:00<00:01, 1564.61it/s]warmup should be done:  43%|     | 1297/3000 [00:00<00:01, 1594.41it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1605.20it/s]warmup should be done:  47%|     | 1399/3000 [00:00<00:00, 1606.75it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1613.30it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1642.70it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1647.29it/s]warmup should be done:  49%|     | 1465/3000 [00:00<00:00, 1603.63it/s]warmup should be done:  48%|     | 1429/3000 [00:00<00:01, 1565.59it/s]warmup should be done:  49%|     | 1459/3000 [00:00<00:00, 1599.86it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1607.38it/s]warmup should be done:  52%|    | 1562/3000 [00:01<00:00, 1611.37it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1641.16it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1610.46it/s]warmup should be done:  55%|    | 1654/3000 [00:01<00:00, 1650.20it/s]warmup should be done:  54%|    | 1629/3000 [00:01<00:00, 1613.29it/s]warmup should be done:  53%|    | 1586/3000 [00:01<00:00, 1566.37it/s]warmup should be done:  54%|    | 1620/3000 [00:01<00:00, 1602.89it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1607.37it/s]warmup should be done:  57%|    | 1725/3000 [00:01<00:00, 1615.59it/s]warmup should be done:  60%|    | 1787/3000 [00:01<00:00, 1611.26it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1654.18it/s]warmup should be done:  61%|    | 1820/3000 [00:01<00:00, 1638.96it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1621.31it/s]warmup should be done:  58%|    | 1743/3000 [00:01<00:00, 1567.00it/s]warmup should be done:  59%|    | 1781/3000 [00:01<00:00, 1602.97it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1608.28it/s]warmup should be done:  63%|   | 1888/3000 [00:01<00:00, 1618.14it/s]warmup should be done:  66%|   | 1988/3000 [00:01<00:00, 1657.17it/s]warmup should be done:  66%|   | 1985/3000 [00:01<00:00, 1639.77it/s]warmup should be done:  65%|   | 1949/3000 [00:01<00:00, 1607.34it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1626.16it/s]warmup should be done:  63%|   | 1901/3000 [00:01<00:00, 1567.93it/s]warmup should be done:  65%|   | 1942/3000 [00:01<00:00, 1598.13it/s]warmup should be done:  65%|   | 1952/3000 [00:01<00:00, 1609.74it/s]warmup should be done:  68%|   | 2050/3000 [00:01<00:00, 1618.38it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1657.69it/s]warmup should be done:  72%|  | 2150/3000 [00:01<00:00, 1640.81it/s]warmup should be done:  71%|   | 2122/3000 [00:01<00:00, 1631.35it/s]warmup should be done:  69%|   | 2058/3000 [00:01<00:00, 1567.19it/s]warmup should be done:  70%|   | 2110/3000 [00:01<00:00, 1600.16it/s]warmup should be done:  70%|   | 2102/3000 [00:01<00:00, 1594.33it/s]warmup should be done:  70%|   | 2114/3000 [00:01<00:00, 1611.42it/s]warmup should be done:  74%|  | 2212/3000 [00:01<00:00, 1618.84it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1655.95it/s]warmup should be done:  76%|  | 2286/3000 [00:01<00:00, 1632.73it/s]warmup should be done:  77%|  | 2315/3000 [00:01<00:00, 1640.00it/s]warmup should be done:  74%|  | 2216/3000 [00:01<00:00, 1568.34it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1593.05it/s]warmup should be done:  75%|  | 2262/3000 [00:01<00:00, 1588.37it/s]warmup should be done:  76%|  | 2276/3000 [00:01<00:00, 1610.45it/s]warmup should be done:  79%|  | 2374/3000 [00:01<00:00, 1616.99it/s]warmup should be done:  83%| | 2486/3000 [00:01<00:00, 1657.14it/s]warmup should be done:  83%| | 2480/3000 [00:01<00:00, 1641.36it/s]warmup should be done:  82%| | 2451/3000 [00:01<00:00, 1635.46it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1566.44it/s]warmup should be done:  81%|  | 2431/3000 [00:01<00:00, 1590.91it/s]warmup should be done:  81%|  | 2421/3000 [00:01<00:00, 1587.90it/s]warmup should be done:  81%| | 2438/3000 [00:01<00:00, 1611.90it/s]warmup should be done:  85%| | 2537/3000 [00:01<00:00, 1618.90it/s]warmup should be done:  88%| | 2653/3000 [00:01<00:00, 1658.70it/s]warmup should be done:  87%| | 2616/3000 [00:01<00:00, 1637.08it/s]warmup should be done:  84%| | 2530/3000 [00:01<00:00, 1566.62it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1587.96it/s]warmup should be done:  86%| | 2580/3000 [00:01<00:00, 1587.14it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1611.57it/s]warmup should be done:  88%| | 2645/3000 [00:01<00:00, 1608.16it/s]warmup should be done:  90%| | 2699/3000 [00:01<00:00, 1615.57it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1658.11it/s]warmup should be done:  93%|| 2781/3000 [00:01<00:00, 1638.71it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1567.40it/s]warmup should be done:  92%|| 2750/3000 [00:01<00:00, 1586.68it/s]warmup should be done:  91%|| 2739/3000 [00:01<00:00, 1587.07it/s]warmup should be done:  92%|| 2762/3000 [00:01<00:00, 1588.03it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1587.48it/s]warmup should be done:  95%|| 2862/3000 [00:01<00:00, 1618.42it/s]warmup should be done: 100%|| 2986/3000 [00:01<00:00, 1659.11it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1643.28it/s]warmup should be done:  95%|| 2844/3000 [00:01<00:00, 1564.98it/s]warmup should be done:  97%|| 2911/3000 [00:01<00:00, 1590.84it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1588.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.93it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1590.08it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1593.50it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1603.88it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1601.23it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1596.63it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1586.42it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1570.41it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1688.19it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.45it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.68it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1613.33it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1682.91it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1621.03it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1652.35it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.29it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1694.77it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1628.16it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1686.60it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1633.13it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1659.95it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1687.96it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1622.21it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1611.27it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1695.96it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1635.42it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1690.66it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1683.22it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1636.55it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1665.01it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1621.73it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1615.51it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1695.20it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1640.71it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1639.99it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1669.45it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1691.10it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1623.60it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1679.56it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1619.54it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1699.26it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1672.33it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1639.15it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1623.83it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1633.97it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1685.79it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1615.34it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1661.56it/s]warmup should be done:  34%|      | 1022/3000 [00:00<00:01, 1703.19it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1668.88it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1620.92it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1686.02it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1614.08it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1660.64it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1613.19it/s]warmup should be done:  40%|      | 1193/3000 [00:00<00:01, 1703.60it/s]warmup should be done:  39%|      | 1172/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1622.28it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1632.94it/s]warmup should be done:  40%|      | 1187/3000 [00:00<00:01, 1680.83it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1616.03it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1657.87it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1625.20it/s]warmup should be done:  45%|     | 1364/3000 [00:00<00:00, 1705.37it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1668.78it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1617.70it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1628.87it/s]warmup should be done:  45%|     | 1357/3000 [00:00<00:00, 1685.05it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1613.58it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1635.32it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1659.47it/s]warmup should be done:  51%|     | 1535/3000 [00:00<00:00, 1706.52it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1670.13it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1620.70it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1686.44it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1630.71it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1617.16it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1644.08it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1660.10it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1706.42it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1671.65it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1622.60it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1688.90it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1637.27it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1621.30it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1662.16it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1652.18it/s]warmup should be done:  63%|   | 1877/3000 [00:01<00:00, 1706.12it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1672.39it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1691.58it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1622.08it/s]warmup should be done:  60%|    | 1808/3000 [00:01<00:00, 1640.07it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1621.37it/s]warmup should be done:  62%|   | 1846/3000 [00:01<00:00, 1664.08it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1657.68it/s]warmup should be done:  68%|   | 2049/3000 [00:01<00:00, 1707.19it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1672.95it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1692.61it/s]warmup should be done:  66%|   | 1973/3000 [00:01<00:00, 1640.72it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1620.40it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1621.90it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1663.28it/s]warmup should be done:  66%|   | 1988/3000 [00:01<00:00, 1659.07it/s]warmup should be done:  74%|  | 2220/3000 [00:01<00:00, 1706.12it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1680.68it/s]warmup should be done:  74%|  | 2206/3000 [00:01<00:00, 1691.71it/s]warmup should be done:  71%|  | 2138/3000 [00:01<00:00, 1642.88it/s]warmup should be done:  71%|   | 2119/3000 [00:01<00:00, 1621.94it/s]warmup should be done:  71%|   | 2118/3000 [00:01<00:00, 1625.83it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1662.95it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1660.74it/s]warmup should be done:  80%|  | 2391/3000 [00:01<00:00, 1706.55it/s]warmup should be done:  78%|  | 2353/3000 [00:01<00:00, 1689.23it/s]warmup should be done:  79%|  | 2376/3000 [00:01<00:00, 1690.70it/s]warmup should be done:  77%|  | 2303/3000 [00:01<00:00, 1641.04it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1624.61it/s]warmup should be done:  76%|  | 2282/3000 [00:01<00:00, 1627.03it/s]warmup should be done:  77%|  | 2322/3000 [00:01<00:00, 1662.81it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1661.33it/s]warmup should be done:  85%| | 2563/3000 [00:01<00:00, 1707.84it/s]warmup should be done:  84%| | 2525/3000 [00:01<00:00, 1695.47it/s]warmup should be done:  85%| | 2546/3000 [00:01<00:00, 1692.28it/s]warmup should be done:  82%| | 2446/3000 [00:01<00:00, 1624.01it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1638.53it/s]warmup should be done:  82%| | 2445/3000 [00:01<00:00, 1624.26it/s]warmup should be done:  83%| | 2489/3000 [00:01<00:00, 1663.60it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1661.47it/s]warmup should be done:  91%| | 2734/3000 [00:01<00:00, 1706.89it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1699.05it/s]warmup should be done:  91%| | 2716/3000 [00:01<00:00, 1694.50it/s]warmup should be done:  87%| | 2609/3000 [00:01<00:00, 1622.71it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1635.58it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1623.31it/s]warmup should be done:  89%| | 2656/3000 [00:01<00:00, 1663.06it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1660.34it/s]warmup should be done:  97%|| 2905/3000 [00:01<00:00, 1702.14it/s]warmup should be done:  96%|| 2867/3000 [00:01<00:00, 1699.63it/s]warmup should be done:  96%|| 2886/3000 [00:01<00:00, 1693.90it/s]warmup should be done:  92%|| 2772/3000 [00:01<00:00, 1624.08it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1636.67it/s]warmup should be done:  92%|| 2771/3000 [00:01<00:00, 1625.19it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1664.17it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1659.41it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1702.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1681.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.67it/s]warmup should be done:  98%|| 2935/3000 [00:01<00:00, 1624.20it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1638.21it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1625.26it/s]warmup should be done: 100%|| 2991/3000 [00:01<00:00, 1666.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.74it/s]2022-12-12 06:14:31.611388: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff2e3833cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:31.611456: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.162016: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff2eb830350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.162097: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.402593: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff2ef158cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.402679: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.657391: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff2f382c630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.657458: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.685776: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff2eb795930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.685846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.693330: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd46c0288a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.693384: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.747595: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff2f3833370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.747667: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.756433: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd46c02cd00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:14:33.756501: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:14:33.855555: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:35.463114: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:35.710161: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:35.917858: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:35.921108: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:35.976497: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:36.056234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:36.057996: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:14:36.813986: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.375037: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.641682: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.779826: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.828851: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.933035: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.939727: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:14:38.963580: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:15:02.706][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:15:02.706][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.715][ERROR][RK0][main]: coll ps creation done
[HCTR][06:15:02.715][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][06:15:02.731][ERROR][RK0][tid #140682173863680]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:15:02.731][ERROR][RK0][tid #140682173863680]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.739][ERROR][RK0][tid #140682173863680]: coll ps creation done
[HCTR][06:15:02.739][ERROR][RK0][tid #140682173863680]: replica 7 waits for coll ps creation barrier
[HCTR][06:15:02.744][ERROR][RK0][tid #140681636992768]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:15:02.745][ERROR][RK0][tid #140681636992768]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.745][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:15:02.745][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.750][ERROR][RK0][tid #140681636992768]: coll ps creation done
[HCTR][06:15:02.750][ERROR][RK0][tid #140681636992768]: replica 2 waits for coll ps creation barrier
[HCTR][06:15:02.750][ERROR][RK0][main]: coll ps creation done
[HCTR][06:15:02.750][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][06:15:02.766][ERROR][RK0][tid #140681636992768]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:15:02.766][ERROR][RK0][tid #140681636992768]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.771][ERROR][RK0][tid #140681636992768]: coll ps creation done
[HCTR][06:15:02.771][ERROR][RK0][tid #140681636992768]: replica 3 waits for coll ps creation barrier
[HCTR][06:15:02.790][ERROR][RK0][tid #140681897035520]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:15:02.790][ERROR][RK0][tid #140681897035520]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.795][ERROR][RK0][tid #140681897035520]: coll ps creation done
[HCTR][06:15:02.795][ERROR][RK0][tid #140681897035520]: replica 5 waits for coll ps creation barrier
[HCTR][06:15:02.813][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:15:02.813][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.822][ERROR][RK0][main]: coll ps creation done
[HCTR][06:15:02.822][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][06:15:02.830][ERROR][RK0][tid #140681762817792]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:15:02.830][ERROR][RK0][tid #140681762817792]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:15:02.835][ERROR][RK0][tid #140681762817792]: coll ps creation done
[HCTR][06:15:02.835][ERROR][RK0][tid #140681762817792]: replica 1 waits for coll ps creation barrier
[HCTR][06:15:02.835][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][06:15:03.719][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][06:15:03.764][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][tid #140681636992768]: replica 3 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][tid #140682173863680]: replica 7 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][tid #140681897035520]: replica 5 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][tid #140681636992768]: replica 2 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][tid #140681762817792]: replica 1 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][06:15:03.764][ERROR][RK0][main]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][tid #140681636992768]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][tid #140682173863680]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][tid #140681897035520]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][tid #140681636992768]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][tid #140681636992768]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][tid #140681762817792]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][main]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][main]: Calling build_v2
[HCTR][06:15:03.764][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][tid #140682173863680]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][tid #140681897035520]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][tid #140681636992768]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][tid #140681762817792]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:15:03.764][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 06:15:032022-12-12 06:15:032022-12-12 06:15:03.2022-12-12 06:15:03.[[2022-12-12 06:15:03[.764717.7647302022-12-12 06:15:03.2022-12-12 06:15:03764733: 764734: 2022-12-12 06:15:03.764741.: E: E.764757: 764742E E 764765: E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :136:136 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] 136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:] using concurrent impl MPS] using concurrent impl MPS:136] 136using concurrent impl MPS
using concurrent impl MPS
136] using concurrent impl MPS] 

] using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS


[2022-12-12 06:15:03.768929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:15:03.768968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 06:15:03196.] 768974assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 06:15:032022-12-12 06:15:03..769017769022: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::2022-12-12 06:15:03178196.] ] 769051[v100x8, slow pcieassigning 8 to cpu: 2022-12-12 06:15:03

E. 769063[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 06:15:03:E.212 769091] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-12 06:15:03E2022-12-12 06:15:03
178. .] 769112/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc769109v100x8, slow pcie: :: [
E196E[2022-12-12 06:15:03 ]  [2022-12-12 06:15:03./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:15:03.769153:
:.769156: 212178769169: E] [] : E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:15:03v100x8, slow pcieE /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.[
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[7692052022-12-12 06:15:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[[2132022-12-12 06:15:03: .:1782022-12-12 06:15:032022-12-12 06:15:03] .E769244196] ..remote time is 8.68421769248 : ] v100x8, slow pcie769269769271
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu
: : E:[ 
EE[ 1782022-12-12 06:15:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  2022-12-12 06:15:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:v100x8, slow pcie769366212::769378178
[: ] 213196: ] 2022-12-12 06:15:03E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] ] Ev100x8, slow pcie. 2022-12-12 06:15:03
remote time is 8.68421assigning 8 to cpu 
769442/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :[769467:2022-12-12 06:15:03[E2142022-12-12 06:15:03: 196.2022-12-12 06:15:03 ] .E] 769531./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588769537[ assigning 8 to cpu: 769551:
: 2022-12-12 06:15:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E: 212E.: E]  769595196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:15:03
:Eassigning 8 to cpu213:.196 [
] 214769687] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:15:03remote time is 8.68421] : assigning 8 to cpu:.
cpu time is 97.0588E[
212769747
 [2022-12-12 06:15:03] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:15:03.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E:.769819
 212769822: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [: [E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:15:03E2022-12-12 06:15:03 213
. ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 769883/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc769888[:remote time is 8.68421: :: 2022-12-12 06:15:03212
E214E.]  ] [ 769952build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.05882022-12-12 06:15:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:
.:E212770008213 [] : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:15:03build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Eremote time is 8.68421:.
 
213770066/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : [:[remote time is 8.68421E2022-12-12 06:15:032142022-12-12 06:15:03
 .] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc770126[cpu time is 97.0588770127:: 2022-12-12 06:15:03
: 213E.E]  770179 remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:E:214 [213] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:15:03] cpu time is 97.0588:.remote time is 8.68421
214770263
] : cpu time is 97.0588[E
2022-12-12 06:15:03 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc770325:: 214E]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-12 06:16:23.233900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:16:23.273848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 06:16:23.402479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:16:23.402544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:16:23.402577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:16:23.402607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:16:23.403085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:16:23.403146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.404147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.404952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.417726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 06:16:23.417788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[[2022-12-12 06:16:232022-12-12 06:16:23[2022-12-12 06:16:23..[2022-12-12 06:16:23.4181824181872022-12-12 06:16:23.418195: : .418218: EE[418217: E [ 2022-12-12 06:16:23: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 06:16:23.E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::.418253 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:202202418278: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] ] : E:1815] 1 solved6 solvedE 202] 4 solved

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] Building Coll Cache with ... num gpu device is 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[[
3 solved:2022022-12-12 06:16:23[2022-12-12 06:16:23
202] [.[2022-12-12 06:16:23.] 2 solved2022-12-12 06:16:234183852022-12-12 06:16:23.4183867 solved
.: .418396: 
418416E418420[: E:  : [2022-12-12 06:16:23E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE2022-12-12 06:16:23. /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc : .418450/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc418461: :205:] :: E205] 1980worker 0 thread 1 initing device 1205E ] worker 0 thread 6 initing device 6] 
]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 4 initing device 4
eager alloc mem 381.47 MBworker 0 thread 3 initing device 3/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:


:205205] ] worker 0 thread 2 initing device 2worker 0 thread 7 initing device 7

[2022-12-12 06:16:23[.2022-12-12 06:16:23418947[.[: 2022-12-12 06:16:234189532022-12-12 06:16:23E.: .[[ 418960E4189632022-12-12 06:16:232022-12-12 06:16:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  : ..:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE4189774189791815 : : : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEEBuilding Coll Cache with ... num gpu device is 8:] :  
1815Building Coll Cache with ... num gpu device is 81815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
] ::Building Coll Cache with ... num gpu device is 8[Building Coll Cache with ... num gpu device is 818151815
2022-12-12 06:16:23
] ] [.Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 82022-12-12 06:16:23419078
[
.[: 2022-12-12 06:16:234190922022-12-12 06:16:23E.[: .[ 4191092022-12-12 06:16:23E4191112022-12-12 06:16:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: . : .:E419140/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE4191431980 : : : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEeager alloc mem 381.47 MB: ] : 
1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :
] :eager alloc mem 381.47 MB1980eager alloc mem 381.47 MB1980
] 
] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 06:16:23.422373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.423734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.423841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.423889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.423942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.424039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.424104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.426427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.428135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.428179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.428272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.428323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.428381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.428488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:16:23.485551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:16:23.491205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:16:23.491337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:16:23.492244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.492944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.494053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.494101: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.510575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-12 06:16:232022-12-12 06:16:23..512623512627: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes[

[[[2022-12-12 06:16:232022-12-12 06:16:232022-12-12 06:16:232022-12-12 06:16:23....512698512700512702512702: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[2022-12-12 06:16:23.516699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:16:23.516820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:16:23.519783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.520373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:16:23.520455: [E2022-12-12 06:16:23 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc520460:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:[6382022-12-12 06:16:23] .eager release cuda mem 5520498
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.520539[: 2022-12-12 06:16:23E. 520563/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[2022-12-12 06:16:23.520634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:16:23.520717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:16:23.520791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 06:16:23.520799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:16:23.520877: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:16:23:.638520877] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:16:23.520962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:16:23.521543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.521581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.521626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.522344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.527731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.528530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.529239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.529889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:16:23.530849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.531092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.531351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.531463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.531544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.531599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:23.531925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.531972: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.532167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.532214: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.532421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.532468: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.532511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.532557: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.532604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.532650: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.532675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:23.532726: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:16:23.558713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:16:23.559365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:16:23.559411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:16:23.586828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:16:23.587469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:16:23.587512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:16:23.594385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:16:23.595020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:16:23.595065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:16:23.595665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:16:23.595753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:16:23.596280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:16:23.596322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:16:23.596358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:16:23.596400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[2022-12-12 06:16:232022-12-12 06:16:23..596828596830: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 06:16:23.597309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 06:16:232022-12-12 06:16:23..597452597455: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-12 06:16:232022-12-12 06:16:23..597516597518: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 11.93 GBeager alloc mem 11.93 GB

[2022-12-12 06:16:23.597915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:16:23.597956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 06:16:272022-12-12 06:16:272022-12-12 06:16:272022-12-12 06:16:272022-12-12 06:16:272022-12-12 06:16:272022-12-12 06:16:272022-12-12 06:16:27........952308952309952308952309952308952308952309952318: : : : : : : : EEEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::192619261926:1926192619261926] ] ] 1926] ] ] ] Device 4 init p2p of link 5Device 2 init p2p of link 1Device 3 init p2p of link 2] Device 1 init p2p of link 7Device 6 init p2p of link 0Device 7 init p2p of link 4Device 5 init p2p of link 6


Device 0 init p2p of link 3




[[[[2022-12-12 06:16:272022-12-12 06:16:27[2022-12-12 06:16:27[2022-12-12 06:16:27..2022-12-12 06:16:27.2022-12-12 06:16:27.952934952934.952934.952936: : 952943[: 952946: EE: 2022-12-12 06:16:27E: E  E. E[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 952977/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:16:27::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:.19809530141980:E1980:1980] : ] 1980 ] 1980] eager alloc mem 611.00 KBEeager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] eager alloc mem 611.00 KB
 
eager alloc mem 611.00 KB:
eager alloc mem 611.00 KB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
1980
:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 06:16:27[.2022-12-12 06:16:27953991.: 953996E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 06:16:27.954114: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 06:16:27:[2022-12-12 06:16:27.6382022-12-12 06:16:27.954128] .954131: eager release cuda mem 625663954134: E
: E E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:16:27:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.638:638954175] 638[] : eager release cuda mem 625663] 2022-12-12 06:16:27eager release cuda mem 625663E
eager release cuda mem 625663.
 
954227/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 06:16:27.971860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:16:27.[9720482022-12-12 06:16:27: .E972043 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1926
] Device 6 init p2p of link 5
[2022-12-12 06:16:27.972248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.973042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.973187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.981328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 06:16:27.981520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.981567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 06:16:27.981637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 06:16:27.981743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 06:16:27
.981752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 06:16:27.981823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.981933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.982259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 06:16:27.982496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:16:272022-12-12 06:16:27.[.9825192022-12-12 06:16:27982528: .: E982528E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638] :] eager alloc mem 611.00 KB1926eager release cuda mem 625663
] 
Device 5 init p2p of link 4
[[2022-12-12 06:16:272022-12-12 06:16:27..982789982794: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB[

2022-12-12 06:16:27.982861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.983443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.983828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.986487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 06:16:27.986623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.986923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 06:16:27.987054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.987555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.987984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.995836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 06:16:27.995969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.996920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:27.996985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:16:27.997160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:27.998168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28.  4260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:16:28.  4392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28.  4876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 06:16:28.  5014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28.  5105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 06:16:28.  5181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28.  5230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28.  5939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28.  6024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28.  6288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 06:16:28.  6414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28.  7348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 11724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:16:28. 11841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 12798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 14102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:16:28. 14219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 15176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 17170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:16:28. 17294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 17703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:16:28. 17824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 18253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 18780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 20439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:16:28. 20559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 20980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 06:16:28. 21150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 21502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 22139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 30723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:16:28. 30845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 31624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 32389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:16:28. 32527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:16:28. 33301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:16:28. 34829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 35632[: 2022-12-12 06:16:28E.  35645/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 100400000:
638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 36493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61739 secs 
[2022-12-12 06:16:28. 37058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61793 secs 
[2022-12-12 06:16:28. 37241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61814 secs 
[2022-12-12 06:16:28. 37775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 39312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.6209 secs 
[2022-12-12 06:16:28. 40469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 42185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 44397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62533 secs 
[2022-12-12 06:16:28. 44700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62562 secs 
[2022-12-12 06:16:28. 45823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 46141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:16:28. 48260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62914 secs 
[2022-12-12 06:16:28. 49111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64599 secs 
[2022-12-12 06:16:28. 50655: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.56 GB
[2022-12-12 06:16:29.415563: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.83 GB
[2022-12-12 06:16:29.416054: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.83 GB
[2022-12-12 06:16:29.417044: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.83 GB
[2022-12-12 06:16:31. 55087: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.09 GB
[2022-12-12 06:16:31. 55363: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.09 GB
[2022-12-12 06:16:31. 55790: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.09 GB
[2022-12-12 06:16:32.222177: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.30 GB
[2022-12-12 06:16:32.222311: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.30 GB
[2022-12-12 06:16:32.222587: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.30 GB
[2022-12-12 06:16:34.107733: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.52 GB
[2022-12-12 06:16:34.108692: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.52 GB
[2022-12-12 06:16:34.111311: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.52 GB
[2022-12-12 06:16:35.662436: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.98 GB
[2022-12-12 06:16:35.663001: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.98 GB
[2022-12-12 06:16:35.664131: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.98 GB
[2022-12-12 06:16:36.937098: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.18 GB
[2022-12-12 06:16:36.937273: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.18 GB
[HCTR][06:16:36.956][ERROR][RK0][tid #140682173863680]: replica 7 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][tid #140681762817792]: replica 1 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][tid #140681636992768]: replica 2 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][tid #140681636992768]: replica 3 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][tid #140681897035520]: replica 5 calling init per replica done, doing barrier
[HCTR][06:16:36.956][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681897035520]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][tid #140682173863680]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681636992768]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681762817792]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681636992768]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:16:36.956][ERROR][RK0][main]: init per replica done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681897035520]: init per replica done
[HCTR][06:16:36.956][ERROR][RK0][main]: init per replica done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681636992768]: init per replica done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681762817792]: init per replica done
[HCTR][06:16:36.956][ERROR][RK0][tid #140681636992768]: init per replica done
[HCTR][06:16:36.956][ERROR][RK0][tid #140682173863680]: init per replica done
[HCTR][06:16:36.958][ERROR][RK0][main]: init per replica done
[HCTR][06:16:36.962][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ff4ddf20000
[HCTR][06:16:36.962][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ff4de400000
[HCTR][06:16:36.962][ERROR][RK0][main]: 5 allocated 3276800 at 0x7ff4dea40000
[HCTR][06:16:36.962][ERROR][RK0][main]: 5 allocated 6553600 at 0x7ff4ded60000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681762817792]: 1 allocated 3276800 at 0x7ff4dff20000
[HCTR][06:16:36.962][ERROR][RK0][main]: 2 allocated 3276800 at 0x7ff4dff20000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681762817792]: 1 allocated 6553600 at 0x7ff4e0400000
[HCTR][06:16:36.962][ERROR][RK0][main]: 2 allocated 6553600 at 0x7ff4e0400000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681762817792]: 1 allocated 3276800 at 0x7ff4e0a40000
[HCTR][06:16:36.962][ERROR][RK0][main]: 2 allocated 3276800 at 0x7ff4e0a40000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681762817792]: 1 allocated 6553600 at 0x7ff4e0d60000
[HCTR][06:16:36.962][ERROR][RK0][main]: 2 allocated 6553600 at 0x7ff4e0d60000
[HCTR][06:16:36.962][ERROR][RK0][main]: 7 allocated 3276800 at 0x7ff4dff20000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681695708928]: 6 allocated 3276800 at 0x7ff4dbf20000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681636992768]: 4 allocated 3276800 at 0x7ff4dff20000
[HCTR][06:16:36.962][ERROR][RK0][main]: 7 allocated 6553600 at 0x7ff4e0400000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681695708928]: 6 allocated 6553600 at 0x7ff4dc400000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681636992768]: 4 allocated 6553600 at 0x7ff4e0400000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681695708928]: 6 allocated 3276800 at 0x7ff4dca40000
[HCTR][06:16:36.962][ERROR][RK0][main]: 7 allocated 3276800 at 0x7ff4e0a40000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681636992768]: 4 allocated 3276800 at 0x7ff4e0a40000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681695708928]: 6 allocated 6553600 at 0x7ff4dcd60000
[HCTR][06:16:36.962][ERROR][RK0][main]: 7 allocated 6553600 at 0x7ff4e0d60000
[HCTR][06:16:36.962][ERROR][RK0][tid #140681636992768]: 4 allocated 6553600 at 0x7ff4e0d60000
[HCTR][06:16:36.963][ERROR][RK0][tid #140681636992768]: 3 allocated 3276800 at 0x7ff4dff20000
[HCTR][06:16:36.963][ERROR][RK0][tid #140681636992768]: 3 allocated 6553600 at 0x7ff4e0400000
[HCTR][06:16:36.963][ERROR][RK0][tid #140681636992768]: 3 allocated 3276800 at 0x7ff4e0a40000
[HCTR][06:16:36.963][ERROR][RK0][tid #140681636992768]: 3 allocated 6553600 at 0x7ff4e0d60000
[HCTR][06:16:36.965][ERROR][RK0][tid #140682173863680]: 0 allocated 3276800 at 0x7ff4e2b20000
[HCTR][06:16:36.965][ERROR][RK0][tid #140682173863680]: 0 allocated 6553600 at 0x7ff4e3000000
[HCTR][06:16:36.965][ERROR][RK0][tid #140682173863680]: 0 allocated 3276800 at 0x7fd6ca50e800
[HCTR][06:16:36.965][ERROR][RK0][tid #140682173863680]: 0 allocated 6553600 at 0x7fd6ca82e800








