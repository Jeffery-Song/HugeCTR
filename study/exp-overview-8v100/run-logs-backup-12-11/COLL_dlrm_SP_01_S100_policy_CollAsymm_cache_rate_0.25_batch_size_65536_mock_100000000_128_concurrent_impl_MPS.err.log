2022-12-12 06:27:22.945058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:22.960072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:22.965917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:22.972643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:22.978361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:22.984884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:22.995604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.006025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.055879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.056919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.066346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.068119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.070743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.073558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.080714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.084094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.085709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.094977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.095455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.098912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.099710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.100860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.101140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.102357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.102404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.103928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.105059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.106153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.107238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.108303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.109271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.110229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.112018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.113113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.114249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.115301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.116349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.117699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.119483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.120130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.120785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.121761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.123070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.124148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.125151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.126143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.126693: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.127121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.128235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.132613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.134147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.135876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.136324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.136767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.137730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.138899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.139291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.140534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.141120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.141408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.141638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.143179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.143847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.143981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.145796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.146674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.146704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.148456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.149267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.149404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.149478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.152332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.152541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.152679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.154877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.155412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.155589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.157711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.158413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.160556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.161271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.161456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.162807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.165166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.165988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.167565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.167657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.168112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.169774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.169912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.171518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.172784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.172839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.173549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.173802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.175285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.175321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.176676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.176888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.196176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.202618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.208767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.211698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.212351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.214525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.214570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.214706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.214769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.215163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.216577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.218085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.220073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.220124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.220194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.220269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.220395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.222422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.226020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.226172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.226256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.226282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.226597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.227271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.229929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.229972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.230033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.230211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.230876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.231089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.234120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.234300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.234377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.234466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.236141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.237997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.238159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.238245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.238275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.239593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.241464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.241605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.241743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.241771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.242917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.244664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.244910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.245091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.245120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.246355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.247910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.248148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.248216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.248607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.249688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.251318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.251618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.251796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.252137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.253176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.254568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.254821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.254931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.255209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.256729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.257909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.258149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.258376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.258463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.260047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.261102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.261580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.261857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.261888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.264224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.264488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.264867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.264901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.267057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.267623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.267643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.267657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.267878: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.268635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.270865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.271406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.271473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.271601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.272296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.273717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.274983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.275169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.275315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.275918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.277200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.277961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.278461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.278675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.278807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.279840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.280184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.281200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.282151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.282741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.283387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.283604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.285547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.285977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.287267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.288361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.289167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.289545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.289728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.290999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.291471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.292266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.294120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.294499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.294765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.296536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.297127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.297863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.299439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.299871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.300535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.301001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.301539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.303624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.304147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.304772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.305209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.305661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.306440: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.307506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.310027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.311104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.311814: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.312431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.312928: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.312956: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.313609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.314902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.316041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.316169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.317526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.318943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.319363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.320224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.321931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.321969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.322451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.323107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.323281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.323989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.325523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.325673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.327324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.327599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.329062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.331709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.331983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.333161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.333436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.334520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.336751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.367066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.370255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.400792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.404418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.407177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.409502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.420918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.423796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.426913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.431034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.433844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.436611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.439714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.443912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.445224: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.454860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.462929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.464191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.467645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.469388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.509305: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:27:23.519009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.606120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:23.612078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.430529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.431181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.431910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.432685: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.432744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:27:24.450319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.450962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.451482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.452066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.452570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.453043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:27:24.499150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.499366: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.540947: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:27:24.657081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.657693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.658622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.659313: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.659371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:27:24.676821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.677451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.677962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.678747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.679419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.679897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:27:24.718752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.719458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.719999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.720455: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.720505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:27:24.738768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.739419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.739933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.740717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.741472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.741953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:27:24.742349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.742656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.743597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.744219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.744337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.745266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.745305: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.745358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:27:24.745980: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.746043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:27:24.762646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.763494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.763506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.764679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.764745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.764924: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.765080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.765816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.766011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.766416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.766706: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:27:24.766851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.767293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.767897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.768338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.768713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:27:24.769031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.769210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.769594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:27:24.770263: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.770263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.770308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:27:24.770981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.771458: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.771502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:27:24.782742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.783397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.784019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.784517: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:27:24.784576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:27:24.787868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.788496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.788813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.789132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.789794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.790087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.790680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.790986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.791700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.791849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:27:24.792293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.792775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:27:24.802050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.802716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.803258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.803863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.804383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:27:24.804865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:27:24.815342: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.815573: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.815646: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.815789: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.817434: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:27:24.817636: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:27:24.819511: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.819698: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.821475: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:27:24.838249: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.838348: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.838448: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.838505: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.840300: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:27:24.840369: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:27:24.850487: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.850659: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:27:24.852724: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.121][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:27:26.122][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 99it [00:01, 84.35it/s]warmup run: 96it [00:01, 81.45it/s]warmup run: 100it [00:01, 84.88it/s]warmup run: 98it [00:01, 82.71it/s]warmup run: 96it [00:01, 80.50it/s]warmup run: 199it [00:01, 183.87it/s]warmup run: 196it [00:01, 180.98it/s]warmup run: 200it [00:01, 183.98it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 195it [00:01, 178.44it/s]warmup run: 194it [00:01, 176.84it/s]warmup run: 299it [00:01, 293.46it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 296it [00:01, 290.51it/s]warmup run: 301it [00:01, 294.37it/s]warmup run: 100it [00:01, 86.64it/s]warmup run: 279it [00:01, 266.63it/s]warmup run: 290it [00:01, 278.82it/s]warmup run: 395it [00:01, 400.34it/s]warmup run: 75it [00:01, 65.39it/s]warmup run: 97it [00:01, 85.15it/s]warmup run: 394it [00:01, 400.96it/s]warmup run: 401it [00:01, 407.56it/s]warmup run: 200it [00:01, 187.31it/s]warmup run: 358it [00:01, 350.30it/s]warmup run: 388it [00:01, 389.72it/s]warmup run: 494it [00:02, 509.87it/s]warmup run: 168it [00:01, 161.31it/s]warmup run: 194it [00:01, 183.90it/s]warmup run: 493it [00:02, 510.49it/s]warmup run: 501it [00:02, 517.74it/s]warmup run: 300it [00:01, 297.79it/s]warmup run: 454it [00:02, 463.51it/s]warmup run: 487it [00:02, 500.24it/s]warmup run: 594it [00:02, 611.85it/s]warmup run: 266it [00:01, 273.12it/s]warmup run: 292it [00:01, 293.08it/s]warmup run: 593it [00:02, 613.40it/s]warmup run: 603it [00:02, 622.99it/s]warmup run: 398it [00:01, 408.52it/s]warmup run: 558it [00:02, 584.09it/s]warmup run: 588it [00:02, 606.51it/s]warmup run: 695it [00:02, 703.04it/s]warmup run: 365it [00:01, 390.15it/s]warmup run: 392it [00:01, 408.49it/s]warmup run: 694it [00:02, 704.68it/s]warmup run: 705it [00:02, 713.79it/s]warmup run: 496it [00:02, 515.45it/s]warmup run: 661it [00:02, 685.69it/s]warmup run: 687it [00:02, 693.41it/s]warmup run: 796it [00:02, 778.82it/s]warmup run: 462it [00:01, 499.70it/s]warmup run: 482it [00:01, 501.13it/s]warmup run: 795it [00:02, 778.50it/s]warmup run: 807it [00:02, 788.63it/s]warmup run: 595it [00:02, 615.37it/s]warmup run: 756it [00:02, 748.01it/s]warmup run: 786it [00:02, 764.91it/s]warmup run: 898it [00:02, 839.76it/s]warmup run: 562it [00:02, 604.85it/s]warmup run: 572it [00:02, 580.93it/s]warmup run: 894it [00:02, 832.95it/s]warmup run: 907it [00:02, 842.58it/s]warmup run: 693it [00:02, 699.23it/s]warmup run: 854it [00:02, 807.42it/s]warmup run: 887it [00:02, 828.22it/s]warmup run: 999it [00:02, 884.65it/s]warmup run: 662it [00:02, 695.43it/s]warmup run: 993it [00:02, 875.46it/s]warmup run: 676it [00:02, 685.91it/s]warmup run: 1007it [00:02, 882.64it/s]warmup run: 792it [00:02, 770.62it/s]warmup run: 951it [00:02, 851.37it/s]warmup run: 985it [00:02, 867.53it/s]warmup run: 1101it [00:02, 920.77it/s]warmup run: 762it [00:02, 769.96it/s]warmup run: 1093it [00:02, 909.74it/s]warmup run: 779it [00:02, 771.03it/s]warmup run: 1108it [00:02, 916.30it/s]warmup run: 890it [00:02, 824.43it/s]warmup run: 1049it [00:02, 886.85it/s]warmup run: 1083it [00:02, 891.28it/s]warmup run: 1203it [00:02, 946.62it/s]warmup run: 861it [00:02, 826.76it/s]warmup run: 880it [00:02, 833.42it/s]warmup run: 1193it [00:02, 934.87it/s]warmup run: 1208it [00:02, 938.15it/s]warmup run: 991it [00:02, 872.69it/s]warmup run: 1151it [00:02, 923.38it/s]warmup run: 1180it [00:02, 885.09it/s]warmup run: 1304it [00:02, 964.41it/s]warmup run: 960it [00:02, 870.43it/s]warmup run: 1294it [00:02, 954.49it/s]warmup run: 982it [00:02, 881.96it/s]warmup run: 1308it [00:02, 949.91it/s]warmup run: 1093it [00:02, 912.28it/s]warmup run: 1251it [00:02, 945.00it/s]warmup run: 1274it [00:02, 887.37it/s]warmup run: 1406it [00:02, 978.58it/s]warmup run: 1060it [00:02, 906.10it/s]warmup run: 1394it [00:02, 967.55it/s]warmup run: 1084it [00:02, 918.16it/s]warmup run: 1407it [00:02, 959.90it/s]warmup run: 1194it [00:02, 938.46it/s]warmup run: 1352it [00:02, 961.41it/s]warmup run: 1375it [00:02, 920.02it/s]warmup run: 1508it [00:03, 988.66it/s]warmup run: 1159it [00:02, 926.86it/s]warmup run: 1184it [00:02, 940.32it/s]warmup run: 1494it [00:03, 962.11it/s]warmup run: 1506it [00:03, 960.76it/s]warmup run: 1294it [00:02, 956.19it/s]warmup run: 1454it [00:03, 977.73it/s]warmup run: 1476it [00:03, 944.05it/s]warmup run: 1609it [00:03, 993.69it/s]warmup run: 1258it [00:02, 942.10it/s]warmup run: 1285it [00:02, 958.86it/s]warmup run: 1593it [00:03, 956.67it/s]warmup run: 1605it [00:03, 959.65it/s]warmup run: 1397it [00:02, 977.44it/s]warmup run: 1554it [00:03, 969.07it/s]warmup run: 1577it [00:03, 961.00it/s]warmup run: 1710it [00:03, 997.03it/s]warmup run: 1357it [00:02, 951.85it/s]warmup run: 1385it [00:02, 967.68it/s]warmup run: 1705it [00:03, 970.78it/s]warmup run: 1691it [00:03, 956.76it/s]warmup run: 1500it [00:03, 992.01it/s]warmup run: 1653it [00:03, 965.19it/s]warmup run: 1676it [00:03, 967.72it/s]warmup run: 1811it [00:03, 998.15it/s]warmup run: 1455it [00:02, 959.53it/s]warmup run: 1485it [00:02, 973.13it/s]warmup run: 1806it [00:03, 980.24it/s]warmup run: 1602it [00:03, 996.25it/s]warmup run: 1788it [00:03, 954.79it/s]warmup run: 1753it [00:03, 972.75it/s]warmup run: 1775it [00:03, 972.84it/s]warmup run: 1912it [00:03, 992.40it/s]warmup run: 1553it [00:03, 964.15it/s]warmup run: 1585it [00:03, 972.71it/s]warmup run: 1907it [00:03, 987.25it/s]warmup run: 1704it [00:03, 999.03it/s]warmup run: 1885it [00:03, 950.11it/s]warmup run: 1852it [00:03, 974.96it/s]warmup run: 1874it [00:03, 967.93it/s]warmup run: 2015it [00:03, 1002.97it/s]warmup run: 1652it [00:03, 969.24it/s]warmup run: 1684it [00:03, 975.71it/s]warmup run: 2010it [00:03, 998.10it/s]warmup run: 1805it [00:03, 999.94it/s]warmup run: 1981it [00:03, 952.98it/s]warmup run: 1951it [00:03, 976.79it/s]warmup run: 1972it [00:03, 964.94it/s]warmup run: 2138it [00:03, 1068.38it/s]warmup run: 1750it [00:03, 970.40it/s]warmup run: 1785it [00:03, 983.49it/s]warmup run: 2129it [00:03, 1054.89it/s]warmup run: 1906it [00:03, 1002.62it/s]warmup run: 2097it [00:03, 1011.96it/s]warmup run: 2062it [00:03, 1015.52it/s]warmup run: 2084it [00:03, 1010.29it/s]warmup run: 2261it [00:03, 1114.78it/s]warmup run: 1848it [00:03, 973.07it/s]warmup run: 1886it [00:03, 988.95it/s]warmup run: 2248it [00:03, 1094.41it/s]warmup run: 2007it [00:03, 1002.44it/s]warmup run: 2218it [00:03, 1069.40it/s]warmup run: 2183it [00:03, 1070.99it/s]warmup run: 2205it [00:03, 1067.15it/s]warmup run: 2384it [00:03, 1147.60it/s]warmup run: 1946it [00:03, 973.77it/s]warmup run: 1988it [00:03, 996.62it/s]warmup run: 2368it [00:03, 1123.13it/s]warmup run: 2126it [00:03, 1058.14it/s]warmup run: 2339it [00:03, 1110.78it/s]warmup run: 2304it [00:03, 1111.38it/s]warmup run: 2325it [00:03, 1104.87it/s]warmup run: 2507it [00:03, 1170.99it/s]warmup run: 2052it [00:03, 998.61it/s]warmup run: 2107it [00:03, 1052.69it/s]warmup run: 2487it [00:03, 1141.83it/s]warmup run: 2246it [00:03, 1097.48it/s]warmup run: 2460it [00:03, 1139.82it/s]warmup run: 2425it [00:03, 1139.28it/s]warmup run: 2445it [00:03, 1131.99it/s]warmup run: 2630it [00:04, 1186.91it/s]warmup run: 2156it [00:03, 1007.66it/s]warmup run: 2230it [00:03, 1103.95it/s]warmup run: 2606it [00:04, 1153.76it/s]warmup run: 2365it [00:03, 1125.02it/s]warmup run: 2581it [00:04, 1160.24it/s]warmup run: 2546it [00:04, 1158.56it/s]warmup run: 2565it [00:04, 1151.68it/s]warmup run: 2752it [00:04, 1194.09it/s]warmup run: 2267it [00:03, 1037.95it/s]warmup run: 2353it [00:03, 1139.91it/s]warmup run: 2724it [00:04, 1158.85it/s]warmup run: 2484it [00:03, 1143.22it/s]warmup run: 2701it [00:04, 1171.29it/s]warmup run: 2667it [00:04, 1172.31it/s]warmup run: 2684it [00:04, 1160.51it/s]warmup run: 2874it [00:04, 1201.07it/s]warmup run: 2387it [00:03, 1085.78it/s]warmup run: 2476it [00:03, 1165.10it/s]warmup run: 2843it [00:04, 1165.70it/s]warmup run: 2603it [00:04, 1156.72it/s]warmup run: 2822it [00:04, 1181.66it/s]warmup run: 2787it [00:04, 1177.75it/s]warmup run: 2804it [00:04, 1170.49it/s]warmup run: 2997it [00:04, 1207.29it/s]warmup run: 2510it [00:04, 1126.61it/s]warmup run: 2598it [00:04, 1180.84it/s]warmup run: 3000it [00:04, 687.94it/s] warmup run: 2962it [00:04, 1170.23it/s]warmup run: 2721it [00:04, 1162.67it/s]warmup run: 2943it [00:04, 1187.41it/s]warmup run: 2908it [00:04, 1185.71it/s]warmup run: 3000it [00:04, 682.53it/s] warmup run: 2924it [00:04, 1177.81it/s]warmup run: 3000it [00:04, 679.44it/s] warmup run: 2630it [00:04, 1145.93it/s]warmup run: 2718it [00:04, 1186.18it/s]warmup run: 2841it [00:04, 1171.92it/s]warmup run: 3000it [00:04, 674.09it/s] warmup run: 3000it [00:04, 671.37it/s] warmup run: 2748it [00:04, 1155.91it/s]warmup run: 2839it [00:04, 1191.31it/s]warmup run: 2962it [00:04, 1181.09it/s]warmup run: 3000it [00:04, 688.98it/s] warmup run: 2867it [00:04, 1165.93it/s]warmup run: 2961it [00:04, 1198.28it/s]warmup run: 3000it [00:04, 691.19it/s] warmup run: 2985it [00:04, 1170.05it/s]warmup run: 3000it [00:04, 678.20it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.82it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.33it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.02it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1627.39it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.63it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1603.07it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.49it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.76it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.23it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.42it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.93it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1612.04it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1643.86it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.02it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.35it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1651.36it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1663.61it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1657.58it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1649.81it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1643.26it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1640.47it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1628.41it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1606.46it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1644.83it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1649.49it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.47it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1643.21it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1640.04it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1626.54it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1649.57it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1604.27it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1641.49it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1647.40it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1659.75it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1641.54it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1623.92it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1638.78it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1602.63it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1641.48it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1636.34it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1643.20it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1654.43it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1632.74it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1630.59it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1615.29it/s]warmup should be done:  32%|███▏      | 968/3000 [00:00<00:01, 1595.21it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1641.36it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1632.99it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1643.60it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1655.56it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1631.01it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1628.59it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1613.63it/s]warmup should be done:  38%|███▊      | 1128/3000 [00:00<00:01, 1590.12it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1636.49it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1621.62it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1643.17it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1654.76it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1628.38it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1631.54it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1612.09it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1639.98it/s]warmup should be done:  43%|████▎     | 1288/3000 [00:00<00:01, 1587.85it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1618.78it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1641.29it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1646.02it/s]warmup should be done:  49%|████▉     | 1481/3000 [00:00<00:00, 1631.92it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1628.94it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1615.79it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1642.13it/s]warmup should be done:  48%|████▊     | 1448/3000 [00:00<00:00, 1588.60it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1620.74it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1638.12it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1633.29it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1630.44it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1620.42it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1643.86it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1637.59it/s]warmup should be done:  54%|█████▎    | 1607/3000 [00:01<00:00, 1588.82it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1623.98it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1634.22it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1623.29it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1630.14it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1644.11it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1624.01it/s]warmup should be done:  59%|█████▉    | 1767/3000 [00:01<00:00, 1589.81it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1633.99it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1627.22it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1634.14it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1624.93it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1629.96it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1644.63it/s]warmup should be done:  64%|██████▍   | 1926/3000 [00:01<00:00, 1588.92it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1629.13it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1630.96it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1604.15it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1633.21it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1626.55it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1629.65it/s]warmup should be done:  72%|███████▏  | 2149/3000 [00:01<00:00, 1644.64it/s]warmup should be done:  70%|██████▉   | 2085/3000 [00:01<00:00, 1588.10it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1630.91it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1627.24it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1590.24it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1629.43it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1626.49it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1641.91it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1626.57it/s]warmup should be done:  75%|███████▍  | 2244/3000 [00:01<00:00, 1584.20it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1627.05it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1620.66it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1587.09it/s]warmup should be done:  82%|████████▏ | 2451/3000 [00:01<00:00, 1635.78it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1626.61it/s]warmup should be done:  80%|████████  | 2404/3000 [00:01<00:00, 1587.01it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1629.63it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1620.16it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1636.27it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1620.87it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1572.71it/s]warmup should be done:  87%|████████▋ | 2616/3000 [00:01<00:00, 1639.37it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1627.61it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1639.75it/s]warmup should be done:  86%|████████▌ | 2566/3000 [00:01<00:00, 1594.22it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1633.42it/s]warmup should be done:  88%|████████▊ | 2627/3000 [00:01<00:00, 1616.09it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1623.19it/s]warmup should be done:  87%|████████▋ | 2616/3000 [00:01<00:00, 1581.92it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1642.79it/s]warmup should be done:  93%|█████████▎| 2787/3000 [00:01<00:00, 1627.76it/s]warmup should be done:  91%|█████████ | 2727/3000 [00:01<00:00, 1598.43it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1643.03it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1635.59it/s]warmup should be done:  94%|█████████▎| 2807/3000 [00:01<00:00, 1624.86it/s]warmup should be done:  93%|█████████▎| 2789/3000 [00:01<00:00, 1608.96it/s]warmup should be done:  92%|█████████▎| 2775/3000 [00:01<00:00, 1578.83it/s]warmup should be done:  98%|█████████▊| 2952/3000 [00:01<00:00, 1634.14it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1651.22it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1605.93it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1642.76it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1640.37it/s]warmup should be done:  99%|█████████▉| 2972/3000 [00:01<00:00, 1630.61it/s]warmup should be done:  98%|█████████▊| 2952/3000 [00:01<00:00, 1613.16it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1589.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.92it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.55it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.49it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.47it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.81it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.33it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.26it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.01it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.61it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1673.14it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.16it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1690.79it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1674.40it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1669.66it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.10it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1640.20it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1678.99it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1679.52it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1693.74it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.27it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.82it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1695.21it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.32it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.52it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1675.32it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1681.45it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1678.22it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1692.43it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1670.14it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1675.62it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1697.06it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1675.81it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1616.37it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1678.79it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1697.58it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1678.99it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1691.88it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1668.55it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1669.00it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1677.64it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1612.32it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1698.65it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1679.95it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1687.88it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1661.74it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1665.85it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1675.63it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1662.91it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1617.32it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1680.60it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1694.45it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1686.02it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1664.16it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1667.15it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1658.07it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1671.05it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1608.09it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1683.04it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1698.41it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1688.87it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1669.08it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1680.83it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1673.25it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1661.95it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1619.33it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1697.85it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1682.15it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1690.50it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1670.71it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1687.54it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1674.16it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1664.78it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1618.29it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1696.69it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1681.25it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1692.10it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1675.44it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1692.28it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1682.09it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1667.97it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1617.68it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1697.01it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1683.05it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1693.41it/s]warmup should be done:  62%|██████▏   | 1860/3000 [00:01<00:00, 1696.44it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1683.64it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1669.13it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1624.04it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1650.85it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1696.24it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1683.38it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1694.16it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1697.38it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1688.78it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1668.22it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1624.99it/s]warmup should be done:  67%|██████▋   | 2018/3000 [00:01<00:00, 1658.55it/s]warmup should be done:  74%|███████▍  | 2214/3000 [00:01<00:00, 1697.55it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1682.99it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1691.63it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1694.18it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1670.12it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1685.92it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1662.37it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1611.22it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1699.67it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1684.01it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1691.99it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1678.12it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1689.70it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1682.51it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1668.42it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1619.01it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1685.64it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1696.29it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1693.41it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1683.46it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1688.41it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1682.04it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1672.47it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1619.83it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1686.50it/s]warmup should be done:  91%|█████████ | 2725/3000 [00:01<00:00, 1693.93it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1694.47it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1680.72it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1686.41it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1678.12it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1671.92it/s]warmup should be done:  87%|████████▋ | 2617/3000 [00:01<00:00, 1624.17it/s]warmup should be done:  96%|█████████▌| 2870/3000 [00:01<00:00, 1685.14it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1692.58it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1685.36it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1681.11it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1668.09it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1672.15it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1629.08it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1634.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1691.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.19it/s]warmup should be done:  98%|█████████▊| 2948/3000 [00:01<00:00, 1637.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.22it/s]2022-12-12 06:29:02.101282: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34ac02d2a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.101346: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.146066: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f53b3833f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.146133: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.166578: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f353002cc60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.166643: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.178924: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34fc029f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.178985: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.189234: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3570029e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.189289: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.437815: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f53b7833e40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.437890: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.482441: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34d402d300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.482518: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:02.504303: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f53b3796560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:29:02.504378: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:29:04.417095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.458531: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.507527: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.509197: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.532537: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.755290: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.789513: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:04.839404: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:29:07.339611: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.408917: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.496709: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.497219: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.499302: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.687244: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.709248: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:29:07.760078: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:29:31.504][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:29:31.504][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.510][ERROR][RK0][main]: coll ps creation done
[HCTR][06:29:31.511][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][06:29:31.537][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:29:31.537][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.546][ERROR][RK0][main]: coll ps creation done
[HCTR][06:29:31.546][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][06:29:31.569][ERROR][RK0][tid #139998141605632]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:29:31.570][ERROR][RK0][tid #139998141605632]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.577][ERROR][RK0][tid #139998141605632]: coll ps creation done
[HCTR][06:29:31.577][ERROR][RK0][tid #139998141605632]: replica 3 waits for coll ps creation barrier
[HCTR][06:29:31.578][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:29:31.578][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.581][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:29:31.581][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.583][ERROR][RK0][main]: coll ps creation done
[HCTR][06:29:31.583][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:29:31.588][ERROR][RK0][main]: coll ps creation done
[HCTR][06:29:31.588][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][06:29:31.620][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:29:31.620][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.629][ERROR][RK0][main]: coll ps creation done
[HCTR][06:29:31.629][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][06:29:31.668][ERROR][RK0][tid #139997613127424]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:29:31.668][ERROR][RK0][tid #139997613127424]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.675][ERROR][RK0][tid #139997613127424]: coll ps creation done
[HCTR][06:29:31.675][ERROR][RK0][tid #139997613127424]: replica 6 waits for coll ps creation barrier
[HCTR][06:29:31.685][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:29:31.685][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][06:29:31.690][ERROR][RK0][main]: coll ps creation done
[HCTR][06:29:31.690][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][06:29:31.690][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][06:29:32.748][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][06:29:32.793][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][tid #139998141605632]: replica 3 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][tid #139997613127424]: replica 6 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:29:32.793][ERROR][RK0][main]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][main]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][main]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][tid #139998141605632]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][main]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][tid #139997613127424]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][main]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][tid #139997613127424]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][main]: Calling build_v2
[HCTR][06:29:32.793][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][tid #139998141605632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:29:32.793][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 06:29:322022-12-12 06:29:322022-12-12 06:29:32[.2022-12-12 06:29:32.2022-12-12 06:29:32.[793937.7939372022-12-12 06:29:32.2022-12-12 06:29:32793944: 7939392022-12-12 06:29:32: .793940.: E: .E793946: 793959E E793958 : E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE: : :136: 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] :] :] using concurrent impl MPS] :using concurrent impl MPS136using concurrent impl MPS136using concurrent impl MPS
using concurrent impl MPS136
] 
] 

] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS


[2022-12-12 06:29:32.798217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 06:29:322022-12-12 06:29:32..798261798264: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178196] ] [v100x8, slow pcieassigning 8 to cpu2022-12-12 06:29:32

.798306: [E2022-12-12 06:29:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc798329:: 178[E] 2022-12-12 06:29:32 [v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:29:32
798346:.: 196[798351E] 2022-12-12 06:29:32:  assigning 8 to cpu.[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
7983782022-12-12 06:29:32 :: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212E798394:]  : 178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 
: [v100x8, slow pcie196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 06:29:32
] :2022-12-12 06:29:32.assigning 8 to cpu[178.[798441
2022-12-12 06:29:32] 7984552022-12-12 06:29:32: .v100x8, slow pcie: .E[798470
E798474[ 2022-12-12 06:29:32:  : 2022-12-12 06:29:32[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[.2022-12-12 06:29:32:798494 : 2022-12-12 06:29:32798508.178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 798527] E:] :798537E: v100x8, slow pcie 213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196:  E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :remote time is 8.68421[assigning 8 to cpu :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178
2022-12-12 06:29:32
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-12 06:29:32:] .[:] .196v100x8, slow pcie7986852022-12-12 06:29:32178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8798700] [
: .] 
: assigning 8 to cpu2022-12-12 06:29:32E798741[v100x8, slow pcieE
. [: 2022-12-12 06:29:32
 798777/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:29:32E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :[. 798813:E1962022-12-12 06:29:32798831[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 213 ] .: 2022-12-12 06:29:32:E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu798868E.214 remote time is 8.68421:
:  798896] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588:] [[ :E
196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:29:322022-12-12 06:29:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 ] 
..:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu799014799016196[remote time is 8.68421
:
[: : ] 2022-12-12 06:29:322122022-12-12 06:29:32EEassigning 8 to cpu.] .  
799092build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8799148/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
: :2022-12-12 06:29:32:EE212.214[  ] 799189] 2022-12-12 06:29:32[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: cpu time is 97.0588.2022-12-12 06:29:32::
E
799231.213214 : [799248] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 06:29:32: remote time is 8.68421cpu time is 97.0588: .E

212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc799315 ] [:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:29:32213E:
.]  212799395remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : [
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E2022-12-12 06:29:32213
[ .] 2022-12-12 06:29:32[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc799453remote time is 8.68421.2022-12-12 06:29:32:: 
799481.214E: [799513]  E2022-12-12 06:29:32: cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .E
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc799557 213:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214E:remote time is 8.68421]  213
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
:[remote time is 8.684212142022-12-12 06:29:32
] .cpu time is 97.0588[799687
2022-12-12 06:29:32: .E799722 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 06:30:50.225064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:30:50.265171: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 06:30:50.265253: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 06:30:50.266302: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 06:30:50.345341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 06:30:50.742397: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 06:30:50.742487: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 06:30:58.749217: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 06:30:58.749312: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 06:31:00.463542: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 06:31:00.463642: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 06:31:00.466596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 06:31:00.466662: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 06:31:00.993860: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 06:31:01. 22410: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 06:31:01. 23804: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 06:31:01. 44101: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 06:31:01.566913: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 06:34:22.541679: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 06:34:22.551349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 06:34:22.561784: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 06:34:22.608581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:34:22.608678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:34:22.608712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:34:22.608743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:34:22.609351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:34:22.609407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.610611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.611332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.623949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 06:34:22.624032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 06:34:22.[[6244972022-12-12 06:34:222022-12-12 06:34:22: ..E624489624494 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:  1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :[:Building Coll Cache with ... num gpu device is 82022022-12-12 06:34:22202
] [.] 1 solved2022-12-12 06:34:226245675 solved
.[: 
6245852022-12-12 06:34:22[E: .2022-12-12 06:34:22[ E624626.2022-12-12 06:34:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc : 624638.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 624646202: E: ] 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E6 solved] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 
3 solved1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
] 205[:eager alloc mem 381.47 MB] 2022-12-12 06:34:22[205
worker 0 thread 1 initing device 1.2022-12-12 06:34:22] 
624729.worker 0 thread 5 initing device 5: 624740
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 6 initing device 6] 
worker 0 thread 3 initing device 3
[2022-12-12 06:34:22[.2022-12-12 06:34:22625167.: 625172E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ [:2022-12-12 06:34:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:34:221815.:.] 6251951815625196Building Coll Cache with ... num gpu device is 8: ] : 
EBuilding Coll Cache with ... num gpu device is 8E 
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:34:22::.18151815[625270] ] 2022-12-12 06:34:22: Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8.E

625288 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[] [:2022-12-12 06:34:22eager alloc mem 381.47 MB2022-12-12 06:34:221980.
.] 625329625331eager alloc mem 381.47 MB: : 
EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 06:34:22.625784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 06:34:22.625842: [E2022-12-12 06:34:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc625829:: 205E]  worker 0 thread 4 initing device 4/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:202] 2 solved
[2022-12-12 06:34:22.625943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 06:34:22.626328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:34:22.626371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.626410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:34:22.626459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.628715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.629210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.629701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.629927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.629983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.630522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.631031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.633135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.633474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.634010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.634136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.634187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.634239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.635332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:34:22.694968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:34:22.700226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:34:22.700335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:34:22.702318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.702986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.704128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:22.704182: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.35 MB
[2022-12-12 06:34:22.714408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:34:22.714447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[[[2022-12-12 06:34:222022-12-12 06:34:222022-12-12 06:34:222022-12-12 06:34:222022-12-12 06:34:22.....714521714521714521714521714528: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes




[2022-12-12 06:34:22.719161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:34:22.719226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:34:22.720404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.720936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.720969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:34:22.721067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 06:34:22] .eager release cuda mem 400000000721065
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:34:22.721137[: 2022-12-12 06:34:22E. 721162/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[2022-12-12 06:34:22.[7212322022-12-12 06:34:22: .E721226 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1024
[2022-12-12 06:34:22[.2022-12-12 06:34:22721323.: 721314E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[[2022-12-12 06:34:222022-12-12 06:34:22..721398721412: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 06:34:22.721497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:34:22.721960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.721999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:22.722046: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.29 MB
[2022-12-12 06:34:22.722968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.723577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.724458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.725129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.725638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:34:22.726345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.726794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.726887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.727061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.727110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.727157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:22.727443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:22.727490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.34 MB
[2022-12-12 06:34:22.727883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:22.727932: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.34 MB
[2022-12-12 06:34:22.727965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:22.728010: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.26 MB
[2022-12-12 06:34:22.728172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:34:222022-12-12 06:34:22..728215728220: : EW [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:34:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:.:63872823943] : ] eager release cuda mem 625663EWORKER[0] alloc host memory 95.35 MB
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:22.728307: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 06:34:22:.43728321] : WORKER[0] alloc host memory 95.26 MBW
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.04 MB
[2022-12-12 06:34:22.768893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.769516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.769560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:34:22.784550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.785142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.785182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:34:22.790781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.791421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.791465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[2022-12-12 06:34:22.791521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.792142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.792182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:34:22.792675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.793167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.793280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.793322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:34:22.793450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.793718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:34:22.793773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.793814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.88 GB
[2022-12-12 06:34:22.794049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.794088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:34:22.794322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:34:22.794364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[[[[[[[[2022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:26........612783612783612782612782612782612782612782612782: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] Device 2 init p2p of link 1] ] ] ] ] ] ] 
Device 4 init p2p of link 5Device 0 init p2p of link 3Device 5 init p2p of link 6Device 6 init p2p of link 0Device 7 init p2p of link 4Device 3 init p2p of link 2Device 1 init p2p of link 7






[2022-12-12 06:34:26.613334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[[[[[2022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:262022-12-12 06:34:26[2022-12-12 06:34:26.....2022-12-12 06:34:26.613354613355613354613354613359.613358: : : : : 613373: EEEEE: E     E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980198019801980:1980] ] ] ] ] 1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB




eager alloc mem 611.00 KB

[2022-12-12 06:34:26.614291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:34:262022-12-12 06:34:26..614467614468: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638[638] 2022-12-12 06:34:26[] eager release cuda mem 625663[.2022-12-12 06:34:26eager release cuda mem 625663
2022-12-12 06:34:26614503[.
.: 2022-12-12 06:34:26614510[614515E.: 2022-12-12 06:34:26:  614527E.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  614543 :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 :E:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 638eager release cuda mem 625663:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
638eager release cuda mem 625663:eager release cuda mem 625663] 
638
eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 06:34:26.632598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 06:34:26.632760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.632850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:34:26.633024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.633702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.633980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.634685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 06:34:26.634838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.635057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 06:34:26.635229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.635387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 06:34:26.635561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.635689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 06:34:26.635747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:34:262022-12-12 06:34:26..635848635863: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 7 init p2p of link 1eager alloc mem 611.00 KB

[[2022-12-12 06:34:262022-12-12 06:34:26..636031636051: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 0 init p2p of link 6eager alloc mem 611.00 KB

[2022-12-12 06:34:26.636181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.636244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.636496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.636804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.637001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.637141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.651099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 06:34:26.651236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.651616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 06:34:26.651747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.652162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.652680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.653770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 06:34:26.653822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 06:34:26.653890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.653952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.654139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:34:26.654268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.654394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:34:26.654521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:34:262022-12-12 06:34:26..654602654604: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 4 init p2p of link 2Device 5 init p2p of link 7

[2022-12-12 06:34:26[.2022-12-12 06:34:26654759.: 654762E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 06:34:26.654812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.654858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.655165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.655357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.655619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.655699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.673773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:34:26.673894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.674806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.675459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:34:26.675505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:34:26.675580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.675623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.676499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.676547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.677022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:34:26.677150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.678066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.679635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:34:26.679757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.680059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 06:34:26.680181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.680685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.681088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.681456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:34:26.681569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.682380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.682897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:34:26.683018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:34:26.683842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:34:26.691809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.692568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24912890 / 100000000 nodes ( 24.91 %~25.00 %) | remote 67998724 / 100000000 nodes ( 68.00 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.88 GB | 4.06724 secs 
[2022-12-12 06:34:26.696057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.696585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24980532 / 100000000 nodes ( 24.98 %~25.00 %) | remote 67931082 / 100000000 nodes ( 67.93 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.92 GB | 4.07014 secs 
[2022-12-12 06:34:26.697495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.698548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.700074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.700230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.702764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.703145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:34:26.722928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24995391 / 100000000 nodes ( 25.00 %~25.00 %) | remote 67916223 / 100000000 nodes ( 67.92 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.92 GB | 4.09765 secs 
[2022-12-12 06:34:26.723112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24973065 / 100000000 nodes ( 24.97 %~25.00 %) | remote 67938549 / 100000000 nodes ( 67.94 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.91 GB | 4.09675 secs 
[2022-12-12 06:34:26.723459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24992507 / 100000000 nodes ( 24.99 %~25.00 %) | remote 67919107 / 100000000 nodes ( 67.92 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.92 GB | 4.09821 secs 
[2022-12-12 06:34:26.723630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24971163 / 100000000 nodes ( 24.97 %~25.00 %) | remote 67940451 / 100000000 nodes ( 67.94 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.91 GB | 4.09831 secs 
[2022-12-12 06:34:26.723818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24993603 / 100000000 nodes ( 24.99 %~25.00 %) | remote 67918011 / 100000000 nodes ( 67.92 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.92 GB | 4.09921 secs 
[2022-12-12 06:34:26.724072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24995811 / 100000000 nodes ( 25.00 %~25.00 %) | remote 67915803 / 100000000 nodes ( 67.92 %) | cpu 7088386 / 100000000 nodes ( 7.09 %) | 11.92 GB | 4.11468 secs 
[2022-12-12 06:34:26.725008: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.56 GB
[2022-12-12 06:34:28. 70368: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.82 GB
[2022-12-12 06:34:28. 70737: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.82 GB
[2022-12-12 06:34:28. 71216: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.82 GB
[2022-12-12 06:34:29.392825: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.09 GB
[2022-12-12 06:34:29.392999: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.09 GB
[2022-12-12 06:34:29.394024: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.09 GB
[2022-12-12 06:34:30.591611: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.30 GB
[2022-12-12 06:34:30.591739: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.30 GB
[2022-12-12 06:34:30.592015: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.30 GB
[2022-12-12 06:34:31.450118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.52 GB
[2022-12-12 06:34:31.450730: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.52 GB
[2022-12-12 06:34:31.451682: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.52 GB
[2022-12-12 06:34:32.797603: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.97 GB
[2022-12-12 06:34:32.798608: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.97 GB
[2022-12-12 06:34:32.799432: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.97 GB
[2022-12-12 06:34:34.378945: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.17 GB
[2022-12-12 06:34:34.379460: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.17 GB
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][tid #139997613127424]: replica 6 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][tid #139998141605632]: replica 3 calling init per replica done, doing barrier
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][tid #139998141605632]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][tid #139997613127424]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:34:35.449][ERROR][RK0][main]: init per replica done
[HCTR][06:34:35.449][ERROR][RK0][main]: init per replica done
[HCTR][06:34:35.449][ERROR][RK0][main]: init per replica done
[HCTR][06:34:35.449][ERROR][RK0][main]: init per replica done
[HCTR][06:34:35.449][ERROR][RK0][tid #139998141605632]: init per replica done
[HCTR][06:34:35.449][ERROR][RK0][main]: init per replica done
[HCTR][06:34:35.449][ERROR][RK0][tid #139997613127424]: init per replica done
[HCTR][06:34:35.452][ERROR][RK0][main]: init per replica done
[HCTR][06:34:35.455][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f55a5f20000
[HCTR][06:34:35.455][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f55a6400000
[HCTR][06:34:35.455][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f55a6a40000
[HCTR][06:34:35.455][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f55a6d60000
[HCTR][06:34:35.455][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f55a1f20000
[HCTR][06:34:35.455][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f55a7f20000
[HCTR][06:34:35.455][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f55a2400000
[HCTR][06:34:35.455][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f55a8400000
[HCTR][06:34:35.455][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f55a2a40000
[HCTR][06:34:35.455][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f55a8a40000
[HCTR][06:34:35.455][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f55a2d60000
[HCTR][06:34:35.455][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f55a5f20000
[HCTR][06:34:35.455][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f55a8d60000
[HCTR][06:34:35.455][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f55a6400000
[HCTR][06:34:35.455][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f55a6a40000
[HCTR][06:34:35.455][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f55a6d60000
[HCTR][06:34:35.455][ERROR][RK0][tid #139997671843584]: 2 allocated 3276800 at 0x7f559bf20000
[HCTR][06:34:35.455][ERROR][RK0][tid #139997671843584]: 2 allocated 6553600 at 0x7f559c400000
[HCTR][06:34:35.455][ERROR][RK0][tid #139997671843584]: 2 allocated 3276800 at 0x7f559ca40000
[HCTR][06:34:35.455][ERROR][RK0][tid #139997671843584]: 2 allocated 6553600 at 0x7f559cd60000
[HCTR][06:34:35.455][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f55a5f20000
[HCTR][06:34:35.455][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f55a6400000
[HCTR][06:34:35.455][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f55a6a40000
[HCTR][06:34:35.455][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f55a6d60000
[HCTR][06:34:35.456][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f55a5f20000
[HCTR][06:34:35.456][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f55a6400000
[HCTR][06:34:35.456][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f55a6a40000
[HCTR][06:34:35.456][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f55a6d60000
[HCTR][06:34:35.458][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f55a8b20000
[HCTR][06:34:35.458][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f55a9000000
[HCTR][06:34:35.458][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f381e50e800
[HCTR][06:34:35.458][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f381e82e800








