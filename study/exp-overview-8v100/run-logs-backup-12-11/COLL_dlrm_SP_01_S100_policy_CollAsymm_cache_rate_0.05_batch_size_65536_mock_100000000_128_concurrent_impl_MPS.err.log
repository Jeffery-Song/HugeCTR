2022-12-12 01:20:59.440158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.449300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.455561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.461237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.466360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.476940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.484570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.488842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.552705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.553754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.554658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.555594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.556771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.557782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.559189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.560841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.561784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.561909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.563333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.563461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.564623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.565022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.566250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.566722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.568246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.568555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.569717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.570198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.571043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.571795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.572343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.573329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.574475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.575503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.576467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.577464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.578366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.579261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.580317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.581324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.585810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.586600: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.586919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.587973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.588952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.589947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.591455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.593035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.593483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.594698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.595795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.597596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.598105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.598147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.600302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.600797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.600808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.600963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.602615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.603507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.603632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.603800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.606432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.606591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.606850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.609661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.609844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.610131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.611285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.612819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.612910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.613199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.614780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.616101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.616459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.617623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.618747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.619120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.620075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.620525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.622206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.622250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.622925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.623513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.625022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.625394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.626099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.627317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.627497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.628253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.629256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.629344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.630590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.632638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.632735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.634362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.634449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.636019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.636325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.642720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.661644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.667577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.669725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.673399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.674165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.674222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.674289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.674339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.674396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.677677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.678110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.678516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.678711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.678759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.678809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.682590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.682948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.684032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.684174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.684222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.684314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.686905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.687252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.688011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.688062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.688107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.688316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.691225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.691728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.692195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.692337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.692385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.692533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.695827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.696359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.696930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.697031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.697158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.697206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.699666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.700380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.701067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.701118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.701250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.701301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.703824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.704346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.705039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.705131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.705178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.705349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.708209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.708657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.708932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.708992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.709031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.709152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.712454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.713883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.714010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.714565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.714612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.714710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.717784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.718427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.718473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.718613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.718704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.718806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.722210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.722863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.722912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.723067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.723108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.723247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.723945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.726834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.727335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.727499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.727747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.727849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.727990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.728839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.731409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.731951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.732145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.732361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.732460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.732577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.733560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.736378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.736804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.737056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.737350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.737405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.737453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.738776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.741716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.742585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.743534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.744315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.744629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.744715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.745184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.747027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.748291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.748828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.748950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.748971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.749764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.751942: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.752030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.752549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.752700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.752802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.752933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.753473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.756196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.756669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.756892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.756919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.757160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.757544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.760168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.760807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.761363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.761556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.761751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.762624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.762680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.766351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.766799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.767042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.767361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.767420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.767952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.768231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.770917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.771628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.772110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.772320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.772512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.772579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.772661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.775683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.776589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.776867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.777170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.777303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.777986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.781403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.781713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.784522: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.785232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.785367: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.785393: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.785670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.786053: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.787995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.790269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.791143: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.792831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.795773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.796176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.796242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.796291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.797039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.799529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.800146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.800154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.800313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.800830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.801535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.803539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.804094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.804151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.804485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.804730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.805851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.842728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.845187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.848585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.854993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.890022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.898975: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:20:59.909425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.914682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:20:59.920072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.934548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.941774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.942335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.942953: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:00.943016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:21:00.960334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.960975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.961482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.962060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.962579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:00.963061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:21:01.009478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.009735: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.052479: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:21:01.169888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.170577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.171334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.171809: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.171869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:21:01.189227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.190255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.190771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.191524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.192061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.192533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:21:01.242337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.242978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.243951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.244016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.244813: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.244872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:21:01.245261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.245806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.246270: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.246318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:21:01.257082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.257725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.258262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.258748: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.258813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:21:01.259853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.260484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.260793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.261164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.262012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.262093: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.262179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:21:01.262782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.263102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.263512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.264157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.264671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.264845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.265626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.266067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:21:01.266467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.266813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.272276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.272646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.273281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.274072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.274173: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.274236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:21:01.274781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:21:01.275195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.275589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.275689: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:21:01.275742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:21:01.276490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.277033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.277604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.278129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.278595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:21:01.280186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.280808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.281407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.281994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.282516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.282995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:21:01.291668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.292329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.292857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.292877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.294111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.294234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.295077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.295316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.296054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:21:01.296318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.296850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:21:01.297317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:21:01.301097: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.301258: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.302950: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:21:01.311365: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.311559: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.313328: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:21:01.324543: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.324744: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.326635: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 01:21:01.328354: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.328489: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.330299: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:21:01.343067: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.343066: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.343258: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.343273: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.345204: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:21:01.345306: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:21:01.372364: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.372561: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:21:01.374450: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][01:21:02.633][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.633][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.636][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.636][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.636][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.637][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.687][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:21:02.687][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 80it [00:01, 67.48it/s]warmup run: 75it [00:01, 64.35it/s]warmup run: 97it [00:01, 83.00it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 100it [00:01, 85.43it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 168it [00:01, 155.05it/s]warmup run: 167it [00:01, 158.10it/s]warmup run: 197it [00:01, 183.13it/s]warmup run: 96it [00:01, 82.65it/s]warmup run: 200it [00:01, 184.99it/s]warmup run: 98it [00:01, 84.96it/s]warmup run: 270it [00:01, 270.27it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 263it [00:01, 266.49it/s]warmup run: 298it [00:01, 294.59it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 189it [00:01, 175.58it/s]warmup run: 297it [00:01, 290.58it/s]warmup run: 195it [00:01, 182.58it/s]warmup run: 371it [00:01, 388.33it/s]warmup run: 97it [00:01, 85.13it/s]warmup run: 361it [00:01, 381.26it/s]warmup run: 398it [00:01, 407.95it/s]warmup run: 101it [00:01, 88.73it/s]warmup run: 284it [00:01, 280.16it/s]warmup run: 391it [00:01, 394.66it/s]warmup run: 291it [00:01, 288.38it/s]warmup run: 472it [00:02, 503.08it/s]warmup run: 197it [00:01, 187.29it/s]warmup run: 457it [00:02, 489.54it/s]warmup run: 489it [00:02, 501.17it/s]warmup run: 198it [00:01, 187.16it/s]warmup run: 378it [00:01, 387.00it/s]warmup run: 484it [00:02, 494.19it/s]warmup run: 387it [00:01, 397.50it/s]warmup run: 573it [00:02, 608.51it/s]warmup run: 298it [00:01, 300.44it/s]warmup run: 555it [00:02, 592.46it/s]warmup run: 580it [00:02, 584.74it/s]warmup run: 295it [00:01, 294.62it/s]warmup run: 472it [00:02, 490.42it/s]warmup run: 580it [00:02, 591.05it/s]warmup run: 482it [00:02, 501.14it/s]warmup run: 675it [00:02, 702.33it/s]warmup run: 394it [00:01, 408.22it/s]warmup run: 652it [00:02, 679.17it/s]warmup run: 671it [00:02, 654.74it/s]warmup run: 393it [00:01, 407.14it/s]warmup run: 572it [00:02, 597.39it/s]warmup run: 677it [00:02, 677.04it/s]warmup run: 578it [00:02, 597.96it/s]warmup run: 776it [00:02, 777.42it/s]warmup run: 493it [00:01, 518.57it/s]warmup run: 750it [00:02, 752.70it/s]warmup run: 763it [00:02, 718.63it/s]warmup run: 491it [00:01, 514.87it/s]warmup run: 670it [00:02, 685.07it/s]warmup run: 771it [00:02, 740.77it/s]warmup run: 676it [00:02, 685.77it/s]warmup run: 875it [00:02, 831.39it/s]warmup run: 595it [00:02, 624.36it/s]warmup run: 848it [00:02, 810.47it/s]warmup run: 854it [00:02, 768.36it/s]warmup run: 590it [00:02, 615.36it/s]warmup run: 767it [00:02, 755.71it/s]warmup run: 866it [00:02, 794.24it/s]warmup run: 774it [00:02, 758.04it/s]warmup run: 973it [00:02, 868.60it/s]warmup run: 696it [00:02, 713.77it/s]warmup run: 945it [00:02, 851.48it/s]warmup run: 945it [00:02, 806.63it/s]warmup run: 689it [00:02, 701.74it/s]warmup run: 863it [00:02, 806.92it/s]warmup run: 964it [00:02, 842.49it/s]warmup run: 872it [00:02, 814.76it/s]warmup run: 1072it [00:02, 901.56it/s]warmup run: 794it [00:02, 772.43it/s]warmup run: 1041it [00:02, 881.60it/s]warmup run: 1036it [00:02, 833.15it/s]warmup run: 788it [00:02, 772.57it/s]warmup run: 959it [00:02, 847.42it/s]warmup run: 1063it [00:02, 883.16it/s]warmup run: 968it [00:02, 853.54it/s]warmup run: 1170it [00:02, 923.06it/s]warmup run: 1137it [00:02, 903.15it/s]warmup run: 891it [00:02, 794.04it/s]warmup run: 1129it [00:02, 858.60it/s]warmup run: 886it [00:02, 826.54it/s]warmup run: 1055it [00:02, 877.84it/s]warmup run: 1161it [00:02, 909.61it/s]warmup run: 1067it [00:02, 890.89it/s]warmup run: 1268it [00:02, 913.45it/s]warmup run: 1235it [00:02, 923.52it/s]warmup run: 1222it [00:02, 876.75it/s]warmup run: 984it [00:02, 867.01it/s]warmup run: 984it [00:02, 813.62it/s]warmup run: 1151it [00:02, 888.87it/s]warmup run: 1166it [00:02, 918.57it/s]warmup run: 1258it [00:02, 907.71it/s]warmup run: 1371it [00:02, 946.54it/s]warmup run: 1333it [00:02, 938.57it/s]warmup run: 1315it [00:02, 889.99it/s]warmup run: 1083it [00:02, 899.96it/s]warmup run: 1082it [00:02, 856.96it/s]warmup run: 1250it [00:02, 915.29it/s]warmup run: 1266it [00:02, 939.61it/s]warmup run: 1358it [00:02, 932.04it/s]warmup run: 1475it [00:03, 971.00it/s]warmup run: 1432it [00:03, 951.26it/s]warmup run: 1182it [00:02, 924.18it/s]warmup run: 1407it [00:03, 893.62it/s]warmup run: 1180it [00:02, 890.05it/s]warmup run: 1348it [00:02, 932.04it/s]warmup run: 1366it [00:02, 956.94it/s]warmup run: 1458it [00:03, 950.82it/s]warmup run: 1577it [00:03, 985.25it/s]warmup run: 1530it [00:03, 954.54it/s]warmup run: 1282it [00:02, 943.75it/s]warmup run: 1502it [00:03, 908.00it/s]warmup run: 1277it [00:02, 911.33it/s]warmup run: 1446it [00:03, 944.34it/s]warmup run: 1468it [00:03, 973.06it/s]warmup run: 1558it [00:03, 964.49it/s]warmup run: 1680it [00:03, 996.54it/s]warmup run: 1627it [00:03, 956.95it/s]warmup run: 1385it [00:02, 967.01it/s]warmup run: 1596it [00:03, 916.37it/s]warmup run: 1378it [00:02, 938.83it/s]warmup run: 1544it [00:03, 953.09it/s]warmup run: 1570it [00:03, 986.67it/s]warmup run: 1658it [00:03, 973.02it/s]warmup run: 1783it [00:03, 1006.29it/s]warmup run: 1724it [00:03, 958.92it/s]warmup run: 1488it [00:02, 984.82it/s]warmup run: 1692it [00:03, 928.66it/s]warmup run: 1482it [00:03, 965.85it/s]warmup run: 1643it [00:03, 961.74it/s]warmup run: 1671it [00:03, 990.50it/s]warmup run: 1758it [00:03, 979.09it/s]warmup run: 1886it [00:03, 1011.40it/s]warmup run: 1821it [00:03, 952.86it/s]warmup run: 1590it [00:03, 994.62it/s]warmup run: 1786it [00:03, 929.33it/s]warmup run: 1585it [00:03, 984.09it/s]warmup run: 1741it [00:03, 963.42it/s]warmup run: 1772it [00:03, 995.14it/s]warmup run: 1860it [00:03, 989.06it/s]warmup run: 1990it [00:03, 1018.00it/s]warmup run: 1918it [00:03, 955.92it/s]warmup run: 1691it [00:03, 992.42it/s]warmup run: 1880it [00:03, 928.86it/s]warmup run: 1688it [00:03, 996.26it/s]warmup run: 1841it [00:03, 972.09it/s]warmup run: 1874it [00:03, 1000.53it/s]warmup run: 1964it [00:03, 1001.47it/s]warmup run: 2110it [00:03, 1069.67it/s]warmup run: 2016it [00:03, 962.80it/s]warmup run: 1792it [00:03, 992.10it/s]warmup run: 1974it [00:03, 928.31it/s]warmup run: 1790it [00:03, 1000.57it/s]warmup run: 1939it [00:03, 967.28it/s]warmup run: 1977it [00:03, 1006.88it/s]warmup run: 2076it [00:03, 1036.71it/s]warmup run: 2232it [00:03, 1114.38it/s]warmup run: 2134it [00:03, 1025.95it/s]warmup run: 2084it [00:03, 977.29it/s]warmup run: 1892it [00:03, 991.38it/s]warmup run: 1891it [00:03, 1002.13it/s]warmup run: 2045it [00:03, 992.76it/s]warmup run: 2093it [00:03, 1051.71it/s]warmup run: 2195it [00:03, 1082.27it/s]warmup run: 2355it [00:03, 1146.36it/s]warmup run: 2253it [00:03, 1072.31it/s]warmup run: 2199it [00:03, 1026.85it/s]warmup run: 1992it [00:03, 990.92it/s]warmup run: 1992it [00:03, 1002.72it/s]warmup run: 2165it [00:03, 1053.17it/s]warmup run: 2213it [00:03, 1095.82it/s]warmup run: 2315it [00:03, 1115.35it/s]warmup run: 2477it [00:03, 1168.06it/s]warmup run: 2372it [00:03, 1106.88it/s]warmup run: 2109it [00:03, 1043.13it/s]warmup run: 2314it [00:03, 1060.84it/s]warmup run: 2109it [00:03, 1051.37it/s]warmup run: 2285it [00:03, 1095.83it/s]warmup run: 2334it [00:03, 1127.64it/s]warmup run: 2435it [00:03, 1138.67it/s]warmup run: 2600it [00:04, 1184.23it/s]warmup run: 2492it [00:04, 1132.57it/s]warmup run: 2228it [00:03, 1091.81it/s]warmup run: 2229it [00:03, 1087.29it/s]warmup run: 2431it [00:04, 1090.81it/s]warmup run: 2405it [00:03, 1125.64it/s]warmup run: 2455it [00:03, 1150.13it/s]warmup run: 2554it [00:04, 1153.51it/s]warmup run: 2722it [00:04, 1192.01it/s]warmup run: 2612it [00:04, 1150.42it/s]warmup run: 2347it [00:03, 1120.90it/s]warmup run: 2348it [00:03, 1117.11it/s]warmup run: 2546it [00:04, 1106.67it/s]warmup run: 2525it [00:04, 1147.09it/s]warmup run: 2576it [00:04, 1166.47it/s]warmup run: 2672it [00:04, 1160.44it/s]warmup run: 2845it [00:04, 1201.02it/s]warmup run: 2731it [00:04, 1159.86it/s]warmup run: 2466it [00:03, 1135.62it/s]warmup run: 2467it [00:03, 1142.80it/s]warmup run: 2660it [00:04, 1114.63it/s]warmup run: 2644it [00:04, 1159.75it/s]warmup run: 2696it [00:04, 1173.98it/s]warmup run: 2791it [00:04, 1168.91it/s]warmup run: 2968it [00:04, 1207.15it/s]warmup run: 2851it [00:04, 1169.81it/s]warmup run: 3000it [00:04, 681.53it/s] warmup run: 2585it [00:03, 1148.98it/s]warmup run: 2587it [00:04, 1157.36it/s]warmup run: 2775it [00:04, 1122.38it/s]warmup run: 2763it [00:04, 1167.30it/s]warmup run: 2815it [00:04, 1178.53it/s]warmup run: 2910it [00:04, 1173.59it/s]warmup run: 2971it [00:04, 1177.81it/s]warmup run: 2707it [00:04, 1169.06it/s]warmup run: 2703it [00:04, 1155.90it/s]warmup run: 2894it [00:04, 1142.44it/s]warmup run: 3000it [00:04, 677.42it/s] warmup run: 2881it [00:04, 1170.75it/s]warmup run: 3000it [00:04, 671.64it/s] warmup run: 2934it [00:04, 1181.52it/s]warmup run: 3000it [00:04, 685.90it/s] warmup run: 3000it [00:04, 660.39it/s] warmup run: 2825it [00:04, 1169.44it/s]warmup run: 2821it [00:04, 1160.74it/s]warmup run: 2999it [00:04, 1169.86it/s]warmup run: 3000it [00:04, 675.45it/s] warmup run: 2939it [00:04, 1165.86it/s]warmup run: 2945it [00:04, 1175.98it/s]warmup run: 3000it [00:04, 686.28it/s] warmup run: 3000it [00:04, 688.83it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1605.05it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.82it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.07it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.08it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.99it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.86it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.24it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.20it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.75it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1610.52it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1609.06it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.53it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.50it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.36it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.03it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1637.70it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.96it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1636.89it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.09it/s]warmup should be done:  16%|█▌        | 484/3000 [00:00<00:01, 1603.54it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1622.19it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1661.89it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1593.48it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1626.52it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1633.10it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1642.57it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1633.68it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1661.25it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1619.35it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1599.07it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1567.35it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1599.09it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1642.08it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1631.04it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1617.63it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1631.25it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1659.99it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1596.44it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1553.66it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1581.05it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1630.63it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1640.07it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1654.26it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1628.00it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1609.05it/s]warmup should be done:  32%|███▏      | 965/3000 [00:00<00:01, 1588.58it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1544.76it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1582.39it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1632.76it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1643.38it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1653.15it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1608.10it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1623.42it/s]warmup should be done:  37%|███▋      | 1124/3000 [00:00<00:01, 1586.58it/s]warmup should be done:  37%|███▋      | 1120/3000 [00:00<00:01, 1568.33it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1603.79it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1634.56it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1645.45it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1653.50it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1607.85it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1623.50it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1584.72it/s]warmup should be done:  43%|████▎     | 1283/3000 [00:00<00:01, 1584.78it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1609.98it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1647.90it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1607.69it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1652.55it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1623.93it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1622.16it/s]warmup should be done:  48%|████▊     | 1442/3000 [00:00<00:00, 1581.58it/s]warmup should be done:  48%|████▊     | 1446/3000 [00:00<00:00, 1595.86it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1612.96it/s]warmup should be done:  54%|█████▍    | 1620/3000 [00:01<00:00, 1608.19it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1649.65it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1653.54it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1632.87it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1622.04it/s]warmup should be done:  53%|█████▎    | 1601/3000 [00:01<00:00, 1579.45it/s]warmup should be done:  54%|█████▎    | 1609/3000 [00:01<00:00, 1604.46it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1616.07it/s]warmup should be done:  59%|█████▉    | 1781/3000 [00:01<00:00, 1607.29it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1650.19it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1653.27it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1641.13it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1622.53it/s]warmup should be done:  59%|█████▊    | 1759/3000 [00:01<00:00, 1578.44it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1616.88it/s]warmup should be done:  59%|█████▉    | 1772/3000 [00:01<00:00, 1610.54it/s]warmup should be done:  65%|██████▍   | 1942/3000 [00:01<00:00, 1606.27it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1650.69it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1652.51it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1622.77it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1647.25it/s]warmup should be done:  64%|██████▍   | 1917/3000 [00:01<00:00, 1578.09it/s]warmup should be done:  65%|██████▌   | 1959/3000 [00:01<00:00, 1617.72it/s]warmup should be done:  64%|██████▍   | 1935/3000 [00:01<00:00, 1614.91it/s]warmup should be done:  70%|███████   | 2103/3000 [00:01<00:00, 1606.05it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1650.37it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1651.48it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1623.18it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1651.50it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1578.48it/s]warmup should be done:  70%|██████▉   | 2099/3000 [00:01<00:00, 1622.12it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1618.70it/s]warmup should be done:  75%|███████▌  | 2264/3000 [00:01<00:00, 1601.88it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1647.49it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1622.87it/s]warmup should be done:  74%|███████▍  | 2234/3000 [00:01<00:00, 1581.83it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1651.88it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1648.37it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1627.23it/s]warmup should be done:  76%|███████▌  | 2284/3000 [00:01<00:00, 1606.54it/s]warmup should be done:  81%|████████  | 2425/3000 [00:01<00:00, 1603.10it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1648.18it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1655.86it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1591.07it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1649.09it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1620.36it/s]warmup should be done:  81%|████████  | 2428/3000 [00:01<00:00, 1632.88it/s]warmup should be done:  82%|████████▏ | 2448/3000 [00:01<00:00, 1616.04it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1603.26it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1648.57it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1622.37it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1597.00it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1649.78it/s]warmup should be done:  86%|████████▋ | 2592/3000 [00:01<00:00, 1633.87it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1621.88it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1604.05it/s]warmup should be done:  94%|█████████▍| 2814/3000 [00:01<00:00, 1650.03it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1661.19it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1623.46it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1602.38it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1651.75it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1637.58it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1630.44it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1610.52it/s]warmup should be done:  99%|█████████▉| 2981/3000 [00:01<00:00, 1654.92it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1666.75it/s]warmup should be done:  96%|█████████▌| 2883/3000 [00:01<00:00, 1610.12it/s]warmup should be done: 100%|█████████▉| 2994/3000 [00:01<00:00, 1656.73it/s]warmup should be done:  98%|█████████▊| 2943/3000 [00:01<00:00, 1629.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.10it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1643.21it/s]warmup should be done:  98%|█████████▊| 2946/3000 [00:01<00:00, 1643.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1625.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1593.89it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.93it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.49it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.49it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.14it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.77it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.81it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.48it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.15it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.60it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1709.22it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1669.24it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1698.35it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1697.88it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.05it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1681.47it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1668.00it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1657.33it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1672.83it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1702.48it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1712.25it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1702.22it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1683.39it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1646.85it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1669.60it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1674.93it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1654.46it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1685.80it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1703.21it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1673.53it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1708.90it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1692.50it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1633.89it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1675.48it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1685.74it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1651.33it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1674.87it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1699.70it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1693.80it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1700.53it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1632.22it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1676.38it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1685.58it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1699.44it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1673.52it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1648.93it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1692.90it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1639.62it/s]warmup should be done:  34%|███▍      | 1029/3000 [00:00<00:01, 1688.70it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1676.61it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1685.87it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1697.90it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1651.61it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1671.26it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1693.27it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1642.77it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1685.39it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1677.67it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1687.87it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1698.36it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1672.91it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1694.76it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1641.03it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1633.69it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1684.29it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1677.96it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1688.25it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1699.15it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1671.93it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1695.81it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1642.70it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1638.10it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1682.45it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1679.02it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1689.37it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1673.06it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1694.96it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1641.57it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1687.11it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1646.00it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1679.99it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1691.04it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1672.63it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1673.89it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1686.01it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1639.98it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1649.00it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1681.01it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1654.14it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1690.48it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1673.86it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1664.78it/s]warmup should be done:  68%|██████▊   | 2044/3000 [00:01<00:00, 1687.63it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1650.42it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1632.88it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1681.14it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1645.88it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1685.93it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1671.99it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1665.09it/s]warmup should be done:  74%|███████▍  | 2214/3000 [00:01<00:00, 1691.03it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1655.73it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1632.64it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1651.60it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1637.88it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1673.46it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1683.69it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1667.86it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1693.76it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1660.92it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1634.31it/s]warmup should be done:  79%|███████▉  | 2376/3000 [00:01<00:00, 1665.74it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1541.75it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1674.51it/s]warmup should be done:  85%|████████▍ | 2540/3000 [00:01<00:00, 1682.93it/s]warmup should be done:  85%|████████▌ | 2556/3000 [00:01<00:00, 1697.59it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1662.48it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1635.93it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1635.05it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1677.32it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1581.66it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1674.13it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1682.84it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1661.39it/s]warmup should be done:  91%|█████████ | 2726/3000 [00:01<00:00, 1689.83it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1644.07it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1649.03it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1681.42it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1611.40it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1672.80it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1680.77it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1659.02it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1650.43it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1671.10it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1660.40it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1671.87it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1626.79it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.20it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1658.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.16it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.71it/s]2022-12-12 01:22:36.935772: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f919b7959e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:36.935828: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.046718: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f919b830130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.046783: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.438904: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f750c02a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.438959: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.919779: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f919b82fde0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.919846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.920325: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f919b82bc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.920390: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.976682: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f758802f630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.976755: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.992747: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f757802fa10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.992825: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:38.997454: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f758802c410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:22:38.997502: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:22:39.155519: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:40.314498: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:40.712829: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:41.164212: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:41.236380: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:41.237094: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:41.237094: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:41.310616: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:22:42.139113: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:43.196733: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:43.569982: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:44.042080: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:44.100448: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:44.111017: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:44.126717: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:22:44.171307: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:23:17.221][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:23:17.221][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.226][ERROR][RK0][main]: coll ps creation done
[HCTR][01:23:17.226][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][01:23:17.262][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:23:17.262][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.272][ERROR][RK0][main]: coll ps creation done
[HCTR][01:23:17.272][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][01:23:17.422][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:23:17.422][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.427][ERROR][RK0][main]: coll ps creation done
[HCTR][01:23:17.427][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][01:23:17.453][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:23:17.454][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.459][ERROR][RK0][main]: coll ps creation done
[HCTR][01:23:17.459][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][01:23:17.507][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:23:17.507][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.515][ERROR][RK0][main]: coll ps creation done
[HCTR][01:23:17.515][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][01:23:17.585][ERROR][RK0][tid #140263498446592]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:23:17.585][ERROR][RK0][tid #140263498446592]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.589][ERROR][RK0][tid #140263498446592]: coll ps creation done
[HCTR][01:23:17.589][ERROR][RK0][tid #140263498446592]: replica 3 waits for coll ps creation barrier
[HCTR][01:23:17.646][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:23:17.646][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.651][ERROR][RK0][main]: coll ps creation done
[HCTR][01:23:17.651][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][01:23:17.726][ERROR][RK0][tid #140263490053888]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:23:17.726][ERROR][RK0][tid #140263490053888]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][01:23:17.731][ERROR][RK0][tid #140263490053888]: coll ps creation done
[HCTR][01:23:17.731][ERROR][RK0][tid #140263490053888]: replica 0 waits for coll ps creation barrier
[HCTR][01:23:17.731][ERROR][RK0][tid #140263490053888]: replica 0 preparing frequency
[HCTR][01:23:18.622][ERROR][RK0][tid #140263490053888]: replica 0 preparing frequency done
[HCTR][01:23:18.656][ERROR][RK0][tid #140263490053888]: replica 0 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][tid #140263490053888]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][tid #140263498446592]: replica 3 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][01:23:18.656][ERROR][RK0][tid #140263490053888]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][main]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][tid #140263498446592]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: Calling build_v2
[HCTR][01:23:18.656][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][tid #140263498446592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:23:18.656][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 01:23:182022-12-12 01:23:18[[.2022-12-12 01:23:18.2022-12-12 01:23:182022-12-12 01:23:182022-12-12 01:23:182022-12-12 01:23:186570142022-12-12 01:23:18.657017....: .657024: 657010657026657025657024E657025: E: : : :  : E EEEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc    : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136::::] :136] 136136136136using concurrent impl MPS136] using concurrent impl MPS] ] ] ] 
] using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS





[2022-12-12 01:23:18.661298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 01:23:18.661339: [E2022-12-12 01:23:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc661343:: 196E]  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:178] v100x8, slow pcie
[2022-12-12 01:23:18.[6613872022-12-12 01:23:18: .E661397 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:[ 1782022-12-12 01:23:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:v100x8, slow pcie661418196
: ] E[assigning 8 to cpu[ 2022-12-12 01:23:18
2022-12-12 01:23:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..:661440661451212: : ] E[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-12 01:23:18 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:23:18:6614932022-12-12 01:23:18:.[178: .1966615272022-12-12 01:23:18] [E661532] : .v100x8, slow pcie2022-12-12 01:23:18 : assigning 8 to cpuE661563
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E
 : 661577:[2022-12-12 01:23:18 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 1782022-12-12 01:23:18./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E] .661624:212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc v100x8, slow pcie[661649: 178] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 01:23:18: E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213:.E [v100x8, slow pcie
] 178661705 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:23:18
remote time is 8.68421] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.
[v100x8, slow pcieE2022-12-12 01:23:18:1786617582022-12-12 01:23:18
[ .196] : .2022-12-12 01:23:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[661802] v100x8, slow pcieE661819.:2022-12-12 01:23:18: assigning 8 to cpu
 : 661847212.E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[: ] 661872 : 2022-12-12 01:23:18Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. 
E:] :[661935/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 213assigning 8 to cpu[1962022-12-12 01:23:18: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
2022-12-12 01:23:18] .E214:remote time is 8.68421.assigning 8 to cpu662003 ] 196
662023
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588] : [E[:
assigning 8 to cpuE2022-12-12 01:23:18 2022-12-12 01:23:18196[
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 2022-12-12 01:23:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc662128:662129assigning 8 to cpu.:: 212: 
662165213E[] E: ]  2022-12-12 01:23:18build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 Eremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
[:662225:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:23:18214[[: 212:.] 2022-12-12 01:23:182022-12-12 01:23:18E] 212662281cpu time is 97.0588.. build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] : 
662306662306/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E: : :
 EE[212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  [2022-12-12 01:23:18] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:23:18.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212::.662417
] 213214662431: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] ] [: E
remote time is 8.68421cpu time is 97.05882022-12-12 01:23:18E 

. [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc662494[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:23:18:: 2022-12-12 01:23:18:.213E.213662527]  662538] : remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421E
:E
 213 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 01:23:18:remote time is 8.68421:2022-12-12 01:23:18.213
214.662597] ] 662603: [remote time is 8.68421cpu time is 97.0588: E2022-12-12 01:23:18

E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[662639/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 01:23:18: :214.E214] 662665 ] cpu time is 97.0588: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588
E:
 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 01:24:35.728579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:24:35.768732: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 01:24:35.768829: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 01:24:35.769838: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 01:24:35.849581: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 01:24:36.240760: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 01:24:36.240853: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 01:24:42.556338: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 01:24:42.556435: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 01:24:44.261268: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 01:24:44.261363: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 01:24:44.264283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 01:24:44.264341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 01:24:44.613897: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 01:24:44.641909: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 01:24:44.643371: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 01:24:44.664010: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 01:24:45.207224: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 01:26:01.252035: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 01:26:01.260200: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 01:26:01.270183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 01:26:01.318116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:26:01.318210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:26:01.318243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:26:01.318282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:26:01.318807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:26:01.318859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.319793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.320468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.333553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 01:26:01.333632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 01:26:01.334065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 01:26:011815.] 334068Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 01:26:01.[3341352022-12-12 01:26:01: .E334140 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :eager alloc mem 381.47 MB205[
] 2022-12-12 01:26:01worker 0 thread 2 initing device 2.
334167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 01:26:01.334234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 01:26:01.334403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 01:26:01.334460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 01:26:01205.] 334450worker 0 thread 4 initing device 4: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 01:26:01.334552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 01:26:01.334603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:26:01.334651[: 2022-12-12 01:26:01E. 334654/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:26:01.334730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.334906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:26:01.334949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.335008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 01:26:01.335022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [3 solved2022-12-12 01:26:01
[.[2022-12-12 01:26:013350672022-12-12 01:26:01.: .335061E335088:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:] :202eager alloc mem 381.47 MB205] 
] 1 solvedworker 0 thread 3 initing device 3

[2022-12-12 01:26:01.335235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 01:26:01.335636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 01:26:012022-12-12 01:26:01..335676335681: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151980] ] Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB

[2022-12-12 01:26:01.335779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.338048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.339105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.339173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.339395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.339521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.340004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.340514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.342025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.343449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.343564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.343734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.343800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.344285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.344843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:26:01.401530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:26:01.406735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:26:01.406870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:26:01.408251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.408862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.409870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:01.409916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.04 MB
[2022-12-12 01:26:01.414308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.415049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:26:01.415091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:26:01.422289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:26:01.422424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[[[:2022-12-12 01:26:012022-12-12 01:26:012022-12-12 01:26:012022-12-12 01:26:011980....] 422464422464422465422464eager alloc mem 1024.00 Bytes: : : : 
EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 01:26:01.427502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:26:01.427595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:26:01.428377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.428633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 01:26:012022-12-12 01:26:01..428710428727: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[[2022-12-12 01:26:012022-12-12 01:26:01..428821428808: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-12 01:26:01.428865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.[4288892022-12-12 01:26:01: .E428912 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1024638
] eager release cuda mem 400000000
[2022-12-12 01:26:01.428969[: 2022-12-12 01:26:01E. 428993/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[2022-12-12 01:26:01.429069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:26:01.429630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.429847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:01.429894: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:26:01.430619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.431225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.431283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 01:26:01.431788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.432389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.432864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.433291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.433401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.433443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.433487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.433834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:01.433878: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 18.90 MB
[2022-12-12 01:26:01.434264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:01.434308: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.03 MB
[2022-12-12 01:26:01.434370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:01.[4344132022-12-12 01:26:01: .W434415 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccE: 43/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :WORKER[0] alloc host memory 19.07 MB638
] eager release cuda mem 625663[
2022-12-12 01:26:01.434457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:26:01
.434482: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.02 MB[
2022-12-12 01:26:01.434507: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 18.97 MB
[2022-12-12 01:26:01.436088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 01:26:01.436174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:26:01.436980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:26:01.437462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:01.438442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:01.438486: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:26:01.444511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.445118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:26:01.445159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:26:01.447590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.447831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.448191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:26:01.448233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.37 GB
[2022-12-12 01:26:01.448440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:26:01.448483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:26:01.448605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.448643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.449196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-12 01:26:012022-12-12 01:26:01..449233449237: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 2.38 GB

[2022-12-12 01:26:01.449307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 01:26:01.449661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.450250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:26:01.450299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:26:01.453284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:26:01.453905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:26:01.453968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[[[2022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:02........219668219669219668219668219669219669219669219669: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 4 init p2p of link 5Device 3 init p2p of link 2Device 2 init p2p of link 1Device 5 init p2p of link 6Device 7 init p2p of link 4Device 1 init p2p of link 7Device 6 init p2p of link 0Device 0 init p2p of link 3







[[[[2022-12-12 01:26:02[2022-12-12 01:26:022022-12-12 01:26:022022-12-12 01:26:02.[2022-12-12 01:26:02[...[2201982022-12-12 01:26:02.2022-12-12 01:26:022201982201982201982022-12-12 01:26:02: .220206.: : : .E220213: 220215EEE220228 : E:    : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu198019801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :1980:] ] ] :eager alloc mem 611.00 KB1980] 1980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB] 


] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 01:26:02.[221345[2022-12-12 01:26:02[: 2022-12-12 01:26:02.[2022-12-12 01:26:02E[[.2213532022-12-12 01:26:02. [2022-12-12 01:26:022022-12-12 01:26:02221356: .221357/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:26:02..: E221363: :.221366221367E : E638221386: :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE ] : EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663E  :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] eager release cuda mem 625663638] :638638eager release cuda mem 625663
] eager release cuda mem 625663638] ] 
eager release cuda mem 625663
] eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 01:26:02.235711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 01:26:02.235869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.235925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[[2022-12-12 01:26:022022-12-12 01:26:02..236064236079: : EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:.:19262361061980] : ] Device 2 init p2p of link 3Eeager alloc mem 611.00 KB
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 01:26:02.236321: E[ 2022-12-12 01:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2363321980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.236445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 01:26:02.236508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 01:26:02.236600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.236664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 01:26:02
.236683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.236862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 01:26:02.236986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 01:26:02
.237006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.237054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 01:26:02.237144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 01:26:02638.] 237160eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.237204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.237419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.237470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.237817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.238003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.250354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 01:26:02.250473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.250710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 01:26:02.250751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 01:26:02.250829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.250866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.250919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 01:26:02.251033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.251070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 01:26:02.251126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 01:26:02.251195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.251264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.251303: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 01:26:02:.638251310] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 01:26:02.251441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.251490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 01:26:02.251612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.251638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.251681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.251821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.252003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.252072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.252231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.252407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.267987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 01:26:02.268106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.268304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 01:26:02.268417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.268760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 01:26:02.[2688912022-12-12 01:26:02: .E268894 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 01:26:02.269072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 01:26:02.269187: [E2022-12-12 01:26:02 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu269186:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 3 init p2p of link 1[
2022-12-12 01:26:02.269240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.269325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.269347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 01:26:02.269463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.269752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.269992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.270118: [E2022-12-12 01:26:02 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc270117:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 0 init p2p of link 2
[2022-12-12 01:26:02[.2022-12-12 01:26:02270262[.: 2022-12-12 01:26:02270260E.:  270287E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926eager release cuda mem 625663:] 
1980Device 6 init p2p of link 7] 
eager alloc mem 611.00 KB
[2022-12-12 01:26:02.270478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:26:02.271187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.271277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:26:02.283361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.284813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.285354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4988610 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14522634 / 100000000 nodes ( 14.52 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.38 GB | 0.951228 secs 
[2022-12-12 01:26:02.285542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4997888 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14513356 / 100000000 nodes ( 14.51 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.39 GB | 0.950898 secs 
[2022-12-12 01:26:02.285673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.286021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4972686 / 100000000 nodes ( 4.97 %~5.00 %) | remote 14538558 / 100000000 nodes ( 14.54 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.38 GB | 0.951079 secs 
[2022-12-12 01:26:02.286577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.286810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.287226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.287797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 01:26:02eager release cuda mem 20400000.
287823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:26:02.289008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4999504 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14511740 / 100000000 nodes ( 14.51 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.39 GB | 0.953236 secs 
[2022-12-12 01:26:02.289152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4987157 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14524087 / 100000000 nodes ( 14.52 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.38 GB | 0.953479 secs 
[2022-12-12 01:26:02.289515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4955739 / 100000000 nodes ( 4.96 %~5.00 %) | remote 14555505 / 100000000 nodes ( 14.56 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.37 GB | 0.954791 secs 
[2022-12-12 01:26:02.290773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4990051 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14521193 / 100000000 nodes ( 14.52 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.38 GB | 0.971928 secs 
[2022-12-12 01:26:02.290868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4998144 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14513100 / 100000000 nodes ( 14.51 %) | cpu 80488756 / 100000000 nodes ( 80.49 %) | 2.39 GB | 0.955813 secs 
[2022-12-12 01:26:02.292721: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.02 GB
[2022-12-12 01:26:03.696827: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.28 GB
[2022-12-12 01:26:03.697447: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.28 GB
[2022-12-12 01:26:03.698000: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.28 GB
[2022-12-12 01:26:05.112106: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.55 GB
[2022-12-12 01:26:05.112797: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.55 GB
[2022-12-12 01:26:05.113819: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.55 GB
[2022-12-12 01:26:06.354191: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.76 GB
[2022-12-12 01:26:06.354327: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.76 GB
[2022-12-12 01:26:06.354769: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.76 GB
[2022-12-12 01:26:07.670694: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.98 GB
[2022-12-12 01:26:07.670852: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.98 GB
[2022-12-12 01:26:07.671183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.98 GB
[2022-12-12 01:26:08.977189: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.44 GB
[2022-12-12 01:26:08.977371: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.44 GB
[2022-12-12 01:26:08.977692: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.44 GB
[2022-12-12 01:26:10.239250: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.63 GB
[2022-12-12 01:26:10.239961: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.63 GB
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][tid #140263490053888]: replica 0 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][tid #140263498446592]: replica 3 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][tid #140263498446592]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][tid #140263490053888]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][main]: init per replica done
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:26:11.073][ERROR][RK0][main]: init per replica done
[HCTR][01:26:11.073][ERROR][RK0][tid #140263498446592]: init per replica done
[HCTR][01:26:11.073][ERROR][RK0][main]: init per replica done
[HCTR][01:26:11.073][ERROR][RK0][main]: init per replica done
[HCTR][01:26:11.073][ERROR][RK0][main]: init per replica done
[HCTR][01:26:11.073][ERROR][RK0][main]: init per replica done
[HCTR][01:26:11.076][ERROR][RK0][tid #140263490053888]: init per replica done
[HCTR][01:26:11.079][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f938bb20000
[HCTR][01:26:11.079][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f938c000000
[HCTR][01:26:11.079][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f938c640000
[HCTR][01:26:11.079][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f938c960000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 3 allocated 3276800 at 0x7f938db20000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 3 allocated 6553600 at 0x7f938e000000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 3 allocated 3276800 at 0x7f938e640000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 3 allocated 6553600 at 0x7f938e960000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 6 allocated 3276800 at 0x7f938bb20000
[HCTR][01:26:11.079][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f938bb20000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 6 allocated 6553600 at 0x7f938c000000
[HCTR][01:26:11.079][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f938c000000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 6 allocated 3276800 at 0x7f938c640000
[HCTR][01:26:11.079][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f938c640000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263498446592]: 6 allocated 6553600 at 0x7f938c960000
[HCTR][01:26:11.079][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f938c960000
[HCTR][01:26:11.079][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f938db20000
[HCTR][01:26:11.079][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f938e000000
[HCTR][01:26:11.079][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f938e640000
[HCTR][01:26:11.079][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f938e960000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263422945024]: 1 allocated 3276800 at 0x7f938db20000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263422945024]: 1 allocated 6553600 at 0x7f938e000000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263422945024]: 1 allocated 3276800 at 0x7f938e640000
[HCTR][01:26:11.079][ERROR][RK0][tid #140263422945024]: 1 allocated 6553600 at 0x7f938e960000
[HCTR][01:26:11.080][ERROR][RK0][tid #140264832231168]: 4 allocated 3276800 at 0x7f9389b20000
[HCTR][01:26:11.080][ERROR][RK0][tid #140264832231168]: 4 allocated 6553600 at 0x7f938a000000
[HCTR][01:26:11.080][ERROR][RK0][tid #140264832231168]: 4 allocated 3276800 at 0x7f938a640000
[HCTR][01:26:11.080][ERROR][RK0][tid #140264832231168]: 4 allocated 6553600 at 0x7f938a960000
[HCTR][01:26:11.082][ERROR][RK0][tid #140263490053888]: 0 allocated 3276800 at 0x7f938e120000
[HCTR][01:26:11.082][ERROR][RK0][tid #140263490053888]: 0 allocated 6553600 at 0x7f938e600000
[HCTR][01:26:11.082][ERROR][RK0][tid #140263490053888]: 0 allocated 3276800 at 0x7f938f30e800
[HCTR][01:26:11.082][ERROR][RK0][tid #140263490053888]: 0 allocated 6553600 at 0x7f938f62e800








