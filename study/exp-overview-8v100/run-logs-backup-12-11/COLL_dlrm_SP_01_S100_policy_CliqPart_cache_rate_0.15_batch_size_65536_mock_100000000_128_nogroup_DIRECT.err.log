2022-12-12 03:28:11.576020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.581956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.593292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.601520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.613190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.617765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.621970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.631632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.661991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.663167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.664151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.665096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.666032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.667077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.668191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.669254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.678043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.679063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.679394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.680830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.681071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.682431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.682729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.683993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.684135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.685583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.685674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.687181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.687268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.687647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.689095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.689415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.689722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.691271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.691926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.693461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.694976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.696686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.696887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.698638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.698902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.699020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.700752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.701496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.701636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.703216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.704307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.704464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.704838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.705632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.706717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.707080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.707594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.708022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.709423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.709840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.710345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.710514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.710947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.712032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.712397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.712889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.712979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.713797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.714573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.714747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.715182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.715627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.715873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.716932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.717760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.718562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.718753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.719149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.719853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.720268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.721180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.721693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.722263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.722391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.723653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.723998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.724046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.724087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.725905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.725947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.725984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.727277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.727458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.727576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.728618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.728999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.729731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.729973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.730617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.730948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.731503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.731916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.732375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.732864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.733304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.734048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.734548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.735049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.735840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.736395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.736422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.736542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.737849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.737862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.737965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.738207: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:11.739303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.739307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.739397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.740719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.740797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.740898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.742055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.742225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.742411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.743764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.744046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.744106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.745123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.745561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.745575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.746565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.747169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.747313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.747338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.748275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.749182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.749373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.750206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.750766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.750931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.751820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.752485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.753176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.753750: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:11.754122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.754712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.755711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.756274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.757262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.757816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.758899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.760188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.760196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.761889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.762729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.762791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.763014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.763062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.765308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.765407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.765583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.765645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.765747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.767873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.767977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.768261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.768441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.768719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.769495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.772599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.778851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.779166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.779403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.779563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.779772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.779816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.783605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.783747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.784285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.784308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.784353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.784587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.820890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.821122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.821537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.821541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.821803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.821964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.825664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.825720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.826184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.826218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.826504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.830785: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:11.830975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.830984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.831431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.831496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.831591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.835732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.836106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.836285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.836439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.839056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.839526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.839546: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:11.839610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.840492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.840702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.843573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.844272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.844338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.845051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.845444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.848013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.848159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.848701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.848856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.849190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.849469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.852716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.852826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.853166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.853878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.854072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.862215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.862556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.863861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.864448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.864725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.868459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.868989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.869549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.870099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.875403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.875846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.876407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.876497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.909276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.909327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.909873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.910081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.914725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.915068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.915274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.916106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.920637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.921714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.922242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.922309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.935104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.936725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.937332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.937437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.940685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.941856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.943394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.943428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.947868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.948763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.949388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.949485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.958015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.959805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.960444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.960517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.963285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.964159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.964885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.965006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.979015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.979758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:11.981557: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:11.990549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.009590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.018107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.018162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.018341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.020518: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:12.029512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.052215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.055659: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:12.055660: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:28:12.056234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.061162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.064619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.064861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.073425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.073469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.080430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:12.080478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.218168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.222550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.223617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.225886: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.225955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:28:13.243532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.244397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.244910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.245492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.246208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.246890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:28:13.282672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.283991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.285554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.286545: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.286610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:28:13.295077: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.295291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.304693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.305926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.306918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.308012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.309019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.310181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:28:13.356535: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:28:13.365621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.366044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.366360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.367097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.367356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.368273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.368643: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.368699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:28:13.369235: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.369293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:28:13.385518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.386621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.386773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.388271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.388428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.390236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.390513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.391957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.392152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.393821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:28:13.393834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.395166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:28:13.404430: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.404618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.406412: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:28:13.441059: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.441265: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.443073: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 03:28:13.451554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.452334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.452860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.453337: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.453388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:28:13.470298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.470929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.471462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.472206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.472722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.473201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:28:13.477720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.478334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.478855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.479638: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.479709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:28:13.481961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.482555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.483085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.483565: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.483616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:28:13.485094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.486351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.488089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.490344: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:28:13.490402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:28:13.490502: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.490683: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.492467: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:28:13.495708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.496831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.497890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.499085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.500091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.500216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.501543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:28:13.501785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.502766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.503905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.504899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.505841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:28:13.507069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.508256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.509375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.511845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.512948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:28:13.514041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:28:13.518600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.518810: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.520633: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 03:28:13.546786: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.546999: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.548772: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 03:28:13.550593: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.550791: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.553584: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 03:28:13.560353: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.560620: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:28:13.562433: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][03:28:14.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.826][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.875][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:28:14.875][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 85it [00:01, 70.74it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 175it [00:01, 159.03it/s]warmup run: 96it [00:01, 80.71it/s]warmup run: 92it [00:01, 75.76it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 98it [00:01, 81.92it/s]warmup run: 268it [00:01, 260.78it/s]warmup run: 192it [00:01, 175.15it/s]warmup run: 185it [00:01, 165.92it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 82it [00:01, 70.08it/s]warmup run: 101it [00:01, 87.89it/s]warmup run: 198it [00:01, 179.98it/s]warmup run: 361it [00:01, 366.92it/s]warmup run: 288it [00:01, 279.37it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 278it [00:01, 265.87it/s]warmup run: 68it [00:01, 58.58it/s]warmup run: 150it [00:01, 136.65it/s]warmup run: 201it [00:01, 188.82it/s]warmup run: 298it [00:01, 288.57it/s]warmup run: 454it [00:02, 470.59it/s]warmup run: 384it [00:01, 387.61it/s]warmup run: 99it [00:01, 87.22it/s]warmup run: 371it [00:01, 370.05it/s]warmup run: 160it [00:01, 153.18it/s]warmup run: 244it [00:01, 244.76it/s]warmup run: 300it [00:01, 298.01it/s]warmup run: 398it [00:01, 401.29it/s]warmup run: 546it [00:02, 563.34it/s]warmup run: 480it [00:02, 493.20it/s]warmup run: 194it [00:01, 183.84it/s]warmup run: 465it [00:02, 473.79it/s]warmup run: 254it [00:01, 259.69it/s]warmup run: 341it [00:01, 361.82it/s]warmup run: 399it [00:01, 410.35it/s]warmup run: 495it [00:02, 505.59it/s]warmup run: 639it [00:02, 646.59it/s]warmup run: 578it [00:02, 593.79it/s]warmup run: 293it [00:01, 294.94it/s]warmup run: 559it [00:02, 569.52it/s]warmup run: 349it [00:01, 371.67it/s]warmup run: 439it [00:02, 477.16it/s]warmup run: 499it [00:01, 521.06it/s]warmup run: 597it [00:02, 611.84it/s]warmup run: 731it [00:02, 712.97it/s]warmup run: 391it [00:01, 407.69it/s]warmup run: 671it [00:02, 654.86it/s]warmup run: 654it [00:02, 655.21it/s]warmup run: 446it [00:02, 484.13it/s]warmup run: 536it [00:02, 580.40it/s]warmup run: 600it [00:02, 624.43it/s]warmup run: 695it [00:02, 696.14it/s]warmup run: 822it [00:02, 758.61it/s]warmup run: 489it [00:01, 516.37it/s]warmup run: 776it [00:02, 749.75it/s]warmup run: 748it [00:02, 725.12it/s]warmup run: 543it [00:02, 586.62it/s]warmup run: 635it [00:02, 674.17it/s]warmup run: 702it [00:02, 715.60it/s]warmup run: 795it [00:02, 770.22it/s]warmup run: 916it [00:02, 806.55it/s]warmup run: 589it [00:02, 618.96it/s]warmup run: 880it [00:02, 823.56it/s]warmup run: 842it [00:02, 780.78it/s]warmup run: 641it [00:02, 677.76it/s]warmup run: 734it [00:02, 751.14it/s]warmup run: 805it [00:02, 792.02it/s]warmup run: 895it [00:02, 828.53it/s]warmup run: 1009it [00:02, 839.67it/s]warmup run: 690it [00:02, 708.77it/s]warmup run: 984it [00:02, 879.60it/s]warmup run: 936it [00:02, 823.04it/s]warmup run: 740it [00:02, 754.65it/s]warmup run: 831it [00:02, 807.96it/s]warmup run: 906it [00:02, 848.66it/s]warmup run: 994it [00:02, 870.27it/s]warmup run: 1102it [00:02, 864.88it/s]warmup run: 790it [00:02, 781.50it/s]warmup run: 1088it [00:02, 923.19it/s]warmup run: 1030it [00:02, 853.05it/s]warmup run: 838it [00:02, 811.63it/s]warmup run: 928it [00:02, 851.71it/s]warmup run: 1007it [00:02, 891.08it/s]warmup run: 1095it [00:02, 908.21it/s]warmup run: 1196it [00:02, 884.72it/s]warmup run: 889it [00:02, 835.86it/s]warmup run: 1189it [00:02, 945.44it/s]warmup run: 1124it [00:02, 876.02it/s]warmup run: 936it [00:02, 855.45it/s]warmup run: 1026it [00:02, 885.86it/s]warmup run: 1109it [00:02, 925.26it/s]warmup run: 1195it [00:02, 933.65it/s]warmup run: 1290it [00:02, 899.76it/s]warmup run: 990it [00:02, 881.91it/s]warmup run: 1290it [00:02, 909.50it/s]warmup run: 1218it [00:02, 892.43it/s]warmup run: 1034it [00:02, 887.80it/s]warmup run: 1123it [00:02, 909.24it/s]warmup run: 1210it [00:02, 944.06it/s]warmup run: 1296it [00:02, 954.68it/s]warmup run: 1385it [00:03, 913.48it/s]warmup run: 1092it [00:02, 919.19it/s]warmup run: 1395it [00:02, 946.51it/s]warmup run: 1312it [00:02, 904.15it/s]warmup run: 1131it [00:02, 910.40it/s]warmup run: 1222it [00:02, 931.66it/s]warmup run: 1311it [00:02, 962.70it/s]warmup run: 1397it [00:02, 969.56it/s]warmup run: 1479it [00:03, 904.15it/s]warmup run: 1192it [00:02, 939.06it/s]warmup run: 1499it [00:03, 972.39it/s]warmup run: 1407it [00:03, 916.41it/s]warmup run: 1229it [00:02, 928.17it/s]warmup run: 1320it [00:02, 944.13it/s]warmup run: 1412it [00:02, 973.31it/s]warmup run: 1498it [00:03, 981.32it/s]warmup run: 1571it [00:03, 907.32it/s]warmup run: 1292it [00:02, 949.47it/s]warmup run: 1603it [00:03, 989.77it/s]warmup run: 1502it [00:03, 923.54it/s]warmup run: 1327it [00:02, 941.90it/s]warmup run: 1418it [00:03, 950.59it/s]warmup run: 1513it [00:03, 960.88it/s]warmup run: 1599it [00:03, 988.40it/s]warmup run: 1666it [00:03, 918.27it/s]warmup run: 1391it [00:02, 958.87it/s]warmup run: 1708it [00:03, 1004.65it/s]warmup run: 1597it [00:03, 930.67it/s]warmup run: 1425it [00:03, 951.93it/s]warmup run: 1516it [00:03, 956.49it/s]warmup run: 1700it [00:03, 985.50it/s]warmup run: 1612it [00:03, 922.20it/s]warmup run: 1761it [00:03, 926.68it/s]warmup run: 1490it [00:02, 967.94it/s]warmup run: 1812it [00:03, 1014.41it/s]warmup run: 1692it [00:03, 935.11it/s]warmup run: 1523it [00:03, 955.54it/s]warmup run: 1614it [00:03, 961.64it/s]warmup run: 1801it [00:03, 990.91it/s]warmup run: 1711it [00:03, 940.97it/s]warmup run: 1856it [00:03, 932.10it/s]warmup run: 1590it [00:03, 974.95it/s]warmup run: 1916it [00:03, 1021.49it/s]warmup run: 1787it [00:03, 936.44it/s]warmup run: 1620it [00:03, 956.83it/s]warmup run: 1712it [00:03, 963.91it/s]warmup run: 1903it [00:03, 998.96it/s]warmup run: 1811it [00:03, 957.90it/s]warmup run: 1952it [00:03, 938.89it/s]warmup run: 1690it [00:03, 979.42it/s]warmup run: 2024it [00:03, 1036.73it/s]warmup run: 1882it [00:03, 936.02it/s]warmup run: 1717it [00:03, 959.50it/s]warmup run: 1810it [00:03, 965.83it/s]warmup run: 2004it [00:03, 999.69it/s]warmup run: 1908it [00:03, 957.99it/s]warmup run: 2052it [00:03, 955.91it/s]warmup run: 1791it [00:03, 985.49it/s]warmup run: 2147it [00:03, 1092.35it/s]warmup run: 1976it [00:03, 936.19it/s]warmup run: 1817it [00:03, 969.56it/s]warmup run: 1908it [00:03, 969.76it/s]warmup run: 2121it [00:03, 1049.34it/s]warmup run: 2011it [00:03, 978.31it/s]warmup run: 2173it [00:03, 1029.83it/s]warmup run: 1893it [00:03, 993.22it/s]warmup run: 2270it [00:03, 1131.27it/s]warmup run: 2083it [00:03, 975.16it/s]warmup run: 1915it [00:03, 968.71it/s]warmup run: 2008it [00:03, 978.08it/s]warmup run: 2239it [00:03, 1086.24it/s]warmup run: 2131it [00:03, 1042.60it/s]warmup run: 2294it [00:03, 1083.36it/s]warmup run: 1995it [00:03, 998.88it/s]warmup run: 2393it [00:03, 1158.75it/s]warmup run: 2199it [00:03, 1030.01it/s]warmup run: 2017it [00:03, 982.86it/s]warmup run: 2128it [00:03, 1043.74it/s]warmup run: 2358it [00:03, 1115.68it/s]warmup run: 2252it [00:03, 1089.71it/s]warmup run: 2415it [00:04, 1120.87it/s]warmup run: 2113it [00:03, 1050.78it/s]warmup run: 2516it [00:03, 1177.56it/s]warmup run: 2317it [00:04, 1074.10it/s]warmup run: 2136it [00:03, 1043.59it/s]warmup run: 2248it [00:03, 1089.89it/s]warmup run: 2477it [00:03, 1135.58it/s]warmup run: 2373it [00:03, 1125.14it/s]warmup run: 2536it [00:04, 1145.37it/s]warmup run: 2232it [00:03, 1091.37it/s]warmup run: 2638it [00:04, 1188.45it/s]warmup run: 2432it [00:04, 1095.28it/s]warmup run: 2255it [00:03, 1087.13it/s]warmup run: 2362it [00:03, 1104.83it/s]warmup run: 2596it [00:04, 1150.68it/s]warmup run: 2490it [00:03, 1137.22it/s]warmup run: 2657it [00:04, 1162.85it/s]warmup run: 2351it [00:03, 1120.55it/s]warmup run: 2761it [00:04, 1199.42it/s]warmup run: 2551it [00:04, 1122.62it/s]warmup run: 2375it [00:03, 1118.26it/s]warmup run: 2479it [00:04, 1123.66it/s]warmup run: 2714it [00:04, 1157.26it/s]warmup run: 2609it [00:04, 1151.77it/s]warmup run: 2778it [00:04, 1176.34it/s]warmup run: 2470it [00:03, 1140.93it/s]warmup run: 2884it [00:04, 1206.66it/s]warmup run: 2670it [00:04, 1141.21it/s]warmup run: 2494it [00:04, 1139.40it/s]warmup run: 2597it [00:04, 1138.21it/s]warmup run: 2833it [00:04, 1166.37it/s]warmup run: 2730it [00:04, 1167.27it/s]warmup run: 2899it [00:04, 1183.67it/s]warmup run: 2589it [00:03, 1154.73it/s]warmup run: 3000it [00:04, 682.00it/s] warmup run: 2787it [00:04, 1147.85it/s]warmup run: 2613it [00:04, 1154.18it/s]warmup run: 2713it [00:04, 1143.65it/s]warmup run: 2952it [00:04, 1170.73it/s]warmup run: 3000it [00:04, 654.84it/s] warmup run: 2849it [00:04, 1171.81it/s]warmup run: 2707it [00:04, 1161.08it/s]warmup run: 3000it [00:04, 678.01it/s] warmup run: 2905it [00:04, 1156.52it/s]warmup run: 2732it [00:04, 1162.61it/s]warmup run: 2831it [00:04, 1153.62it/s]warmup run: 2969it [00:04, 1179.53it/s]warmup run: 2827it [00:04, 1170.94it/s]warmup run: 3000it [00:04, 687.07it/s] warmup run: 3000it [00:04, 653.75it/s] warmup run: 2851it [00:04, 1167.89it/s]warmup run: 2951it [00:04, 1167.45it/s]warmup run: 2946it [00:04, 1173.67it/s]warmup run: 3000it [00:04, 669.19it/s] warmup run: 2968it [00:04, 1164.89it/s]warmup run: 3000it [00:04, 690.67it/s] warmup run: 3000it [00:04, 672.50it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1618.89it/s]warmup should be done:   5%|         | 151/3000 [00:00<00:01, 1508.07it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1612.94it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.04it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1605.97it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.25it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.76it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.47it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1626.56it/s]warmup should be done:  11%|         | 316/3000 [00:00<00:01, 1588.88it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1617.46it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1624.36it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1624.67it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1655.39it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1609.97it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1639.06it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1657.17it/s]warmup should be done:  16%|        | 479/3000 [00:00<00:01, 1603.67it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1628.89it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1623.52it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1607.65it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1610.62it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1634.44it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1617.52it/s]warmup should be done:  21%|       | 641/3000 [00:00<00:01, 1609.97it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1657.14it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1632.28it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1623.12it/s]warmup should be done:  22%|       | 646/3000 [00:00<00:01, 1607.22it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1632.92it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1607.60it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1613.65it/s]warmup should be done:  27%|       | 804/3000 [00:00<00:01, 1615.41it/s]warmup should be done:  27%|       | 807/3000 [00:00<00:01, 1607.91it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1655.14it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1623.18it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1628.60it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1619.43it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1605.94it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1630.05it/s]warmup should be done:  32%|      | 966/3000 [00:00<00:01, 1614.17it/s]warmup should be done:  32%|      | 968/3000 [00:00<00:01, 1601.90it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1648.96it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1617.73it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1617.36it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1632.21it/s]warmup should be done:  33%|      | 981/3000 [00:00<00:01, 1619.57it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1597.98it/s]warmup should be done:  38%|      | 1128/3000 [00:00<00:01, 1609.69it/s]warmup should be done:  38%|      | 1129/3000 [00:00<00:01, 1602.38it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1647.38it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1617.53it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1640.14it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1618.66it/s]warmup should be done:  38%|      | 1143/3000 [00:00<00:01, 1618.37it/s]warmup should be done:  38%|      | 1130/3000 [00:00<00:01, 1597.97it/s]warmup should be done:  43%|     | 1289/3000 [00:00<00:01, 1609.17it/s]warmup should be done:  43%|     | 1290/3000 [00:00<00:01, 1602.19it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1619.82it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1619.73it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:01, 1637.69it/s]warmup should be done:  43%|     | 1290/3000 [00:00<00:01, 1597.01it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1634.61it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1604.46it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1620.62it/s]warmup should be done:  48%|     | 1451/3000 [00:00<00:00, 1601.60it/s]warmup should be done:  48%|     | 1450/3000 [00:00<00:00, 1602.81it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1620.33it/s]warmup should be done:  48%|     | 1450/3000 [00:00<00:00, 1595.97it/s]warmup should be done:  49%|     | 1481/3000 [00:00<00:00, 1631.30it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1602.38it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1624.72it/s]warmup should be done:  54%|    | 1627/3000 [00:01<00:00, 1622.34it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1620.44it/s]warmup should be done:  54%|    | 1611/3000 [00:01<00:00, 1599.97it/s]warmup should be done:  54%|    | 1612/3000 [00:01<00:00, 1596.31it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1595.48it/s]warmup should be done:  55%|    | 1645/3000 [00:01<00:00, 1625.52it/s]warmup should be done:  54%|    | 1627/3000 [00:01<00:00, 1600.78it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1618.81it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1623.03it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1621.26it/s]warmup should be done:  59%|    | 1773/3000 [00:01<00:00, 1603.90it/s]warmup should be done:  59%|    | 1773/3000 [00:01<00:00, 1599.03it/s]warmup should be done:  59%|    | 1770/3000 [00:01<00:00, 1595.63it/s]warmup should be done:  60%|    | 1788/3000 [00:01<00:00, 1599.42it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1603.74it/s]warmup should be done:  60%|    | 1808/3000 [00:01<00:00, 1547.31it/s]warmup should be done:  65%|   | 1953/3000 [00:01<00:00, 1623.11it/s]warmup should be done:  65%|   | 1936/3000 [00:01<00:00, 1609.85it/s]warmup should be done:  64%|   | 1930/3000 [00:01<00:00, 1596.50it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1621.07it/s]warmup should be done:  64%|   | 1934/3000 [00:01<00:00, 1600.40it/s]warmup should be done:  65%|   | 1948/3000 [00:01<00:00, 1599.03it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1605.00it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1552.92it/s]warmup should be done:  71%|   | 2116/3000 [00:01<00:00, 1623.86it/s]warmup should be done:  70%|   | 2090/3000 [00:01<00:00, 1596.13it/s]warmup should be done:  70%|   | 2100/3000 [00:01<00:00, 1615.95it/s]warmup should be done:  70%|   | 2095/3000 [00:01<00:00, 1601.57it/s]warmup should be done:  71%|   | 2119/3000 [00:01<00:00, 1620.13it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1599.25it/s]warmup should be done:  71%|  | 2138/3000 [00:01<00:00, 1607.15it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1582.67it/s]warmup should be done:  76%|  | 2279/3000 [00:01<00:00, 1621.14it/s]warmup should be done:  75%|  | 2264/3000 [00:01<00:00, 1620.17it/s]warmup should be done:  75%|  | 2250/3000 [00:01<00:00, 1591.84it/s]warmup should be done:  75%|  | 2256/3000 [00:01<00:00, 1599.03it/s]warmup should be done:  76%|  | 2282/3000 [00:01<00:00, 1617.39it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1595.01it/s]warmup should be done:  77%|  | 2299/3000 [00:01<00:00, 1606.93it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1601.88it/s]warmup should be done:  81%| | 2442/3000 [00:01<00:00, 1619.54it/s]warmup should be done:  81%|  | 2427/3000 [00:01<00:00, 1622.81it/s]warmup should be done:  81%|  | 2417/3000 [00:01<00:00, 1600.94it/s]warmup should be done:  81%| | 2444/3000 [00:01<00:00, 1618.13it/s]warmup should be done:  80%|  | 2410/3000 [00:01<00:00, 1591.53it/s]warmup should be done:  81%|  | 2428/3000 [00:01<00:00, 1595.96it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1606.96it/s]warmup should be done:  82%| | 2462/3000 [00:01<00:00, 1618.02it/s]warmup should be done:  87%| | 2604/3000 [00:01<00:00, 1619.48it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1627.24it/s]warmup should be done:  87%| | 2606/3000 [00:01<00:00, 1617.43it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1592.70it/s]warmup should be done:  86%| | 2578/3000 [00:01<00:00, 1601.19it/s]warmup should be done:  86%| | 2588/3000 [00:01<00:00, 1596.18it/s]warmup should be done:  87%| | 2622/3000 [00:01<00:00, 1608.60it/s]warmup should be done:  88%| | 2628/3000 [00:01<00:00, 1629.18it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1619.80it/s]warmup should be done:  92%|| 2755/3000 [00:01<00:00, 1628.29it/s]warmup should be done:  92%|| 2768/3000 [00:01<00:00, 1617.48it/s]warmup should be done:  91%| | 2730/3000 [00:01<00:00, 1594.08it/s]warmup should be done:  91%|| 2739/3000 [00:01<00:00, 1601.72it/s]warmup should be done:  92%|| 2749/3000 [00:01<00:00, 1597.96it/s]warmup should be done:  93%|| 2783/3000 [00:01<00:00, 1608.96it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1637.31it/s]warmup should be done:  98%|| 2931/3000 [00:01<00:00, 1624.61it/s]warmup should be done:  97%|| 2920/3000 [00:01<00:00, 1634.68it/s]warmup should be done:  98%|| 2932/3000 [00:01<00:00, 1622.88it/s]warmup should be done:  96%|| 2892/3000 [00:01<00:00, 1600.62it/s]warmup should be done:  97%|| 2901/3000 [00:01<00:00, 1606.23it/s]warmup should be done:  97%|| 2911/3000 [00:01<00:00, 1603.23it/s]warmup should be done:  98%|| 2946/3000 [00:01<00:00, 1614.58it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1645.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1622.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1615.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1606.84it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1602.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1598.82it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.70it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1676.94it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1656.18it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.23it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1665.08it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1644.30it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1701.52it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.74it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1671.63it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.55it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.49it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.23it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1663.26it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.14it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1651.25it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1698.01it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1675.17it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1653.24it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1683.35it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1658.88it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1647.98it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1656.89it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1661.91it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1698.72it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1674.01it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1652.71it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1686.96it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1664.64it/s]warmup should be done:  23%|       | 682/3000 [00:00<00:01, 1698.81it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1660.78it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1646.30it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1663.34it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1646.15it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1689.05it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1670.10it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1660.93it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1650.49it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1665.85it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1693.46it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1648.29it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1662.77it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1689.30it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1651.55it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1660.65it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1669.22it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1664.12it/s]warmup should be done:  34%|      | 1022/3000 [00:00<00:01, 1683.55it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1648.76it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1653.80it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1690.50it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1660.66it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1660.28it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1665.00it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1661.58it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1674.00it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1645.09it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1691.99it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:01, 1662.22it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1661.58it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1650.06it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:01, 1662.89it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1661.76it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1672.56it/s]warmup should be done:  50%|     | 1487/3000 [00:00<00:00, 1646.48it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1692.75it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1662.80it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1663.35it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1651.57it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1663.63it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1659.44it/s]warmup should be done:  51%|     | 1527/3000 [00:00<00:00, 1670.00it/s]warmup should be done:  55%|    | 1654/3000 [00:01<00:00, 1651.13it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1693.74it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1664.95it/s]warmup should be done:  56%|    | 1670/3000 [00:01<00:00, 1664.07it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1653.98it/s]warmup should be done:  56%|    | 1671/3000 [00:01<00:00, 1665.96it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1659.08it/s]warmup should be done:  56%|    | 1695/3000 [00:01<00:00, 1669.82it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1662.14it/s]warmup should be done:  61%|    | 1835/3000 [00:01<00:00, 1664.88it/s]warmup should be done:  62%|   | 1867/3000 [00:01<00:00, 1695.96it/s]warmup should be done:  61%|   | 1838/3000 [00:01<00:00, 1666.63it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1656.31it/s]warmup should be done:  61%|   | 1839/3000 [00:01<00:00, 1668.43it/s]warmup should be done:  61%|   | 1840/3000 [00:01<00:00, 1657.41it/s]warmup should be done:  62%|   | 1863/3000 [00:01<00:00, 1671.61it/s]warmup should be done:  66%|   | 1991/3000 [00:01<00:00, 1665.84it/s]warmup should be done:  68%|   | 2037/3000 [00:01<00:00, 1696.33it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1665.49it/s]warmup should be done:  67%|   | 2002/3000 [00:01<00:00, 1663.13it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1653.26it/s]warmup should be done:  67%|   | 2006/3000 [00:01<00:00, 1667.11it/s]warmup should be done:  67%|   | 2006/3000 [00:01<00:00, 1654.04it/s]warmup should be done:  68%|   | 2031/3000 [00:01<00:00, 1667.03it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1663.98it/s]warmup should be done:  74%|  | 2207/3000 [00:01<00:00, 1695.36it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1664.97it/s]warmup should be done:  72%|  | 2173/3000 [00:01<00:00, 1667.23it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1653.78it/s]warmup should be done:  72%|  | 2169/3000 [00:01<00:00, 1658.46it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1651.73it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1665.64it/s]warmup should be done:  78%|  | 2325/3000 [00:01<00:00, 1665.06it/s]warmup should be done:  79%|  | 2377/3000 [00:01<00:00, 1695.87it/s]warmup should be done:  78%|  | 2340/3000 [00:01<00:00, 1666.52it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1669.57it/s]warmup should be done:  78%|  | 2325/3000 [00:01<00:00, 1656.26it/s]warmup should be done:  78%|  | 2335/3000 [00:01<00:00, 1657.25it/s]warmup should be done:  78%|  | 2338/3000 [00:01<00:00, 1651.94it/s]warmup should be done:  79%|  | 2365/3000 [00:01<00:00, 1666.81it/s]warmup should be done:  85%| | 2547/3000 [00:01<00:00, 1696.91it/s]warmup should be done:  83%| | 2492/3000 [00:01<00:00, 1665.39it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1667.37it/s]warmup should be done:  84%| | 2508/3000 [00:01<00:00, 1667.62it/s]warmup should be done:  83%| | 2491/3000 [00:01<00:00, 1656.65it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1658.31it/s]warmup should be done:  83%| | 2504/3000 [00:01<00:00, 1652.61it/s]warmup should be done:  84%| | 2533/3000 [00:01<00:00, 1668.87it/s]warmup should be done:  91%| | 2718/3000 [00:01<00:00, 1697.93it/s]warmup should be done:  89%| | 2674/3000 [00:01<00:00, 1666.18it/s]warmup should be done:  89%| | 2659/3000 [00:01<00:00, 1658.93it/s]warmup should be done:  89%| | 2657/3000 [00:01<00:00, 1654.69it/s]warmup should be done:  89%| | 2675/3000 [00:01<00:00, 1662.50it/s]warmup should be done:  89%| | 2669/3000 [00:01<00:00, 1659.32it/s]warmup should be done:  89%| | 2670/3000 [00:01<00:00, 1650.67it/s]warmup should be done:  90%| | 2701/3000 [00:01<00:00, 1670.40it/s]warmup should be done:  96%|| 2888/3000 [00:01<00:00, 1697.02it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1665.82it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1654.76it/s]warmup should be done:  94%|| 2835/3000 [00:01<00:00, 1659.07it/s]warmup should be done:  95%|| 2842/3000 [00:01<00:00, 1656.45it/s]warmup should be done:  94%|| 2825/3000 [00:01<00:00, 1645.01it/s]warmup should be done:  95%|| 2836/3000 [00:01<00:00, 1645.64it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1680.46it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1693.00it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.85it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1665.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.39it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1657.49it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.85it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1638.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1653.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.35it/s]2022-12-12 03:29:50.403001: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f312802d9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:50.403063: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.438812: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4e8f95c3c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.438876: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.511042: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f319002a460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.511116: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.560221: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4e83f921c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.560297: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.756135: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4e97830350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.756201: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.762408: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f30c0031670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.762477: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.842234: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3148029f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.842299: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:51.842478: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f311c02da10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:29:51.842527: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:29:52.668167: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:53.745121: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:53.832276: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:53.901125: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:54.070819: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:54.072156: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:54.140181: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:54.203508: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:29:55.612302: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:56.651253: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:56.751564: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:56.854458: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:56.924641: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:56.973357: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:57.026454: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:29:57.106029: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:30:23.010][ERROR][RK0][tid #139975668528896]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:30:23.010][ERROR][RK0][tid #139975668528896]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.010][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:30:23.010][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.019][ERROR][RK0][main]: coll ps creation done
[HCTR][03:30:23.019][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][03:30:23.019][ERROR][RK0][tid #139975668528896]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:30:23.019][ERROR][RK0][tid #139975668528896]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.023][ERROR][RK0][tid #139975668528896]: coll ps creation done
[HCTR][03:30:23.023][ERROR][RK0][tid #139975668528896]: replica 2 waits for coll ps creation barrier
[HCTR][03:30:23.024][ERROR][RK0][tid #139975668528896]: coll ps creation done
[HCTR][03:30:23.024][ERROR][RK0][tid #139975668528896]: replica 5 waits for coll ps creation barrier
[HCTR][03:30:23.027][ERROR][RK0][tid #139975660136192]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:30:23.027][ERROR][RK0][tid #139975660136192]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.035][ERROR][RK0][tid #139975660136192]: coll ps creation done
[HCTR][03:30:23.035][ERROR][RK0][tid #139975660136192]: replica 6 waits for coll ps creation barrier
[HCTR][03:30:23.067][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:30:23.067][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.072][ERROR][RK0][main]: coll ps creation done
[HCTR][03:30:23.072][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][03:30:23.095][ERROR][RK0][tid #139975601420032]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:30:23.095][ERROR][RK0][tid #139975601420032]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.102][ERROR][RK0][tid #139975601420032]: coll ps creation done
[HCTR][03:30:23.102][ERROR][RK0][tid #139975601420032]: replica 3 waits for coll ps creation barrier
[HCTR][03:30:23.135][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:30:23.135][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.140][ERROR][RK0][main]: coll ps creation done
[HCTR][03:30:23.140][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][03:30:23.179][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:30:23.179][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:30:23.183][ERROR][RK0][main]: coll ps creation done
[HCTR][03:30:23.183][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][03:30:23.183][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][03:30:24.061][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][03:30:24.100][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][tid #139975601420032]: replica 3 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][tid #139975660136192]: replica 6 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][tid #139975668528896]: replica 2 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][tid #139975668528896]: replica 5 calling init per replica
[HCTR][03:30:24.100][ERROR][RK0][main]: Calling build_v2
[HCTR][03:30:24.100][ERROR][RK0][main]: Calling build_v2
[HCTR][03:30:24.100][ERROR][RK0][main]: Calling build_v2
[HCTR][03:30:24.100][ERROR][RK0][tid #139975601420032]: Calling build_v2
[HCTR][03:30:24.100][ERROR][RK0][tid #139975660136192]: Calling build_v2
[HCTR][03:30:24.100][ERROR][RK0][tid #139975668528896]: Calling build_v2
[HCTR][03:30:24.101][ERROR][RK0][main]: Calling build_v2
[HCTR][03:30:24.101][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][tid #139975668528896]: Calling build_v2
[HCTR][03:30:24.101][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][tid #139975601420032]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][tid #139975660136192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][tid #139975668528896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:30:24.101][ERROR][RK0][tid #139975668528896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 03:30:24.105033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:178] v100x8, slow pcie2022-12-12 03:30:24
.105070: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:30:24:[.178105119] : v100x8, slow pcieE
 2022-12-12 03:30:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:105115[196: 2022-12-12 03:30:24] [E.assigning 0 to cpu 105164
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 03:30:24:E.178 105163] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pcie:[E
196[2022-12-12 03:30:24 ] .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu1052162022-12-12 03:30:242022-12-12 03:30:24:
: ..178E105237105209]  : : v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE
: [ 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 03:30:24:2022-12-12 03:30:24:[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[.196.1782022-12-12 03:30:24
105263] 105295] 2022-12-12 03:30:24.: assigning 0 to cpu: [v100x8, slow pcie.[105313E
E
1053102022-12-12 03:30:24: [  : 2022-12-12 03:30:24.E2022-12-12 03:30:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.105372 .::2022-12-12 03:30:24 105353: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc105422178196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E:: ] ] 105441:E 212Ev100x8, slow pcieassigning 0 to cpu: 178 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  

E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [[v100x8, slow pcie:213
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:30:242022-12-12 03:30:24[
178] 196:..2022-12-12 03:30:24] remote time is 8.68421] 212[105590105607.v100x8, slow pcie
assigning 0 to cpu] 2022-12-12 03:30:24[: : 105635

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-12 03:30:24EE: 
[105652.  E2022-12-12 03:30:24: [105692/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .[E2022-12-12 03:30:24: ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1057162022-12-12 03:30:24 .E196212:: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc105743 ] ] 213E105756:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]  : 196E:

remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE]  214
: [assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 03:30:24
:cpu time is 97.0588] :2022-12-12 03:30:24.[213
assigning 0 to cpu212.1059112022-12-12 03:30:24] 
] 105926: .[remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E1059482022-12-12 03:30:24

E : . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[E[105985/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 03:30:242022-12-12 03:30:24 2022-12-12 03:30:24: :213../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E214] 106028106031:106034 ] remote time is 8.68421: : 212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588
EE] E:
 [ build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:30:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] :.:213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214106161[212] 
] : 2022-12-12 03:30:24] remote time is 8.68421cpu time is 97.0588E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8

[ 106233
[2022-12-12 03:30:24/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 03:30:24.[:E.1062842022-12-12 03:30:24214 106304: .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E106327cpu time is 97.0588:E : 
213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214] :] [remote time is 8.68421213cpu time is 97.05882022-12-12 03:30:24
] 
.remote time is 8.68421106478[
: 2022-12-12 03:30:24E. [106514/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:30:24: :.E214106534 ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588E:
 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 03:31:42.910460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:31:42.950573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 03:31:43. 68398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:31:43. 68462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:31:43. 68494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:31:43. 68524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:31:43. 69032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:31:43. 69084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 70079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 70884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 83780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 03:31:43. 83844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 03:31:43. 83910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 03:31:43. 83993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 03:31:43. 84052[: 2022-12-12 03:31:43E.  84067/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc6 solved:
202] 5 solved[
2022-12-12 03:31:43. 84170: E[ 2022-12-12 03:31:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.: 84187205: ] Eworker 0 thread 6 initing device 6 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 03:31:43. 84298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:31:43. 84364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 84463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:31:43. 84521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 84646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:31:43. 84672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-12 03:31:43
. 84695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 84733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 85355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 03:31:43. 85424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 03:31:43. 86008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:31:43. 86080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 93453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 93502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43.[[ 935652022-12-12 03:31:432022-12-12 03:31:43: ..E 93567 93567 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] ::eager alloc mem 381.47 MB202202
] ] 2 solved4 solved

[[[2022-12-12 03:31:432022-12-12 03:31:432022-12-12 03:31:43... 93674 93672 93674: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:::2051980205] ] ] worker 0 thread 4 initing device 4eager alloc mem 381.47 MB[worker 0 thread 2 initing device 2

2022-12-12 03:31:43
. 93779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 03:31:432022-12-12 03:31:43.. 94341 94341: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 03:31:432022-12-12 03:31:43.. 94416 94416: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 03:31:43. 97744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 97937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 98044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 98096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 98173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43. 98694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 03:31:43. 98747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43.102698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43.102743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:31:43.148524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 03:31:43.154570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:31:43.154705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:31:43.155662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.156500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.157579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43.157628: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[[[2022-12-12 03:31:43[[2022-12-12 03:31:432022-12-12 03:31:43.2022-12-12 03:31:432022-12-12 03:31:43..181950..181980181980: 181996181993: : E: : EE EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980::19801980] 19801980] ] eager alloc mem 5.00 Bytes] eager alloc mem 5.00 Bytes] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

eager alloc mem 5.00 Bytes


[[2022-12-12 03:31:432022-12-12 03:31:43..185071185071: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 03:31:43.188437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:31:43[.2022-12-12 03:31:43188509.: 188536E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5] 
eager release cuda mem 400000000
[2022-12-12 03:31:43.[1885902022-12-12 03:31:43: .E188614 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 5638
] eager release cuda mem 400000000
[2022-12-12 03:31:43.188660: [E2022-12-12 03:31:43 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc188687:: 638E]  eager release cuda mem 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[2022-12-12 03:31:43.188750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:31:43638.] 188752eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:31:43.188838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:31:43.189884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.190278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:31:43.190361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:31:43638.] 190362eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:31:43.190462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:31:43.190487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.191145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.191758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.192559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.193884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.194399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:31:43.194858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.195229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.195338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.195380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.195601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.195844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.195881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43.195928: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-12 03:31:4343.] 195931WORKER[0] alloc host memory 57.22 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:43.196249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43.196295: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:31:43.196357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43[.2022-12-12 03:31:43196396.: 196402E:  W/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc638:] 43eager release cuda mem 625663] 
WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:31:43.196462: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:31:43.196626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43.196673: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:31:43.196893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43.196943: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:31:43.196994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:43.197040: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:31:43.197611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:31:43.198250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:31:43.198298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:31:43.233981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:31:43.234070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 03:31:43eager alloc mem 25.25 KB.
234099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:31:43.234382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:31:43.234610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:31:43.234654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:31:43.[2347032022-12-12 03:31:43: .E234712 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 25855638
] eager release cuda mem 25855
[2022-12-12 03:31:43.[2347692022-12-12 03:31:43: .E234774 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 7.16 GB1980
] eager alloc mem 7.16 GB
[[2022-12-12 03:31:432022-12-12 03:31:43..234985235002: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[2022-12-12 03:31:43.235067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:31:43.235112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:31:43.235389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:31:43.235614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:31:43.235658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:31:43.235740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:31:43.235782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:31:43.235999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:31:43.236041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[[[[[[[[2022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:45........990433990431990431990431990432990432990432990432: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 0 init p2p of link 3Device 4 init p2p of link 5Device 2 init p2p of link 1Device 6 init p2p of link 0Device 1 init p2p of link 7Device 3 init p2p of link 2Device 5 init p2p of link 6Device 7 init p2p of link 4







[[[[[2022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:45[[[.....2022-12-12 03:31:452022-12-12 03:31:452022-12-12 03:31:45990983990980990980990980990980...: : : : : 990993990993990999EEEEE: : :      EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   :::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980198019801980:::] ] ] ] ] 198019801980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] ] 




eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 03:31:45.[9920372022-12-12 03:31:45: .E992046 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 03:31:45[.[2022-12-12 03:31:45[992146[2022-12-12 03:31:45[.2022-12-12 03:31:45: 2022-12-12 03:31:45.2022-12-12 03:31:45992151.E.992152.: 992160 992158: 992159E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E:  E:E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:eager release cuda mem 625663:638:] 638
638] 638eager release cuda mem 625663] ] eager release cuda mem 625663] 
eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 03:31:46.  5200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 03:31:46.  5357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46.  5492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 03:31:46.  5637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46.  6246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46.  6551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 12797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 03:31:46. 12965[: [2022-12-12 03:31:46E2022-12-12 03:31:46. . 12962/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 12967: :: E1980E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
:19261926] ] Device 2 init p2p of link 3Device 0 init p2p of link 6

[[2022-12-12 03:31:462022-12-12 03:31:46.. 13169 13169: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
[
2022-12-12 03:31:46. 13203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 03:31:46. 13365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 13438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 03:31:46. 13585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 13905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 13978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 14102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 14145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 03:31:46. 14248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 14308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 14356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 15221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 17555[: 2022-12-12 03:31:46E.  17568/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 1 init p2p of link 3:
1926] Device 6 init p2p of link 4
[2022-12-12 03:31:46. 17692: E[ 2022-12-12 03:31:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: 177031980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 03:31:462022-12-12 03:31:46.. 18587 18591: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 03:31:46. 26370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 03:31:46. 26500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 27393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 27554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 03:31:46. 27679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 28594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 32980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 03:31:46. 33112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 33772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[[2022-12-12 03:31:462022-12-12 03:31:46.. 33897 33899: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 03:31:46. 34268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 03:31:46. 34393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 34676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 03:31:46. 34796: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:31:46:.638 34810] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 35172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 35722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 40074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 03:31:46. 40191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 41093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 42802[: 2022-12-12 03:31:46E.  42814/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 3 init p2p of link 1:
1926] Device 6 init p2p of link 7
[2022-12-12 03:31:46.[ 429702022-12-12 03:31:46: .E 42977 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 43089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 03:31:46. 43214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 43910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 03:31:46] .eager release cuda mem 625663 43930
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 44107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 46104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 03:31:46. 46230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 46500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 03:31:46. 46618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 47121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 47512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 56337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 03:31:46. 56465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 57160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 03:31:46. 57238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 57288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:31:46. 58068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:31:46. 60676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 61301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 61491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96709 secs 
[2022-12-12 03:31:46. 61821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 62188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96779 secs 
[2022-12-12 03:31:46. 62434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97771 secs 
[2022-12-12 03:31:46. 66643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 66995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 67038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.98098 secs 
[2022-12-12 03:31:46. 67381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955[] 2022-12-12 03:31:46Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.9827 secs .
 67396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 67775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.98343 secs 
[2022-12-12 03:31:46. 68835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 69215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 3.00014 secs [
2022-12-12 03:31:46. 69230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:31:46. 69623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.98512 secs 
[HCTR][03:31:46.069][ERROR][RK0][tid #139975668528896]: replica 5 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][tid #139975601420032]: replica 3 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][tid #139975668528896]: replica 2 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][tid #139975660136192]: replica 6 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975668528896]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975660136192]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975601420032]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975668528896]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:31:46.069][ERROR][RK0][main]: init per replica done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975668528896]: init per replica done
[HCTR][03:31:46.069][ERROR][RK0][main]: init per replica done
[HCTR][03:31:46.069][ERROR][RK0][main]: init per replica done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975660136192]: init per replica done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975601420032]: init per replica done
[HCTR][03:31:46.069][ERROR][RK0][tid #139975668528896]: init per replica done
[HCTR][03:31:46.072][ERROR][RK0][main]: init per replica done








