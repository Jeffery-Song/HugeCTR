2022-12-12 02:17:45.473124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.480371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.486745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.493565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.497651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.509813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.516428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.528700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.581162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.583890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.585258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.586928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.587657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.588163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.589417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.589597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.591032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.591064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.592532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.592586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.593987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.594093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.595607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.595699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.597102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.597335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.598564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.599083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.600019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.601135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.602024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.603046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.604872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.605982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.606993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.608024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.608974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.609938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.610982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.612019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.617390: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.618247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.619553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.621183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.622899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.623522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.625173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.625333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.625363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.627109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.627399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.627534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.627575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.629611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.630064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.630219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.630317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.631932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.632757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.632942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.632987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.635358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.635738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.637962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.638289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.639938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.640514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.640782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.642690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.643276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.644969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.645468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.647186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.647793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.649924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.650309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.651504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.652695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.652887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.654465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.655065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.655247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.655366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.657122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.665519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.666709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.666962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.668553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.669032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.669601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.671315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.671360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.672300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.680305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.686994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.688701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.707415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.707664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.708817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.709107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.709160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.709190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.711174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.712318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.713064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.713341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.713526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.713554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.713898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.716427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.717729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.718151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.719245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.719356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.719592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.720237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.721565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.723642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.723977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.724025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.724269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.725168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.725816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.727947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.728116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.728353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.728618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.730331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.731950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.732085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.732282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.732474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.733776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.735305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.735491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.735718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.735853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.736961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.738491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.738674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.738922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.739057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.741235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.741555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.741755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.741931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.742082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.744717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.744947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.745271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.745503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.745548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.748030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.748276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.748356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.748565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.748751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.751420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.751588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.751715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.751913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.752052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.754677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.754823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.755069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.755212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.755351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.757994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.758040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.758212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.758412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.758520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.761294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.761303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.761436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.761643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.761684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.764745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.765065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.765783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.765910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.766122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.767446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.767542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.768963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.769164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.769530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.770562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.771778: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.771942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.771955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.772082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.772431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.773663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.774404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.775147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.775334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.775350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.775680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.777502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.778404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.779416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.779801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.779845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.780094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.781493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.781557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.782382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.783556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.783991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.784209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.784290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.786493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.786525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.787314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.788183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.789146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.789345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.790056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.791142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.791175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.791884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.793183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.793942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.793978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.794937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.795709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.796734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.798869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.800324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.800885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.801917: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.801999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.802182: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.803150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.803496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.803507: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.805921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.806516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.808384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.808418: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.808935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.810560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.811424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.811490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.811978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.813253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.813757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.814948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.815195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.815705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.817284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.817672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.817806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.819058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.820063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.821006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.822923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.823250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.823641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.824905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.855912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.856466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.856706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.867536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.867828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.872655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.873518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.906628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.907052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.911657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.913318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.918397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.918789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.924205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.924684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.929220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.930316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.934767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.937825: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.944558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.947238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.958109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.960733: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:17:45.962821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:45.970182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.051703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.083599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.898482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.899373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.899945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.900621: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:46.900677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:17:46.918938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.919598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.920111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.920700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.921634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:46.922275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:17:46.968818: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:46.969040: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.016459: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:17:47.120520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.121134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.121895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.122376: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.122429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:17:47.140184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.141003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.141598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.142228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.142766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.143245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:17:47.176724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.177341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.177868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.178339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.178395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:17:47.188564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.189156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.189855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.190329: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.190384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:17:47.196819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.197473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.197977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.198810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.198920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.200418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.200609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.201593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:17:47.202069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.202582: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.202633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:17:47.208196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.208800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.209307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.209878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.210399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.210866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:17:47.213744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.214328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.215341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.215823: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.215865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:17:47.220619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.221473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.222003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.222599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.223109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.223601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:17:47.225540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.225713: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.227481: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:17:47.232902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.233520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.234047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.234054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.235339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.235420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:17:47.240640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.241209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.241815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.242367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.242841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:17:47.252837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.253464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.253982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.254454: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:17:47.254507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:17:47.256852: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.257029: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.259219: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:17:47.259363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.260049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.260579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.261151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.261676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.262155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:17:47.269166: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.269370: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.271277: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:17:47.272702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.273347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.273852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.274434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.274941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:17:47.275423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:17:47.289180: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.289391: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.291097: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:17:47.299704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.299898: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.301749: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 02:17:47.309051: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.309230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.311035: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:17:47.320985: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.321163: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:17:47.322912: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][02:17:48.603][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:17:48.604][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 100it [00:01, 83.05it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 199it [00:01, 179.42it/s]warmup run: 100it [00:01, 85.02it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 299it [00:01, 287.31it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 200it [00:01, 184.36it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 82.68it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 396it [00:01, 395.25it/s]warmup run: 97it [00:01, 82.87it/s]warmup run: 98it [00:01, 85.51it/s]warmup run: 98it [00:01, 85.51it/s]warmup run: 298it [00:01, 291.15it/s]warmup run: 99it [00:01, 85.62it/s]warmup run: 194it [00:01, 179.25it/s]warmup run: 100it [00:01, 88.37it/s]warmup run: 495it [00:02, 504.24it/s]warmup run: 194it [00:01, 179.49it/s]warmup run: 196it [00:01, 184.73it/s]warmup run: 196it [00:01, 184.91it/s]warmup run: 392it [00:01, 394.98it/s]warmup run: 199it [00:01, 186.39it/s]warmup run: 294it [00:01, 289.27it/s]warmup run: 201it [00:01, 191.79it/s]warmup run: 596it [00:02, 608.52it/s]warmup run: 292it [00:01, 287.25it/s]warmup run: 294it [00:01, 293.49it/s]warmup run: 295it [00:01, 294.85it/s]warmup run: 488it [00:02, 499.93it/s]warmup run: 300it [00:01, 298.28it/s]warmup run: 392it [00:01, 400.45it/s]warmup run: 302it [00:01, 304.90it/s]warmup run: 696it [00:02, 697.30it/s]warmup run: 390it [00:01, 398.72it/s]warmup run: 393it [00:01, 406.96it/s]warmup run: 393it [00:01, 406.67it/s]warmup run: 585it [00:02, 598.42it/s]warmup run: 402it [00:01, 415.04it/s]warmup run: 491it [00:02, 509.98it/s]warmup run: 404it [00:01, 422.54it/s]warmup run: 796it [00:02, 770.63it/s]warmup run: 488it [00:02, 506.59it/s]warmup run: 491it [00:01, 515.27it/s]warmup run: 491it [00:01, 515.07it/s]warmup run: 683it [00:02, 686.26it/s]warmup run: 502it [00:02, 524.79it/s]warmup run: 591it [00:02, 612.44it/s]warmup run: 505it [00:01, 534.34it/s]warmup run: 586it [00:02, 606.12it/s]warmup run: 591it [00:02, 617.76it/s]warmup run: 894it [00:02, 763.97it/s]warmup run: 585it [00:02, 598.52it/s]warmup run: 780it [00:02, 755.61it/s]warmup run: 605it [00:02, 630.52it/s]warmup run: 691it [00:02, 701.27it/s]warmup run: 606it [00:02, 636.05it/s]warmup run: 683it [00:02, 690.13it/s]warmup run: 691it [00:02, 706.16it/s]warmup run: 985it [00:02, 799.71it/s]warmup run: 683it [00:02, 685.83it/s]warmup run: 877it [00:02, 810.59it/s]warmup run: 708it [00:02, 721.94it/s]warmup run: 791it [00:02, 774.70it/s]warmup run: 703it [00:02, 710.96it/s]warmup run: 780it [00:02, 759.05it/s]warmup run: 789it [00:02, 773.08it/s]warmup run: 1083it [00:02, 847.66it/s]warmup run: 783it [00:02, 763.09it/s]warmup run: 977it [00:02, 860.02it/s]warmup run: 810it [00:02, 794.48it/s]warmup run: 890it [00:02, 830.17it/s]warmup run: 800it [00:02, 770.02it/s]warmup run: 878it [00:02, 814.76it/s]warmup run: 886it [00:02, 824.30it/s]warmup run: 1180it [00:02, 881.09it/s]warmup run: 886it [00:02, 830.89it/s]warmup run: 1077it [00:02, 896.90it/s]warmup run: 913it [00:02, 855.35it/s]warmup run: 989it [00:02, 872.51it/s]warmup run: 896it [00:02, 810.36it/s]warmup run: 977it [00:02, 860.61it/s]warmup run: 986it [00:02, 869.99it/s]warmup run: 1276it [00:02, 903.34it/s]warmup run: 1177it [00:02, 924.67it/s]warmup run: 988it [00:02, 872.85it/s]warmup run: 1017it [00:02, 904.68it/s]warmup run: 1088it [00:02, 903.56it/s]warmup run: 991it [00:02, 839.88it/s]warmup run: 1075it [00:02, 892.13it/s]warmup run: 1087it [00:02, 908.30it/s]warmup run: 1374it [00:03, 924.40it/s]warmup run: 1278it [00:02, 948.47it/s]warmup run: 1090it [00:02, 913.42it/s]warmup run: 1120it [00:02, 939.33it/s]warmup run: 1187it [00:02, 926.50it/s]warmup run: 1085it [00:02, 860.13it/s]warmup run: 1173it [00:02, 915.88it/s]warmup run: 1189it [00:02, 939.86it/s]warmup run: 1470it [00:03, 932.17it/s]warmup run: 1377it [00:02, 957.74it/s]warmup run: 1191it [00:02, 939.24it/s]warmup run: 1224it [00:02, 967.54it/s]warmup run: 1288it [00:02, 948.91it/s]warmup run: 1178it [00:02, 876.43it/s]warmup run: 1272it [00:02, 935.30it/s]warmup run: 1290it [00:02, 957.42it/s]warmup run: 1570it [00:03, 951.89it/s]warmup run: 1293it [00:02, 962.18it/s]warmup run: 1476it [00:03, 958.47it/s]warmup run: 1328it [00:02, 986.30it/s]warmup run: 1392it [00:02, 973.05it/s]warmup run: 1274it [00:02, 898.72it/s]warmup run: 1370it [00:02, 946.34it/s]warmup run: 1390it [00:02, 968.06it/s]warmup run: 1670it [00:03, 963.82it/s]warmup run: 1395it [00:02, 978.10it/s]warmup run: 1574it [00:03, 944.80it/s]warmup run: 1432it [00:02, 1000.85it/s]warmup run: 1493it [00:03, 979.73it/s]warmup run: 1373it [00:02, 923.66it/s]warmup run: 1468it [00:03, 952.56it/s]warmup run: 1490it [00:02, 975.17it/s]warmup run: 1768it [00:03, 968.11it/s]warmup run: 1497it [00:03, 989.74it/s]warmup run: 1670it [00:03, 946.94it/s]warmup run: 1536it [00:03, 1010.25it/s]warmup run: 1593it [00:03, 980.20it/s]warmup run: 1472it [00:02, 941.61it/s]warmup run: 1566it [00:03, 954.02it/s]warmup run: 1590it [00:03, 980.01it/s]warmup run: 1868it [00:03, 975.89it/s]warmup run: 1599it [00:03, 997.32it/s]warmup run: 1766it [00:03, 947.83it/s]warmup run: 1640it [00:03, 1018.07it/s]warmup run: 1693it [00:03, 984.54it/s]warmup run: 1571it [00:03, 955.20it/s]warmup run: 1663it [00:03, 958.23it/s]warmup run: 1690it [00:03, 980.59it/s]warmup run: 1967it [00:03, 979.21it/s]warmup run: 1862it [00:03, 946.23it/s]warmup run: 1745it [00:03, 1024.83it/s]warmup run: 1701it [00:03, 969.74it/s]warmup run: 1794it [00:03, 990.30it/s]warmup run: 1672it [00:03, 970.78it/s]warmup run: 1762it [00:03, 966.83it/s]warmup run: 1790it [00:03, 983.06it/s]warmup run: 2078it [00:03, 1016.30it/s]warmup run: 1958it [00:03, 943.65it/s]warmup run: 1849it [00:03, 1023.47it/s]warmup run: 1895it [00:03, 994.57it/s]warmup run: 1800it [00:03, 951.00it/s]warmup run: 1775it [00:03, 987.78it/s]warmup run: 1865it [00:03, 982.82it/s]warmup run: 1891it [00:03, 988.32it/s]warmup run: 2196it [00:03, 1064.89it/s]warmup run: 2064it [00:03, 976.26it/s]warmup run: 1953it [00:03, 1025.31it/s]warmup run: 1997it [00:03, 1000.63it/s]warmup run: 1903it [00:03, 972.25it/s]warmup run: 1875it [00:03, 984.23it/s]warmup run: 1966it [00:03, 989.06it/s]warmup run: 1994it [00:03, 999.76it/s]warmup run: 2314it [00:03, 1098.85it/s]warmup run: 2181it [00:03, 1033.07it/s]warmup run: 2064it [00:03, 1050.45it/s]warmup run: 2118it [00:03, 1061.37it/s]warmup run: 2007it [00:03, 990.52it/s]warmup run: 1974it [00:03, 978.50it/s]warmup run: 2080it [00:03, 1032.76it/s]warmup run: 2113it [00:03, 1055.88it/s]warmup run: 2433it [00:04, 1124.46it/s]warmup run: 2298it [00:03, 1071.95it/s]warmup run: 2184it [00:03, 1093.12it/s]warmup run: 2240it [00:03, 1108.43it/s]warmup run: 2129it [00:03, 1057.06it/s]warmup run: 2086it [00:03, 1018.27it/s]warmup run: 2200it [00:03, 1081.82it/s]warmup run: 2234it [00:03, 1101.83it/s]warmup run: 2552it [00:04, 1143.07it/s]warmup run: 2415it [00:03, 1100.24it/s]warmup run: 2304it [00:03, 1123.42it/s]warmup run: 2363it [00:03, 1142.65it/s]warmup run: 2252it [00:03, 1106.22it/s]warmup run: 2204it [00:03, 1065.59it/s]warmup run: 2320it [00:03, 1115.76it/s]warmup run: 2356it [00:03, 1135.29it/s]warmup run: 2670it [00:04, 1152.92it/s]warmup run: 2532it [00:04, 1120.58it/s]warmup run: 2424it [00:03, 1144.52it/s]warmup run: 2486it [00:03, 1167.40it/s]warmup run: 2375it [00:03, 1142.06it/s]warmup run: 2323it [00:03, 1102.43it/s]warmup run: 2440it [00:03, 1139.88it/s]warmup run: 2478it [00:03, 1158.69it/s]warmup run: 2789it [00:04, 1162.48it/s]warmup run: 2648it [00:04, 1132.21it/s]warmup run: 2544it [00:03, 1159.19it/s]warmup run: 2609it [00:04, 1184.09it/s]warmup run: 2498it [00:03, 1167.19it/s]warmup run: 2442it [00:03, 1127.65it/s]warmup run: 2561it [00:04, 1158.12it/s]warmup run: 2600it [00:04, 1174.21it/s]warmup run: 2908it [00:04, 1168.84it/s]warmup run: 2765it [00:04, 1143.01it/s]warmup run: 2663it [00:04, 1165.72it/s]warmup run: 2621it [00:04, 1184.45it/s]warmup run: 2731it [00:04, 1192.55it/s]warmup run: 2561it [00:03, 1145.63it/s]warmup run: 2680it [00:04, 1166.65it/s]warmup run: 2721it [00:04, 1184.40it/s]warmup run: 3000it [00:04, 666.85it/s] warmup run: 2882it [00:04, 1149.64it/s]warmup run: 2782it [00:04, 1172.49it/s]warmup run: 2853it [00:04, 1199.00it/s]warmup run: 2743it [00:04, 1192.67it/s]warmup run: 2680it [00:04, 1158.29it/s]warmup run: 2800it [00:04, 1174.49it/s]warmup run: 2842it [00:04, 1189.13it/s]warmup run: 2999it [00:04, 1155.61it/s]warmup run: 2901it [00:04, 1177.53it/s]warmup run: 3000it [00:04, 671.82it/s] warmup run: 2975it [00:04, 1203.69it/s]warmup run: 2866it [00:04, 1202.31it/s]warmup run: 3000it [00:04, 686.32it/s] warmup run: 2796it [00:04, 1158.33it/s]warmup run: 2919it [00:04, 1178.66it/s]warmup run: 2963it [00:04, 1194.95it/s]warmup run: 3000it [00:04, 696.49it/s] warmup run: 3000it [00:04, 690.76it/s] warmup run: 2989it [00:04, 1209.44it/s]warmup run: 3000it [00:04, 690.45it/s] warmup run: 3000it [00:04, 679.51it/s] warmup run: 2913it [00:04, 1160.28it/s]warmup run: 3000it [00:04, 685.61it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.18it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.58it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.20it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.97it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.18it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.34it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1622.84it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1613.04it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1671.37it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1677.31it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1655.32it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1637.03it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1657.15it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1632.90it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1623.14it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1619.44it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1635.79it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1673.97it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1652.72it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1629.80it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1630.19it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1620.52it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1647.63it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1654.19it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1636.00it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1674.61it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1650.88it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1629.64it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1619.27it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1641.59it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1646.29it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1593.79it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1635.54it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1672.47it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1634.74it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1647.57it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1615.97it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1641.27it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1641.82it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1605.87it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1633.68it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1667.28it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1633.44it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1640.00it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1609.40it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1635.84it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1636.05it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1590.92it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1629.93it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1635.52it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1606.45it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1637.15it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1635.53it/s]warmup should be done:  39%|      | 1163/3000 [00:00<00:01, 1635.12it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1603.21it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1628.18it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1634.99it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1664.83it/s]warmup should be done:  43%|     | 1298/3000 [00:00<00:01, 1604.91it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1637.43it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1636.00it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1635.45it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1613.62it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1627.09it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1635.57it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1663.97it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1636.76it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1636.40it/s]warmup should be done:  49%|     | 1459/3000 [00:00<00:00, 1603.32it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1634.81it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1619.97it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1626.52it/s]warmup should be done:  55%|    | 1642/3000 [00:01<00:00, 1637.02it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1664.15it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1636.80it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1637.99it/s]warmup should be done:  54%|    | 1620/3000 [00:01<00:00, 1602.00it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1633.47it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1626.35it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1638.61it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1663.58it/s]warmup should be done:  60%|    | 1814/3000 [00:01<00:00, 1636.24it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1637.06it/s]warmup should be done:  59%|    | 1781/3000 [00:01<00:00, 1604.37it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1634.02it/s]warmup should be done:  60%|    | 1797/3000 [00:01<00:00, 1628.24it/s]warmup should be done:  65%|   | 1963/3000 [00:01<00:00, 1625.79it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1639.05it/s]warmup should be done:  66%|   | 1978/3000 [00:01<00:00, 1636.27it/s]warmup should be done:  67%|   | 2010/3000 [00:01<00:00, 1657.19it/s]warmup should be done:  66%|   | 1981/3000 [00:01<00:00, 1637.51it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1634.69it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1596.86it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1631.32it/s]warmup should be done:  71%|   | 2126/3000 [00:01<00:00, 1625.12it/s]warmup should be done:  71%|   | 2136/3000 [00:01<00:00, 1639.17it/s]warmup should be done:  71%|  | 2142/3000 [00:01<00:00, 1635.49it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1636.79it/s]warmup should be done:  73%|  | 2176/3000 [00:01<00:00, 1649.37it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1634.30it/s]warmup should be done:  70%|   | 2107/3000 [00:01<00:00, 1607.57it/s]warmup should be done:  71%|   | 2125/3000 [00:01<00:00, 1629.89it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1625.24it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1636.04it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1634.23it/s]warmup should be done:  77%|  | 2306/3000 [00:01<00:00, 1628.81it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1642.84it/s]warmup should be done:  76%|  | 2270/3000 [00:01<00:00, 1612.78it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1631.26it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1620.20it/s]warmup should be done:  82%| | 2452/3000 [00:01<00:00, 1622.84it/s]warmup should be done:  82%| | 2464/3000 [00:01<00:00, 1634.54it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1634.35it/s]warmup should be done:  82%| | 2470/3000 [00:01<00:00, 1630.38it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1643.30it/s]warmup should be done:  81%|  | 2434/3000 [00:01<00:00, 1619.24it/s]warmup should be done:  82%| | 2475/3000 [00:01<00:00, 1632.20it/s]warmup should be done:  82%| | 2452/3000 [00:01<00:00, 1621.65it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1623.40it/s]warmup should be done:  88%| | 2629/3000 [00:01<00:00, 1636.38it/s]warmup should be done:  88%| | 2637/3000 [00:01<00:00, 1635.25it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1632.13it/s]warmup should be done:  87%| | 2598/3000 [00:01<00:00, 1623.78it/s]warmup should be done:  88%| | 2639/3000 [00:01<00:00, 1632.22it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1637.97it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1622.30it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1624.01it/s]warmup should be done:  93%|| 2793/3000 [00:01<00:00, 1636.93it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1637.91it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1631.73it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1625.25it/s]warmup should be done:  94%|| 2835/3000 [00:01<00:00, 1635.33it/s]warmup should be done:  93%|| 2803/3000 [00:01<00:00, 1625.66it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1605.66it/s]warmup should be done:  98%|| 2943/3000 [00:01<00:00, 1629.46it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1643.48it/s]warmup should be done:  99%|| 2969/3000 [00:01<00:00, 1644.59it/s]warmup should be done:  99%|| 2965/3000 [00:01<00:00, 1642.26it/s]warmup should be done:  98%|| 2926/3000 [00:01<00:00, 1632.25it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1654.94it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1628.00it/s]warmup should be done:  98%|| 2943/3000 [00:01<00:00, 1616.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.95it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.33it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1615.83it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1679.12it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1679.81it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.33it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1684.12it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.72it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.33it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.16it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.05it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1678.47it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1684.28it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1686.40it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1660.80it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1697.88it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1670.45it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1684.33it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1660.71it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1689.27it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1680.89it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1667.53it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1688.75it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1700.88it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1671.34it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1671.69it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1419.07it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1683.17it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1690.07it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1692.26it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1668.85it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1704.66it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1672.44it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1671.45it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1504.21it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1694.67it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1682.44it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1667.31it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1687.71it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1673.18it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1706.62it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1672.07it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1572.37it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1693.39it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1687.77it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1669.28it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1679.44it/s]warmup should be done:  34%|      | 1027/3000 [00:00<00:01, 1705.95it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1671.80it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1616.29it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1669.27it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1688.75it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1690.34it/s]warmup should be done:  39%|      | 1180/3000 [00:00<00:01, 1676.99it/s]warmup should be done:  40%|      | 1198/3000 [00:00<00:01, 1703.57it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1670.08it/s]warmup should be done:  39%|      | 1180/3000 [00:00<00:01, 1662.34it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1644.22it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1670.55it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:00, 1692.78it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1678.66it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1689.28it/s]warmup should be done:  46%|     | 1370/3000 [00:00<00:00, 1706.34it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1671.19it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1659.69it/s]warmup should be done:  45%|     | 1354/3000 [00:00<00:00, 1666.46it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1671.14it/s]warmup should be done:  51%|     | 1528/3000 [00:00<00:00, 1693.37it/s]warmup should be done:  51%|     | 1517/3000 [00:00<00:00, 1678.00it/s]warmup should be done:  51%|     | 1525/3000 [00:00<00:00, 1687.78it/s]warmup should be done:  51%|    | 1541/3000 [00:00<00:00, 1706.02it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1670.60it/s]warmup should be done:  50%|     | 1513/3000 [00:00<00:00, 1654.94it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1680.92it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1673.24it/s]warmup should be done:  57%|    | 1698/3000 [00:01<00:00, 1693.73it/s]warmup should be done:  56%|    | 1685/3000 [00:01<00:00, 1677.09it/s]warmup should be done:  56%|    | 1694/3000 [00:01<00:00, 1687.42it/s]warmup should be done:  57%|    | 1712/3000 [00:01<00:00, 1705.81it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1670.58it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1655.97it/s]warmup should be done:  57%|    | 1698/3000 [00:01<00:00, 1690.58it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1674.49it/s]warmup should be done:  62%|   | 1868/3000 [00:01<00:00, 1694.13it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1678.63it/s]warmup should be done:  62%|   | 1864/3000 [00:01<00:00, 1688.64it/s]warmup should be done:  63%|   | 1884/3000 [00:01<00:00, 1707.48it/s]warmup should be done:  62%|   | 1848/3000 [00:01<00:00, 1672.01it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1662.45it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1697.89it/s]warmup should be done:  68%|   | 2038/3000 [00:01<00:00, 1695.09it/s]warmup should be done:  67%|   | 2011/3000 [00:01<00:00, 1672.70it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1679.66it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1689.25it/s]warmup should be done:  69%|   | 2056/3000 [00:01<00:00, 1708.90it/s]warmup should be done:  67%|   | 2016/3000 [00:01<00:00, 1671.61it/s]warmup should be done:  67%|   | 2015/3000 [00:01<00:00, 1665.09it/s]warmup should be done:  68%|   | 2042/3000 [00:01<00:00, 1701.90it/s]warmup should be done:  74%|  | 2208/3000 [00:01<00:00, 1692.93it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1671.12it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1678.31it/s]warmup should be done:  73%|  | 2203/3000 [00:01<00:00, 1687.79it/s]warmup should be done:  74%|  | 2227/3000 [00:01<00:00, 1706.86it/s]warmup should be done:  73%|  | 2184/3000 [00:01<00:00, 1670.64it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1665.82it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1703.44it/s]warmup should be done:  79%|  | 2359/3000 [00:01<00:00, 1678.33it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1672.61it/s]warmup should be done:  79%|  | 2378/3000 [00:01<00:00, 1692.43it/s]warmup should be done:  79%|  | 2372/3000 [00:01<00:00, 1687.22it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1704.62it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1671.64it/s]warmup should be done:  78%|  | 2350/3000 [00:01<00:00, 1669.39it/s]warmup should be done:  79%|  | 2384/3000 [00:01<00:00, 1704.20it/s]warmup should be done:  85%| | 2548/3000 [00:01<00:00, 1693.36it/s]warmup should be done:  85%| | 2541/3000 [00:01<00:00, 1687.64it/s]warmup should be done:  84%| | 2528/3000 [00:01<00:00, 1679.43it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1671.39it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1706.34it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1672.77it/s]warmup should be done:  84%| | 2518/3000 [00:01<00:00, 1672.20it/s]warmup should be done:  85%| | 2556/3000 [00:01<00:00, 1706.34it/s]warmup should be done:  91%| | 2718/3000 [00:01<00:00, 1694.19it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1678.47it/s]warmup should be done:  90%| | 2710/3000 [00:01<00:00, 1683.14it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1669.74it/s]warmup should be done:  91%|| 2741/3000 [00:01<00:00, 1706.72it/s]warmup should be done:  90%| | 2688/3000 [00:01<00:00, 1672.48it/s]warmup should be done:  90%| | 2686/3000 [00:01<00:00, 1672.83it/s]warmup should be done:  91%| | 2728/3000 [00:01<00:00, 1707.92it/s]warmup should be done:  96%|| 2888/3000 [00:01<00:00, 1693.11it/s]warmup should be done:  95%|| 2864/3000 [00:01<00:00, 1676.40it/s]warmup should be done:  95%|| 2850/3000 [00:01<00:00, 1666.44it/s]warmup should be done:  97%|| 2912/3000 [00:01<00:00, 1707.09it/s]warmup should be done:  96%|| 2879/3000 [00:01<00:00, 1675.81it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1671.19it/s]warmup should be done:  95%|| 2854/3000 [00:01<00:00, 1671.69it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1705.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1705.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1692.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1684.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.57it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1671.60it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1669.30it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1668.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.74it/s]2022-12-12 02:19:23.063453: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb283833b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:23.063512: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.022258: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb28382cd80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.022331: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.081176: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f95280299d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.081233: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.083900: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f953c02ed10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.083945: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.671056: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f95d402d930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.671148: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.689347: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb27bf931d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.689412: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.714366: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb283833860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.714443: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:24.727641: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb283831190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:19:24.727712: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:19:25.299697: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:26.337564: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:26.363003: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:26.363092: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:26.944724: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:26.998478: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:26.998983: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:27.001128: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:19:28.186839: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.188006: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.260301: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.261596: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.852444: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.924493: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.940605: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:19:29.941812: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:19:57.682][ERROR][RK0][tid #140404829714176]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:19:57.682][ERROR][RK0][tid #140404829714176]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:57.689][ERROR][RK0][tid #140404829714176]: coll ps creation done
[HCTR][02:19:57.689][ERROR][RK0][tid #140404829714176]: replica 2 waits for coll ps creation barrier
[HCTR][02:19:57.800][ERROR][RK0][tid #140405249120000]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:19:57.800][ERROR][RK0][tid #140405249120000]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:57.809][ERROR][RK0][tid #140405249120000]: coll ps creation done
[HCTR][02:19:57.809][ERROR][RK0][tid #140405249120000]: replica 3 waits for coll ps creation barrier
[HCTR][02:19:57.950][ERROR][RK0][tid #140404829714176]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:19:57.951][ERROR][RK0][tid #140404829714176]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:57.957][ERROR][RK0][tid #140404829714176]: coll ps creation done
[HCTR][02:19:57.957][ERROR][RK0][tid #140404829714176]: replica 6 waits for coll ps creation barrier
[HCTR][02:19:58.016][ERROR][RK0][tid #140405249120000]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:19:58.016][ERROR][RK0][tid #140405249120000]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:58.024][ERROR][RK0][tid #140405249120000]: coll ps creation done
[HCTR][02:19:58.024][ERROR][RK0][tid #140405249120000]: replica 7 waits for coll ps creation barrier
[HCTR][02:19:58.088][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:19:58.088][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:58.093][ERROR][RK0][tid #140405718882048]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:19:58.093][ERROR][RK0][tid #140405718882048]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:58.095][ERROR][RK0][main]: coll ps creation done
[HCTR][02:19:58.095][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][02:19:58.098][ERROR][RK0][tid #140405718882048]: coll ps creation done
[HCTR][02:19:58.098][ERROR][RK0][tid #140405718882048]: replica 4 waits for coll ps creation barrier
[HCTR][02:19:58.103][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:19:58.103][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:58.111][ERROR][RK0][main]: coll ps creation done
[HCTR][02:19:58.111][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][02:19:58.144][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:19:58.145][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:19:58.150][ERROR][RK0][main]: coll ps creation done
[HCTR][02:19:58.150][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:19:58.150][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][02:19:59.002][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][02:19:59.040][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][tid #140404829714176]: replica 2 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][tid #140405249120000]: replica 3 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][tid #140405249120000]: replica 7 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][tid #140404829714176]: replica 6 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][tid #140405718882048]: replica 4 calling init per replica
[HCTR][02:19:59.040][ERROR][RK0][main]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][tid #140404829714176]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][main]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][tid #140405249120000]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][tid #140405249120000]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][main]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][tid #140404829714176]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][tid #140405718882048]: Calling build_v2
[HCTR][02:19:59.040][ERROR][RK0][tid #140404829714176]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][tid #140405249120000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][tid #140405249120000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][tid #140404829714176]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:19:59.040][ERROR][RK0][tid #140405718882048]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-12 02:19:592022-12-12 02:19:592022-12-12 02:19:592022-12-12 02:19:592022-12-12 02:19:592022-12-12 02:19:59.2022-12-12 02:19:59..2022-12-12 02:19:59... 40813. 40825 40830. 40828 40820 40825:  40834: :  40835: : : E: EE: EEE E  E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136:136136:136136136] 136] ] 136] ] ] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS

using concurrent impl MPS




[2022-12-12 02:19:59. 45743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:19:59. 45782: E[ 2022-12-12 02:19:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 45787196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:19:59. 45837: E[ 2022-12-12 02:19:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[ 458401962022-12-12 02:19:59: ] .[Eassigning 8 to cpu 458622022-12-12 02:19:59 
: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 45886: : 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E] :2022-12-12 02:19:59 v100x8, slow pcie212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
]  45934[:2022-12-12 02:19:59build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [2022-12-12 02:19:59178.
E2022-12-12 02:19:59.]  45970 . 45996v100x8, slow pcie: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 45983[: 
E:: 2022-12-12 02:19:59E 178[E.[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 02:19:59  460502022-12-12 02:19:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .:212
[ 460822022-12-12 02:19:59:E 46080196] : [.178 : ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E2022-12-12 02:19:59 46137] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu
 .: v100x8, slow pcie: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 46179E
 213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [2022-12-12 02:19:59:196E:[remote time is 8.684212022-12-12 02:19:59.178]  1782022-12-12 02:19:59
. 46266] assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] . 46291: [v100x8, slow pcie
: 46308v100x8, slow pcie: E2022-12-12 02:19:59
196: 
E .] E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[ 46370[assigning 8 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 02:19:592022-12-12 02:19:59: 2022-12-12 02:19:59
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] 213..E.:assigning 8 to cpu]  46431 46428  46434212
remote time is 8.68421: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: ] 
EE:2022-12-12 02:19:59Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [ 214. 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 02:19:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  46540/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 02:19:59.:[cpu time is 97.0588: :212. 465811962022-12-12 02:19:59
E196]  46597: ] . ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: Eassigning 8 to cpu 46633/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu
E 
: :
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 02:19:59] :214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212[] : 46754
[] 2022-12-12 02:19:59cpu time is 97.0588213: 2022-12-12 02:19:59build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[
] E.
 468102022-12-12 02:19:59remote time is 8.68421  46806: .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: E 46840:2022-12-12 02:19:59[E : 213.2022-12-12 02:19:59 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE]  46884./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421:  46898:212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E: 212] : E[] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 02:19:59build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
remote time is 8.68421213: 46966
] [214: remote time is 8.684212022-12-12 02:19:59[[] E
.2022-12-12 02:19:592022-12-12 02:19:59cpu time is 97.0588  47011.[
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  470252022-12-12 02:19:59 47023:: E.: 214EE  47049]   /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E
:: 213214213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] ] :remote time is 8.68421cpu time is 97.0588remote time is 8.68421214


] cpu time is 97.0588
[[2022-12-12 02:19:592022-12-12 02:19:59.. 47188 47193: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 02:21:18.965002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:21:19.  4892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 02:21:19.112924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:21:19.112985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:21:19.113018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:21:19.113049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:21:19.113645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:21:19.113690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.114576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.115253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.128305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 02:21:19.128371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 02:21:19.128888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:21:19.128932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.129112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 02:21:19.129186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 02:21:19.129202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 02:21:19.129265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 02:21:19.129326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2022022-12-12 02:21:19] .7 solved129353
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 02:21:19202.] 129415: 2 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 7 initing device 72022-12-12 02:21:19
.129457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 02:21:19.129693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:21:19.129744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.129780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:21:19.129825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.[1299602022-12-12 02:21:19: .E129968 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81815
] [Building Coll Cache with ... num gpu device is 82022-12-12 02:21:19
[.[2022-12-12 02:21:19130025[2022-12-12 02:21:19.: 2022-12-12 02:21:19.130053E.130045:  130073: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: E :E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu202 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19805 solved:202] 
1980] eager alloc mem 381.47 MB] [4 solved
eager alloc mem 381.47 MB2022-12-12 02:21:19

.130210: [E2022-12-12 02:21:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc130247:: 205E]  worker 0 thread 5 initing device 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 4 initing device 4
[2022-12-12 02:21:19.130772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:21:19.130800: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:21:19:.1815130816] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.130870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.137737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.141827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.141874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.141930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.141985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.142056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.142118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.142169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:21:19.198772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 02:21:19.204148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:21:19.204280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:21:19.205171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.205906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.206949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:19.206997: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[[[2022-12-12 02:21:192022-12-12 02:21:192022-12-12 02:21:19...225528225528225521: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 02:21:19.231279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:21:19.231369: [E2022-12-12 02:21:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc231364:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 5
[2022-12-12 02:21:19.231467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:21:19.231483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:21:19.231563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:21:19.232438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.233073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[[2022-12-12 02:21:192022-12-12 02:21:19..233508233508: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] [] eager alloc mem 5.00 Bytes[2022-12-12 02:21:19eager alloc mem 5.00 Bytes
2022-12-12 02:21:19.
.233584233587: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 02:21:19.233682[: 2022-12-12 02:21:19E. 233708/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 25.25 KB:
1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.234324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.234368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:21:19.234489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.234641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.234722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.235494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:19.235540: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:21:19.235651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:19.235700: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:21:19.235727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:19.235774: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:21:19.243466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 02:21:192022-12-12 02:21:19..243537243557: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 02:21:19[.2022-12-12 02:21:19243642.: 243631E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-12 02:21:19.243710: [E2022-12-12 02:21:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc243736:: 638E]  eager release cuda mem 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[2022-12-12 02:21:19.243810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:21:19.245096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.245606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.246155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.246681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:21:19.247973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.248010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.248062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.248111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:19.248981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:19.[2490192022-12-12 02:21:19: .E249027 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 62566343
] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:21:19[.2022-12-12 02:21:19249088.: 249095E:  W/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc638:] [43eager release cuda mem 6256632022-12-12 02:21:19] 
.WORKER[0] alloc host memory 38.15 MB249126
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:21:19eager release cuda mem 625663.
249162: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:21:19.249192: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:21:19.261372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:21:19.261723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:21:19.261788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:21:19.261985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.262027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:21:19.262318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.262361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:21:19.262403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.262447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:21:19.275174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:21:19.275558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:21:19.275702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:21:19.275788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-12 02:21:192022-12-12 02:21:19..275817275832: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 4.77 GB

[2022-12-12 02:21:19.276172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.276221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:21:19.276299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.276342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:21:19.276437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:21:19.276481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:21........107067107067107067107067107067107067107067107067: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 1 init p2p of link 7Device 4 init p2p of link 5Device 3 init p2p of link 2Device 5 init p2p of link 6Device 7 init p2p of link 4Device 0 init p2p of link 3Device 6 init p2p of link 0







[[[[2022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:21[[[[2022-12-12 02:21:21...2022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:212022-12-12 02:21:21.107637107638107638....107638: : : 107649107649107649107649: EEE: : : : E   EEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:198019801980::::1980] ] ] 1980198019801980] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] ] ] eager alloc mem 611.00 KB


eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 02:21:21.108657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 02:21:21.108682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:21:212022-12-12 02:21:21..108738108740: : E[E[ [2022-12-12 02:21:21 [2022-12-12 02:21:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:21:21./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:21:21.:.108759:.108770638108768: 638108774: ] : E] : Eeager release cuda mem 625663E eager release cuda mem 625663E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638:638638] 638] ] eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 02:21:21.121807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:21:21.121976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.122039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:21:21.122198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.122831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.123055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.126779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 02:21:21.126939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.127024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 02:21:21.127196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.127255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 02:21:21.127439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.127681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 02:21:21.127804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:21:212022-12-12 02:21:21.[.1278482022-12-12 02:21:21127839: .: E127855E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926] :] eager alloc mem 611.00 KB1926Device 5 init p2p of link 4
] 
Device 0 init p2p of link 6
[2022-12-12 02:21:21.128067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:21:212022-12-12 02:21:21..128103128105: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 02:21:21.128213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.128840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.128900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.129008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.131147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:21:21.131265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.131405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:21:21.131525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.132113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.132367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.140997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:21:21.141113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.141496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 02:21:21.141609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.141997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.142497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.144665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:21:21.144794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.145371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:21:21.145490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.145586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.145831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 02:21:21.145975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.146267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 02:21:21
.146280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:21:21.146421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.146822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.147266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.151252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:21:21.151372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.151589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:21:21.151707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.152225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.152567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.155380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:21:21.155499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.155840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 02:21:21.155956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.156348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.156809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.156911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:21:21.157028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.157893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.158700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 02:21:21.158816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.159679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.165322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 02:21:21.165454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.165606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:21:21.165723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:21:21.166222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.166490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:21:21.171081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.171431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.171767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.0417 secs 
[2022-12-12 02:21:21.171982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04117 secs 
[2022-12-12 02:21:21.172426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.172708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.172790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04297 secs 
[2022-12-12 02:21:21.174423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.0455 secs 
[2022-12-12 02:21:21.175160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.175555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.175892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.177703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:21:21.178502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04764 secs 
[2022-12-12 02:21:21.178638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04891 secs 
[2022-12-12 02:21:21.178758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04871 secs 
[2022-12-12 02:21:21.180074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.06639 secs 
[2022-12-12 02:21:21.181794: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.41 GB
[2022-12-12 02:21:22.553807: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.67 GB
[2022-12-12 02:21:22.576758: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.67 GB
[2022-12-12 02:21:22.601019: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.67 GB
[2022-12-12 02:21:23.850698: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.94 GB
[2022-12-12 02:21:23.851154: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.94 GB
[2022-12-12 02:21:23.851682: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.94 GB
[2022-12-12 02:21:25. 70613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.15 GB
[2022-12-12 02:21:25. 70750: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.15 GB
[2022-12-12 02:21:25. 71031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.15 GB
[2022-12-12 02:21:26.599960: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.37 GB
[2022-12-12 02:21:26.600459: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.37 GB
[2022-12-12 02:21:26.600767: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.37 GB
[2022-12-12 02:21:28.193512: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.82 GB
[2022-12-12 02:21:28.193699: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.82 GB
[2022-12-12 02:21:28.194022: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.82 GB
[2022-12-12 02:21:29.848768: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.02 GB
[2022-12-12 02:21:29.848961: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.02 GB
[HCTR][02:21:30.098][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][tid #140404829714176]: replica 6 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][tid #140405718882048]: replica 4 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][tid #140405249120000]: replica 3 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][tid #140404829714176]: replica 2 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][tid #140405249120000]: replica 7 calling init per replica done, doing barrier
[HCTR][02:21:30.098][ERROR][RK0][tid #140405718882048]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][tid #140404829714176]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][tid #140405249120000]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][tid #140404829714176]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][tid #140405249120000]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:21:30.098][ERROR][RK0][tid #140405718882048]: init per replica done
[HCTR][02:21:30.098][ERROR][RK0][main]: init per replica done
[HCTR][02:21:30.098][ERROR][RK0][tid #140404829714176]: init per replica done
[HCTR][02:21:30.098][ERROR][RK0][tid #140405249120000]: init per replica done
[HCTR][02:21:30.098][ERROR][RK0][tid #140404829714176]: init per replica done
[HCTR][02:21:30.098][ERROR][RK0][tid #140405249120000]: init per replica done
[HCTR][02:21:30.098][ERROR][RK0][main]: init per replica done
[HCTR][02:21:30.101][ERROR][RK0][main]: init per replica done
[HCTR][02:21:30.104][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fb477d20000
[HCTR][02:21:30.104][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fb478200000
[HCTR][02:21:30.104][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fb478840000
[HCTR][02:21:30.104][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fb478b60000
[HCTR][02:21:30.105][ERROR][RK0][tid #140404829714176]: 6 allocated 3276800 at 0x7fb475d20000
[HCTR][02:21:30.105][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fb471d20000
[HCTR][02:21:30.105][ERROR][RK0][tid #140404829714176]: 6 allocated 6553600 at 0x7fb476200000
[HCTR][02:21:30.105][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fb472200000
[HCTR][02:21:30.105][ERROR][RK0][tid #140404829714176]: 6 allocated 3276800 at 0x7fb476840000
[HCTR][02:21:30.105][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fb472840000
[HCTR][02:21:30.105][ERROR][RK0][tid #140404829714176]: 6 allocated 6553600 at 0x7fb476b60000
[HCTR][02:21:30.105][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fb472b60000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 3 allocated 3276800 at 0x7fb479d20000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 3 allocated 6553600 at 0x7fb47a200000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 3 allocated 3276800 at 0x7fb47a840000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 3 allocated 6553600 at 0x7fb47ab60000
[HCTR][02:21:30.105][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fb477d20000
[HCTR][02:21:30.105][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fb478200000
[HCTR][02:21:30.105][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fb478840000
[HCTR][02:21:30.105][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fb478b60000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405718882048]: 4 allocated 3276800 at 0x7fb479d20000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405718882048]: 4 allocated 6553600 at 0x7fb47a200000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405718882048]: 4 allocated 3276800 at 0x7fb47a840000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 1 allocated 3276800 at 0x7fb479d20000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405718882048]: 4 allocated 6553600 at 0x7fb47ab60000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 1 allocated 6553600 at 0x7fb47a200000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 1 allocated 3276800 at 0x7fb47a840000
[HCTR][02:21:30.105][ERROR][RK0][tid #140405249120000]: 1 allocated 6553600 at 0x7fb47ab60000
[HCTR][02:21:30.108][ERROR][RK0][tid #140405651773184]: 0 allocated 3276800 at 0x7fb47c520000
[HCTR][02:21:30.108][ERROR][RK0][tid #140405651773184]: 0 allocated 6553600 at 0x7fb47ca00000
[HCTR][02:21:30.108][ERROR][RK0][tid #140405651773184]: 0 allocated 3276800 at 0x7fb47d70e800
[HCTR][02:21:30.108][ERROR][RK0][tid #140405651773184]: 0 allocated 6553600 at 0x7fb47da2e800








