2022-12-12 07:52:38.449357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.454695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.463401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.468814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.474548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.485334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.493120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.505064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.556095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.559849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.562763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.563823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.564880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.566118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.567832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.568805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.568925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.570523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.570546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.572038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.572200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.573465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.573812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.575068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.575474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.576760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.577070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.578252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.578731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.579666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.580455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.581073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.583184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.584342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.585359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.586371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.587427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.588693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.590351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.590938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.591671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.592584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.593852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.594823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.595812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.596806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.597571: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.597842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.598828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.605601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.606862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.607882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.608540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.610127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.610509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.610936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.612503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.613051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.613104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.613232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.615873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.616073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.616140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.618910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.619114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.619316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.619787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.621995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.622254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.622430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.623008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.623566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.625117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.625293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.626005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.626650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.627103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.627971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.628099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.628889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.629727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.630067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.631156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.631242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.632029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.632907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.633018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.634326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.635062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.635661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.635816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.637593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.637901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.638024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.639382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.639643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.639803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.643460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.650272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.650463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.650583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.652065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.652358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.669495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.678136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.679951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.689672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.692604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.693286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.694891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.695105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.695106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.695161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.695249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.695483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.696921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.699712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.699797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.699940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.699999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.700042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.700079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.702284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.705496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.705726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.706636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.706704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.706731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.706778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.707862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.710207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.710644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.710713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.710777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.710895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.710942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.712005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.714897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.715079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.715241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.715325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.715423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.715510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.716483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.719506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.719751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.719876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.719997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.720020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.720131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.720986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.723655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.723966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.724098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.724217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.724254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.724343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.725298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.727812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.728080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.728165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.728344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.728431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.729154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.729253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.731791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.732097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.732164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.732331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.732414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.733196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.733572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.735999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.736419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.736453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.736645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.736676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.737649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.737891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.740192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.740619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.740660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.740793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.740895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.741716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.741912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.744581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.744824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.744900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.745142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.745185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.745991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.746118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.748565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.749069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.749101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.749145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.750068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.750308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.750335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.752760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.752960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.753040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.753212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.754270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.754370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.754528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.756817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.757040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.757124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.757168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.758424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.758429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.758965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.761368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.761725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.761907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.762523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.762605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.763710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.765265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.765290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.765487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.766004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.766054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.767226: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.767326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.768931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.768998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.769175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.769739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.769937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.770975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.772783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.773001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.773169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.773560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.773676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.774691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.776451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.776747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.776932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.777186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.777240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.777557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.778676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.780723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.781054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.781324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.781427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.781590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.782702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.784442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.784998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.785273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.785442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.785847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.785865: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.786313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.788023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.788536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.788912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.789444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.790217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.791799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.795508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.795573: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.795935: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.796376: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.796762: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.797384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.797627: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:52:38.799389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.805778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.805821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.806290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.806659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.807433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.809444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.809486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.810097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.810940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.811762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.853456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.853562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.854146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.854764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:38.855539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.915791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.916388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.917130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.917606: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:39.917659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:52:39.935884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.936509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.937013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.939728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.940661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:39.941529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:52:39.985740: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:39.985960: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.039818: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:52:40.135782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.136461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.137003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.137861: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.137918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:52:40.155937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.156795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.157314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.157894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.158418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.158888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:52:40.197031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.197749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.198407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.199286: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.199342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:52:40.216994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.217631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.218137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.218721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.219335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.219855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:52:40.222416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.223033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.224261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.224750: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.224805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:52:40.227104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.227703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.228230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.228699: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.228745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:52:40.230597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.231385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.231918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.232260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.232569: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.232621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:52:40.233355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.233891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.234353: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.234395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:52:40.242440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.243094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.243622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.244205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.244724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.245192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:52:40.246369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.246978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.247521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.248117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.248639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.249102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:52:40.250710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.251308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.251828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.251844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.252957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.253017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.254006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.254086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:52:40.255386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.255915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.256377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:52:40.265935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.266567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.267093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.267583: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:52:40.267638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:52:40.281072: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.281293: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.283295: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:52:40.285765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.286410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.286934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.287553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.288079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:52:40.288566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:52:40.289323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.289489: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.291370: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:52:40.294230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.294424: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.296301: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:52:40.299704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.299831: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.299871: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.299985: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.301839: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.301928: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 07:52:40.301990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.302008: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 07:52:40.303776: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:52:40.333918: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.334146: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:52:40.336134: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:52:41.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 88it [00:01, 74.17it/s]warmup run: 96it [00:01, 80.90it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 100it [00:01, 86.56it/s]warmup run: 100it [00:01, 86.71it/s]warmup run: 101it [00:01, 87.87it/s]warmup run: 90it [00:01, 78.91it/s]warmup run: 180it [00:01, 165.33it/s]warmup run: 193it [00:01, 176.61it/s]warmup run: 94it [00:01, 82.27it/s]warmup run: 97it [00:01, 84.57it/s]warmup run: 198it [00:01, 185.10it/s]warmup run: 200it [00:01, 187.48it/s]warmup run: 202it [00:01, 189.95it/s]warmup run: 184it [00:01, 174.84it/s]warmup run: 274it [00:01, 268.57it/s]warmup run: 293it [00:01, 286.10it/s]warmup run: 188it [00:01, 177.78it/s]warmup run: 197it [00:01, 186.06it/s]warmup run: 298it [00:01, 295.80it/s]warmup run: 300it [00:01, 298.18it/s]warmup run: 302it [00:01, 300.51it/s]warmup run: 276it [00:01, 276.83it/s]warmup run: 369it [00:01, 377.09it/s]warmup run: 395it [00:01, 403.22it/s]warmup run: 274it [00:01, 271.19it/s]warmup run: 296it [00:01, 295.95it/s]warmup run: 396it [00:01, 407.07it/s]warmup run: 401it [00:01, 413.48it/s]warmup run: 403it [00:01, 415.91it/s]warmup run: 368it [00:01, 381.73it/s]warmup run: 465it [00:02, 484.45it/s]warmup run: 497it [00:02, 517.44it/s]warmup run: 394it [00:01, 407.67it/s]warmup run: 493it [00:02, 513.11it/s]warmup run: 352it [00:01, 347.05it/s]warmup run: 501it [00:02, 523.93it/s]warmup run: 504it [00:01, 527.63it/s]warmup run: 562it [00:02, 585.65it/s]warmup run: 460it [00:01, 483.02it/s]warmup run: 600it [00:02, 624.84it/s]warmup run: 493it [00:01, 517.57it/s]warmup run: 595it [00:02, 620.31it/s]warmup run: 444it [00:01, 453.36it/s]warmup run: 604it [00:02, 630.58it/s]warmup run: 608it [00:02, 635.19it/s]warmup run: 553it [00:02, 578.06it/s]warmup run: 704it [00:02, 720.00it/s]warmup run: 660it [00:02, 675.04it/s]warmup run: 594it [00:02, 621.59it/s]warmup run: 695it [00:02, 707.65it/s]warmup run: 538it [00:02, 554.99it/s]warmup run: 707it [00:02, 722.22it/s]warmup run: 711it [00:02, 725.61it/s]warmup run: 647it [00:02, 662.22it/s]warmup run: 761it [00:02, 757.57it/s]warmup run: 807it [00:02, 795.32it/s]warmup run: 695it [00:02, 712.02it/s]warmup run: 796it [00:02, 782.66it/s]warmup run: 633it [00:02, 644.90it/s]warmup run: 808it [00:02, 793.26it/s]warmup run: 814it [00:02, 799.25it/s]warmup run: 740it [00:02, 728.75it/s]warmup run: 862it [00:02, 821.57it/s]warmup run: 908it [00:02, 849.72it/s]warmup run: 796it [00:02, 785.83it/s]warmup run: 898it [00:02, 844.81it/s]warmup run: 728it [00:02, 719.11it/s]warmup run: 909it [00:02, 849.93it/s]warmup run: 915it [00:02, 853.15it/s]warmup run: 834it [00:02, 783.83it/s]warmup run: 962it [00:02, 867.93it/s]warmup run: 1009it [00:02, 890.70it/s]warmup run: 897it [00:02, 842.86it/s]warmup run: 1000it [00:02, 891.36it/s]warmup run: 822it [00:02, 775.99it/s]warmup run: 1010it [00:02, 892.95it/s]warmup run: 1017it [00:02, 896.98it/s]warmup run: 928it [00:02, 824.29it/s]warmup run: 1110it [00:02, 920.10it/s]warmup run: 1060it [00:02, 892.19it/s]warmup run: 997it [00:02, 884.64it/s]warmup run: 1102it [00:02, 926.77it/s]warmup run: 916it [00:02, 820.44it/s]warmup run: 1111it [00:02, 925.36it/s]warmup run: 1120it [00:02, 932.36it/s]warmup run: 1022it [00:02, 856.05it/s]warmup run: 1211it [00:02, 944.45it/s]warmup run: 1157it [00:02, 907.98it/s]warmup run: 1098it [00:02, 918.84it/s]warmup run: 1204it [00:02, 953.37it/s]warmup run: 1010it [00:02, 852.12it/s]warmup run: 1212it [00:02, 942.77it/s]warmup run: 1222it [00:02, 954.94it/s]warmup run: 1117it [00:02, 880.92it/s]warmup run: 1255it [00:02, 927.34it/s]warmup run: 1312it [00:02, 948.21it/s]warmup run: 1199it [00:02, 943.59it/s]warmup run: 1306it [00:02, 971.06it/s]warmup run: 1105it [00:02, 877.63it/s]warmup run: 1312it [00:02, 946.45it/s]warmup run: 1324it [00:02, 972.44it/s]warmup run: 1212it [00:02, 899.08it/s]warmup run: 1352it [00:02, 933.50it/s]warmup run: 1413it [00:02, 964.53it/s]warmup run: 1299it [00:02, 958.28it/s]warmup run: 1410it [00:02, 990.02it/s]warmup run: 1199it [00:02, 893.84it/s]warmup run: 1411it [00:02, 945.08it/s]warmup run: 1428it [00:02, 990.97it/s]warmup run: 1307it [00:02, 911.90it/s]warmup run: 1513it [00:03, 974.20it/s]warmup run: 1449it [00:03, 931.78it/s]warmup run: 1400it [00:02, 970.73it/s]warmup run: 1514it [00:03, 1002.92it/s]warmup run: 1293it [00:02, 900.86it/s]warmup run: 1509it [00:03, 949.65it/s]warmup run: 1530it [00:03, 998.84it/s]warmup run: 1403it [00:02, 926.00it/s]warmup run: 1613it [00:03, 978.20it/s]warmup run: 1545it [00:03, 932.59it/s]warmup run: 1503it [00:02, 987.23it/s]warmup run: 1618it [00:03, 1012.00it/s]warmup run: 1386it [00:03, 907.31it/s]warmup run: 1608it [00:03, 959.73it/s]warmup run: 1632it [00:03, 1004.15it/s]warmup run: 1498it [00:03, 931.17it/s]warmup run: 1713it [00:03, 970.48it/s]warmup run: 1640it [00:03, 926.57it/s]warmup run: 1606it [00:03, 998.49it/s]warmup run: 1721it [00:03, 1015.73it/s]warmup run: 1479it [00:03, 910.88it/s]warmup run: 1708it [00:03, 969.06it/s]warmup run: 1734it [00:03, 1006.83it/s]warmup run: 1593it [00:03, 934.80it/s]warmup run: 1812it [00:03, 964.90it/s]warmup run: 1734it [00:03, 925.44it/s]warmup run: 1709it [00:03, 1007.76it/s]warmup run: 1825it [00:03, 1021.01it/s]warmup run: 1573it [00:03, 917.58it/s]warmup run: 1809it [00:03, 979.12it/s]warmup run: 1836it [00:03, 1006.70it/s]warmup run: 1688it [00:03, 936.33it/s]warmup run: 1828it [00:03, 926.51it/s]warmup run: 1910it [00:03, 958.21it/s]warmup run: 1813it [00:03, 1014.47it/s]warmup run: 1929it [00:03, 1024.91it/s]warmup run: 1667it [00:03, 921.90it/s]warmup run: 1909it [00:03, 984.55it/s]warmup run: 1938it [00:03, 1002.64it/s]warmup run: 1783it [00:03, 937.33it/s]warmup run: 1928it [00:03, 946.19it/s]warmup run: 2007it [00:03, 958.42it/s]warmup run: 1916it [00:03, 1018.97it/s]warmup run: 2033it [00:03, 1026.91it/s]warmup run: 1767it [00:03, 943.12it/s]warmup run: 2010it [00:03, 991.05it/s]warmup run: 2042it [00:03, 1013.08it/s]warmup run: 1878it [00:03, 940.32it/s]warmup run: 2033it [00:03, 976.80it/s]warmup run: 2125it [00:03, 1022.72it/s]warmup run: 2022it [00:03, 1029.58it/s]warmup run: 2153it [00:03, 1076.75it/s]warmup run: 1867it [00:03, 958.60it/s]warmup run: 2128it [00:03, 1046.20it/s]warmup run: 2162it [00:03, 1067.31it/s]warmup run: 1973it [00:03, 939.79it/s]warmup run: 2152it [00:03, 1039.36it/s]warmup run: 2243it [00:03, 1068.32it/s]warmup run: 2143it [00:03, 1081.31it/s]warmup run: 2273it [00:03, 1112.33it/s]warmup run: 1968it [00:03, 971.86it/s]warmup run: 2246it [00:03, 1085.33it/s]warmup run: 2282it [00:03, 1106.29it/s]warmup run: 2086it [00:03, 994.92it/s]warmup run: 2271it [00:03, 1083.82it/s]warmup run: 2362it [00:03, 1101.96it/s]warmup run: 2265it [00:03, 1119.93it/s]warmup run: 2393it [00:03, 1136.50it/s]warmup run: 2078it [00:03, 1009.33it/s]warmup run: 2365it [00:03, 1114.52it/s]warmup run: 2402it [00:03, 1133.64it/s]warmup run: 2207it [00:03, 1057.62it/s]warmup run: 2391it [00:03, 1116.68it/s]warmup run: 2481it [00:03, 1126.10it/s]warmup run: 2386it [00:03, 1146.36it/s]warmup run: 2513it [00:03, 1153.57it/s]warmup run: 2194it [00:03, 1052.90it/s]warmup run: 2484it [00:03, 1135.06it/s]warmup run: 2522it [00:03, 1152.43it/s]warmup run: 2328it [00:03, 1101.39it/s]warmup run: 2511it [00:04, 1140.37it/s]warmup run: 2600it [00:04, 1142.33it/s]warmup run: 2508it [00:03, 1165.66it/s]warmup run: 2633it [00:04, 1165.54it/s]warmup run: 2310it [00:03, 1083.74it/s]warmup run: 2603it [00:04, 1148.68it/s]warmup run: 2642it [00:04, 1166.36it/s]warmup run: 2449it [00:03, 1132.37it/s]warmup run: 2631it [00:04, 1157.10it/s]warmup run: 2719it [00:04, 1154.56it/s]warmup run: 2630it [00:04, 1179.44it/s]warmup run: 2753it [00:04, 1174.47it/s]warmup run: 2426it [00:04, 1104.95it/s]warmup run: 2722it [00:04, 1158.27it/s]warmup run: 2761it [00:04, 1172.71it/s]warmup run: 2570it [00:04, 1154.35it/s]warmup run: 2751it [00:04, 1168.09it/s]warmup run: 2837it [00:04, 1160.10it/s]warmup run: 2752it [00:04, 1189.38it/s]warmup run: 2872it [00:04, 1176.30it/s]warmup run: 2542it [00:04, 1118.72it/s]warmup run: 2839it [00:04, 1161.67it/s]warmup run: 2881it [00:04, 1180.79it/s]warmup run: 2691it [00:04, 1169.89it/s]warmup run: 2869it [00:04, 1170.75it/s]warmup run: 2956it [00:04, 1168.60it/s]warmup run: 2872it [00:04, 1190.32it/s]warmup run: 2992it [00:04, 1181.92it/s]warmup run: 2657it [00:04, 1126.43it/s]warmup run: 3000it [00:04, 693.29it/s] warmup run: 3000it [00:04, 678.29it/s] warmup run: 2958it [00:04, 1167.91it/s]warmup run: 3000it [00:04, 695.15it/s] warmup run: 2809it [00:04, 1170.27it/s]warmup run: 3000it [00:04, 686.91it/s] warmup run: 2988it [00:04, 1173.58it/s]warmup run: 2992it [00:04, 1191.39it/s]warmup run: 3000it [00:04, 667.54it/s] warmup run: 3000it [00:04, 694.77it/s] warmup run: 2773it [00:04, 1135.80it/s]warmup run: 2929it [00:04, 1177.10it/s]warmup run: 2893it [00:04, 1153.71it/s]warmup run: 3000it [00:04, 672.14it/s] warmup run: 3000it [00:04, 665.14it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.77it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.44it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.56it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.10it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.61it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1601.10it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.82it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.50it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.60it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.11it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1638.43it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.35it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.75it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1615.56it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.34it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1626.81it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1644.89it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1636.79it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1646.13it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1630.82it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1609.15it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1639.02it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1606.01it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1627.52it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1636.69it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1642.26it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1643.06it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1627.93it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1631.51it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1612.43it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1598.17it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1626.30it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1638.13it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1640.20it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1623.31it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1598.31it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1630.73it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1626.78it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1617.25it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1597.09it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1635.85it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1598.12it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1636.29it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1636.94it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1619.35it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1622.12it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1623.81it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1591.24it/s]warmup should be done:  38%|███▊      | 1128/3000 [00:00<00:01, 1599.25it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1638.23it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1629.38it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1633.04it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1614.53it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1617.01it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1594.68it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1595.90it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1605.87it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1642.63it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1624.99it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1632.64it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1612.52it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1617.82it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1600.37it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1597.96it/s]warmup should be done:  48%|████▊     | 1454/3000 [00:00<00:00, 1611.02it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1645.17it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1628.27it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1632.73it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1613.21it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1619.26it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1600.83it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1609.60it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1616.25it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1645.62it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1628.61it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1610.01it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1620.35it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1624.00it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1602.94it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1616.85it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1619.02it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1628.66it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1648.61it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1609.74it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1621.41it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1626.15it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1604.99it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1618.14it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1650.87it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1630.12it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1622.42it/s]warmup should be done:  65%|██████▌   | 1955/3000 [00:01<00:00, 1611.25it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1622.36it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1628.13it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1603.02it/s]warmup should be done:  66%|██████▌   | 1967/3000 [00:01<00:00, 1630.91it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1623.98it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1651.32it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1630.39it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1623.51it/s]warmup should be done:  71%|███████   | 2117/3000 [00:01<00:00, 1612.01it/s]warmup should be done:  71%|███████▏  | 2139/3000 [00:01<00:00, 1623.26it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1600.34it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1636.05it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1652.24it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1630.60it/s]warmup should be done:  76%|███████▌  | 2270/3000 [00:01<00:00, 1621.36it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1612.62it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1619.36it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1624.92it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1598.04it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1641.06it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1621.48it/s]warmup should be done:  82%|████████▏ | 2462/3000 [00:01<00:00, 1626.85it/s]warmup should be done:  81%|████████▏ | 2441/3000 [00:01<00:00, 1611.68it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1623.57it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1634.75it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1612.58it/s]warmup should be done:  81%|████████  | 2423/3000 [00:01<00:00, 1594.19it/s]warmup should be done:  82%|████████▏ | 2463/3000 [00:01<00:00, 1639.53it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1621.49it/s]warmup should be done:  88%|████████▊ | 2626/3000 [00:01<00:00, 1627.99it/s]warmup should be done:  87%|████████▋ | 2603/3000 [00:01<00:00, 1611.96it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1625.64it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1614.73it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1624.67it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1593.33it/s]warmup should be done:  88%|████████▊ | 2627/3000 [00:01<00:00, 1634.17it/s]warmup should be done:  92%|█████████▏| 2759/3000 [00:01<00:00, 1620.22it/s]warmup should be done:  93%|█████████▎| 2790/3000 [00:01<00:00, 1629.82it/s]warmup should be done:  93%|█████████▎| 2777/3000 [00:01<00:00, 1617.40it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1627.66it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1597.91it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1629.53it/s]warmup should be done:  91%|█████████▏| 2743/3000 [00:01<00:00, 1560.91it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1609.16it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1623.69it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1633.40it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1633.60it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1619.69it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1636.05it/s]warmup should be done:  98%|█████████▊| 2925/3000 [00:01<00:00, 1584.16it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1555.40it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1589.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1590.64it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.77it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1537.76it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.63it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.14it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1692.02it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.50it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.57it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.23it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1685.23it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1648.82it/s]warmup should be done:  11%|█         | 318/3000 [00:00<00:01, 1595.04it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.74it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1656.57it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1637.00it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1636.39it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1681.89it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1686.93it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1618.94it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1668.61it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.39it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1636.30it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1664.40it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1638.18it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1679.65it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1692.39it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1632.46it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1679.75it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1678.09it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1645.13it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1670.43it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1641.59it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1680.83it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1695.70it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1683.09it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1642.94it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1683.36it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1655.58it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1671.39it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1650.17it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1681.70it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1696.85it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1682.45it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1646.76it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1686.54it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1660.85it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1652.49it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1667.00it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1673.94it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1693.84it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1648.69it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1688.72it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1679.84it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1664.21it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1661.69it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1666.74it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1673.20it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1696.82it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1681.67it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1690.14it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1644.44it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1668.26it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1670.31it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1662.43it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1677.62it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1697.68it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1682.54it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1691.44it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1651.22it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1672.20it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1675.33it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1665.29it/s]warmup should be done:  51%|█████     | 1521/3000 [00:00<00:00, 1676.53it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1697.32it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1693.16it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1656.64it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1681.86it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1674.45it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1668.50it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1673.83it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1674.97it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1699.16it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1694.68it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1659.10it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1675.62it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1684.35it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1667.62it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1668.53it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1659.71it/s]warmup should be done:  68%|██████▊   | 2041/3000 [00:01<00:00, 1700.62it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1695.64it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1658.97it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1685.33it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1675.42it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1661.94it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1663.24it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1661.98it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1695.30it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1699.90it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1660.33it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1684.84it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1674.98it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1654.77it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1659.60it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1663.06it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1694.94it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1698.17it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1663.03it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1685.67it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1676.71it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1652.33it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1659.79it/s]warmup should be done:  79%|███████▊  | 2359/3000 [00:01<00:00, 1665.48it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1695.24it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1663.87it/s]warmup should be done:  85%|████████▌ | 2553/3000 [00:01<00:00, 1699.79it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1686.06it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1672.51it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1660.48it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1650.40it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1667.66it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1695.02it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1700.62it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1661.84it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1685.92it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1673.02it/s]warmup should be done:  89%|████████▉ | 2672/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1646.25it/s]warmup should be done:  90%|████████▉ | 2694/3000 [00:01<00:00, 1666.39it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1693.36it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1663.43it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1698.58it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1683.29it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1673.72it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1657.91it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1643.92it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1667.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.17it/s]warmup should be done:  99%|█████████▉| 2983/3000 [00:01<00:00, 1666.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.02it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1644.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.56it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c5574310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c55722b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c55630a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c5564100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c5564100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c5876e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c587edf0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd8c58789d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 07:54:11.071520: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3fb028dd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.071577: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.080432: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.107011: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3eb030fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.107073: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.109737: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3fa833cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.109790: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.116352: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.118433: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.812187: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3fb029f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.812253: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.815254: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3f68306d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.815311: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.815534: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3f68300f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.815579: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.822180: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.823052: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.823049: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.860556: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3f6f92ae0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.860620: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.870608: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:11.874712: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd3fa830af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:54:11.874766: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:54:11.885039: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:54:18.186954: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.313428: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.493606: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.632108: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.654536: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.687107: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.708140: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:54:18.749306: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:55:07.457][ERROR][RK0][tid #140548685940480]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:55:07.458][ERROR][RK0][tid #140548685940480]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:07.463][ERROR][RK0][tid #140548685940480]: coll ps creation done
[HCTR][07:55:07.463][ERROR][RK0][tid #140548685940480]: replica 5 waits for coll ps creation barrier
[HCTR][07:55:07.708][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:55:07.708][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:07.713][ERROR][RK0][main]: coll ps creation done
[HCTR][07:55:07.713][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][07:55:07.781][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:55:07.781][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:07.788][ERROR][RK0][main]: coll ps creation done
[HCTR][07:55:07.788][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][07:55:07.971][ERROR][RK0][tid #140548761442048]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:55:07.972][ERROR][RK0][tid #140548761442048]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:07.976][ERROR][RK0][tid #140548761442048]: coll ps creation done
[HCTR][07:55:07.976][ERROR][RK0][tid #140548761442048]: replica 3 waits for coll ps creation barrier
[HCTR][07:55:07.990][ERROR][RK0][tid #140549633857280]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:55:07.990][ERROR][RK0][tid #140549633857280]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:07.997][ERROR][RK0][tid #140549633857280]: coll ps creation done
[HCTR][07:55:07.997][ERROR][RK0][tid #140549633857280]: replica 6 waits for coll ps creation barrier
[HCTR][07:55:08.002][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:55:08.002][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:08.009][ERROR][RK0][main]: coll ps creation done
[HCTR][07:55:08.009][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][07:55:08.054][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:55:08.054][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:08.059][ERROR][RK0][main]: coll ps creation done
[HCTR][07:55:08.059][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][07:55:08.117][ERROR][RK0][tid #140548962768640]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:55:08.117][ERROR][RK0][tid #140548962768640]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:55:08.124][ERROR][RK0][tid #140548962768640]: coll ps creation done
[HCTR][07:55:08.124][ERROR][RK0][tid #140548962768640]: replica 2 waits for coll ps creation barrier
[HCTR][07:55:08.124][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][07:55:08.997][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][07:55:09.046][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][tid #140548761442048]: replica 3 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][main]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][tid #140548962768640]: replica 2 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][tid #140548685940480]: replica 5 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][tid #140549633857280]: replica 6 calling init per replica
[HCTR][07:55:09.046][ERROR][RK0][main]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][tid #140548761442048]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][main]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][tid #140548962768640]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][tid #140548685940480]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][main]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][tid #140548685940480]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][tid #140549633857280]: Calling build_v2
[HCTR][07:55:09.046][ERROR][RK0][tid #140548761442048]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][tid #140548962768640]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:55:09.046][ERROR][RK0][tid #140549633857280]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 07:55:09. 50829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:55:09. 50901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 0 to cpu
[2022-12-12 07:55:09. 50919: E[ 2022-12-12 07:55:09/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 50959178[: ] Ev100x8, slow pcie 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:55:09:.212[ 50961] 2022-12-12 07:55:09: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E
 50996 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] :2022-12-12 07:55:09[v100x8, slow pcie196.
]  51031assigning 0 to cpu: 2022-12-12 07:55:09
E[. 2022-12-12 07:55:09 51014/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: :[ 51065E213[:  ] 2022-12-12 07:55:092022-12-12 07:55:09E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421.. :
 51082 51111[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178: : [:[] 2022-12-12 07:55:09E2022-12-12 07:55:09E2022-12-12 07:55:09196v100x8, slow pcie. . .] 
[ 51138/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 51189/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 51190assigning 0 to cpu[: :: :: 
2022-12-12 07:55:09E2022-12-12 07:55:09178E212E. .]  ]   51283/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 51237[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :: 2022-12-12 07:55:09
:
:E[178E.178214 [2022-12-12 07:55:09]   51373] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:55:09.v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pciecpu time is 97.0588:. 51456
:E

196 51468: 178 [] [: E]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:55:09assigning 0 to cpu2022-12-12 07:55:09Ev100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.
. 
:212 51576 51586/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196] : [: :] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E2022-12-12 07:55:09[E213assigning 0 to cpu

 .2022-12-12 07:55:09 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 51666.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:2022-12-12 07:55:09:  516932022-12-12 07:55:09:
196.E: .196] [ 51743 E 51769] assigning 0 to cpu2022-12-12 07:55:09: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : assigning 0 to cpu
.E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
 51826 196: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[:assigning 0 to cpu] :[ 2022-12-12 07:55:09213
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82122022-12-12 07:55:09/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 
] .: 51949remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 51950[214[: 

: 2022-12-12 07:55:09] 2022-12-12 07:55:09E[E.cpu time is 97.0588.[ 2022-12-12 07:55:09  52036
 520392022-12-12 07:55:09/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : .: 52080:EE 52089212: 212  : ] E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] ] [[:214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.684212022-12-12 07:55:092022-12-12 07:55:09213] 

..] cpu time is 97.0588 52307 52310[[remote time is 8.68421
: : 2022-12-12 07:55:092022-12-12 07:55:09
EE..   52368[ 52369/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 07:55:09: ::E.E213213  52416 ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421remote time is 8.68421:E:

214 213] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] cpu time is 97.05882022-12-12 07:55:09:2022-12-12 07:55:09remote time is 8.68421
.214.
 52535]  52534: [cpu time is 97.0588: E2022-12-12 07:55:09
E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 52613/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: :214E214]  ] cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588
:
214] cpu time is 97.0588
[2022-12-12 07:56:26.557062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:56:26.596964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 07:56:26.729099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:56:26.729158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:56:26.729187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:56:26.729215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:56:26.729714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:56:26.729755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.730608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.731297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.744343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 07:56:26.744412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 07:56:26.2022-12-12 07:56:26744701.: 744712E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 2022 solved] 
[4 solved[
2022-12-12 07:56:262022-12-12 07:56:26..744762[744785: 2022-12-12 07:56:26: E.E 744804 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:E:202 205] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] [7 solved:worker 0 thread 2 initing device 22022-12-12 07:56:26
205
.] 744844[worker 0 thread 4 initing device 4: 2022-12-12 07:56:26
E. 744870/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccBuilding Coll Cache with ... num gpu device is 8:
205] worker 0 thread 7 initing device 7
[2022-12-12 07:56:26.744929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.744982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[[2022-12-12 07:56:262022-12-12 07:56:26..745017745034: : EE[  2022-12-12 07:56:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.::745044202205: ] ] E3 solvedworker 0 thread 6 initing device 6 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-12 07:56:261 solved.
745110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 07:56:26205.] 745129worker 0 thread 3 initing device 3: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[[2022-12-12 07:56:262022-12-12 07:56:26..745265745266: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[2022-12-12 07:56:26.745315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[:2022-12-12 07:56:262022-12-12 07:56:261815..] 745332745332Building Coll Cache with ... num gpu device is 8: : 
EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[:19802022-12-12 07:56:261980] .] eager alloc mem 381.47 MB745378eager alloc mem 381.47 MB
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.745485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:56:26.745526: E[ 2022-12-12 07:56:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7455331980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:56:261815.] 745562Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:56:26.745598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 07:56:26] .eager alloc mem 381.47 MB745613
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.749187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.749507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.749566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.749623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.749673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.750161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.750266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.753588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.753748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.753805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.753860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.753906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.753959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.754440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:56:26.812154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:56:26.818912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:56:26.819056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:56:26.819970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.820696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.821798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:26.821850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:56:26.838187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[[[2022-12-12 07:56:262022-12-12 07:56:262022-12-12 07:56:262022-12-12 07:56:26....838243838243838243838243: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[2022-12-12 07:56:26.839154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:56:26.839201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:56:26.844249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:56:26.844358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:56:26.844850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 07:56:262022-12-12 07:56:26..844923844941: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 07:56:26.845025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:56:26.845199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:56:26.845278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 07:56:26638.] 845279eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 07:56:262022-12-12 07:56:26..845352845364: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[[2022-12-12 07:56:262022-12-12 07:56:26..845448845438: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-12 07:56:26.845533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:56:26.857935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.858434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.858939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.859455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.859982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.860488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.860998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:56:26.863264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.863304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 07:56:26] .eager alloc mem 611.00 KB863325
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.863436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.863483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.863540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.863575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:26.864332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:26[.2022-12-12 07:56:26864370.: 864377[E: 2022-12-12 07:56:26 W./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 864388:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: 638:E] 43 eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
WORKER[0] alloc host memory 95.37 MB:
638] eager release cuda mem 625663
[2022-12-12 07:56:26.864453: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 07:56:26:.43864465] : WORKER[0] alloc host memory 95.37 MBW
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:56:26.864498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:26[.2022-12-12 07:56:26864544.: 864544W:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43:] 638WORKER[0] alloc host memory 95.37 MB] 
eager release cuda mem 625663
[[2022-12-12 07:56:262022-12-12 07:56:26..864607864612: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[::2022-12-12 07:56:2663843.] ] 864635eager release cuda mem 625663WORKER[0] alloc host memory 95.37 MB: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:26.864685: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 07:56:26WORKER[0] alloc host memory 95.37 MB.
864702: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:56:26.886010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.886656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.886697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.927977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.928192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.928615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.928657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.928688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.928812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.928855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.928978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.929295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.929337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.929583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.929623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.930019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.930363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.930495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:56:26.930620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.930662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.930969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.931012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:56:26.931104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:56:26.931156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[2022-12-12 07:56:31.[2022-12-12 07:56:31310062[2022-12-12 07:56:31.[: 2022-12-12 07:56:31[.310061[E2022-12-12 07:56:312022-12-12 07:56:31.2022-12-12 07:56:31310061:  2022-12-12 07:56:31..310061.: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.310061310061: 310067E :310063: : E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926: EE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :1926Device 6 init p2p of link 0 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:] Device 5 init p2p of link 6:19261926] 1926Device 2 init p2p of link 1
1926] ] Device 7 init p2p of link 4] 
] Device 3 init p2p of link 2Device 4 init p2p of link 5
Device 0 init p2p of link 3Device 1 init p2p of link 7



[2022-12-12 07:56:31.310551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:56:311980[.] 2022-12-12 07:56:31310570eager alloc mem 611.00 KB.: [
[310575[E2022-12-12 07:56:31[2022-12-12 07:56:31[: 2022-12-12 07:56:31 .2022-12-12 07:56:31.2022-12-12 07:56:31E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu310588.310590. 310594:: 310598: 310602/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 1980E: E: :E]  E E1980 eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:1980:1980:
1980] 1980] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[2022-12-12 07:56:31.311383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.311536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.311639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 07:56:31.311663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[[2022-12-12 07:56:312022-12-12 07:56:312022-12-12 07:56:31...311688[311690311692: 2022-12-12 07:56:31: : E.EE 311709  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E::638 638638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] eager release cuda mem 625663:eager release cuda mem 625663eager release cuda mem 625663
638

] eager release cuda mem 625663
[2022-12-12 07:56:31.325095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:56:31.325257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.325317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:56:31.325473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.326225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.326438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.334741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:56:31[.2022-12-12 07:56:31334896.: 334888E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] [
Device 5 init p2p of link 42022-12-12 07:56:31
.334931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 07:56:31.335079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 07:56:31] .eager alloc mem 611.00 KB335095
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.335144: [E2022-12-12 07:56:31 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu335157:: 1926E]  Device 2 init p2p of link 3/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 7 init p2p of link 1
[2022-12-12 07:56:31.335326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:56:311980.] 335340eager alloc mem 611.00 KB: [
E2022-12-12 07:56:31 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu335345:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 0 init p2p of link 6
[2022-12-12 07:56:31.335529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.335872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.336041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 07:56:31.336062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.336148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 07:56:312022-12-12 07:56:31..336305336307: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 07:56:31.339309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 07:56:31.339423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.339541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 07:56:31.339670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.340348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.340594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.349232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 07:56:31.349352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.349913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:56:31.350040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.350324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.351004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.356995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 07:56:31.357119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.357559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 07:56:31.357699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.357790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:56:31.357907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 07:56:31eager release cuda mem 625663.
357925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.358142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 07:56:31.358273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.358474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.358861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.359196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.365234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:56:31.365349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.365687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:56:31.365808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.366299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.366755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.367916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:56:31.368032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.368213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:56:31.368336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.368968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.369280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.372043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:56:31.372160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.372446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 07:56:31.372571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.373107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.373510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.384167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 07:56:31.384294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.384520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:56:31.384645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:56:31.385070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.385421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:56:31.388990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.389252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.389424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.389826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.6449 secs 
[2022-12-12 07:56:31.390199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64461 secs 
[2022-12-12 07:56:31.390402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64508 secs 
[2022-12-12 07:56:31.390588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.391040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64571 secs 
[2022-12-12 07:56:31.393340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.393790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64827 secs 
[2022-12-12 07:56:31.394200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.394656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64905 secs 
[2022-12-12 07:56:31.396648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.397020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:56:31.397105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.65173 secs 
[2022-12-12 07:56:31.397473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.66773 secs 
[HCTR][07:56:31.397][ERROR][RK0][tid #140548761442048]: replica 3 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][tid #140548962768640]: replica 2 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][tid #140548685940480]: replica 5 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][tid #140549633857280]: replica 6 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][tid #140548761442048]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][tid #140548962768640]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][tid #140548685940480]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][tid #140549633857280]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:56:31.397][ERROR][RK0][main]: init per replica done
[HCTR][07:56:31.397][ERROR][RK0][tid #140548761442048]: init per replica done
[HCTR][07:56:31.397][ERROR][RK0][tid #140548962768640]: init per replica done
[HCTR][07:56:31.397][ERROR][RK0][tid #140548685940480]: init per replica done
[HCTR][07:56:31.397][ERROR][RK0][main]: init per replica done
[HCTR][07:56:31.397][ERROR][RK0][main]: init per replica done
[HCTR][07:56:31.397][ERROR][RK0][tid #140549633857280]: init per replica done
[HCTR][07:56:31.400][ERROR][RK0][main]: init per replica done
