2022-12-11 19:15:33.761583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.771748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.777670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.782448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.794883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.800851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.805257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.818584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.868750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.874669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.877463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.878338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.879453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.880696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.882354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.883402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.883488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.885026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.885188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.886453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.886889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.888009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.888608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.889719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.890259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.891201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.891920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.892690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.893682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.894114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.895397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.895631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.898262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.899235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.900198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.901112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.902014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.902885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.903856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.904792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.909511: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:33.912076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.913591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.915292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.915354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.916873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.917014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.918770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.918914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.919054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.921036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.921085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.921149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.921411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.923596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.923637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.923772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.923913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.926195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.926391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.926481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.928967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.929091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.930998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.931126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.933060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.934957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.937257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.937462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.940058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.940412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.941731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.942223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.942929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.942968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.944482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.944743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.945575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.945670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.961251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.961725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.962582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.962720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.963661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.964417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.965443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.965490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.966231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.973234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.977000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.983492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.997972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.999253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.999297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:33.999890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.001740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.001917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.001997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.002692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.003880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.004341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.004652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.006213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.006247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.006391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.009182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.009277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.009435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.011445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.011486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.011527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.013883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.014141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.015285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.015319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.015545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.018273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.019168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.019207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.019351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.021716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.022518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.022549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.022725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.024801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.025462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.025594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.025684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.027715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.028330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.028381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.028530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.030834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.031239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.031399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.031438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.034494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.035303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.035387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.035515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.038130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.038503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.038555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.038656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.041725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.041766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.041821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.042279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.044722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.044762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.044803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.045634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.047723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.047741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.047781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.049049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.050591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.050602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.050908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.051926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.052548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.055654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.055705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.056094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.056353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.056556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.057601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.058965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.059496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.059974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.060488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.060644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.061643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.061954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.063211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.063674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.064212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.065004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.065005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.066301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.066718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.067834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.068231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.068984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.069297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.069656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.070913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.071331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.072538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.073223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.074323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.074845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.077111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.077419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.078034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.078416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.079283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.079464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.080577: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.080743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.081151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.081909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.082238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.082970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.083108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.084466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.084768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.085635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.085956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.086943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.087186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.088303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.088579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.090312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.090869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.092556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.092889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.093927: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.094079: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.094479: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.094957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.095427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.096170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.096598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.097891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.098730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.099827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.099958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.101764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.102801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.102917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.103894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.104130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.104203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.104856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.106926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.107014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.108403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.109707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.109717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.110775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.113771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.113805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.114901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.115565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.115652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.116411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.118446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.118487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.121867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.122877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.123208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.127000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.127969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.128358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.172446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.174084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.174504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.177757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.180163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.180582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.184261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.185729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.185923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.193368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.194416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.194719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.199470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.201949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.202204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.204600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.206215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.207421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.214052: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.223579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.236830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.236961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.239451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.242214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.245626: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.246922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.254283: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:15:34.254823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.263668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.336698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.338427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.343726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:34.345146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.270901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.271853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.272604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.273076: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.273132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:15:35.291832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.292477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.292984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.293582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.294095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.294950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:15:35.343297: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.343507: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.407769: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:15:35.506767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.507413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.507943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.508411: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.508466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:15:35.524251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.524866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.525404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.526201: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.526272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:15:35.526710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.527362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.527871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.528455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.528964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.529439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:15:35.539521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.540125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.540663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.541120: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.541170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:15:35.543726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.544354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.544858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.545436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.546146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.546855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:15:35.559024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.559684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.560763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.561145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.561451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.562228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.562308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.563435: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.563437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.563485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:15:35.564091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:15:35.582284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.582923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.583453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.584025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.584558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.585136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:15:35.590564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.591207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.591740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.592260: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.592331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:15:35.594384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.594967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.595524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.595992: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.596044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:15:35.607265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.607890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.608442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.608907: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:15:35.608959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:15:35.610326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.610398: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.610589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.610926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.611466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.611810: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:15:35.612025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.612544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.613004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:15:35.614606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.615278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.615791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.616375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.616896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.617377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:15:35.626121: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.626323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.627342: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:15:35.627420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.628058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.628625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.629221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.629754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:15:35.630241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:15:35.630267: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.630444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.632256: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 19:15:35.640236: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.640396: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.642301: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:15:35.658086: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.658285: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.663497: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.663652: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.663823: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:15:35.664474: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:15:35.675854: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.676018: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:15:35.676844: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][19:15:36.940][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.940][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.940][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.940][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.940][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.941][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.941][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:15:36.941][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.59it/s]warmup run: 1it [00:00,  2.52it/s]warmup run: 1it [00:00,  2.51it/s]warmup run: 1it [00:00,  2.50it/s]warmup run: 1it [00:00,  2.49it/s]warmup run: 60it [00:00, 161.14it/s]warmup run: 69it [00:00, 181.27it/s]warmup run: 63it [00:00, 164.83it/s]warmup run: 63it [00:00, 164.59it/s]warmup run: 72it [00:00, 187.62it/s]warmup run: 109it [00:00, 251.98it/s]warmup run: 121it [00:00, 272.86it/s]warmup run: 113it [00:00, 255.16it/s]warmup run: 128it [00:00, 288.30it/s]warmup run: 116it [00:00, 263.16it/s]warmup run: 162it [00:00, 330.61it/s]warmup run: 176it [00:00, 349.96it/s]warmup run: 162it [00:00, 320.79it/s]warmup run: 184it [00:00, 363.68it/s]warmup run: 169it [00:00, 336.83it/s]warmup run: 219it [00:00, 398.26it/s]warmup run: 232it [00:00, 409.13it/s]warmup run: 212it [00:00, 370.58it/s]warmup run: 222it [00:00, 390.77it/s]warmup run: 240it [00:00, 417.57it/s]warmup run: 272it [00:00, 435.35it/s]warmup run: 288it [00:00, 451.89it/s]warmup run: 262it [00:00, 406.46it/s]warmup run: 295it [00:00, 455.35it/s]warmup run: 276it [00:00, 432.10it/s]warmup run: 323it [00:00, 457.04it/s]warmup run: 344it [00:01, 483.18it/s]warmup run: 313it [00:01, 434.36it/s]warmup run: 349it [00:01, 478.00it/s]warmup run: 332it [00:01, 467.77it/s]warmup run: 375it [00:01, 473.76it/s]warmup run: 401it [00:01, 506.67it/s]warmup run: 365it [00:01, 457.92it/s]warmup run: 403it [00:01, 495.03it/s]warmup run: 385it [00:01, 476.36it/s]warmup run: 429it [00:01, 491.89it/s]warmup run: 458it [00:01, 523.75it/s]warmup run: 460it [00:01, 516.37it/s]warmup run: 417it [00:01, 474.30it/s]warmup run: 437it [00:01, 479.17it/s]warmup run: 486it [00:01, 514.27it/s]warmup run: 515it [00:01, 535.84it/s]warmup run: 517it [00:01, 531.57it/s]warmup run: 469it [00:01, 486.47it/s]warmup run: 492it [00:01, 498.03it/s]warmup run: 544it [00:01, 531.92it/s]warmup run: 1it [00:01,  1.40s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 573it [00:01, 546.09it/s]warmup run: 575it [00:01, 543.96it/s]warmup run: 520it [00:01, 477.97it/s]warmup run: 548it [00:01, 515.61it/s]warmup run: 600it [00:01, 538.61it/s]warmup run: 99it [00:01, 91.51it/s]warmup run: 95it [00:01, 87.81it/s]warmup run: 96it [00:01, 88.69it/s]warmup run: 630it [00:01, 551.57it/s]warmup run: 631it [00:01, 546.18it/s]warmup run: 570it [00:01, 481.33it/s]warmup run: 605it [00:01, 528.90it/s]warmup run: 657it [00:01, 545.12it/s]warmup run: 196it [00:01, 194.14it/s]warmup run: 189it [00:01, 187.28it/s]warmup run: 193it [00:01, 191.60it/s]warmup run: 687it [00:01, 549.41it/s]warmup run: 687it [00:01, 531.74it/s]warmup run: 662it [00:01, 539.09it/s]warmup run: 620it [00:01, 475.71it/s]warmup run: 714it [00:01, 549.77it/s]warmup run: 293it [00:01, 304.92it/s]warmup run: 285it [00:01, 297.50it/s]warmup run: 287it [00:01, 298.39it/s]warmup run: 743it [00:01, 549.01it/s]warmup run: 719it [00:01, 546.46it/s]warmup run: 669it [00:01, 476.42it/s]warmup run: 741it [00:01, 511.21it/s]warmup run: 392it [00:01, 420.72it/s]warmup run: 771it [00:01, 553.68it/s]warmup run: 382it [00:01, 410.85it/s]warmup run: 381it [00:01, 406.42it/s]warmup run: 799it [00:01, 544.97it/s]warmup run: 775it [00:01, 549.26it/s]warmup run: 722it [00:01, 491.47it/s]warmup run: 793it [00:01, 500.10it/s]warmup run: 491it [00:01, 531.45it/s]warmup run: 480it [00:01, 521.43it/s]warmup run: 828it [00:01, 556.28it/s]warmup run: 475it [00:01, 509.83it/s]warmup run: 854it [00:01, 544.59it/s]warmup run: 832it [00:01, 554.75it/s]warmup run: 772it [00:01, 491.92it/s]warmup run: 844it [00:01, 495.03it/s]warmup run: 590it [00:01, 630.76it/s]warmup run: 577it [00:02, 618.80it/s]warmup run: 885it [00:02, 559.22it/s]warmup run: 569it [00:02, 602.45it/s]warmup run: 911it [00:02, 549.93it/s]warmup run: 889it [00:02, 559.21it/s]warmup run: 822it [00:02, 490.88it/s]warmup run: 894it [00:02, 489.38it/s]warmup run: 689it [00:02, 715.36it/s]warmup run: 675it [00:02, 703.33it/s]warmup run: 662it [00:02, 678.95it/s]warmup run: 942it [00:02, 556.56it/s]warmup run: 967it [00:02, 549.72it/s]warmup run: 946it [00:02, 559.72it/s]warmup run: 872it [00:02, 489.72it/s]warmup run: 944it [00:02, 486.38it/s]warmup run: 789it [00:02, 786.33it/s]warmup run: 773it [00:02, 771.98it/s]warmup run: 758it [00:02, 748.32it/s]warmup run: 998it [00:02, 550.56it/s]warmup run: 1024it [00:02, 554.66it/s]warmup run: 1003it [00:02, 549.52it/s]warmup run: 922it [00:02, 491.02it/s]warmup run: 998it [00:02, 499.94it/s]warmup run: 888it [00:02, 840.32it/s]warmup run: 871it [00:02, 824.77it/s]warmup run: 852it [00:02, 797.25it/s]warmup run: 1054it [00:02, 551.60it/s]warmup run: 1081it [00:02, 558.16it/s]warmup run: 972it [00:02, 491.14it/s]warmup run: 1059it [00:02, 544.16it/s]warmup run: 1052it [00:02, 509.25it/s]warmup run: 987it [00:02, 881.20it/s]warmup run: 969it [00:02, 866.30it/s]warmup run: 946it [00:02, 834.04it/s]warmup run: 1110it [00:02, 542.23it/s]warmup run: 1138it [00:02, 559.54it/s]warmup run: 1022it [00:02, 489.55it/s]warmup run: 1114it [00:02, 536.84it/s]warmup run: 1105it [00:02, 514.31it/s]warmup run: 1086it [00:02, 911.48it/s]warmup run: 1067it [00:02, 897.68it/s]warmup run: 1040it [00:02, 863.59it/s]warmup run: 1165it [00:02, 536.37it/s]warmup run: 1195it [00:02, 559.85it/s]warmup run: 1071it [00:02, 489.59it/s]warmup run: 1168it [00:02, 531.92it/s]warmup run: 1160it [00:02, 522.80it/s]warmup run: 1186it [00:02, 935.65it/s]warmup run: 1166it [00:02, 921.83it/s]warmup run: 1134it [00:02, 885.09it/s]warmup run: 1221it [00:02, 542.58it/s]warmup run: 1252it [00:02, 560.92it/s]warmup run: 1121it [00:02, 490.07it/s]warmup run: 1222it [00:02, 522.25it/s]warmup run: 1217it [00:02, 534.11it/s]warmup run: 1287it [00:02, 956.80it/s]warmup run: 1265it [00:02, 939.75it/s]warmup run: 1228it [00:02, 896.05it/s]warmup run: 1278it [00:02, 549.39it/s]warmup run: 1309it [00:02, 561.99it/s]warmup run: 1176it [00:02, 504.45it/s]warmup run: 1275it [00:02, 514.72it/s]warmup run: 1273it [00:02, 539.01it/s]warmup run: 1364it [00:02, 951.89it/s]warmup run: 1321it [00:02, 892.61it/s]warmup run: 1387it [00:02, 931.66it/s]warmup run: 1335it [00:02, 554.73it/s]warmup run: 1366it [00:02, 563.35it/s]warmup run: 1230it [00:02, 512.37it/s]warmup run: 1327it [00:02, 510.47it/s]warmup run: 1329it [00:02, 542.76it/s]warmup run: 1392it [00:02, 557.36it/s]warmup run: 1423it [00:02, 564.32it/s]warmup run: 1282it [00:02, 503.92it/s]warmup run: 1385it [00:02, 545.41it/s]warmup run: 1379it [00:02, 505.99it/s]warmup run: 1462it [00:02, 794.26it/s]warmup run: 1413it [00:02, 739.17it/s]warmup run: 1483it [00:02, 760.94it/s]warmup run: 1448it [00:03, 551.66it/s]warmup run: 1480it [00:03, 564.77it/s]warmup run: 1333it [00:03, 496.81it/s]warmup run: 1443it [00:03, 553.88it/s]warmup run: 1430it [00:03, 499.62it/s]warmup run: 1505it [00:03, 554.65it/s]warmup run: 1548it [00:03, 710.38it/s]warmup run: 1537it [00:03, 558.77it/s]warmup run: 1493it [00:03, 666.27it/s]warmup run: 1566it [00:03, 664.51it/s]warmup run: 1383it [00:03, 491.66it/s]warmup run: 1500it [00:03, 556.98it/s]warmup run: 1480it [00:03, 491.17it/s]warmup run: 1561it [00:03, 551.44it/s]warmup run: 1593it [00:03, 558.80it/s]warmup run: 1433it [00:03, 488.84it/s]warmup run: 1565it [00:03, 624.92it/s]warmup run: 1556it [00:03, 547.67it/s]warmup run: 1530it [00:03, 483.49it/s]warmup run: 1625it [00:03, 624.51it/s]warmup run: 1639it [00:03, 616.16it/s]warmup run: 1617it [00:03, 550.86it/s]warmup run: 1650it [00:03, 561.76it/s]warmup run: 1482it [00:03, 486.36it/s]warmup run: 1611it [00:03, 543.54it/s]warmup run: 1632it [00:03, 601.32it/s]warmup run: 1579it [00:03, 477.84it/s]warmup run: 1693it [00:03, 602.88it/s]warmup run: 1674it [00:03, 555.66it/s]warmup run: 1707it [00:03, 558.79it/s]warmup run: 1706it [00:03, 570.12it/s]warmup run: 1531it [00:03, 485.78it/s]warmup run: 1666it [00:03, 537.61it/s]warmup run: 1630it [00:03, 485.43it/s]warmup run: 1695it [00:03, 590.44it/s]warmup run: 1730it [00:03, 553.81it/s]warmup run: 1757it [00:03, 586.85it/s]warmup run: 1764it [00:03, 558.84it/s]warmup run: 1767it [00:03, 559.36it/s]warmup run: 1580it [00:03, 484.53it/s]warmup run: 1723it [00:03, 545.07it/s]warmup run: 1686it [00:03, 507.04it/s]warmup run: 1756it [00:03, 581.10it/s]warmup run: 1786it [00:03, 555.49it/s]warmup run: 1821it [00:03, 559.55it/s]warmup run: 1818it [00:03, 582.61it/s]warmup run: 1825it [00:03, 559.99it/s]warmup run: 1636it [00:03, 505.81it/s]warmup run: 1780it [00:03, 551.50it/s]warmup run: 1744it [00:03, 526.12it/s]warmup run: 1816it [00:03, 570.78it/s]warmup run: 1842it [00:03, 553.72it/s]warmup run: 1877it [00:03, 555.46it/s]warmup run: 1878it [00:03, 564.20it/s]warmup run: 1692it [00:03, 519.78it/s]warmup run: 1883it [00:03, 544.48it/s]warmup run: 1836it [00:03, 542.39it/s]warmup run: 1801it [00:03, 538.65it/s]warmup run: 1875it [00:03, 574.89it/s]warmup run: 1898it [00:03, 555.41it/s]warmup run: 1933it [00:03, 553.08it/s]warmup run: 1936it [00:03, 553.18it/s]warmup run: 1749it [00:03, 533.01it/s]warmup run: 1939it [00:03, 544.83it/s]warmup run: 1858it [00:03, 546.36it/s]warmup run: 1891it [00:03, 528.06it/s]warmup run: 1934it [00:03, 578.34it/s]warmup run: 1955it [00:03, 559.53it/s]warmup run: 1990it [00:03, 557.04it/s]warmup run: 1992it [00:03, 537.21it/s]warmup run: 1803it [00:03, 519.84it/s]warmup run: 1996it [00:03, 549.85it/s]warmup run: 1915it [00:04, 550.86it/s]warmup run: 1944it [00:04, 518.35it/s]warmup run: 1993it [00:04, 581.53it/s]warmup run: 2012it [00:04, 561.09it/s]warmup run: 2048it [00:04, 560.95it/s]warmup run: 2053it [00:04, 554.12it/s]warmup run: 1856it [00:04, 518.94it/s]warmup run: 2047it [00:04, 517.19it/s]warmup run: 1971it [00:04, 533.64it/s]warmup run: 1996it [00:04, 512.29it/s]warmup run: 2052it [00:04, 571.81it/s]warmup run: 2069it [00:04, 563.19it/s]warmup run: 2106it [00:04, 564.24it/s]warmup run: 2110it [00:04, 558.09it/s]warmup run: 1911it [00:04, 526.34it/s]warmup run: 2099it [00:04, 512.59it/s]warmup run: 2025it [00:04, 531.94it/s]warmup run: 2048it [00:04, 511.13it/s]warmup run: 2127it [00:04, 565.58it/s]warmup run: 2110it [00:04, 565.89it/s]warmup run: 2164it [00:04, 565.83it/s]warmup run: 2167it [00:04, 560.53it/s]warmup run: 1968it [00:04, 536.95it/s]warmup run: 2155it [00:04, 523.29it/s]warmup run: 2082it [00:04, 541.18it/s]warmup run: 2100it [00:04, 506.48it/s]warmup run: 2185it [00:04, 568.58it/s]warmup run: 2167it [00:04, 565.83it/s]warmup run: 2221it [00:04, 565.65it/s]warmup run: 2224it [00:04, 561.70it/s]warmup run: 2025it [00:04, 544.53it/s]warmup run: 2210it [00:04, 528.55it/s]warmup run: 2139it [00:04, 548.88it/s]warmup run: 2151it [00:04, 501.94it/s]warmup run: 2243it [00:04, 569.33it/s]warmup run: 2224it [00:04, 564.61it/s]warmup run: 2278it [00:04, 563.88it/s]warmup run: 2281it [00:04, 561.78it/s]warmup run: 2083it [00:04, 552.92it/s]warmup run: 2267it [00:04, 538.99it/s]warmup run: 2194it [00:04, 537.95it/s]warmup run: 2202it [00:04, 496.20it/s]warmup run: 2300it [00:04, 569.45it/s]warmup run: 2336it [00:04, 565.73it/s]warmup run: 2281it [00:04, 540.07it/s]warmup run: 2338it [00:04, 559.61it/s]warmup run: 2141it [00:04, 559.70it/s]warmup run: 2324it [00:04, 546.64it/s]warmup run: 2255it [00:04, 505.62it/s]warmup run: 2248it [00:04, 529.36it/s]warmup run: 2357it [00:04, 567.17it/s]warmup run: 2393it [00:04, 565.29it/s]warmup run: 2336it [00:04, 533.60it/s]warmup run: 2198it [00:04, 560.94it/s]warmup run: 2395it [00:04, 550.89it/s]warmup run: 2380it [00:04, 550.36it/s]warmup run: 2312it [00:04, 524.13it/s]warmup run: 2302it [00:04, 514.90it/s]warmup run: 2414it [00:04, 564.06it/s]warmup run: 2450it [00:04, 564.87it/s]warmup run: 2390it [00:04, 530.65it/s]warmup run: 2255it [00:04, 559.55it/s]warmup run: 2451it [00:04, 552.84it/s]warmup run: 2438it [00:04, 556.48it/s]warmup run: 2369it [00:04, 536.44it/s]warmup run: 2357it [00:04, 523.25it/s]warmup run: 2471it [00:04, 558.88it/s]warmup run: 2507it [00:04, 563.88it/s]warmup run: 2444it [00:04, 530.20it/s]warmup run: 2312it [00:04, 561.06it/s]warmup run: 2507it [00:04, 544.27it/s]warmup run: 2495it [00:04, 558.92it/s]warmup run: 2425it [00:04, 541.07it/s]warmup run: 2531it [00:04, 569.34it/s]warmup run: 2410it [00:04, 513.80it/s]warmup run: 2565it [00:04, 565.93it/s]warmup run: 2498it [00:04, 510.47it/s]warmup run: 2369it [00:05, 559.08it/s]warmup run: 2552it [00:05, 560.38it/s]warmup run: 2562it [00:05, 537.92it/s]warmup run: 2480it [00:05, 528.99it/s]warmup run: 2465it [00:05, 524.04it/s]warmup run: 2588it [00:05, 559.37it/s]warmup run: 2622it [00:05, 564.25it/s]warmup run: 2552it [00:05, 518.10it/s]warmup run: 2425it [00:05, 557.40it/s]warmup run: 2609it [00:05, 561.58it/s]warmup run: 2616it [00:05, 536.55it/s]warmup run: 2523it [00:05, 538.53it/s]warmup run: 2533it [00:05, 515.02it/s]warmup run: 2606it [00:05, 522.09it/s]warmup run: 2481it [00:05, 552.36it/s]warmup run: 2666it [00:05, 562.00it/s]warmup run: 2673it [00:05, 546.08it/s]warmup run: 2581it [00:05, 548.08it/s]warmup run: 2585it [00:05, 505.96it/s]warmup run: 2659it [00:05, 520.66it/s]warmup run: 2537it [00:05, 546.71it/s]warmup run: 2644it [00:05, 365.28it/s]warmup run: 2679it [00:05, 361.23it/s]warmup run: 2638it [00:05, 553.78it/s]warmup run: 2636it [00:05, 500.92it/s]warmup run: 2594it [00:05, 552.33it/s]warmup run: 2701it [00:05, 407.82it/s]warmup run: 2735it [00:05, 403.36it/s]warmup run: 2723it [00:05, 358.79it/s]warmup run: 2728it [00:05, 347.54it/s]warmup run: 2758it [00:05, 444.12it/s]warmup run: 2792it [00:05, 441.06it/s]warmup run: 2712it [00:05, 341.22it/s]warmup run: 2780it [00:05, 402.95it/s]warmup run: 2785it [00:05, 393.62it/s]warmup run: 2815it [00:05, 473.78it/s]warmup run: 2849it [00:05, 472.03it/s]warmup run: 2687it [00:05, 321.60it/s]warmup run: 2762it [00:05, 374.95it/s]warmup run: 2694it [00:05, 324.18it/s]warmup run: 2834it [00:05, 434.40it/s]warmup run: 2650it [00:05, 350.87it/s]warmup run: 2842it [00:05, 433.46it/s]warmup run: 2872it [00:05, 497.82it/s]warmup run: 2906it [00:05, 496.80it/s]warmup run: 2737it [00:05, 357.42it/s]warmup run: 2815it [00:05, 409.42it/s]warmup run: 2751it [00:05, 371.90it/s]warmup run: 2704it [00:05, 390.19it/s]warmup run: 2885it [00:05, 448.07it/s]warmup run: 2899it [00:05, 466.25it/s]warmup run: 2929it [00:05, 515.28it/s]warmup run: 2963it [00:05, 514.51it/s]warmup run: 2787it [00:05, 390.00it/s]warmup run: 2864it [00:05, 427.95it/s]warmup run: 2808it [00:05, 415.40it/s]warmup run: 2755it [00:05, 417.06it/s]warmup run: 2936it [00:05, 463.70it/s]warmup run: 2954it [00:05, 486.19it/s]warmup run: 3000it [00:05, 505.58it/s]warmup run: 2986it [00:05, 528.96it/s]warmup run: 2844it [00:05, 432.94it/s]warmup run: 3000it [00:05, 501.51it/s]warmup run: 2914it [00:05, 445.96it/s]warmup run: 2859it [00:06, 435.29it/s]warmup run: 3000it [00:06, 499.24it/s]warmup run: 2808it [00:06, 444.34it/s]warmup run: 2988it [00:06, 478.84it/s]warmup run: 3000it [00:06, 496.22it/s]warmup run: 2898it [00:06, 460.45it/s]warmup run: 2969it [00:06, 472.50it/s]warmup run: 2914it [00:06, 463.86it/s]warmup run: 2863it [00:06, 469.59it/s]warmup run: 3000it [00:06, 488.08it/s]warmup run: 2955it [00:06, 488.68it/s]warmup run: 2968it [00:06, 482.73it/s]warmup run: 2917it [00:06, 487.97it/s]warmup run: 3000it [00:06, 479.09it/s]warmup run: 3000it [00:06, 478.23it/s]warmup run: 2972it [00:06, 504.49it/s]warmup run: 3000it [00:06, 470.41it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.36it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.55it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.49it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.94it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1652.98it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1682.49it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1673.14it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1670.99it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1692.33it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1685.46it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1693.95it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1659.28it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.79it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1664.95it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1686.72it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1684.61it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1692.54it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1683.56it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1698.32it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1686.11it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1669.32it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1661.03it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1685.82it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1682.01it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1695.34it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1687.13it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1686.48it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1664.84it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1683.09it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1657.98it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1679.75it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1683.30it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1694.98it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1686.31it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1684.63it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1683.29it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1666.77it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1674.01it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1650.61it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1689.94it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1684.52it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1689.68it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1666.84it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1681.55it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1683.20it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1691.32it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1645.88it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1663.16it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1682.39it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1666.83it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1684.88it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1689.93it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1694.00it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1673.08it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1642.22it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1659.24it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1683.97it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1668.10it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1678.91it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1695.78it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:00, 1681.94it/s]warmup should be done:  45%|     | 1351/3000 [00:00<00:00, 1667.84it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1640.19it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1656.97it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1682.98it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1680.27it/s]warmup should be done:  51%|     | 1527/3000 [00:00<00:00, 1684.23it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1662.39it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1697.35it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1665.47it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1642.51it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1657.30it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1684.43it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1679.74it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1680.41it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1696.61it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1661.95it/s]warmup should be done:  55%|    | 1661/3000 [00:01<00:00, 1650.62it/s]warmup should be done:  56%|    | 1685/3000 [00:01<00:00, 1661.86it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1662.60it/s]warmup should be done:  62%|   | 1859/3000 [00:01<00:00, 1682.40it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1687.18it/s]warmup should be done:  62%|   | 1859/3000 [00:01<00:00, 1681.15it/s]warmup should be done:  62%|   | 1872/3000 [00:01<00:00, 1694.98it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1673.87it/s]warmup should be done:  61%|    | 1828/3000 [00:01<00:00, 1656.43it/s]warmup should be done:  62%|   | 1848/3000 [00:01<00:00, 1669.39it/s]warmup should be done:  62%|   | 1852/3000 [00:01<00:00, 1655.03it/s]warmup should be done:  68%|   | 2029/3000 [00:01<00:00, 1686.33it/s]warmup should be done:  68%|   | 2035/3000 [00:01<00:00, 1684.79it/s]warmup should be done:  68%|   | 2028/3000 [00:01<00:00, 1680.89it/s]warmup should be done:  68%|   | 2042/3000 [00:01<00:00, 1694.56it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1676.91it/s]warmup should be done:  66%|   | 1995/3000 [00:01<00:00, 1660.34it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1674.08it/s]warmup should be done:  67%|   | 2018/3000 [00:01<00:00, 1651.57it/s]warmup should be done:  73%|  | 2199/3000 [00:01<00:00, 1690.26it/s]warmup should be done:  74%|  | 2212/3000 [00:01<00:00, 1694.80it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1676.85it/s]warmup should be done:  73%|  | 2197/3000 [00:01<00:00, 1680.36it/s]warmup should be done:  73%|  | 2204/3000 [00:01<00:00, 1681.14it/s]warmup should be done:  72%|  | 2163/3000 [00:01<00:00, 1663.63it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1676.40it/s]warmup should be done:  73%|  | 2184/3000 [00:01<00:00, 1648.17it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1692.73it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1677.40it/s]warmup should be done:  79%|  | 2382/3000 [00:01<00:00, 1694.70it/s]warmup should be done:  79%|  | 2366/3000 [00:01<00:00, 1680.37it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1679.57it/s]warmup should be done:  78%|  | 2331/3000 [00:01<00:00, 1666.55it/s]warmup should be done:  78%|  | 2355/3000 [00:01<00:00, 1678.37it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1646.16it/s]warmup should be done:  85%| | 2540/3000 [00:01<00:00, 1696.34it/s]warmup should be done:  84%| | 2518/3000 [00:01<00:00, 1679.74it/s]warmup should be done:  85%| | 2552/3000 [00:01<00:00, 1695.48it/s]warmup should be done:  84%| | 2535/3000 [00:01<00:00, 1680.78it/s]warmup should be done:  85%| | 2542/3000 [00:01<00:00, 1681.27it/s]warmup should be done:  83%| | 2499/3000 [00:01<00:00, 1669.30it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1680.61it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1645.41it/s]warmup should be done:  90%| | 2711/3000 [00:01<00:00, 1699.44it/s]warmup should be done:  91%| | 2722/3000 [00:01<00:00, 1696.55it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1681.35it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1681.22it/s]warmup should be done:  90%| | 2711/3000 [00:01<00:00, 1682.39it/s]warmup should be done:  89%| | 2667/3000 [00:01<00:00, 1670.47it/s]warmup should be done:  90%| | 2693/3000 [00:01<00:00, 1681.89it/s]warmup should be done:  89%| | 2680/3000 [00:01<00:00, 1647.22it/s]warmup should be done:  96%|| 2882/3000 [00:01<00:00, 1702.35it/s]warmup should be done:  96%|| 2893/3000 [00:01<00:00, 1699.20it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1678.59it/s]warmup should be done:  96%|| 2874/3000 [00:01<00:00, 1684.94it/s]warmup should be done:  96%|| 2880/3000 [00:01<00:00, 1681.76it/s]warmup should be done:  95%|| 2863/3000 [00:01<00:00, 1685.38it/s]warmup should be done:  94%|| 2835/3000 [00:01<00:00, 1659.68it/s]warmup should be done:  95%|| 2846/3000 [00:01<00:00, 1649.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1694.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.21it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1685.33it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1682.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.49it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1670.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1657.23it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1729.75it/s]warmup should be done:   6%|         | 174/3000 [00:00<00:01, 1736.56it/s]warmup should be done:   6%|         | 175/3000 [00:00<00:01, 1745.00it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1726.56it/s]warmup should be done:   6%|         | 176/3000 [00:00<00:01, 1755.54it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1723.66it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1703.72it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1710.84it/s]warmup should be done:  12%|        | 350/3000 [00:00<00:01, 1747.94it/s]warmup should be done:  12%|        | 348/3000 [00:00<00:01, 1736.34it/s]warmup should be done:  12%|        | 353/3000 [00:00<00:01, 1759.91it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1714.08it/s]warmup should be done:  12%|        | 347/3000 [00:00<00:01, 1730.39it/s]warmup should be done:  12%|        | 347/3000 [00:00<00:01, 1729.51it/s]warmup should be done:  12%|        | 352/3000 [00:00<00:01, 1754.21it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1708.88it/s]warmup should be done:  17%|        | 522/3000 [00:00<00:01, 1737.81it/s]warmup should be done:  18%|        | 526/3000 [00:00<00:01, 1751.51it/s]warmup should be done:  18%|        | 530/3000 [00:00<00:01, 1762.24it/s]warmup should be done:  17%|        | 521/3000 [00:00<00:01, 1732.13it/s]warmup should be done:  17%|        | 517/3000 [00:00<00:01, 1717.96it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1711.97it/s]warmup should be done:  17%|        | 523/3000 [00:00<00:01, 1740.16it/s]warmup should be done:  18%|        | 529/3000 [00:00<00:01, 1756.92it/s]warmup should be done:  23%|       | 698/3000 [00:00<00:01, 1744.83it/s]warmup should be done:  23%|       | 702/3000 [00:00<00:01, 1749.87it/s]warmup should be done:  23%|       | 695/3000 [00:00<00:01, 1732.94it/s]warmup should be done:  23%|       | 689/3000 [00:00<00:01, 1716.97it/s]warmup should be done:  23%|       | 687/3000 [00:00<00:01, 1711.83it/s]warmup should be done:  24%|       | 705/3000 [00:00<00:01, 1755.49it/s]warmup should be done:  24%|       | 707/3000 [00:00<00:01, 1758.15it/s]warmup should be done:  23%|       | 698/3000 [00:00<00:01, 1691.48it/s]warmup should be done:  29%|       | 861/3000 [00:00<00:01, 1715.65it/s]warmup should be done:  29%|       | 873/3000 [00:00<00:01, 1739.44it/s]warmup should be done:  29%|       | 877/3000 [00:00<00:01, 1745.61it/s]warmup should be done:  29%|       | 859/3000 [00:00<00:01, 1710.40it/s]warmup should be done:  29%|       | 883/3000 [00:00<00:01, 1747.67it/s]warmup should be done:  29%|       | 881/3000 [00:00<00:01, 1744.21it/s]warmup should be done:  29%|       | 869/3000 [00:00<00:01, 1716.86it/s]warmup should be done:  29%|       | 873/3000 [00:00<00:01, 1709.94it/s]warmup should be done:  35%|      | 1047/3000 [00:00<00:01, 1739.31it/s]warmup should be done:  34%|      | 1033/3000 [00:00<00:01, 1714.46it/s]warmup should be done:  35%|      | 1053/3000 [00:00<00:01, 1747.35it/s]warmup should be done:  34%|      | 1031/3000 [00:00<00:01, 1709.43it/s]warmup should be done:  35%|      | 1056/3000 [00:00<00:01, 1741.37it/s]warmup should be done:  35%|      | 1043/3000 [00:00<00:01, 1723.12it/s]warmup should be done:  35%|      | 1058/3000 [00:00<00:01, 1742.50it/s]warmup should be done:  35%|      | 1048/3000 [00:00<00:01, 1722.61it/s]warmup should be done:  41%|      | 1221/3000 [00:00<00:01, 1737.81it/s]warmup should be done:  40%|      | 1207/3000 [00:00<00:01, 1720.46it/s]warmup should be done:  41%|      | 1228/3000 [00:00<00:01, 1746.44it/s]warmup should be done:  40%|      | 1202/3000 [00:00<00:01, 1704.17it/s]warmup should be done:  41%|      | 1217/3000 [00:00<00:01, 1726.85it/s]warmup should be done:  41%|      | 1231/3000 [00:00<00:01, 1738.65it/s]warmup should be done:  41%|      | 1233/3000 [00:00<00:01, 1739.34it/s]warmup should be done:  41%|      | 1221/3000 [00:00<00:01, 1716.21it/s]warmup should be done:  47%|     | 1396/3000 [00:00<00:00, 1740.18it/s]warmup should be done:  46%|     | 1383/3000 [00:00<00:00, 1730.55it/s]warmup should be done:  47%|     | 1404/3000 [00:00<00:00, 1749.60it/s]warmup should be done:  46%|     | 1373/3000 [00:00<00:00, 1703.29it/s]warmup should be done:  46%|     | 1391/3000 [00:00<00:00, 1730.98it/s]warmup should be done:  47%|     | 1406/3000 [00:00<00:00, 1740.30it/s]warmup should be done:  47%|     | 1407/3000 [00:00<00:00, 1737.34it/s]warmup should be done:  46%|     | 1393/3000 [00:00<00:00, 1695.99it/s]warmup should be done:  52%|    | 1558/3000 [00:00<00:00, 1736.20it/s]warmup should be done:  52%|    | 1571/3000 [00:00<00:00, 1738.50it/s]warmup should be done:  53%|    | 1580/3000 [00:00<00:00, 1750.81it/s]warmup should be done:  51%|    | 1544/3000 [00:00<00:00, 1701.69it/s]warmup should be done:  52%|    | 1565/3000 [00:00<00:00, 1731.85it/s]warmup should be done:  53%|    | 1581/3000 [00:00<00:00, 1741.57it/s]warmup should be done:  53%|    | 1582/3000 [00:00<00:00, 1740.30it/s]warmup should be done:  52%|    | 1563/3000 [00:00<00:00, 1689.30it/s]warmup should be done:  58%|    | 1733/3000 [00:01<00:00, 1740.23it/s]warmup should be done:  59%|    | 1756/3000 [00:01<00:00, 1751.13it/s]warmup should be done:  58%|    | 1747/3000 [00:01<00:00, 1742.24it/s]warmup should be done:  57%|    | 1715/3000 [00:01<00:00, 1702.21it/s]warmup should be done:  58%|    | 1739/3000 [00:01<00:00, 1731.75it/s]warmup should be done:  59%|    | 1758/3000 [00:01<00:00, 1745.55it/s]warmup should be done:  59%|    | 1758/3000 [00:01<00:00, 1748.29it/s]warmup should be done:  58%|    | 1732/3000 [00:01<00:00, 1686.56it/s]warmup should be done:  64%|   | 1909/3000 [00:01<00:00, 1743.89it/s]warmup should be done:  64%|   | 1924/3000 [00:01<00:00, 1747.87it/s]warmup should be done:  64%|   | 1932/3000 [00:01<00:00, 1750.33it/s]warmup should be done:  63%|   | 1887/3000 [00:01<00:00, 1706.43it/s]warmup should be done:  64%|   | 1913/3000 [00:01<00:00, 1732.69it/s]warmup should be done:  64%|   | 1934/3000 [00:01<00:00, 1748.98it/s]warmup should be done:  64%|   | 1934/3000 [00:01<00:00, 1751.34it/s]warmup should be done:  63%|   | 1901/3000 [00:01<00:00, 1684.91it/s]warmup should be done:  69%|   | 2084/3000 [00:01<00:00, 1743.75it/s]warmup should be done:  70%|   | 2100/3000 [00:01<00:00, 1750.60it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1751.10it/s]warmup should be done:  69%|   | 2058/3000 [00:01<00:00, 1707.04it/s]warmup should be done:  70%|   | 2087/3000 [00:01<00:00, 1733.53it/s]warmup should be done:  70%|   | 2110/3000 [00:01<00:00, 1749.97it/s]warmup should be done:  70%|   | 2111/3000 [00:01<00:00, 1754.58it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1684.54it/s]warmup should be done:  75%|  | 2259/3000 [00:01<00:00, 1742.90it/s]warmup should be done:  76%|  | 2277/3000 [00:01<00:00, 1753.71it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1751.20it/s]warmup should be done:  74%|  | 2229/3000 [00:01<00:00, 1706.79it/s]warmup should be done:  75%|  | 2261/3000 [00:01<00:00, 1734.34it/s]warmup should be done:  76%|  | 2288/3000 [00:01<00:00, 1758.52it/s]warmup should be done:  76%|  | 2285/3000 [00:01<00:00, 1748.17it/s]warmup should be done:  75%|  | 2239/3000 [00:01<00:00, 1682.53it/s]warmup should be done:  81%|  | 2434/3000 [00:01<00:00, 1739.88it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1749.91it/s]warmup should be done:  80%|  | 2400/3000 [00:01<00:00, 1704.88it/s]warmup should be done:  82%| | 2461/3000 [00:01<00:00, 1751.10it/s]warmup should be done:  82%| | 2465/3000 [00:01<00:00, 1759.87it/s]warmup should be done:  81%|  | 2435/3000 [00:01<00:00, 1727.68it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1739.95it/s]warmup should be done:  80%|  | 2408/3000 [00:01<00:00, 1682.67it/s]warmup should be done:  87%| | 2609/3000 [00:01<00:00, 1742.29it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1750.74it/s]warmup should be done:  86%| | 2572/3000 [00:01<00:00, 1708.46it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1759.43it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1753.96it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1727.57it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1747.68it/s]warmup should be done:  86%| | 2583/3000 [00:01<00:00, 1701.16it/s]warmup should be done:  93%|| 2785/3000 [00:01<00:00, 1744.91it/s]warmup should be done:  91%|| 2743/3000 [00:01<00:00, 1708.73it/s]warmup should be done:  94%|| 2812/3000 [00:01<00:00, 1750.96it/s]warmup should be done:  94%|| 2814/3000 [00:01<00:00, 1754.43it/s]warmup should be done:  93%|| 2782/3000 [00:01<00:00, 1730.72it/s]warmup should be done:  94%|| 2807/3000 [00:01<00:00, 1753.54it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1738.75it/s]warmup should be done:  92%|| 2757/3000 [00:01<00:00, 1709.77it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1745.07it/s]warmup should be done: 100%|| 2988/3000 [00:01<00:00, 1751.11it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1755.76it/s]warmup should be done:  99%|| 2956/3000 [00:01<00:00, 1733.43it/s]warmup should be done:  99%|| 2983/3000 [00:01<00:00, 1748.98it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1690.02it/s]warmup should be done: 100%|| 2994/3000 [00:01<00:00, 1745.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1749.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1748.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1747.48it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1744.28it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1713.64it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1734.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1729.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1703.33it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1702.78it/s]2022-12-11 19:16:42.548214: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0472fa2fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.548272: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.556836: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f046ec22e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.556888: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.558637: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f046efa2cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.558691: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.623717: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:42.640021: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:42.660887: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:42.817919: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0462fa3080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.817981: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.855063: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0466fa2f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.855142: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.876932: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:42.916531: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f046ec22bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.916597: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.917163: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f045ef82dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.917223: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.921987: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0472f05bf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:16:42.922035: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:16:42.937770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:43.000487: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:43.002350: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:43.007350: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:16:44.313838: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.319594: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.322451: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.515574: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.563186: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.597406: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.638373: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:16:44.663194: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:17:05.142][ERROR][RK0][tid #139658419750656]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:17:05.142][ERROR][RK0][tid #139658419750656]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.155][ERROR][RK0][tid #139658419750656]: coll ps creation done
[HCTR][19:17:05.155][ERROR][RK0][tid #139658419750656]: replica 3 waits for coll ps creation barrier
[HCTR][19:17:05.273][ERROR][RK0][tid #139657547335424]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:17:05.273][ERROR][RK0][tid #139657547335424]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.282][ERROR][RK0][tid #139657547335424]: coll ps creation done
[HCTR][19:17:05.282][ERROR][RK0][tid #139657547335424]: replica 4 waits for coll ps creation barrier
[HCTR][19:17:05.336][ERROR][RK0][tid #139657748662016]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:17:05.336][ERROR][RK0][tid #139657748662016]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.344][ERROR][RK0][tid #139657748662016]: coll ps creation done
[HCTR][19:17:05.344][ERROR][RK0][tid #139657748662016]: replica 5 waits for coll ps creation barrier
[HCTR][19:17:05.352][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:17:05.352][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.358][ERROR][RK0][main]: coll ps creation done
[HCTR][19:17:05.358][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][19:17:05.412][ERROR][RK0][tid #139657413117696]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:17:05.412][ERROR][RK0][tid #139657413117696]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.420][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:17:05.420][ERROR][RK0][tid #139657413117696]: coll ps creation done
[HCTR][19:17:05.420][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.420][ERROR][RK0][tid #139657413117696]: replica 2 waits for coll ps creation barrier
[HCTR][19:17:05.425][ERROR][RK0][tid #139657547335424]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:17:05.425][ERROR][RK0][tid #139657547335424]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.425][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:17:05.425][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:17:05.428][ERROR][RK0][main]: coll ps creation done
[HCTR][19:17:05.428][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][19:17:05.430][ERROR][RK0][tid #139657547335424]: coll ps creation done
[HCTR][19:17:05.430][ERROR][RK0][tid #139657547335424]: replica 7 waits for coll ps creation barrier
[HCTR][19:17:05.430][ERROR][RK0][main]: coll ps creation done
[HCTR][19:17:05.430][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][19:17:05.430][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][19:17:12.579][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][19:17:12.616][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139657547335424]: replica 7 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139657413117696]: replica 2 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139657547335424]: replica 4 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139658419750656]: replica 3 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139657748662016]: replica 5 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139658419750656]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][19:17:12.616][ERROR][RK0][tid #139658419750656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][main]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][tid #139657547335424]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][tid #139657413117696]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][main]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][tid #139657547335424]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][tid #139657748662016]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][main]: Calling build_v2
[HCTR][19:17:12.616][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][tid #139657547335424]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][tid #139657413117696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][tid #139657547335424]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][tid #139657748662016]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:17:12.616][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 19:17:12.[[[616534[[[2022-12-11 19:17:12: 2022-12-11 19:17:122022-12-11 19:17:122022-12-11 19:17:12.2022-12-11 19:17:12E[.2022-12-11 19:17:12..616557. 616558.2022-12-11 19:17:12616555616558: 616557/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: 616555.: : E: :E: 616598EE E136 E:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccusing concurrent impl MPS:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ::136:
136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136] 136] 136:] ] using concurrent impl MPS] using concurrent impl MPS] 136using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPS] 



using concurrent impl MPS
[2022-12-11 19:17:12.621401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:17:12.621440: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:17:12:.196621448] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:17:12[.2022-12-11 19:17:12621497.: 621499E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196:] 212assigning 8 to cpu] 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-11 19:17:12.621545: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:17:12:[.1782022-12-11 19:17:12621563] .: v100x8, slow pcie621570E
:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 19:17:12 :2022-12-11 19:17:12./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213.621593:] 621601: 212[remote time is 8.68421: E] 2022-12-11 19:17:12
E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[
621640/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 19:17:122022-12-11 19:17:12: :178[..E196[] 2022-12-11 19:17:12621691621691 ] 2022-12-11 19:17:12v100x8, slow pcie.: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu.
[621732[EE:
6217402022-12-11 19:17:12: 2022-12-11 19:17:12  178: .E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E621791 621824:[:v100x8, slow pcie : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2142022-12-11 19:17:12178
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:E] .] :[ 213 cpu time is 97.0588621888v100x8, slow pcie1782022-12-11 19:17:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 
] .:remote time is 8.68421:Ev100x8, slow pcie621964[178
196 [
: 2022-12-11 19:17:12] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:17:12E[.v100x8, slow pcieassigning 8 to cpu:. 2022-12-11 19:17:12622037

212622079/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: ] [: :622090Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:17:12E196: [ 
. ] E2022-12-11 19:17:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc622145/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[ .:: :
2022-12-11 19:17:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc622176196E214.:: ]  ] 622225196Eassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588: []  
:
E2022-12-11 19:17:12assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196 .
:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc622316[212assigning 8 to cpu:: 2022-12-11 19:17:12] 
[213E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:17:12]  622391
.[remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 6224192022-12-11 19:17:12
:[E: .2122022-12-11 19:17:12[ E622458] .2022-12-11 19:17:12/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8622490.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
: 622512212: E: ] 212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 2022-12-11 19:17:12:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.212:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
622586] [213:: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:17:12[] 214E
.2022-12-11 19:17:12remote time is 8.68421]  622639.
[cpu time is 97.0588[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 6226582022-12-11 19:17:12
2022-12-11 19:17:12:E: ..213 E622702622725] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : : remote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE
213:  ] 213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421] 2022-12-11 19:17:12::
remote time is 8.68421.213214
622818[[] ] : 2022-12-11 19:17:122022-12-11 19:17:12remote time is 8.68421cpu time is 97.0588E..

 622863[622878/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 19:17:12: :E.E214 622921 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:E:
214 214] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] cpu time is 97.0588:cpu time is 97.0588
214
] cpu time is 97.0588
[2022-12-11 19:18:57.109720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:18:57.459984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 19:18:57.460069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 35310982
[2022-12-11 19:18:58.860678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:18:58.860760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:18:58.860798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:18:58.860827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:18:58.861218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.865291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.869184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.993473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 19:18:58.993546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 19:18:58.993925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.995665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:18:58.995732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:18:58.996107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.996486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:18:58.996542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:18:58.996905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.997971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 19:18:58.998029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 19:18:58.998373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:58.998668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 19:18:58.998723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:18:58.999066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59.  3135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 19:18:59.  3186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 19:18:59.  3524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59.  3959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 19:18:59.  4011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 19:18:59.  4350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 22864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 23251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 23429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 27062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 27215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 27303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 30965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 49876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 50156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 50755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 50799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 54477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 54562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59. 58259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:18:59.530433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:18:59.530841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:18:59.530894: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[2022-12-11 19:18:59.548529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:18:59.548653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:18:59.548699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:18:59.553088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.553974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.561392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.561958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.567703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.567762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:18:59.759960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:18:59.760350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:18:59.760398: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[2022-12-11 19:18:59.777854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:18:59.777965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:18:59.778010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:18:59.782413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.783337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.790836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.791481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.797238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.797297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[[2022-12-11 19:18:592022-12-11 19:18:59..819422819422: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-11 19:18:592022-12-11 19:18:59..819822819824: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[[2022-12-11 19:18:592022-12-11 19:18:59..819894819895: : WW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::16831683] ] using empty feat=27using empty feat=27

[2022-12-11 19:18:59.820636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:18:59.821005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:18:59.821053: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[[2022-12-11 19:18:592022-12-11 19:18:59..832362832370: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-11 19:18:59.832446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[2022-12-11 19:18:592022-12-11 19:18:59..832737832738: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980[1980] 2022-12-11 19:18:59] eager alloc mem 1024.00 Bytes.eager alloc mem 1024.00 Bytes
832771
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-11 19:18:592022-12-11 19:18:59..832819832820: : W[W 2022-12-11 19:18:59 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:832838:1683: 1683] W] using empty feat=27 using empty feat=27
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1683] using empty feat=27
[2022-12-11 19:18:59.930029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:18:59.930103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 19:18:59] .eager release cuda mem 2930103
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:18:59.930155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340[
2022-12-11 19:18:59.930176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:18:59.930219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:18:59.930276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:18:59.930364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 22022-12-11 19:18:59
.930362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[[:2022-12-11 19:18:592022-12-11 19:18:59638..] 930416930404: eager release cuda mem 1024: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 3531098340eager release cuda mem 1024
[
2022-12-11 19:18:59.930479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:18:59.930507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[2022-12-11 19:18:596382022-12-11 19:18:59.] .930505eager release cuda mem 2930527: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] [] eager release cuda mem 10242022-12-11 19:18:59eager release cuda mem 3531098340
.
930574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:18:59.930611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:18:59.930655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:18:59.934727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.938873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.943688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.947682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.951766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.955869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:18:59.957133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.957442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.957966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.958054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.958283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.958317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:18:59.965814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.965977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.966378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.966495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.966581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.966623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:18:59.969409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.969473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.969558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.969632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.969694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.969773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:18:59.975123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.975177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 16.84 GB2022-12-11 19:18:59
.975192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.975240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:18:59.975266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.975312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:18:59.975353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.975397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:18:59.975437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.975480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB[
2022-12-11 19:18:59.975495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:18:59.975543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[[[[[[[[2022-12-11 19:19:032022-12-11 19:19:032022-12-11 19:19:032022-12-11 19:19:032022-12-11 19:19:032022-12-11 19:19:032022-12-11 19:19:032022-12-11 19:19:03........232377232380232371232371232385232371232376232387: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB







[2022-12-11 19:19:03.241696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.241750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.241796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.241834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.241903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 19:19:03638.] 241923eager release cuda mem 5518079: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.242006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.242051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.242291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.242633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.243205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.243675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.243743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.243879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.243960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.244236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.251456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.251741: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:19:03:.638251760] : eager release cuda mem 5518079E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.252076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.252177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.252226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.252267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 55180792022-12-11 19:19:03
.252291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.252319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.252356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.253155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.253406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.253539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.253602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.253919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.253988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.258603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.258902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.260591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.260891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.261495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.261671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.261738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.261772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 55180792022-12-11 19:19:03
.261799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB[
2022-12-11 19:19:03.261823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.261881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.262386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.262571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.263004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.263333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.263402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.265279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.265581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.267255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.267563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.269436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.269740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.270331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.270475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.270640: [E2022-12-11 19:19:03 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu270648:: 1980E]  eager alloc mem 5.26 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.270702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.270756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.271069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.271743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.271816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[[2022-12-11 19:19:032022-12-11 19:19:03..271935271937: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 5.26 MBeager release cuda mem 5518079

[2022-12-11 19:19:03.273162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.273934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.274239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.276105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.276400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.277926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.278217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.278673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.278985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.279151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.279195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.279248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.279587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.279747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.279930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.280051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.280612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.280789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.281647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.282778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.283086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.284589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.284885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.285807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.286126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.287358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.287404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.287512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.287686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.287828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.287898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.288008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.288272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:19:03.288876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.289234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.289478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.289781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.29587 secs 
[2022-12-11 19:19:03.290100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.29105 secs 
[2022-12-11 19:19:03.290223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.290639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.28713 secs 
[2022-12-11 19:19:03.291240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.292486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.292986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.294449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.295273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.295380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.295557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:19:03.295667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.29133 secs 
[2022-12-11 19:19:03.297123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.30023 secs 
[2022-12-11 19:19:03.297374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.298039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.298110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:19:03.300345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.43914 secs 
[2022-12-11 19:19:03.301001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.30491 secs 
[2022-12-11 19:19:03.301221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.30286 secs 
[2022-12-11 19:19:03.301910: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 27.75 GB
[2022-12-11 19:19:04.762263: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.01 GB
[2022-12-11 19:19:04.762611: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.01 GB
[2022-12-11 19:19:04.763168: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.01 GB
[2022-12-11 19:19:06.649216: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.28 GB
[2022-12-11 19:19:06.649711: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.28 GB
[2022-12-11 19:19:06.677682: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.28 GB
[2022-12-11 19:19:08.246962: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.49 GB
[2022-12-11 19:19:08.248363: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.49 GB
[2022-12-11 19:19:08.249030: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.49 GB
[2022-12-11 19:19:10.275483: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.71 GB
[2022-12-11 19:19:10.276697: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.71 GB
[2022-12-11 19:19:10.278052: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.71 GB
[2022-12-11 19:19:12.180715: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.16 GB
[2022-12-11 19:19:12.181396: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.16 GB
[2022-12-11 19:19:12.186513: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.16 GB
[2022-12-11 19:19:13.885798: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.36 GB
[2022-12-11 19:19:13.885994: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.36 GB
[HCTR][19:19:13.887][ERROR][RK0][tid #139657547335424]: replica 4 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][tid #139657748662016]: replica 5 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][tid #139657547335424]: replica 7 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][tid #139657413117696]: replica 2 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][tid #139658419750656]: replica 3 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][19:19:13.887][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][tid #139658419750656]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657547335424]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657748662016]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657547335424]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657413117696]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:19:13.887][ERROR][RK0][main]: init per replica done
[HCTR][19:19:13.887][ERROR][RK0][tid #139658419750656]: init per replica done
[HCTR][19:19:13.887][ERROR][RK0][main]: init per replica done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657547335424]: init per replica done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657748662016]: init per replica done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657547335424]: init per replica done
[HCTR][19:19:13.887][ERROR][RK0][tid #139657413117696]: init per replica done
[HCTR][19:19:13.907][ERROR][RK0][main]: init per replica done








