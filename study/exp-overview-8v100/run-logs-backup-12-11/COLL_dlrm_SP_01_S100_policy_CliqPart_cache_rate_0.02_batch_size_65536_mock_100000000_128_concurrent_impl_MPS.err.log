2022-12-11 21:30:47.478328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.485977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.491178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.498213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.503404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.515706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.523358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.527519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.585513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.594425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.597058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.599152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.600025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.600908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.601943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.603199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.605024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.606073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.606092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.607738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.607893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.609116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.609519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.610842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.611322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.612450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.612885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.614009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.614540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.615376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.616176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.617497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.617526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.619091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.620201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.621134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.622082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.623184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.624167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.625100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.630363: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:47.631504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.632261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.633025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.633956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.634663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.635549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.636464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.636937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.638054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.638276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.639272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.639788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.639962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.641157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.641824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.641992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.642907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.643852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.643980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.649091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.650717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.652553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.654652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.655000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.656676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.657049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.658581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.658945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.660258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.660976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.664035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.664303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.666323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.667844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.668005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.669392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.670802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.671020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.671444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.672245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.690752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.690752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.707964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.708756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.709413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.710174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.710372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.710421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.712059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.712455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.713546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.714384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.714916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.714958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.717304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.717599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.718519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.720048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.720128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.720287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.721949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.722140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.723445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.724105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.724308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.724357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.725853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.726742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.728201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.728469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.728663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.729041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.730518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.730964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.732801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.732891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.733054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.733894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.734904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.734938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.736899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.737117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.738028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.738774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.738915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.740409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.740548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.742175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.742333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.743544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.743690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.745065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.745117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.747700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.747829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.749321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.749400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.750896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.750978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.752315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.752495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.753976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.754124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.755364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.755415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.756915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.756997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.758033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.759046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.759395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.759500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.759689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.761023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.762650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.762688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.762728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.763038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.764380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.767117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.767363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.767953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.767960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.768763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.770704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.770808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.771154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.771291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.773559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.773819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.774085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.774514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.774563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.777287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.777423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.777547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.777945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.777992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.780774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.780907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.781003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.781446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.781738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.781982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.785837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.786034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.786173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.786290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.786415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.786692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.786865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.790407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.790622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.790926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.791067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.791495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.791928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.794023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.794249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.794405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.794574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.795054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.795575: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:47.795770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.797640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.797864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.797917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.798133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.801631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.802184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.802189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.802337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.803059: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:47.803061: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:47.804723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.806283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.806557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.806768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.808229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.809683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.809838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.809917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.810610: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:47.811076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.811999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.812091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.812807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.812860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.812992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.816139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.816206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.816729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.816770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.816952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.819300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.822045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.822136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.822740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.822976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.823608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.824688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.827484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.827629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.828590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.829464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.832120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.832355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.833213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.866358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.866573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.867345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.871081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.871262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.872492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.877323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.877444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.878843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.882462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.885041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.885502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.889393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.889656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.891278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.894859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.899900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.902868: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:47.904448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.905092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.907300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.908451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.911949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.942508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.945819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.948157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.951363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.952417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.954806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.956297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:47.957966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.016192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.047108: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:48.051044: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:30:48.056354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.059938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.061515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.066030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.068515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.072807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.942878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.943532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.944356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.944827: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:48.944882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:30:48.962547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.963476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.964408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.964987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.965650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:48.966130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:30:49.013159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.013366: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.055717: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:30:49.221139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.221781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.222316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.222778: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.222837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:30:49.231061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.231710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.232464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.232937: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.233008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:30:49.240276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.240897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.241412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.241978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.242500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.242964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:30:49.250562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.251207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.252116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.253806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.255108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.255717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:30:49.260112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.260640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.261125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.261820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.262174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.262731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.263241: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.263317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:30:49.264599: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.264654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:30:49.280732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.281372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.281837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.281882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.282995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.283095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.283996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.284135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.285257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.285273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:30:49.285781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.286255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:30:49.304491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.305183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.305732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.306210: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.306268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:30:49.315211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.315828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.316362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.316819: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.316875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:30:49.322379: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.322588: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.323599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.324252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.324514: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:30:49.324759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.325344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.325854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.326336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:30:49.327525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.328129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.328673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.329133: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:30:49.329191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:30:49.332262: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.332421: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.333788: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.333957: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.334058: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:30:49.335412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.335726: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:30:49.336036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.336558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.337128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.337649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.338110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:30:49.344788: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.344932: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.346648: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:30:49.347716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.348379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.348895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.349479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.349991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:30:49.350462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:30:49.372727: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.372908: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.375038: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:30:49.384122: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.384307: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.386073: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:30:49.397271: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.397452: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:30:49.399220: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:30:50.680][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 99it [00:01, 81.79it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 198it [00:01, 177.85it/s]warmup run: 91it [00:01, 77.36it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 99it [00:01, 84.45it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 298it [00:01, 285.60it/s]warmup run: 181it [00:01, 166.50it/s]warmup run: 98it [00:01, 84.28it/s]warmup run: 98it [00:01, 84.06it/s]warmup run: 98it [00:01, 85.04it/s]warmup run: 99it [00:01, 84.95it/s]warmup run: 194it [00:01, 178.59it/s]warmup run: 96it [00:01, 82.97it/s]warmup run: 399it [00:01, 399.36it/s]warmup run: 273it [00:01, 267.47it/s]warmup run: 196it [00:01, 182.43it/s]warmup run: 198it [00:01, 184.13it/s]warmup run: 195it [00:01, 182.91it/s]warmup run: 197it [00:01, 182.95it/s]warmup run: 291it [00:01, 285.05it/s]warmup run: 193it [00:01, 180.58it/s]warmup run: 500it [00:02, 510.71it/s]warmup run: 368it [00:01, 377.03it/s]warmup run: 295it [00:01, 291.47it/s]warmup run: 299it [00:01, 295.33it/s]warmup run: 294it [00:01, 292.84it/s]warmup run: 295it [00:01, 290.57it/s]warmup run: 389it [00:01, 396.49it/s]warmup run: 291it [00:01, 288.91it/s]warmup run: 602it [00:02, 615.42it/s]warmup run: 465it [00:02, 486.84it/s]warmup run: 395it [00:01, 406.08it/s]warmup run: 400it [00:01, 410.63it/s]warmup run: 392it [00:01, 404.76it/s]warmup run: 395it [00:01, 404.87it/s]warmup run: 485it [00:02, 501.58it/s]warmup run: 391it [00:01, 403.89it/s]warmup run: 703it [00:02, 704.23it/s]warmup run: 565it [00:02, 593.63it/s]warmup run: 497it [00:02, 521.07it/s]warmup run: 501it [00:02, 522.38it/s]warmup run: 490it [00:01, 513.33it/s]warmup run: 494it [00:02, 514.18it/s]warmup run: 583it [00:02, 601.49it/s]warmup run: 488it [00:02, 509.50it/s]warmup run: 804it [00:02, 778.71it/s]warmup run: 663it [00:02, 682.68it/s]warmup run: 598it [00:02, 623.99it/s]warmup run: 604it [00:02, 628.59it/s]warmup run: 590it [00:02, 615.62it/s]warmup run: 594it [00:02, 616.62it/s]warmup run: 683it [00:02, 691.97it/s]warmup run: 586it [00:02, 608.31it/s]warmup run: 904it [00:02, 836.00it/s]warmup run: 759it [00:02, 750.64it/s]warmup run: 698it [00:02, 709.94it/s]warmup run: 705it [00:02, 716.27it/s]warmup run: 691it [00:02, 705.97it/s]warmup run: 695it [00:02, 706.63it/s]warmup run: 779it [00:02, 758.21it/s]warmup run: 685it [00:02, 696.59it/s]warmup run: 1003it [00:02, 876.02it/s]warmup run: 854it [00:02, 802.60it/s]warmup run: 798it [00:02, 780.70it/s]warmup run: 805it [00:02, 786.14it/s]warmup run: 792it [00:02, 780.82it/s]warmup run: 797it [00:02, 782.54it/s]warmup run: 876it [00:02, 813.17it/s]warmup run: 783it [00:02, 766.98it/s]warmup run: 1103it [00:02, 908.21it/s]warmup run: 949it [00:02, 842.16it/s]warmup run: 898it [00:02, 835.98it/s]warmup run: 893it [00:02, 838.78it/s]warmup run: 904it [00:02, 832.39it/s]warmup run: 897it [00:02, 838.50it/s]warmup run: 976it [00:02, 862.27it/s]warmup run: 881it [00:02, 821.76it/s]warmup run: 1203it [00:02, 933.49it/s]warmup run: 1045it [00:02, 873.07it/s]warmup run: 997it [00:02, 877.61it/s]warmup run: 993it [00:02, 881.38it/s]warmup run: 1002it [00:02, 863.86it/s]warmup run: 996it [00:02, 879.45it/s]warmup run: 1077it [00:02, 900.99it/s]warmup run: 980it [00:02, 865.08it/s]warmup run: 1305it [00:02, 956.57it/s]warmup run: 1141it [00:02, 895.07it/s]warmup run: 1099it [00:02, 915.66it/s]warmup run: 1093it [00:02, 913.80it/s]warmup run: 1100it [00:02, 894.28it/s]warmup run: 1096it [00:02, 911.78it/s]warmup run: 1178it [00:02, 929.62it/s]warmup run: 1078it [00:02, 896.68it/s]warmup run: 1406it [00:02, 971.51it/s]warmup run: 1236it [00:02, 908.88it/s]warmup run: 1199it [00:02, 939.62it/s]warmup run: 1193it [00:02, 938.04it/s]warmup run: 1197it [00:02, 914.13it/s]warmup run: 1195it [00:02, 932.28it/s]warmup run: 1278it [00:02, 949.84it/s]warmup run: 1176it [00:02, 906.39it/s]warmup run: 1507it [00:03, 979.84it/s]warmup run: 1331it [00:02, 916.74it/s]warmup run: 1293it [00:02, 952.50it/s]warmup run: 1299it [00:02, 940.01it/s]warmup run: 1294it [00:02, 927.37it/s]warmup run: 1295it [00:02, 950.41it/s]warmup run: 1379it [00:02, 965.05it/s]warmup run: 1276it [00:02, 930.80it/s]warmup run: 1608it [00:03, 986.80it/s]warmup run: 1426it [00:03, 911.76it/s]warmup run: 1394it [00:02, 966.81it/s]warmup run: 1400it [00:02, 959.04it/s]warmup run: 1391it [00:02, 934.71it/s]warmup run: 1394it [00:02, 960.80it/s]warmup run: 1479it [00:03, 972.12it/s]warmup run: 1375it [00:02, 946.48it/s]warmup run: 1709it [00:03, 984.51it/s]warmup run: 1520it [00:03, 916.56it/s]warmup run: 1494it [00:03, 971.21it/s]warmup run: 1501it [00:03, 972.26it/s]warmup run: 1488it [00:03, 943.02it/s]warmup run: 1493it [00:03, 968.91it/s]warmup run: 1580it [00:03, 980.79it/s]warmup run: 1474it [00:03, 958.68it/s]warmup run: 1809it [00:03, 984.94it/s]warmup run: 1614it [00:03, 921.09it/s]warmup run: 1595it [00:03, 980.20it/s]warmup run: 1602it [00:03, 981.33it/s]warmup run: 1585it [00:03, 945.49it/s]warmup run: 1594it [00:03, 980.68it/s]warmup run: 1681it [00:03, 988.78it/s]warmup run: 1573it [00:03, 967.50it/s]warmup run: 1911it [00:03, 992.21it/s]warmup run: 1708it [00:03, 923.90it/s]warmup run: 1696it [00:03, 986.32it/s]warmup run: 1703it [00:03, 986.96it/s]warmup run: 1684it [00:03, 957.08it/s]warmup run: 1695it [00:03, 987.27it/s]warmup run: 1783it [00:03, 995.59it/s]warmup run: 1672it [00:03, 973.51it/s]warmup run: 2013it [00:03, 1000.21it/s]warmup run: 1802it [00:03, 927.32it/s]warmup run: 1803it [00:03, 990.62it/s]warmup run: 1782it [00:03, 960.93it/s]warmup run: 1795it [00:03, 989.70it/s]warmup run: 1796it [00:03, 969.81it/s]warmup run: 1885it [00:03, 999.98it/s]warmup run: 1771it [00:03, 976.74it/s]warmup run: 2130it [00:03, 1049.16it/s]warmup run: 1896it [00:03, 929.24it/s]warmup run: 1906it [00:03, 999.98it/s]warmup run: 1882it [00:03, 972.08it/s]warmup run: 1896it [00:03, 993.24it/s]warmup run: 1895it [00:03, 975.32it/s]warmup run: 1986it [00:03, 1002.58it/s]warmup run: 1870it [00:03, 976.94it/s]warmup run: 2248it [00:03, 1086.15it/s]warmup run: 1991it [00:03, 932.68it/s]warmup run: 2009it [00:03, 1007.32it/s]warmup run: 1986it [00:03, 989.96it/s]warmup run: 1998it [00:03, 998.61it/s]warmup run: 1996it [00:03, 982.77it/s]warmup run: 2104it [00:03, 1054.80it/s]warmup run: 1969it [00:03, 978.37it/s]warmup run: 2365it [00:03, 1109.93it/s]warmup run: 2105it [00:03, 993.36it/s]warmup run: 2128it [00:03, 1059.45it/s]warmup run: 2103it [00:03, 1043.30it/s]warmup run: 2118it [00:03, 1057.31it/s]warmup run: 2114it [00:03, 1039.93it/s]warmup run: 2225it [00:03, 1099.83it/s]warmup run: 2079it [00:03, 1013.37it/s]warmup run: 2481it [00:03, 1124.66it/s]warmup run: 2222it [00:03, 1045.48it/s]warmup run: 2248it [00:03, 1100.99it/s]warmup run: 2220it [00:03, 1078.44it/s]warmup run: 2239it [00:03, 1102.32it/s]warmup run: 2233it [00:03, 1083.80it/s]warmup run: 2346it [00:03, 1132.15it/s]warmup run: 2196it [00:03, 1059.02it/s]warmup run: 2598it [00:04, 1135.83it/s]warmup run: 2340it [00:03, 1084.81it/s]warmup run: 2368it [00:03, 1129.96it/s]warmup run: 2340it [00:03, 1112.01it/s]warmup run: 2360it [00:03, 1134.09it/s]warmup run: 2352it [00:03, 1114.68it/s]warmup run: 2467it [00:03, 1155.26it/s]warmup run: 2313it [00:03, 1090.67it/s]warmup run: 2714it [00:04, 1141.63it/s]warmup run: 2458it [00:04, 1111.65it/s]warmup run: 2488it [00:03, 1149.66it/s]warmup run: 2459it [00:03, 1135.06it/s]warmup run: 2481it [00:03, 1156.06it/s]warmup run: 2472it [00:03, 1137.69it/s]warmup run: 2589it [00:04, 1172.59it/s]warmup run: 2431it [00:03, 1114.56it/s]warmup run: 2830it [00:04, 1144.30it/s]warmup run: 2575it [00:04, 1128.20it/s]warmup run: 2608it [00:04, 1163.07it/s]warmup run: 2602it [00:04, 1171.81it/s]warmup run: 2579it [00:04, 1151.87it/s]warmup run: 2594it [00:04, 1159.28it/s]warmup run: 2710it [00:04, 1181.79it/s]warmup run: 2549it [00:04, 1131.38it/s]warmup run: 2947it [00:04, 1150.11it/s]warmup run: 2691it [00:04, 1136.34it/s]warmup run: 3000it [00:04, 673.89it/s] warmup run: 2727it [00:04, 1168.70it/s]warmup run: 2722it [00:04, 1180.08it/s]warmup run: 2698it [00:04, 1160.34it/s]warmup run: 2716it [00:04, 1174.59it/s]warmup run: 2831it [00:04, 1190.02it/s]warmup run: 2665it [00:04, 1137.55it/s]warmup run: 2807it [00:04, 1142.48it/s]warmup run: 2847it [00:04, 1177.86it/s]warmup run: 2844it [00:04, 1190.32it/s]warmup run: 2818it [00:04, 1171.52it/s]warmup run: 2952it [00:04, 1195.77it/s]warmup run: 2837it [00:04, 1182.47it/s]warmup run: 2782it [00:04, 1146.21it/s]warmup run: 3000it [00:04, 683.89it/s] warmup run: 2923it [00:04, 1146.54it/s]warmup run: 2968it [00:04, 1184.65it/s]warmup run: 2965it [00:04, 1195.11it/s]warmup run: 2938it [00:04, 1179.77it/s]warmup run: 2959it [00:04, 1192.26it/s]warmup run: 2900it [00:04, 1153.36it/s]warmup run: 3000it [00:04, 687.13it/s] warmup run: 3000it [00:04, 662.70it/s] warmup run: 3000it [00:04, 687.52it/s] warmup run: 3000it [00:04, 687.60it/s] warmup run: 3000it [00:04, 681.69it/s] warmup run: 3000it [00:04, 678.99it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.09it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.24it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1588.73it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1603.37it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.86it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1635.23it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1580.96it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.87it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1612.82it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1643.66it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1603.24it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.68it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1621.69it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.04it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1588.66it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1622.96it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1639.73it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1591.14it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1608.21it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1599.77it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1618.95it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1621.53it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1655.56it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1620.25it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1608.82it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1638.88it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1599.15it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1624.35it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1589.99it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1653.99it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1617.92it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1617.69it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1608.66it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1597.67it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1636.70it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1624.47it/s]warmup should be done:  27%|██▋       | 798/3000 [00:00<00:01, 1588.67it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1615.57it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1612.26it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1632.02it/s]warmup should be done:  32%|███▏      | 968/3000 [00:00<00:01, 1606.78it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1623.98it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1631.89it/s]warmup should be done:  32%|███▏      | 962/3000 [00:00<00:01, 1591.89it/s]warmup should be done:  32%|███▏      | 957/3000 [00:00<00:01, 1583.44it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1610.19it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1609.08it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1631.39it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1607.51it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1627.50it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1632.40it/s]warmup should be done:  37%|███▋      | 1122/3000 [00:00<00:01, 1592.09it/s]warmup should be done:  37%|███▋      | 1116/3000 [00:00<00:01, 1579.99it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1610.03it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1604.36it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1635.16it/s]warmup should be done:  43%|████▎     | 1292/3000 [00:00<00:01, 1611.55it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1628.25it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1631.96it/s]warmup should be done:  43%|████▎     | 1282/3000 [00:00<00:01, 1592.34it/s]warmup should be done:  42%|████▎     | 1275/3000 [00:00<00:01, 1581.15it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1609.65it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1604.94it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1637.76it/s]warmup should be done:  48%|████▊     | 1454/3000 [00:00<00:00, 1611.18it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1631.71it/s]warmup should be done:  48%|████▊     | 1442/3000 [00:00<00:00, 1592.09it/s]warmup should be done:  48%|████▊     | 1434/3000 [00:00<00:00, 1580.17it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1601.58it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1638.35it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1592.90it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1596.26it/s]warmup should be done:  54%|█████▍    | 1616/3000 [00:01<00:00, 1610.52it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1631.12it/s]warmup should be done:  53%|█████▎    | 1602/3000 [00:01<00:00, 1591.05it/s]warmup should be done:  53%|█████▎    | 1593/3000 [00:01<00:00, 1576.29it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1597.87it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1598.84it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1631.32it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1593.33it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1593.44it/s]warmup should be done:  59%|█████▉    | 1778/3000 [00:01<00:00, 1606.03it/s]warmup should be done:  58%|█████▊    | 1751/3000 [00:01<00:00, 1573.37it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1618.96it/s]warmup should be done:  59%|█████▉    | 1782/3000 [00:01<00:00, 1595.89it/s]warmup should be done:  59%|█████▉    | 1783/3000 [00:01<00:00, 1603.73it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1624.13it/s]warmup should be done:  60%|█████▉    | 1788/3000 [00:01<00:00, 1573.66it/s]warmup should be done:  65%|██████▍   | 1940/3000 [00:01<00:00, 1609.06it/s]warmup should be done:  64%|██████▍   | 1923/3000 [00:01<00:00, 1596.46it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1621.53it/s]warmup should be done:  64%|██████▎   | 1909/3000 [00:01<00:00, 1572.83it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1603.77it/s]warmup should be done:  65%|██████▍   | 1942/3000 [00:01<00:00, 1592.61it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1618.99it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1594.16it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1610.24it/s]warmup should be done:  69%|██████▉   | 2084/3000 [00:01<00:00, 1597.83it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1623.54it/s]warmup should be done:  69%|██████▉   | 2067/3000 [00:01<00:00, 1574.09it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1605.59it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1591.74it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1618.83it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1610.47it/s]warmup should be done:  75%|███████▍  | 2244/3000 [00:01<00:00, 1594.86it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1622.67it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1576.08it/s]warmup should be done:  75%|███████▌  | 2264/3000 [00:01<00:00, 1596.05it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1606.28it/s]warmup should be done:  75%|███████▌  | 2262/3000 [00:01<00:00, 1588.08it/s]warmup should be done:  77%|███████▋  | 2305/3000 [00:01<00:00, 1613.47it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1618.45it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1599.79it/s]warmup should be done:  82%|████████▏ | 2458/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  81%|████████  | 2427/3000 [00:01<00:00, 1604.88it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1573.86it/s]warmup should be done:  81%|████████  | 2427/3000 [00:01<00:00, 1603.96it/s]warmup should be done:  81%|████████  | 2421/3000 [00:01<00:00, 1588.48it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1610.33it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1623.84it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1600.59it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1625.67it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1575.37it/s]warmup should be done:  86%|████████▋ | 2590/3000 [00:01<00:00, 1611.02it/s]warmup should be done:  86%|████████▋ | 2588/3000 [00:01<00:00, 1604.06it/s]warmup should be done:  86%|████████▌ | 2580/3000 [00:01<00:00, 1588.90it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1609.04it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1624.66it/s]warmup should be done:  91%|█████████ | 2728/3000 [00:01<00:00, 1601.04it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1626.03it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1616.26it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1577.93it/s]warmup should be done:  92%|█████████▏| 2749/3000 [00:01<00:00, 1602.32it/s]warmup should be done:  91%|█████████▏| 2739/3000 [00:01<00:00, 1588.25it/s]warmup should be done:  92%|█████████▏| 2772/3000 [00:01<00:00, 1625.88it/s]warmup should be done:  93%|█████████▎| 2790/3000 [00:01<00:00, 1606.31it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1604.37it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1630.67it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1624.18it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1582.37it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1606.81it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1592.95it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1626.98it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1612.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1625.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1611.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1606.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1599.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1597.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1580.31it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1598.01it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.52it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.03it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.73it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.75it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.96it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.94it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.32it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1644.57it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.07it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.97it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1605.83it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1658.72it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.58it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1639.81it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1602.84it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1651.50it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1661.04it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.30it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1642.77it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1661.10it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1612.67it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1638.10it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1608.92it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1656.32it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1615.44it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1666.92it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1641.35it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1656.78it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1628.41it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1640.99it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1614.88it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1657.81it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1669.26it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1613.74it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1640.81it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1654.51it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1626.77it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1642.43it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1617.89it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1661.33it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1615.54it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1672.08it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1642.36it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1654.85it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1628.00it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1643.82it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1614.78it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1663.65it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1617.82it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1671.78it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1643.54it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1657.18it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1624.16it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1640.26it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1602.12it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1666.30it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1669.28it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1641.58it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1611.79it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1662.08it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1621.79it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1636.53it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1603.21it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1673.52it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1666.62it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1642.33it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1617.92it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1668.30it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1624.40it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1635.83it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1608.90it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1678.96it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1644.48it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1624.25it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1674.81it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1674.13it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1627.30it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1637.51it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1613.86it/s]warmup should be done:  61%|██████▏   | 1840/3000 [00:01<00:00, 1684.58it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1645.99it/s]warmup should be done:  60%|█████▉    | 1787/3000 [00:01<00:00, 1625.84it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1679.33it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1679.66it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1627.66it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1641.57it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1615.11it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1685.48it/s]warmup should be done:  65%|██████▌   | 1950/3000 [00:01<00:00, 1625.68it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1678.62it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1642.96it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1683.70it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1624.10it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1642.15it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1613.62it/s]warmup should be done:  73%|███████▎  | 2178/3000 [00:01<00:00, 1685.47it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1677.87it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1630.07it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1685.54it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1643.24it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1625.25it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1640.92it/s]warmup should be done:  71%|███████   | 2116/3000 [00:01<00:00, 1616.90it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1684.31it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1632.93it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1688.44it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1674.00it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1645.53it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1626.19it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1643.69it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1618.33it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1682.48it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1691.78it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1632.39it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1645.63it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1669.45it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1625.18it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1643.90it/s]warmup should be done:  81%|████████▏ | 2441/3000 [00:01<00:00, 1617.29it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1684.25it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1693.01it/s]warmup should be done:  87%|████████▋ | 2607/3000 [00:01<00:00, 1632.20it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1643.68it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1665.81it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1623.83it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1650.00it/s]warmup should be done:  87%|████████▋ | 2603/3000 [00:01<00:00, 1617.33it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1684.97it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1694.21it/s]warmup should be done:  92%|█████████▏| 2772/3000 [00:01<00:00, 1634.45it/s]warmup should be done:  94%|█████████▎| 2805/3000 [00:01<00:00, 1643.50it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1623.67it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1660.78it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1656.33it/s]warmup should be done:  92%|█████████▏| 2766/3000 [00:01<00:00, 1619.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.74it/s]warmup should be done:  98%|█████████▊| 2936/3000 [00:01<00:00, 1635.70it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1643.88it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1626.24it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1661.90it/s]warmup should be done:  98%|█████████▊| 2929/3000 [00:01<00:00, 1620.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1625.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.88it/s]2022-12-11 21:32:26.890316: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa56802d570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:26.890385: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:26.899923: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc14b82fb20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:26.899976: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:26.900769: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc147832a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:26.900813: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:27.250561: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa5e402a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:27.250626: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:27.281011: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc147794de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:27.281083: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:27.418526: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc14782fc00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:27.418598: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:27.418904: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc14b82b720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:27.418953: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:27.445263: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa4f802f660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:32:27.445353: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:32:29.132815: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.182336: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.237701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.505479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.539372: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.705351: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.731318: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:29.744755: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:32:32.000244: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.078520: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.106626: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.363060: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.477431: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.562895: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.616391: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:32:32.630449: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:33:12.206][ERROR][RK0][tid #140468541171456]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:33:12.206][ERROR][RK0][tid #140468541171456]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.212][ERROR][RK0][tid #140468541171456]: coll ps creation done
[HCTR][21:33:12.212][ERROR][RK0][tid #140468541171456]: replica 7 waits for coll ps creation barrier
[HCTR][21:33:12.224][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:33:12.224][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.229][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.229][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:33:12.330][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:33:12.330][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.338][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.338][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:33:12.347][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:33:12.347][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.355][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.355][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][21:33:12.380][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:33:12.380][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.387][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.387][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:33:12.394][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:33:12.395][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.395][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:33:12.395][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.399][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.399][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][21:33:12.402][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.402][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][21:33:12.412][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:33:12.412][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:33:12.416][ERROR][RK0][main]: coll ps creation done
[HCTR][21:33:12.416][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][21:33:12.416][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][21:33:13.306][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][tid #140468541171456]: replica 7 calling init per replica
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][tid #140468541171456]: Calling build_v2
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:33:13.342][ERROR][RK0][tid #140468541171456]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 21:33:132022-12-11 21:33:132022-12-11 21:33:132022-12-11 21:33:132022-12-11 21:33:132022-12-11 21:33:13...2022-12-11 21:33:13...2022-12-11 21:33:13342444342445342445.342444342456342456.: : : 342464: : : 342465EEE: EEE:    E   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136:136] 136136:] ] ] 136using concurrent impl MPS] ] 136using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] 
using concurrent impl MPSusing concurrent impl MPS] 


using concurrent impl MPS

using concurrent impl MPS

[2022-12-11 21:33:13.347710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:33:13.347755: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:33:13:.196347761] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:33:13[.2022-12-11 21:33:13347804.: 347813E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 196[v100x8, slow pcie] 2022-12-11 21:33:13
assigning 8 to cpu.
347846[: 2022-12-11 21:33:13E. 347867/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E212 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:
196] assigning 8 to cpu[
2022-12-11 21:33:13.347902[: 2022-12-11 21:33:13[E.2022-12-11 21:33:13 347918./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 347921:E: 178 [E[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:33:13 2022-12-11 21:33:13v100x8, slow pcie:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
212347950:347945] : 213[: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[] 2022-12-11 21:33:13E
 2022-12-11 21:33:13remote time is 8.68421. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
347989[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:347997: 2022-12-11 21:33:13[:[212: E.2022-12-11 21:33:131782022-12-11 21:33:13] E 348032.] [.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 348042v100x8, slow pcie2022-12-11 21:33:13348050
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E: 
.: :196 [E348091E[178] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:33:13 :  2022-12-11 21:33:13] assigning 8 to cpu:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.v100x8, slow pcie
213348175: :348189
] : 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: remote time is 8.68421[E] :] E[
2022-12-11 21:33:13 v100x8, slow pcie178cpu time is 97.0588 2022-12-11 21:33:13.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.3483012022-12-11 21:33:13:v100x8, slow pcie[:348311: .213
2022-12-11 21:33:13196: E348348] .] [E : remote time is 8.68421348384assigning 8 to cpu2022-12-11 21:33:13 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
: 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E[348432:196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 21:33:13: [212] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E2022-12-11 21:33:13] assigning 8 to cpu214:348507 .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] 196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc348537
cpu time is 97.0588] E:: 
assigning 8 to cpu [196E[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:33:13]  2022-12-11 21:33:13:.assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.214348663
212348671] : ] [: cpu time is 97.0588Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:33:13E
 
. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc348760[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: [2022-12-11 21:33:13:213E2022-12-11 21:33:13.212]  .348804] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc348812: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:: E
212E[ ]  2022-12-11 21:33:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:2022-12-11 21:33:13
:348905212.213[: ] 348927] 2022-12-11 21:33:13Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: remote time is 8.68421. 
E
348971/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : [[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 21:33:132022-12-11 21:33:13214: ..] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc349033349035cpu time is 97.0588] :: : 
remote time is 8.68421213EE
]   remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2022-12-11 21:33:13:213.[214] 3491332022-12-11 21:33:13] remote time is 8.68421: .cpu time is 97.0588
E349157
 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 21:33:13: .214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc349209] :: cpu time is 97.0588214E
]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-11 21:34:32.101330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:34:32.141676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 21:34:32.251973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:34:32.252035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:34:32.252068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:34:32.252100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:34:32.252564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:34:32.252608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.253474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.254130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.267363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 21:34:32.267428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 21:34:32.267894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:34:32.267940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.267952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 21:34:32.268021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 21:34:32.268089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:34:32202.] 2681117 solved: 
E[ [2022-12-11 21:34:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 21:34:32.:.268140202268159: ] : E6 solvedE 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202[205] 2022-12-11 21:34:32] 3 solved.worker 0 thread 7 initing device 7
268208
: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 21:34:32:.205268237] : worker 0 thread 6 initing device 6E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 21:34:32.268465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:34:32.268510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.268640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:34:32[.[2022-12-11 21:34:322686742022-12-11 21:34:32.: .268678E268688:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1815Building Coll Cache with ... num gpu device is 81980] 
] Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB

[2022-12-11 21:34:32.268767[: 2022-12-11 21:34:32E. 268774/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.270848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.271912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.272054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.272105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.272163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.273679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.275262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.275315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.275375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.275685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 21:34:322022-12-11 21:34:32..277206277205: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved1 solved

[[2022-12-11 21:34:322022-12-11 21:34:32..277288277288: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 4 initing device 4worker 0 thread 1 initing device 1

[[2022-12-11 21:34:322022-12-11 21:34:32..277874277874: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 21:34:322022-12-11 21:34:32..277946277946: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 21:34:32.279784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.279841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.281868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.281989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:34:32.331411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 21:34:32.341419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.341571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:34:32.344995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.345759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.346842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.348650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:34:32.349414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:34:32.349461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.355566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 21:34:32.[3578582022-12-11 21:34:32: .E357881 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 5.00 Bytes1980
] [[eager alloc mem 5.00 Bytes2022-12-11 21:34:322022-12-11 21:34:32
..357929357929: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 21:34:32.360977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.361076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:34:32.362907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.363416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.363713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.363796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:34:32.363904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.363960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.364002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:34:32.364044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:34:32eager release cuda mem 400000000.
364050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.364135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:34:32.364365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.364580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.365659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.366169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.366682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.370395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:34:32.371117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:34:32.371174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.371505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.372450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.373287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.373344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.373414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.374276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.374327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.374425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.379720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:34:32.380334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:34:32.380377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.380645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:34:32[.2022-12-11 21:34:32381267.: 381260E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 25855] 
eager alloc mem 25.25 KB
[2022-12-11 21:34:32.381339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.381782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:34:32.381904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:34:32.381953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.382391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:34:32.382447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.385332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 21:34:32.386105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 21:34:32.390218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.390298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:34:32.391008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:34:32.391086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-11 21:34:32
.391101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.391969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:34:32.392256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.392451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.393224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.393418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.399333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:34:32.400050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
2022-12-11 21:34:32.400062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:34:32eager alloc mem 25.25 KB.
400099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:34:32.400786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:34:32.400830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[[[[2022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:32........735277735277735277735277735277735277735278735279: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] Device 6 init p2p of link 0] Device 7 init p2p of link 4Device 1 init p2p of link 7Device 4 init p2p of link 5Device 3 init p2p of link 2Device 2 init p2p of link 1Device 5 init p2p of link 6
Device 0 init p2p of link 3






[[[[2022-12-11 21:34:32[2022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:32.[[2022-12-11 21:34:32...7357322022-12-11 21:34:32[2022-12-11 21:34:32.735732735732735732: .2022-12-11 21:34:32.735738: : : E735747.735747: EEE : 735760: E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:198019801980] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] ] ] eager alloc mem 611.00 KB1980:1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
] 1980] eager alloc mem 611.00 KB


eager alloc mem 611.00 KB] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[[[2022-12-11 21:34:322022-12-11 21:34:322022-12-11 21:34:32.[[..7367112022-12-11 21:34:32[2022-12-11 21:34:32736712736712[: [.2022-12-11 21:34:32.: : 2022-12-11 21:34:32E2022-12-11 21:34:32736722.736722EE. .: 736732:   736737/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc736738E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ::  E ::E638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638 ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:638eager release cuda mem 625663eager release cuda mem 625663:
:] 638] 

638638eager release cuda mem 625663] eager release cuda mem 625663] ] 
eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 21:34:32.749850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 21:34:32.750010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.750424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 21:34:32.750490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 21:34:32.750583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.750655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.750822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.750887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 21:34:32.751043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.751110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 21:34:32.751278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.751381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.751483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.751523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 21:34:32.751677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.751765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 21:34:32.751813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.751928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.751945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 21:34:32.752070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.752119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.752475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.752720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.752889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.763903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 21:34:32.764035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.764093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 21:34:32.764221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.764252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 21:34:32.764385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.764712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 21:34:32.764825: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:34:32:.638764838] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.764966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 21:34:32.765016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.765070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 21:34:32.765107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.765158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.765197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.765246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 21:34:32.765375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.765486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 21:34:32.765616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.765638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.765920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.765984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.766141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.766414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.779718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 21:34:32.779841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.780277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 21:34:32.780402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.780654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.780836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 21:34:32.780961: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:34:32:.1980780964] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 21:34:32.781098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.781203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.781625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 21:34:32.781747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.781770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.781890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.781915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 21:34:32.782038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.782250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 21:34:32.782367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.782531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.782823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.782866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 21:34:32.782991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:34:32.783143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.783766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:34:32.796398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.796951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.797133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.797352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.797522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.797802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.798087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.798417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:34:32.798658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.520717 secs 
[2022-12-11 21:34:32.799224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.521287 secs 
[2022-12-11 21:34:32.799693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.531763 secs 
[2022-12-11 21:34:32.799839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.531163 secs 
[2022-12-11 21:34:32.800067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.531565 secs 
[2022-12-11 21:34:32.800409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.531642 secs 
[2022-12-11 21:34:32.800578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.531819 secs 
[2022-12-11 21:34:32.800989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.548388 secs 
[2022-12-11 21:34:32.802797: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.60 GB
[2022-12-11 21:34:34.298072: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.86 GB
[2022-12-11 21:34:34.298543: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.86 GB
[2022-12-11 21:34:34.298901: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.86 GB
[2022-12-11 21:34:35.728077: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.12 GB
[2022-12-11 21:34:35.729192: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.12 GB
[2022-12-11 21:34:35.730241: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.12 GB
[2022-12-11 21:34:36.941933: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.34 GB
[2022-12-11 21:34:36.942064: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.34 GB
[2022-12-11 21:34:36.943146: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.34 GB
[2022-12-11 21:34:38.208590: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.55 GB
[2022-12-11 21:34:38.209099: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.55 GB
[2022-12-11 21:34:38.210043: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.55 GB
[2022-12-11 21:34:39.171841: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.01 GB
[2022-12-11 21:34:39.172707: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.01 GB
[2022-12-11 21:34:39.173358: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.01 GB
[2022-12-11 21:34:40.872833: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.21 GB
[2022-12-11 21:34:40.873613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.21 GB
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][tid #140468541171456]: replica 7 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][tid #140468541171456]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:34:42.203][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.203][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.203][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.203][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.203][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.203][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.203][ERROR][RK0][tid #140468541171456]: init per replica done
[HCTR][21:34:42.206][ERROR][RK0][main]: init per replica done
[HCTR][21:34:42.209][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fc332320000
[HCTR][21:34:42.209][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fc332800000
[HCTR][21:34:42.209][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fc332e40000
[HCTR][21:34:42.209][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fc333160000
[HCTR][21:34:42.209][ERROR][RK0][tid #140468474062592]: 3 allocated 3276800 at 0x7fc332320000
[HCTR][21:34:42.209][ERROR][RK0][tid #140468474062592]: 3 allocated 6553600 at 0x7fc332800000
[HCTR][21:34:42.209][ERROR][RK0][tid #140468474062592]: 3 allocated 3276800 at 0x7fc332e40000
[HCTR][21:34:42.209][ERROR][RK0][tid #140468474062592]: 3 allocated 6553600 at 0x7fc333160000
[HCTR][21:34:42.209][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fc332320000
[HCTR][21:34:42.209][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fc332800000
[HCTR][21:34:42.209][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fc332e40000
[HCTR][21:34:42.209][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fc333160000
[HCTR][21:34:42.209][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fc32e320000
[HCTR][21:34:42.209][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fc32e800000
[HCTR][21:34:42.209][ERROR][RK0][tid #140468608280320]: 6 allocated 3276800 at 0x7fc332320000
[HCTR][21:34:42.209][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fc32ee40000
[HCTR][21:34:42.210][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fc32f160000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468608280320]: 6 allocated 6553600 at 0x7fc332800000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468608280320]: 6 allocated 3276800 at 0x7fc332e40000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468608280320]: 6 allocated 6553600 at 0x7fc333160000
[HCTR][21:34:42.209][ERROR][RK0][tid #140468675389184]: 4 allocated 3276800 at 0x7fc330320000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468675389184]: 4 allocated 6553600 at 0x7fc330800000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468675389184]: 4 allocated 3276800 at 0x7fc330e40000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468675389184]: 4 allocated 6553600 at 0x7fc331160000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468482455296]: 5 allocated 3276800 at 0x7fc332320000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468482455296]: 5 allocated 6553600 at 0x7fc332800000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468482455296]: 5 allocated 3276800 at 0x7fc332e40000
[HCTR][21:34:42.210][ERROR][RK0][tid #140468482455296]: 5 allocated 6553600 at 0x7fc333160000
[HCTR][21:34:42.213][ERROR][RK0][tid #140468474062592]: 0 allocated 3276800 at 0x7fc334520000
[HCTR][21:34:42.213][ERROR][RK0][tid #140468474062592]: 0 allocated 6553600 at 0x7fc334a00000
[HCTR][21:34:42.213][ERROR][RK0][tid #140468474062592]: 0 allocated 3276800 at 0x7fc33570e800
[HCTR][21:34:42.213][ERROR][RK0][tid #140468474062592]: 0 allocated 6553600 at 0x7fc335a2e800








