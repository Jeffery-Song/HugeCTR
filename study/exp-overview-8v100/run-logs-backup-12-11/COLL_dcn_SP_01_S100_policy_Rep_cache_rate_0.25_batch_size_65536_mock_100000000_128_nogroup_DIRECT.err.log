2022-12-12 06:53:33.999474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.004600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.013170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.018043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.022447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.036161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.043429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.064610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.115545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.121949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.124695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.125581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.126555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.127500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.128873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.130594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.131399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.131796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.133108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.133277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.134578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.134607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.136073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.136135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.137487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.137587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.138799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.139109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.140057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.140655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.141353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.142175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.143416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.144553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.145573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.146609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.147557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.148496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.149426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.150317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.155689: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.156744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.158520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.160025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.160057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.161717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.162011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.163219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.163739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.164860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.165342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.165781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.167018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.167856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.168386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.170256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.171194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.171303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.171438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.174434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.174658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.175012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.177350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.177602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.178072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.180923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.181228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.183652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.184026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.184619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.186062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.186521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.187217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.188064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.188510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.189205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.190138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.190890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.191612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.192179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.193327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.193763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.194366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.195106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.196175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.196331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.196875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.198447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.198496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.210189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.212470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.212682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.213354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.214327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.214965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.215488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.218207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.225866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.239320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.241594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.251099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.253274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.254520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.254619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.254697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.254732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.254746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.257695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.258693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.258869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.258901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.258942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.258992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.262358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.264008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.264169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.264198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.264283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.264367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.267775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.267900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.267923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.268037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.268099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.271267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.271432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.271462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.271503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.271640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.275028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.275086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.275242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.275368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.275495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.278511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.278665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.278695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.278798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.278939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.281748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.281862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.281977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.282029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.282127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.284919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.285082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.285153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.285201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.286317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.288301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.288551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.288583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.288669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.289973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.291432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.291800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.291870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.291915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.293433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.294784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.295135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.295179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.295278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.296670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.296688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.298493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.298980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.299015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.299209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.300871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.301175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.302558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.303120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.303157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.303295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.305130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.305391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.306677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.307153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.307179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.307366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.309408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.309577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.311655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.311768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.312338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.312954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.313063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.313096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.313183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.315465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.315704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.316816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.317868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.317896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.317920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.320043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.320311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.321087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.321970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.322094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.322100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.322197: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.324244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.324404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.325237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.325990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.326105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.326166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.328394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.328530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.329791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.330542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.330581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.330681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.332048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.333211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.333360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.335505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.336048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.336158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.336236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.337431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.338710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.339645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.340587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.340896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.341247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.341358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.342202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.343138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.344218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.345546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.345734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.345944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.345948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.347767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.350949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.351959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.353339: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.353551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.354292: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.354296: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.354448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.354801: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.356198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.357032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.358758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.359730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.361940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.362088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.363804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.364513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.364530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.364614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.364770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.365025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.367907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.369176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.369231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.369601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.369720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.369959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.372633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.373602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.374139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.374244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.374387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.374482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.407735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.407964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.425080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.425326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.430175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.430505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.465431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.465645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.470840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.474374: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.476635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.482148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.483796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.489208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.491197: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:53:34.493651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.500330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.531645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:34.536101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.553957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.554795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.556327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.556795: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.556847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:53:35.574759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.575787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.576317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.576886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.577486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.577993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:53:35.625430: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.625625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.659763: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:53:35.756771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.757549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.758092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.758556: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.758608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:53:35.775886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.777056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.777560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.778140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.779168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.779638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:53:35.843126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.843770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.844309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.844665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.844665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.845117: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.845133: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.845197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:53:35.845298: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.846341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.846423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.846956: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:53:35.847474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.847637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.848286: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.848335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:53:35.848682: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.848728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:53:35.862553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.863501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.863541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.864577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.864584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.865407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.865611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.865689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.866959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.867002: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.867033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.867052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:53:35.867221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.869268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.869311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.869314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:53:35.870369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.870473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.871432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.871511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.872691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:53:35.872698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.873178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:53:35.878278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.878868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.879476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.879963: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.880011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:53:35.884400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.885020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.885521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.886089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.886596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.887065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:53:35.894869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.895484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.896014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.896470: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:53:35.896513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:53:35.897142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.897778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.898293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.898861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.899400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.899871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:53:35.913123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.913729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.913889: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.914076: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.914245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.914806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.915355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:53:35.915824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:53:35.915828: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:53:35.918042: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.918199: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.919910: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:53:35.933850: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.934046: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.936002: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:53:35.945670: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.945849: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.947560: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:53:35.959572: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.959750: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.961656: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:53:35.961983: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.962117: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:53:35.963882: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][06:53:37.221][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.225][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.225][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.226][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.229][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.230][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.231][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:53:37.233][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 98it [00:01, 83.54it/s]warmup run: 99it [00:01, 84.25it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 85.07it/s]warmup run: 90it [00:01, 76.97it/s]warmup run: 99it [00:01, 83.75it/s]warmup run: 196it [00:01, 181.11it/s]warmup run: 197it [00:01, 181.45it/s]warmup run: 94it [00:01, 80.71it/s]warmup run: 95it [00:01, 81.87it/s]warmup run: 92it [00:01, 79.38it/s]warmup run: 198it [00:01, 184.20it/s]warmup run: 181it [00:01, 167.92it/s]warmup run: 197it [00:01, 180.55it/s]warmup run: 297it [00:01, 292.31it/s]warmup run: 296it [00:01, 289.76it/s]warmup run: 195it [00:01, 182.40it/s]warmup run: 190it [00:01, 177.22it/s]warmup run: 191it [00:01, 179.53it/s]warmup run: 298it [00:01, 294.42it/s]warmup run: 270it [00:01, 265.28it/s]warmup run: 295it [00:01, 287.11it/s]warmup run: 397it [00:01, 406.01it/s]warmup run: 396it [00:01, 403.71it/s]warmup run: 296it [00:01, 294.06it/s]warmup run: 285it [00:01, 281.90it/s]warmup run: 290it [00:01, 289.45it/s]warmup run: 399it [00:01, 409.73it/s]warmup run: 362it [00:01, 370.81it/s]warmup run: 394it [00:01, 399.30it/s]warmup run: 495it [00:02, 512.82it/s]warmup run: 497it [00:02, 515.93it/s]warmup run: 398it [00:01, 411.29it/s]warmup run: 381it [00:01, 391.60it/s]warmup run: 389it [00:01, 402.95it/s]warmup run: 499it [00:02, 520.37it/s]warmup run: 455it [00:02, 474.67it/s]warmup run: 494it [00:02, 510.07it/s]warmup run: 598it [00:02, 621.18it/s]warmup run: 599it [00:02, 621.87it/s]warmup run: 501it [00:02, 526.80it/s]warmup run: 477it [00:02, 498.05it/s]warmup run: 487it [00:02, 511.38it/s]warmup run: 601it [00:02, 625.64it/s]warmup run: 547it [00:02, 568.06it/s]warmup run: 597it [00:02, 618.11it/s]warmup run: 699it [00:02, 709.73it/s]warmup run: 701it [00:02, 712.23it/s]warmup run: 576it [00:02, 602.08it/s]warmup run: 605it [00:02, 634.28it/s]warmup run: 588it [00:02, 615.73it/s]warmup run: 703it [00:02, 716.63it/s]warmup run: 639it [00:02, 649.05it/s]warmup run: 700it [00:02, 712.86it/s]warmup run: 797it [00:02, 775.60it/s]warmup run: 802it [00:02, 785.62it/s]warmup run: 674it [00:02, 689.53it/s]warmup run: 709it [00:02, 727.10it/s]warmup run: 689it [00:02, 705.68it/s]warmup run: 805it [00:02, 790.56it/s]warmup run: 731it [00:02, 715.29it/s]warmup run: 802it [00:02, 788.47it/s]warmup run: 895it [00:02, 826.16it/s]warmup run: 903it [00:02, 843.57it/s]warmup run: 772it [00:02, 760.33it/s]warmup run: 811it [00:02, 799.77it/s]warmup run: 790it [00:02, 779.68it/s]warmup run: 907it [00:02, 848.45it/s]warmup run: 822it [00:02, 765.23it/s]warmup run: 904it [00:02, 847.41it/s]warmup run: 996it [00:02, 873.73it/s]warmup run: 1004it [00:02, 887.91it/s]warmup run: 871it [00:02, 818.36it/s]warmup run: 912it [00:02, 845.63it/s]warmup run: 891it [00:02, 838.08it/s]warmup run: 1007it [00:02, 886.69it/s]warmup run: 913it [00:02, 802.80it/s]warmup run: 1006it [00:02, 893.14it/s]warmup run: 1097it [00:02, 909.56it/s]warmup run: 1106it [00:02, 922.79it/s]warmup run: 970it [00:02, 864.94it/s]warmup run: 991it [00:02, 881.14it/s]warmup run: 1012it [00:02, 838.42it/s]warmup run: 1107it [00:02, 911.20it/s]warmup run: 1004it [00:02, 831.76it/s]warmup run: 1108it [00:02, 928.03it/s]warmup run: 1197it [00:02, 934.95it/s]warmup run: 1208it [00:02, 947.87it/s]warmup run: 1069it [00:02, 897.49it/s]warmup run: 1091it [00:02, 913.98it/s]warmup run: 1111it [00:02, 877.68it/s]warmup run: 1206it [00:02, 917.60it/s]warmup run: 1097it [00:02, 858.30it/s]warmup run: 1209it [00:02, 950.73it/s]warmup run: 1297it [00:02, 952.02it/s]warmup run: 1309it [00:02, 956.41it/s]warmup run: 1167it [00:02, 918.87it/s]warmup run: 1191it [00:02, 936.54it/s]warmup run: 1207it [00:02, 896.15it/s]warmup run: 1304it [00:02, 913.65it/s]warmup run: 1189it [00:02, 875.22it/s]warmup run: 1310it [00:02, 961.59it/s]warmup run: 1397it [00:02, 964.19it/s]warmup run: 1269it [00:02, 946.54it/s]warmup run: 1409it [00:02, 951.08it/s]warmup run: 1291it [00:02, 942.12it/s]warmup run: 1305it [00:02, 919.57it/s]warmup run: 1400it [00:02, 921.04it/s]warmup run: 1281it [00:02, 886.85it/s]warmup run: 1411it [00:02, 975.52it/s]warmup run: 1497it [00:03, 974.28it/s]warmup run: 1372it [00:02, 969.89it/s]warmup run: 1507it [00:03, 958.60it/s]warmup run: 1390it [00:02, 941.79it/s]warmup run: 1404it [00:02, 938.28it/s]warmup run: 1500it [00:03, 943.30it/s]warmup run: 1376it [00:03, 904.56it/s]warmup run: 1512it [00:03, 983.96it/s]warmup run: 1598it [00:03, 983.42it/s]warmup run: 1475it [00:03, 986.94it/s]warmup run: 1608it [00:03, 971.45it/s]warmup run: 1487it [00:03, 944.51it/s]warmup run: 1503it [00:03, 953.16it/s]warmup run: 1601it [00:03, 962.43it/s]warmup run: 1476it [00:03, 932.63it/s]warmup run: 1614it [00:03, 994.21it/s]warmup run: 1699it [00:03, 989.95it/s]warmup run: 1576it [00:03, 993.36it/s]warmup run: 1709it [00:03, 981.72it/s]warmup run: 1584it [00:03, 945.99it/s]warmup run: 1601it [00:03, 949.83it/s]warmup run: 1702it [00:03, 974.07it/s]warmup run: 1575it [00:03, 948.33it/s]warmup run: 1715it [00:03, 986.35it/s]warmup run: 1800it [00:03, 994.34it/s]warmup run: 1677it [00:03, 998.12it/s]warmup run: 1809it [00:03, 980.36it/s]warmup run: 1680it [00:03, 941.55it/s]warmup run: 1698it [00:03, 952.82it/s]warmup run: 1803it [00:03, 982.06it/s]warmup run: 1673it [00:03, 957.64it/s]warmup run: 1816it [00:03, 992.84it/s]warmup run: 1901it [00:03, 997.55it/s]warmup run: 1780it [00:03, 1006.39it/s]warmup run: 1911it [00:03, 991.17it/s]warmup run: 1776it [00:03, 946.22it/s]warmup run: 1799it [00:03, 967.11it/s]warmup run: 1903it [00:03, 987.31it/s]warmup run: 1771it [00:03, 963.20it/s]warmup run: 1917it [00:03, 989.87it/s]warmup run: 2003it [00:03, 1002.28it/s]warmup run: 1882it [00:03, 1000.25it/s]warmup run: 2012it [00:03, 996.09it/s]warmup run: 1872it [00:03, 948.80it/s]warmup run: 1899it [00:03, 976.17it/s]warmup run: 2004it [00:03, 993.10it/s]warmup run: 1868it [00:03, 958.14it/s]warmup run: 2124it [00:03, 1064.16it/s]warmup run: 2021it [00:03, 1003.25it/s]warmup run: 2133it [00:03, 1059.34it/s]warmup run: 1983it [00:03, 981.41it/s] warmup run: 1969it [00:03, 954.42it/s]warmup run: 1999it [00:03, 983.15it/s]warmup run: 2123it [00:03, 1050.15it/s]warmup run: 1966it [00:03, 962.63it/s]warmup run: 2246it [00:03, 1109.91it/s]warmup run: 2140it [00:03, 1057.98it/s]warmup run: 2255it [00:03, 1104.61it/s]warmup run: 2100it [00:03, 1034.57it/s]warmup run: 2079it [00:03, 996.34it/s]warmup run: 2118it [00:03, 1042.61it/s]warmup run: 2239it [00:03, 1065.02it/s]warmup run: 2078it [00:03, 1006.93it/s]warmup run: 2368it [00:03, 1142.34it/s]warmup run: 2259it [00:03, 1096.58it/s]warmup run: 2377it [00:03, 1136.83it/s]warmup run: 2222it [00:03, 1088.53it/s]warmup run: 2196it [00:03, 1046.38it/s]warmup run: 2238it [00:03, 1088.21it/s]warmup run: 2359it [00:03, 1102.53it/s]warmup run: 2198it [00:03, 1062.21it/s]warmup run: 2490it [00:03, 1165.10it/s]warmup run: 2379it [00:03, 1125.76it/s]warmup run: 2499it [00:03, 1160.01it/s]warmup run: 2344it [00:03, 1127.12it/s]warmup run: 2313it [00:03, 1082.41it/s]warmup run: 2358it [00:03, 1120.15it/s]warmup run: 2479it [00:03, 1130.62it/s]warmup run: 2317it [00:03, 1099.92it/s]warmup run: 2612it [00:04, 1181.49it/s]warmup run: 2499it [00:03, 1147.44it/s]warmup run: 2621it [00:04, 1175.61it/s]warmup run: 2466it [00:03, 1154.09it/s]warmup run: 2430it [00:03, 1108.08it/s]warmup run: 2478it [00:03, 1142.62it/s]warmup run: 2599it [00:04, 1149.45it/s]warmup run: 2436it [00:04, 1125.99it/s]warmup run: 2619it [00:04, 1163.06it/s]warmup run: 2735it [00:04, 1193.21it/s]warmup run: 2743it [00:04, 1186.99it/s]warmup run: 2588it [00:04, 1171.71it/s]warmup run: 2547it [00:04, 1125.91it/s]warmup run: 2598it [00:04, 1157.39it/s]warmup run: 2719it [00:04, 1163.02it/s]warmup run: 2555it [00:04, 1145.07it/s]warmup run: 2739it [00:04, 1174.03it/s]warmup run: 2856it [00:04, 1196.35it/s]warmup run: 2863it [00:04, 1190.47it/s]warmup run: 2709it [00:04, 1181.23it/s]warmup run: 2664it [00:04, 1137.58it/s]warmup run: 2718it [00:04, 1167.35it/s]warmup run: 2838it [00:04, 1168.94it/s]warmup run: 2675it [00:04, 1158.64it/s]warmup run: 2858it [00:04, 1176.66it/s]warmup run: 2978it [00:04, 1202.76it/s]warmup run: 2985it [00:04, 1197.08it/s]warmup run: 3000it [00:04, 687.16it/s] warmup run: 2828it [00:04, 1180.48it/s]warmup run: 2782it [00:04, 1148.27it/s]warmup run: 3000it [00:04, 685.73it/s] warmup run: 2836it [00:04, 1168.78it/s]warmup run: 2959it [00:04, 1179.10it/s]warmup run: 2795it [00:04, 1170.42it/s]warmup run: 2977it [00:04, 1180.52it/s]warmup run: 2947it [00:04, 1178.70it/s]warmup run: 3000it [00:04, 683.58it/s] warmup run: 3000it [00:04, 683.09it/s] warmup run: 2900it [00:04, 1156.73it/s]warmup run: 2955it [00:04, 1174.38it/s]warmup run: 3000it [00:04, 684.58it/s] warmup run: 3000it [00:04, 681.16it/s] warmup run: 2915it [00:04, 1176.25it/s]warmup run: 3000it [00:04, 677.83it/s] warmup run: 3000it [00:04, 664.54it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.62it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.12it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.45it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1639.14it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1618.16it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.34it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.53it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.35it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1644.04it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1649.69it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.89it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1644.08it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.40it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1626.69it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1648.26it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.20it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1649.70it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1643.30it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1645.34it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1653.78it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1621.80it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1636.66it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1634.00it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1630.41it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1648.74it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1641.93it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1644.75it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1656.70it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1635.72it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1609.81it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1624.08it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1618.75it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1646.82it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1641.44it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1642.97it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1655.24it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1640.87it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1616.30it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1599.19it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1610.56it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1639.83it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1654.23it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1642.36it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1639.69it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1632.76it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1612.21it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1592.19it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1604.13it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1635.32it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1650.60it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1639.80it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1632.67it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1634.74it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1605.88it/s]warmup should be done:  38%|      | 1133/3000 [00:00<00:01, 1583.22it/s]warmup should be done:  38%|      | 1144/3000 [00:00<00:01, 1596.89it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:01, 1635.63it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1642.10it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1634.53it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1630.78it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1641.05it/s]warmup should be done:  44%|     | 1307/3000 [00:00<00:01, 1605.43it/s]warmup should be done:  43%|     | 1292/3000 [00:00<00:01, 1580.61it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1590.02it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1643.23it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1637.11it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1633.05it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1625.12it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1630.76it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1605.87it/s]warmup should be done:  48%|     | 1451/3000 [00:00<00:00, 1581.27it/s]warmup should be done:  49%|     | 1465/3000 [00:00<00:00, 1593.82it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1645.14it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1637.72it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1632.63it/s]warmup should be done:  55%|    | 1645/3000 [00:01<00:00, 1615.60it/s]warmup should be done:  54%|    | 1629/3000 [00:01<00:00, 1604.11it/s]warmup should be done:  55%|    | 1657/3000 [00:01<00:00, 1628.72it/s]warmup should be done:  54%|    | 1611/3000 [00:01<00:00, 1585.31it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1595.32it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1636.85it/s]warmup should be done:  60%|    | 1814/3000 [00:01<00:00, 1644.59it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1626.11it/s]warmup should be done:  61%|    | 1820/3000 [00:01<00:00, 1625.79it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1600.10it/s]warmup should be done:  59%|    | 1771/3000 [00:01<00:00, 1588.06it/s]warmup should be done:  60%|    | 1785/3000 [00:01<00:00, 1595.24it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1595.61it/s]warmup should be done:  66%|   | 1974/3000 [00:01<00:00, 1636.24it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1644.01it/s]warmup should be done:  66%|   | 1973/3000 [00:01<00:00, 1620.61it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1625.65it/s]warmup should be done:  65%|   | 1951/3000 [00:01<00:00, 1597.68it/s]warmup should be done:  64%|   | 1931/3000 [00:01<00:00, 1589.22it/s]warmup should be done:  65%|   | 1945/3000 [00:01<00:00, 1595.01it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1583.29it/s]warmup should be done:  71%|  | 2138/3000 [00:01<00:00, 1635.09it/s]warmup should be done:  71%|  | 2144/3000 [00:01<00:00, 1636.47it/s]warmup should be done:  71%|   | 2136/3000 [00:01<00:00, 1613.47it/s]warmup should be done:  70%|   | 2091/3000 [00:01<00:00, 1590.73it/s]warmup should be done:  70%|   | 2111/3000 [00:01<00:00, 1595.31it/s]warmup should be done:  72%|  | 2146/3000 [00:01<00:00, 1615.37it/s]warmup should be done:  70%|   | 2105/3000 [00:01<00:00, 1591.56it/s]warmup should be done:  71%|   | 2126/3000 [00:01<00:00, 1577.63it/s]warmup should be done:  77%|  | 2302/3000 [00:01<00:00, 1635.33it/s]warmup should be done:  77%|  | 2308/3000 [00:01<00:00, 1630.61it/s]warmup should be done:  75%|  | 2251/3000 [00:01<00:00, 1591.87it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1593.77it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1618.36it/s]warmup should be done:  76%|  | 2265/3000 [00:01<00:00, 1592.47it/s]warmup should be done:  77%|  | 2298/3000 [00:01<00:00, 1592.39it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1574.80it/s]warmup should be done:  82%| | 2466/3000 [00:01<00:00, 1632.16it/s]warmup should be done:  82%| | 2472/3000 [00:01<00:00, 1621.82it/s]warmup should be done:  80%|  | 2412/3000 [00:01<00:00, 1595.22it/s]warmup should be done:  81%|  | 2431/3000 [00:01<00:00, 1591.03it/s]warmup should be done:  81%|  | 2425/3000 [00:01<00:00, 1591.11it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1607.26it/s]warmup should be done:  81%| | 2443/3000 [00:01<00:00, 1577.24it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1564.21it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1633.88it/s]warmup should be done:  86%| | 2574/3000 [00:01<00:00, 1601.81it/s]warmup should be done:  88%| | 2635/3000 [00:01<00:00, 1612.30it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1590.01it/s]warmup should be done:  86%| | 2585/3000 [00:01<00:00, 1591.50it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1612.40it/s]warmup should be done:  87%| | 2603/3000 [00:01<00:00, 1583.10it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1529.00it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1634.24it/s]warmup should be done:  91%| | 2736/3000 [00:01<00:00, 1606.09it/s]warmup should be done:  92%|| 2745/3000 [00:01<00:00, 1593.59it/s]warmup should be done:  93%|| 2797/3000 [00:01<00:00, 1603.27it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1588.38it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1611.98it/s]warmup should be done:  92%|| 2762/3000 [00:01<00:00, 1579.49it/s]warmup should be done:  92%|| 2769/3000 [00:01<00:00, 1520.93it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1639.85it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1612.75it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1603.59it/s]warmup should be done:  97%|| 2911/3000 [00:01<00:00, 1591.28it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1606.02it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1618.46it/s]warmup should be done:  97%|| 2921/3000 [00:01<00:00, 1580.61it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.18it/s]warmup should be done:  98%|| 2927/3000 [00:01<00:00, 1537.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.36it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1603.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1602.46it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1601.86it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1599.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1596.33it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.19it/s]warmup should be done:   5%|         | 156/3000 [00:00<00:01, 1557.16it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1657.34it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.14it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1633.25it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1682.30it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.50it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.10it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1638.03it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1603.45it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.60it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.54it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1655.96it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1657.07it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1676.33it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1675.20it/s]warmup should be done:  16%|        | 481/3000 [00:00<00:01, 1606.25it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1644.00it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1661.87it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1647.14it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1658.08it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1658.09it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1676.60it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1681.12it/s]warmup should be done:  22%|       | 646/3000 [00:00<00:01, 1622.61it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1649.97it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1669.65it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1667.74it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1660.43it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1679.10it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1685.10it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1634.45it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1632.26it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1653.31it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1672.33it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1671.17it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1662.18it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1682.11it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1683.79it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1644.88it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1633.00it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1653.07it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1671.72it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1669.24it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1682.32it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1661.96it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1649.63it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1667.53it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1636.93it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1654.44it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1677.95it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1680.83it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1659.33it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1653.36it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1659.58it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1659.70it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1637.33it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1652.88it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1685.21it/s]warmup should be done:  45%|     | 1351/3000 [00:00<00:00, 1683.28it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1661.72it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:01, 1655.61it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1647.33it/s]warmup should be done:  45%|     | 1348/3000 [00:00<00:00, 1657.22it/s]warmup should be done:  49%|     | 1473/3000 [00:00<00:00, 1652.81it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1653.60it/s]warmup should be done:  50%|     | 1515/3000 [00:00<00:00, 1689.25it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1662.09it/s]warmup should be done:  51%|     | 1520/3000 [00:00<00:00, 1680.47it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1654.38it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1645.49it/s]warmup should be done:  50%|     | 1514/3000 [00:00<00:00, 1656.31it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1660.39it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1654.55it/s]warmup should be done:  56%|    | 1686/3000 [00:01<00:00, 1693.01it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1663.01it/s]warmup should be done:  56%|    | 1689/3000 [00:01<00:00, 1677.88it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1652.54it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1645.26it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1655.56it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1666.80it/s]warmup should be done:  61%|    | 1822/3000 [00:01<00:00, 1655.48it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1695.62it/s]warmup should be done:  61%|    | 1836/3000 [00:01<00:00, 1665.02it/s]warmup should be done:  62%|   | 1858/3000 [00:01<00:00, 1680.78it/s]warmup should be done:  61%|    | 1835/3000 [00:01<00:00, 1654.26it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1658.77it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1671.18it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1660.35it/s]warmup should be done:  68%|   | 2028/3000 [00:01<00:00, 1697.15it/s]warmup should be done:  67%|   | 2003/3000 [00:01<00:00, 1664.20it/s]warmup should be done:  68%|   | 2027/3000 [00:01<00:00, 1682.42it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1653.49it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1641.66it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1660.59it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1675.58it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1663.41it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1696.62it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1663.66it/s]warmup should be done:  73%|  | 2196/3000 [00:01<00:00, 1681.69it/s]warmup should be done:  72%|  | 2167/3000 [00:01<00:00, 1653.07it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1641.26it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1658.59it/s]warmup should be done:  77%|  | 2318/3000 [00:01<00:00, 1679.99it/s]warmup should be done:  78%|  | 2326/3000 [00:01<00:00, 1666.69it/s]warmup should be done:  79%|  | 2368/3000 [00:01<00:00, 1697.00it/s]warmup should be done:  78%|  | 2337/3000 [00:01<00:00, 1664.13it/s]warmup should be done:  79%|  | 2365/3000 [00:01<00:00, 1682.17it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1656.53it/s]warmup should be done:  77%|  | 2316/3000 [00:01<00:00, 1641.42it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1657.36it/s]warmup should be done:  83%| | 2487/3000 [00:01<00:00, 1682.53it/s]warmup should be done:  83%| | 2494/3000 [00:01<00:00, 1670.03it/s]warmup should be done:  85%| | 2538/3000 [00:01<00:00, 1695.31it/s]warmup should be done:  83%| | 2504/3000 [00:01<00:00, 1665.19it/s]warmup should be done:  84%| | 2534/3000 [00:01<00:00, 1683.55it/s]warmup should be done:  83%| | 2501/3000 [00:01<00:00, 1659.36it/s]warmup should be done:  84%| | 2513/3000 [00:01<00:00, 1657.31it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1615.27it/s]warmup should be done:  89%| | 2662/3000 [00:01<00:00, 1670.40it/s]warmup should be done:  89%| | 2656/3000 [00:01<00:00, 1671.57it/s]warmup should be done:  90%| | 2708/3000 [00:01<00:00, 1693.43it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1664.26it/s]warmup should be done:  90%| | 2703/3000 [00:01<00:00, 1682.04it/s]warmup should be done:  89%| | 2679/3000 [00:01<00:00, 1655.28it/s]warmup should be done:  89%| | 2667/3000 [00:01<00:00, 1640.87it/s]warmup should be done:  88%| | 2643/3000 [00:01<00:00, 1613.46it/s]warmup should be done:  94%|| 2830/3000 [00:01<00:00, 1671.62it/s]warmup should be done:  96%|| 2878/3000 [00:01<00:00, 1692.81it/s]warmup should be done:  95%|| 2838/3000 [00:01<00:00, 1662.36it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1679.02it/s]warmup should be done:  94%|| 2824/3000 [00:01<00:00, 1651.09it/s]warmup should be done:  94%|| 2834/3000 [00:01<00:00, 1648.14it/s]warmup should be done:  95%|| 2845/3000 [00:01<00:00, 1652.62it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1625.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1685.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.69it/s]warmup should be done: 100%|| 2998/3000 [00:01<00:00, 1672.61it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1657.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.65it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1646.29it/s]warmup should be done:  99%|| 2977/3000 [00:01<00:00, 1639.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1649.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.78it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34901b80d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34904bc730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34901aa280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34901a9130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34904bbe80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34901a91f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34901ac2e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34901b71c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 06:55:07.071425: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fce830c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:07.071493: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:07.079414: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:07.105727: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fcf02d930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:07.105768: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:07.108430: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fca82cdc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:07.108488: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:07.113692: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:07.118598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:08.058220: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fce82c130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:08.058286: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:08.068042: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:08.071095: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fca830970 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:08.071178: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:08.079853: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:08.147842: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fc6834d20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:08.147908: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:08.154338: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fc2f919b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:08.154397: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:08.158306: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:08.162278: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:08.173802: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2fc6834a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:55:08.173858: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:55:08.183613: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:55:14.439680: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:14.510591: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:14.615333: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:14.895942: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:14.968379: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:15.029404: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:15.219282: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:55:15.220781: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:56:04.818][ERROR][RK0][tid #139843690538752]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:56:04.818][ERROR][RK0][tid #139843690538752]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:04.824][ERROR][RK0][tid #139843690538752]: coll ps creation done
[HCTR][06:56:04.824][ERROR][RK0][tid #139843690538752]: replica 7 waits for coll ps creation barrier
[HCTR][06:56:04.960][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:56:04.960][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:04.965][ERROR][RK0][main]: coll ps creation done
[HCTR][06:56:04.965][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][06:56:05.079][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:56:05.080][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:05.085][ERROR][RK0][tid #139844026083072]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:56:05.085][ERROR][RK0][tid #139844026083072]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:05.089][ERROR][RK0][main]: coll ps creation done
[HCTR][06:56:05.089][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:56:05.090][ERROR][RK0][tid #139844026083072]: coll ps creation done
[HCTR][06:56:05.090][ERROR][RK0][tid #139844026083072]: replica 6 waits for coll ps creation barrier
[HCTR][06:56:05.154][ERROR][RK0][tid #139844286125824]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:56:05.154][ERROR][RK0][tid #139844286125824]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:05.159][ERROR][RK0][tid #139844286125824]: coll ps creation done
[HCTR][06:56:05.159][ERROR][RK0][tid #139844286125824]: replica 3 waits for coll ps creation barrier
[HCTR][06:56:05.223][ERROR][RK0][tid #139843749254912]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:56:05.223][ERROR][RK0][tid #139843749254912]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:05.232][ERROR][RK0][tid #139843749254912]: coll ps creation done
[HCTR][06:56:05.232][ERROR][RK0][tid #139843749254912]: replica 0 waits for coll ps creation barrier
[HCTR][06:56:05.240][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:56:05.240][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:05.248][ERROR][RK0][main]: coll ps creation done
[HCTR][06:56:05.248][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][06:56:05.342][ERROR][RK0][tid #139843329849088]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:56:05.342][ERROR][RK0][tid #139843329849088]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:56:05.346][ERROR][RK0][tid #139843329849088]: coll ps creation done
[HCTR][06:56:05.347][ERROR][RK0][tid #139843329849088]: replica 2 waits for coll ps creation barrier
[HCTR][06:56:05.347][ERROR][RK0][tid #139843749254912]: replica 0 preparing frequency
[HCTR][06:56:06.181][ERROR][RK0][tid #139843749254912]: replica 0 preparing frequency done
[HCTR][06:56:06.227][ERROR][RK0][tid #139843749254912]: replica 0 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][tid #139843329849088]: replica 2 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][tid #139843690538752]: replica 7 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][tid #139844026083072]: replica 6 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][tid #139844286125824]: replica 3 calling init per replica
[HCTR][06:56:06.227][ERROR][RK0][tid #139843749254912]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][main]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][main]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][main]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][tid #139844286125824]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][tid #139843329849088]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][tid #139843690538752]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][tid #139844026083072]: Calling build_v2
[HCTR][06:56:06.227][ERROR][RK0][tid #139843749254912]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][tid #139844286125824]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][tid #139843329849088]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][tid #139843690538752]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:56:06.227][ERROR][RK0][tid #139844026083072]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 06:56:06.232088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 06:56:06.232169: E 2022-12-12 06:56:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:232142196[: ] Eassigning 0 to cpu 2022-12-12 06:56:06
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[:232188178: ] E2022-12-12 06:56:06v100x8, slow pcie .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc232229:: 178E]  [[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 06:56:062022-12-12 06:56:06
:..2022-12-12 06:56:06178232281[232280.] [: 2022-12-12 06:56:06: 2322792022-12-12 06:56:06v100x8, slow pcieE.E: .
 232322 E232321/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ : :[E:2022-12-12 06:56:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212 196.: 2022-12-12 06:56:06[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 232391178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:assigning 0 to cpu: ] :232381
196
Ev100x8, slow pcie1782022-12-12 06:56:06: ]  
] .[Eassigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie[2324242022-12-12 06:56:06[ 
:
2022-12-12 06:56:06: .[2022-12-12 06:56:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196.E232526[2022-12-12 06:56:06.] :232558 : 2022-12-12 06:56:06.232541assigning 0 to cpu178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.232587: 
] E: 232623: Ev100x8, slow pcie 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[v100x8, slow pcie213 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2122022-12-12 06:56:06
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:56:06:196] .remote time is 8.68421:.[196] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8232728
2122327302022-12-12 06:56:06] assigning 0 to cpu
: ] [: .assigning 0 to cpu
Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:56:06E[232774
 
. 2022-12-12 06:56:06: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc232824/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E[2022-12-12 06:56:06:: :232851[ 2022-12-12 06:56:06.212E196: 2022-12-12 06:56:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.232902]  ] E.:232904: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu 232924196: E
:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] E 214:[Eassigning 0 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2132022-12-12 06:56:06 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:cpu time is 97.0588] .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213
remote time is 8.684212330502022-12-12 06:56:06:212] remote time is 8.68421[2022-12-12 06:56:06
: .212] [
.E233092] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:56:06233126 [: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:56:06E
 233165E:.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  [2132331792022-12-12 06:56:06:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:56:06] : .212 :.remote time is 8.68421E233216] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212233236
 : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[
214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E: 2022-12-12 06:56:06] 
 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :2333222022-12-12 06:56:06
[:cpu time is 97.0588213: .2022-12-12 06:56:06213
] E233355.] remote time is 8.68421 : 233377remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 
:[ E2142022-12-12 06:56:06[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] .2022-12-12 06:56:06:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588233457.213:
: 233470] 213E: remote time is 8.68421]  E
remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[214:[2022-12-12 06:56:06] 2142022-12-12 06:56:06.cpu time is 97.0588] .233574
cpu time is 97.0588233584: 
: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 06:57:24.304879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:57:24.345038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 06:57:24.345101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 25000000
[2022-12-12 06:57:24.461055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:57:24.461149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:57:24.602917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:57:24.603007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:57:24.603552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.604514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.605327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.618071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 06:57:24.618146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 06:57:24.[618375[2022-12-12 06:57:24: [2022-12-12 06:57:24.E2022-12-12 06:57:24.618392 .618398: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc618406: E:: E 202E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:6 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202
:202] 202] [] 7 solved2 solved2022-12-12 06:57:245 solved

.
618510[: [[2022-12-12 06:57:24E[2022-12-12 06:57:242022-12-12 06:57:24. 2022-12-12 06:57:24..618535/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.618535618537: :618542: : E205: EE ] E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 6 initing device 6 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205:1980205] 205] ] worker 0 thread 7 initing device 7] eager alloc mem 381.47 MBworker 0 thread 2 initing device 2
worker 0 thread 5 initing device 5


[2022-12-12 06:57:24.618964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 06:57:242022-12-12 06:57:24[..2022-12-12 06:57:24618995618997.: : 619001EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:] ] 1980eager alloc mem 381.47 MBeager alloc mem 381.47 MB] 

eager alloc mem 381.47 MB
[2022-12-12 06:57:24.619244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 06:57:24.619300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-12 06:57:24] .worker 0 thread 3 initing device 3619304
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 06:57:24.619380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 06:57:24.619686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.619751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.623122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.623317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.623457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.623521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.623571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.624130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.624634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.627658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.627770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.627891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.627993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.628047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.628097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.628607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:57:24.686059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:57:24.686432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:57:24.691544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:57:24.691617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:57:24.691661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:57:24.692448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.693008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.694105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:24.694193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:57:24.694861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:57:24.694899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.719080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[[[2022-12-12 06:57:242022-12-12 06:57:242022-12-12 06:57:242022-12-12 06:57:24....719141719141719140719141: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[2022-12-12 06:57:24.719415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[2022-12-12 06:57:24[2022-12-12 06:57:242022-12-12 06:57:24.2022-12-12 06:57:24..719532.719533719535: 719540: : E: EE E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:19801980] 1980] ] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes


[[2022-12-12 06:57:242022-12-12 06:57:24..721017721019: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 06:57:24.721331[: 2022-12-12 06:57:24E. 721338/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes:
1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:57:24.741218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:57:24.741296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:57:24.741337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:57:24.745475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.745591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:57:24.745659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:57:24.745701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:57:24.745801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:57:24.745865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:57:24.745877: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:57:24:.638745908] : eager release cuda mem 1024E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:57:24.745950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-12 06:57:242022-12-12 06:57:24..745975745993: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-12 06:57:24.746035[: 2022-12-12 06:57:24E. 746058/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 1024638
] eager release cuda mem 2
[2022-12-12 06:57:24.746118[: 2022-12-12 06:57:24E[. 2022-12-12 06:57:24746125/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: :746116E638:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[2022-12-12 06:57:24.746195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 06:57:24
.746214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:57:24.746259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:57:24.746603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.747409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.747678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:24.747766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:57:24.748326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.748432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:57:24.748474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.748979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.749509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.750017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.750698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:57:24.776334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.776770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.776824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.776877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.777156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.777222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:24.777327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:24.777411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:57:24.777771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:24.777855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 06:57:241980.] 777867eager alloc mem 25.25 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:24.777922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:57:242022-12-12 06:57:24..777959777961: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[2022-12-12 06:57:24.778004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 06:57:24] .eager alloc mem 25.25 KB778022
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.778196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:24.778258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:57:24
.778279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:57:24.778344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:57:24.778414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:57:24.778454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.778537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:57:24.778570: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:57:24:.638778584] : eager release cuda mem 25855E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.778621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.778836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:57:24.778874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:57:24.778897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:57:24.778934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[[[[[[[[2022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:27........346609346615346619346615346609346609346609346609: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[[2022-12-12 06:57:27[[2022-12-12 06:57:272022-12-12 06:57:27[[.2022-12-12 06:57:27[2022-12-12 06:57:27..2022-12-12 06:57:272022-12-12 06:57:27347749.2022-12-12 06:57:27.347750347753..: 347757.347756: : 347763347765E: 347772: EE: :  E: E  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638::] 638:638] ] 638638eager release cuda mem 625663] 638] eager release cuda mem 625663eager release cuda mem 625663] ] 
eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663



[2022-12-12 06:57:27.348098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:[2022-12-12 06:57:2719802022-12-12 06:57:27.[] .348112[2022-12-12 06:57:27[[[eager alloc mem 611.00 KB348115: 2022-12-12 06:57:27.2022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:27
: E.348129...E 348136: 348140348136348142 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E: : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E EEE:1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 611.00 KB:1980:::eager alloc mem 611.00 KB
1980] 198019801980
] eager alloc mem 611.00 KB] ] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-12 06:57:27.348934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.349010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.349123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:57:27
.349147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.349175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:57:27[2022-12-12 06:57:27[.[2022-12-12 06:57:27.[2022-12-12 06:57:273492012022-12-12 06:57:27.3492032022-12-12 06:57:27.: .[349207: .349209E3492152022-12-12 06:57:27: E349222:  : .E : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE349249 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] : 638] :638eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 611.00 KB1980] 
] :eager release cuda mem 625663
] eager release cuda mem 625663eager release cuda mem 6256631980
eager alloc mem 611.00 KB

] 
eager alloc mem 611.00 KB
[2022-12-12 06:57:27.349462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27[.[2022-12-12 06:57:273494892022-12-12 06:57:27.: .349494E349497:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1980eager alloc mem 611.00 KB1980] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 06:57:27.349760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.349826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.350155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 06:57:27[.2022-12-12 06:57:27350180.: 350187E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[:] 2022-12-12 06:57:27638[eager release cuda mem 625663.] 2022-12-12 06:57:27
350215eager release cuda mem 625663.: 
350230E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638[:] 2022-12-12 06:57:271980eager release cuda mem 625663.[] 
3503022022-12-12 06:57:27[eager alloc mem 611.00 KB: .[[2022-12-12 06:57:27
E3503212022-12-12 06:57:272022-12-12 06:57:27. : ..350335/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[E350345350346: :2022-12-12 06:57:27 : : E1980./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE ] 350386:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
E] ::638 eager alloc mem 611.00 KB638638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
] ] eager release cuda mem 625663:eager release cuda mem 625663eager release cuda mem 625663
1980

[] 2022-12-12 06:57:27eager alloc mem 611.00 KB.
350571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[[2022-12-12 06:57:272022-12-12 06:57:272022-12-12 06:57:27...350619350622350624[: : : 2022-12-12 06:57:27EEE.   350652/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :::E198019801980 ] ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB:


1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.351091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.351169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.351233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.351285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 06:57:27] .eager release cuda mem 625663351302
: [E2022-12-12 06:57:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu351316:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
:2022-12-12 06:57:27638.] 351364eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.351447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27[[.2022-12-12 06:57:27[2022-12-12 06:57:27351505.2022-12-12 06:57:27.: 351509.351511E: 351518:  E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] 638:638eager release cuda mem 625663] 638] 
eager release cuda mem 625663] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-12 06:57:27.[3516912022-12-12 06:57:27[[: .2022-12-12 06:57:272022-12-12 06:57:27E351698.. : 351703351704/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: : : EE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980::
] 19801980eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 06:57:27.351917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.351990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.352104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.352141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.352173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.352199: [E2022-12-12 06:57:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc352209:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.352304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.352545: [E2022-12-12 06:57:27[[ .2022-12-12 06:57:272022-12-12 06:57:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc352557..:: 352565352566638E: : ]  EEeager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638::] 638638eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 06:57:27.352697: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[2022-12-12 06:57:27:2022-12-12 06:57:27[2022-12-12 06:57:27.1980.2022-12-12 06:57:27.352719] 352725.352728: eager alloc mem 611.00 KB: 352734: E
E: E  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:19801980:1980] ] 638] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB

eager release cuda mem 625663

[2022-12-12 06:57:27.352920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.352960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:57:272022-12-12 06:57:27..352990352991: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-12 06:57:27.353051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.353083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.353119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:57:27.353548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.353619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[[] [2022-12-12 06:57:272022-12-12 06:57:27eager alloc mem 611.00 KB2022-12-12 06:57:27..
.353634353633353637: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::638638638[] ] ] 2022-12-12 06:57:27eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663.


353710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] [2022-12-12 06:57:27eager release cuda mem 6256632022-12-12 06:57:27.
.353760[[353760: 2022-12-12 06:57:27[2022-12-12 06:57:27: E.2022-12-12 06:57:27.E [353792.353796 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:57:27: 353806[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.E: 2022-12-12 06:57:27E:638353835 E. 638] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc353865/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager release cuda mem 625663E::: :eager release cuda mem 100400000
 1980638E1980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] [ ] :eager alloc mem 611.00 KBeager release cuda mem 1004000002022-12-12 06:57:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB638

.:
] 354003638eager release cuda mem 625663: ] 
Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 06:57:27638.] 354110eager release cuda mem 100400000[: 
2022-12-12 06:57:27E. 354124/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 100400000:
638] eager release cuda mem 100400000
[2022-12-12 06:57:27.354399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:57:27.354438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:57:27.354785: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:57:27:.638354798] : eager release cuda mem 625663E
[ 2022-12-12 06:57:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.:2022-12-12 06:57:27354814638.: ] 354834Eeager release cuda mem 625663:  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1793:] [638Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.73586 secs 2022-12-12 06:57:27] 
.eager release cuda mem 100400000354880
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:57:27.355284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.75174 secs 
[2022-12-12 06:57:27.355593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.73585 secs 
[2022-12-12 06:57:27.355902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.73622 secs 
[2022-12-12 06:57:27.356204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.73721 secs 
[2022-12-12 06:57:27.356597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.7376 secs 
[2022-12-12 06:57:27.356789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.73827 secs 
[2022-12-12 06:57:27.356978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.73799 secs 
[HCTR][06:57:27.357][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][tid #139843690538752]: replica 7 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][tid #139844026083072]: replica 6 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][tid #139843329849088]: replica 2 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][tid #139843749254912]: replica 0 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][tid #139844286125824]: replica 3 calling init per replica done, doing barrier
[HCTR][06:57:27.357][ERROR][RK0][tid #139844286125824]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][tid #139843749254912]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][tid #139844026083072]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][tid #139843690538752]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][tid #139844026083072]: init per replica done
[HCTR][06:57:27.357][ERROR][RK0][tid #139843329849088]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:57:27.357][ERROR][RK0][tid #139844286125824]: init per replica done
[HCTR][06:57:27.357][ERROR][RK0][main]: init per replica done
[HCTR][06:57:27.357][ERROR][RK0][main]: init per replica done
[HCTR][06:57:27.357][ERROR][RK0][tid #139843329849088]: init per replica done
[HCTR][06:57:27.357][ERROR][RK0][main]: init per replica done
[HCTR][06:57:27.357][ERROR][RK0][tid #139843690538752]: init per replica done
[HCTR][06:57:27.359][ERROR][RK0][tid #139843749254912]: init per replica done
