2022-12-12 02:45:44.947951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:44.953303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:44.959944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:44.967327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:44.979544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:44.986246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:44.991742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.004121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.058288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.062390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.062805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.066613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.066682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.068301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.068393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.070223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.070278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.071899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.071988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.073499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.073735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.075041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.075681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.076793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.077611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.078294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.079699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.080861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.081941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.083103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.084185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.085148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.087020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.088271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.089353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.090414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.091465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.092625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.093663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.094691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.097719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.098711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.099912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.100607: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.101017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.101994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.102988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.103980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.105323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.108919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.110078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.110719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.112085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.112452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.112803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.113954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.114842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.114996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.116963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.117145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.119543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.119724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.122598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.122732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.123165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.123190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.126125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.126325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.126906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.127068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.130032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.130557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.131010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.131566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.133534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.133735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.134531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.135118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.135647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.136786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.137707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.138169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.138785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.139357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.140406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.140943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.141359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.141655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.142768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.143262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.148804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.148994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.149859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.151451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.152154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.152702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.152747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.153217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.155308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.155540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.155849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.156062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.174737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.180529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.191463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.191504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.192024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.192046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.193276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.193331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.195198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.195844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.196072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.197298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.197472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.197940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.198800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.200364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.201719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.202692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.203093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.203663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.204384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.205877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.206013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.206578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.207206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.207860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.209742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.209873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.210339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.210976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.212031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.213702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.213848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.214230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.215598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.216117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.217596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.217859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.218307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.219268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.219542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.221027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.221201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.221539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.222605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.223102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.224619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.224848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.225105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.226303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.226552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.228753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.228799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.229687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.231316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.231491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.233073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.233120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.233606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.234904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.234909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.236673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.236833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.237198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.238402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.238578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.239004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.240181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.240391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.240872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.242502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.242840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.242951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.243925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.243932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.244059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.244637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.247481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.247639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.248389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.248498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.248533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.249019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.251449: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.251533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.251761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.252902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.253555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.253728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.254431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.256563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.256787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.257489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.258416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.258421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.258788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.260461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.261100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.261572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.262058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.262646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.263156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.263266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.264910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.265438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.266098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.266379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.267022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.267916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.268011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.269182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.269842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.270587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.271043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.271480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.272326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.272565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.275083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.276666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.276981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.277445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.278460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.278659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.280132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.281409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.282663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.282696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.282919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.283840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.285135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.285512: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.286255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.286316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.286467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.287359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.288853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.290276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.291148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.292509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.293706: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.293706: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.294327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.294459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.295288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.296617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.298090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.298817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.299848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.301069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.301400: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.301542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.302611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.302621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.302631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.306134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.306412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.306474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.306521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.309680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.310858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.311522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.311694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.312070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.322716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.323520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.324739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.326754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.327641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.329130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.331987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.391083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.393060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.395604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.398589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.402134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.405856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.407516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.413556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.414896: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.423850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.427469: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:45:45.430271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.433954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.436003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.477700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:45.486260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.449014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.450122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.450891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.451871: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.451955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:45:46.475691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.476621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.477643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.478495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.479457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.480388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:45:46.532378: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.532592: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.567816: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:45:46.710471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.711081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.711691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.712178: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.712234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:45:46.731255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.732290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.732993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.733726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.734281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.734747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:45:46.736745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.737361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.738108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.738576: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.738628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:45:46.746262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.746864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.747414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.747884: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.747935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:45:46.754918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.754918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.756226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.756252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.757260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.757301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.758110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.758533: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.758587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:45:46.758673: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.758719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:45:46.759690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.760238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.760803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.761330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.761795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:45:46.766995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.767612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.768128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.768694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.769216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.769682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:45:46.776522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.776924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.777252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.777931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.778136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.778839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.779121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.780314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.780522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.781230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.781386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:45:46.781765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:45:46.812255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.812904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.813265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.813611: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.813776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.813781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.814224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.814638: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.814703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:45:46.815025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.815366: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.815500: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.815544: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:45:46.815587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:45:46.815655: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:45:46.817214: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:45:46.827171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.827371: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.827689: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.827833: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.829287: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:45:46.831495: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:45:46.832285: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.832425: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.833581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.834186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.834241: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 02:45:46.834256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.835279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.835283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.836260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.836377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.837234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.837302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.838235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:45:46.838276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:45:46.838760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:45:46.884488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.884686: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.885137: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.885284: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:45:46.886526: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:45:46.887057: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][02:45:48.135][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.135][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.135][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.136][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.201][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.201][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.201][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:45:48.201][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 99it [00:01, 84.03it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 192it [00:01, 175.56it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 92it [00:01, 78.79it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 288it [00:01, 280.44it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 100it [00:01, 85.00it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 98it [00:01, 85.38it/s]warmup run: 99it [00:01, 85.31it/s]warmup run: 95it [00:01, 82.13it/s]warmup run: 183it [00:01, 169.55it/s]warmup run: 386it [00:01, 392.41it/s]warmup run: 96it [00:01, 82.97it/s]warmup run: 201it [00:01, 185.24it/s]warmup run: 85it [00:01, 74.51it/s]warmup run: 197it [00:01, 185.71it/s]warmup run: 200it [00:01, 186.80it/s]warmup run: 192it [00:01, 179.87it/s]warmup run: 277it [00:01, 273.22it/s]warmup run: 485it [00:02, 503.64it/s]warmup run: 192it [00:01, 179.47it/s]warmup run: 297it [00:01, 289.03it/s]warmup run: 175it [00:01, 166.49it/s]warmup run: 290it [00:01, 287.25it/s]warmup run: 300it [00:01, 296.81it/s]warmup run: 283it [00:01, 279.08it/s]warmup run: 372it [00:01, 382.18it/s]warmup run: 585it [00:02, 606.74it/s]warmup run: 289it [00:01, 286.73it/s]warmup run: 397it [00:01, 402.73it/s]warmup run: 275it [00:01, 280.92it/s]warmup run: 390it [00:01, 403.24it/s]warmup run: 395it [00:01, 402.08it/s]warmup run: 378it [00:01, 387.74it/s]warmup run: 463it [00:02, 480.61it/s]warmup run: 685it [00:02, 697.14it/s]warmup run: 387it [00:01, 398.97it/s]warmup run: 497it [00:02, 513.85it/s]warmup run: 375it [00:01, 398.77it/s]warmup run: 491it [00:01, 517.56it/s]warmup run: 488it [00:02, 501.04it/s]warmup run: 475it [00:02, 497.24it/s]warmup run: 555it [00:02, 573.02it/s]warmup run: 784it [00:02, 769.49it/s]warmup run: 483it [00:02, 503.72it/s]warmup run: 598it [00:02, 617.38it/s]warmup run: 474it [00:01, 510.95it/s]warmup run: 593it [00:02, 623.22it/s]warmup run: 583it [00:02, 594.97it/s]warmup run: 577it [00:02, 607.40it/s]warmup run: 653it [00:02, 666.97it/s]warmup run: 884it [00:02, 827.82it/s]warmup run: 580it [00:02, 600.97it/s]warmup run: 699it [00:02, 707.50it/s]warmup run: 574it [00:02, 614.44it/s]warmup run: 696it [00:02, 717.72it/s]warmup run: 679it [00:02, 702.63it/s]warmup run: 679it [00:02, 678.00it/s]warmup run: 754it [00:02, 750.49it/s]warmup run: 984it [00:02, 872.25it/s]warmup run: 678it [00:02, 687.50it/s]warmup run: 799it [00:02, 778.44it/s]warmup run: 674it [00:02, 703.49it/s]warmup run: 799it [00:02, 794.35it/s]warmup run: 772it [00:02, 739.32it/s]warmup run: 781it [00:02, 780.19it/s]warmup run: 849it [00:02, 789.13it/s]warmup run: 1085it [00:02, 908.18it/s]warmup run: 775it [00:02, 757.07it/s]warmup run: 898it [00:02, 832.57it/s]warmup run: 775it [00:02, 779.33it/s]warmup run: 902it [00:02, 854.75it/s]warmup run: 881it [00:02, 837.68it/s]warmup run: 865it [00:02, 788.98it/s]warmup run: 946it [00:02, 835.46it/s]warmup run: 1186it [00:02, 937.05it/s]warmup run: 879it [00:02, 829.52it/s]warmup run: 997it [00:02, 868.45it/s]warmup run: 876it [00:02, 838.39it/s]warmup run: 1005it [00:02, 901.26it/s]warmup run: 982it [00:02, 883.89it/s]warmup run: 959it [00:02, 829.17it/s]warmup run: 1043it [00:02, 871.19it/s]warmup run: 1286it [00:02, 952.30it/s]warmup run: 983it [00:02, 885.79it/s]warmup run: 1095it [00:02, 895.42it/s]warmup run: 977it [00:02, 882.85it/s]warmup run: 1108it [00:02, 935.89it/s]warmup run: 1084it [00:02, 921.03it/s]warmup run: 1054it [00:02, 861.18it/s]warmup run: 1140it [00:02, 898.32it/s]warmup run: 1386it [00:02, 965.98it/s]warmup run: 1088it [00:02, 931.65it/s]warmup run: 1193it [00:02, 914.56it/s]warmup run: 1077it [00:02, 915.49it/s]warmup run: 1212it [00:02, 964.12it/s]warmup run: 1185it [00:02, 945.40it/s]warmup run: 1153it [00:02, 896.06it/s]warmup run: 1237it [00:02, 917.67it/s]warmup run: 1488it [00:03, 980.51it/s]warmup run: 1193it [00:02, 964.13it/s]warmup run: 1290it [00:02, 927.43it/s]warmup run: 1177it [00:02, 937.26it/s]warmup run: 1315it [00:02, 977.21it/s]warmup run: 1286it [00:02, 962.02it/s]warmup run: 1254it [00:02, 928.91it/s]warmup run: 1334it [00:02, 930.61it/s]warmup run: 1589it [00:03, 981.74it/s]warmup run: 1296it [00:02, 980.48it/s]warmup run: 1387it [00:02, 936.35it/s]warmup run: 1277it [00:02, 940.15it/s]warmup run: 1418it [00:02, 992.40it/s]warmup run: 1358it [00:02, 960.39it/s]warmup run: 1389it [00:02, 979.32it/s]warmup run: 1432it [00:03, 943.82it/s]warmup run: 1690it [00:03, 989.91it/s]warmup run: 1401it [00:02, 998.19it/s]warmup run: 1485it [00:03, 946.75it/s]warmup run: 1375it [00:02, 940.42it/s]warmup run: 1521it [00:03, 1001.87it/s]warmup run: 1459it [00:03, 972.07it/s]warmup run: 1492it [00:03, 991.34it/s]warmup run: 1530it [00:03, 952.96it/s]warmup run: 1790it [00:03, 992.52it/s]warmup run: 1505it [00:03, 1009.64it/s]warmup run: 1586it [00:03, 963.52it/s]warmup run: 1472it [00:02, 948.97it/s]warmup run: 1624it [00:03, 1008.76it/s]warmup run: 1595it [00:03, 1000.42it/s]warmup run: 1559it [00:03, 973.88it/s]warmup run: 1628it [00:03, 960.42it/s]warmup run: 1609it [00:03, 1017.18it/s]warmup run: 1890it [00:03, 976.85it/s]warmup run: 1690it [00:03, 983.33it/s]warmup run: 1569it [00:03, 948.98it/s]warmup run: 1727it [00:03, 1003.97it/s]warmup run: 1697it [00:03, 1003.77it/s]warmup run: 1658it [00:03, 978.62it/s]warmup run: 1725it [00:03, 961.38it/s]warmup run: 1713it [00:03, 1022.00it/s]warmup run: 1989it [00:03, 980.28it/s]warmup run: 1794it [00:03, 998.71it/s]warmup run: 1666it [00:03, 947.01it/s]warmup run: 1829it [00:03, 1004.54it/s]warmup run: 1760it [00:03, 988.91it/s]warmup run: 1799it [00:03, 1005.07it/s]warmup run: 1824it [00:03, 967.16it/s]warmup run: 1817it [00:03, 1027.22it/s]warmup run: 2105it [00:03, 1032.53it/s]warmup run: 1896it [00:03, 1003.85it/s]warmup run: 1763it [00:03, 951.24it/s]warmup run: 1931it [00:03, 1004.44it/s]warmup run: 1863it [00:03, 1000.40it/s]warmup run: 1902it [00:03, 1009.31it/s]warmup run: 1922it [00:03, 955.91it/s]warmup run: 1921it [00:03, 1028.64it/s]warmup run: 2223it [00:03, 1074.99it/s]warmup run: 1997it [00:03, 1003.58it/s]warmup run: 1859it [00:03, 953.31it/s]warmup run: 2039it [00:03, 1024.30it/s]warmup run: 1965it [00:03, 1005.21it/s]warmup run: 2006it [00:03, 1016.60it/s]warmup run: 2025it [00:03, 976.28it/s]warmup run: 2027it [00:03, 1035.61it/s]warmup run: 2338it [00:03, 1095.11it/s]warmup run: 2116it [00:03, 1058.26it/s]warmup run: 1956it [00:03, 957.17it/s]warmup run: 2161it [00:03, 1080.14it/s]warmup run: 2079it [00:03, 1044.15it/s]warmup run: 2128it [00:03, 1075.65it/s]warmup run: 2143it [00:03, 1035.70it/s]warmup run: 2148it [00:03, 1086.84it/s]warmup run: 2457it [00:03, 1122.62it/s]warmup run: 2237it [00:03, 1101.65it/s]warmup run: 2065it [00:03, 996.32it/s]warmup run: 2283it [00:03, 1120.03it/s]warmup run: 2199it [00:03, 1088.81it/s]warmup run: 2250it [00:03, 1117.57it/s]warmup run: 2261it [00:03, 1076.98it/s]warmup run: 2269it [00:03, 1122.22it/s]warmup run: 2576it [00:04, 1142.53it/s]warmup run: 2358it [00:03, 1132.16it/s]warmup run: 2185it [00:03, 1056.60it/s]warmup run: 2405it [00:03, 1147.80it/s]warmup run: 2319it [00:03, 1121.27it/s]warmup run: 2372it [00:03, 1145.77it/s]warmup run: 2379it [00:03, 1106.31it/s]warmup run: 2390it [00:03, 1146.91it/s]warmup run: 2693it [00:04, 1148.87it/s]warmup run: 2478it [00:03, 1150.47it/s]warmup run: 2306it [00:03, 1100.99it/s]warmup run: 2528it [00:03, 1170.41it/s]warmup run: 2439it [00:03, 1144.06it/s]warmup run: 2494it [00:03, 1166.07it/s]warmup run: 2498it [00:04, 1129.16it/s]warmup run: 2511it [00:03, 1164.57it/s]warmup run: 2811it [00:04, 1156.86it/s]warmup run: 2598it [00:04, 1162.57it/s]warmup run: 2427it [00:03, 1132.30it/s]warmup run: 2651it [00:04, 1187.35it/s]warmup run: 2559it [00:04, 1159.71it/s]warmup run: 2615it [00:04, 1178.21it/s]warmup run: 2617it [00:04, 1145.62it/s]warmup run: 2633it [00:04, 1178.57it/s]warmup run: 2929it [00:04, 1163.57it/s]warmup run: 2718it [00:04, 1173.35it/s]warmup run: 2549it [00:04, 1157.51it/s]warmup run: 3000it [00:04, 678.54it/s] warmup run: 2772it [00:04, 1193.83it/s]warmup run: 2678it [00:04, 1168.67it/s]warmup run: 2733it [00:04, 1161.57it/s]warmup run: 2735it [00:04, 1154.45it/s]warmup run: 2754it [00:04, 1185.66it/s]warmup run: 2837it [00:04, 1177.58it/s]warmup run: 2671it [00:04, 1175.83it/s]warmup run: 2895it [00:04, 1203.06it/s]warmup run: 2796it [00:04, 1170.77it/s]warmup run: 2852it [00:04, 1157.73it/s]warmup run: 2850it [00:04, 1142.69it/s]warmup run: 2875it [00:04, 1192.23it/s]warmup run: 2958it [00:04, 1185.38it/s]warmup run: 2792it [00:04, 1184.34it/s]warmup run: 3000it [00:04, 682.57it/s] warmup run: 3000it [00:04, 697.07it/s] warmup run: 2915it [00:04, 1174.65it/s]warmup run: 2970it [00:04, 1164.11it/s]warmup run: 2965it [00:04, 1126.20it/s]warmup run: 2995it [00:04, 1193.99it/s]warmup run: 3000it [00:04, 692.85it/s] warmup run: 3000it [00:04, 670.05it/s] warmup run: 3000it [00:04, 686.29it/s] warmup run: 2913it [00:04, 1189.75it/s]warmup run: 3000it [00:04, 681.25it/s] warmup run: 3000it [00:04, 684.22it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.39it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.44it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.89it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.99it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.03it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.00it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.46it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.84it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1619.13it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1671.38it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.01it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.10it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.63it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1627.87it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1633.87it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1677.11it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1636.01it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1658.19it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1641.57it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1614.56it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1662.49it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1668.06it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1622.70it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1673.10it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1644.20it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1656.31it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1612.87it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1667.06it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1660.07it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1672.80it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1618.50it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1599.33it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1645.29it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1654.75it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1613.89it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1665.08it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1671.63it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1612.71it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1648.19it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1600.49it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1644.84it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1619.23it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1653.56it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1661.67it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1669.08it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1605.48it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1637.48it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1599.16it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1643.45it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1623.80it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1648.44it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1658.40it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1664.96it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1598.70it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1633.15it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1595.01it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1628.53it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1643.19it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1648.47it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:01, 1658.09it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1665.86it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1596.99it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1639.96it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1595.50it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1630.67it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1642.26it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1648.73it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1666.43it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1659.29it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1595.73it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1644.31it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1596.26it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1629.33it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1639.94it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1649.29it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1661.12it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1668.27it/s]warmup should be done:  54%|█████▍    | 1618/3000 [00:01<00:00, 1595.07it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1648.58it/s]warmup should be done:  54%|█████▍    | 1618/3000 [00:01<00:00, 1597.37it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1629.82it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1637.92it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1650.57it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1661.70it/s]warmup should be done:  59%|█████▉    | 1778/3000 [00:01<00:00, 1596.33it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1651.35it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1598.67it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1510.56it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1631.59it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1639.38it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1648.69it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1661.13it/s]warmup should be done:  65%|██████▍   | 1938/3000 [00:01<00:00, 1594.47it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1650.77it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1597.33it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1412.21it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1633.41it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1639.02it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1647.01it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1656.26it/s]warmup should be done:  70%|██████▉   | 2098/3000 [00:01<00:00, 1592.58it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1646.92it/s]warmup should be done:  70%|██████▉   | 2099/3000 [00:01<00:00, 1596.77it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1454.69it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1634.80it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1638.18it/s]warmup should be done:  77%|███████▋  | 2321/3000 [00:01<00:00, 1638.48it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1654.95it/s]warmup should be done:  75%|███████▌  | 2258/3000 [00:01<00:00, 1592.21it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1646.77it/s]warmup should be done:  75%|███████▌  | 2259/3000 [00:01<00:00, 1596.24it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1501.03it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1633.30it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1635.92it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1652.95it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1634.11it/s]warmup should be done:  81%|████████  | 2418/3000 [00:01<00:00, 1589.59it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1644.54it/s]warmup should be done:  81%|████████  | 2419/3000 [00:01<00:00, 1594.45it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1531.49it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1634.88it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1636.04it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1654.53it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1637.89it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1595.18it/s]warmup should be done:  86%|████████▌ | 2578/3000 [00:01<00:00, 1590.49it/s]warmup should be done:  89%|████████▊ | 2656/3000 [00:01<00:00, 1646.12it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1555.82it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1635.70it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1635.82it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1658.92it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1640.83it/s]warmup should be done:  91%|█████████▏| 2739/3000 [00:01<00:00, 1595.51it/s]warmup should be done:  94%|█████████▍| 2822/3000 [00:01<00:00, 1649.00it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1591.29it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1573.71it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1641.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.41it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1640.50it/s]warmup should be done:  99%|█████████▉| 2982/3000 [00:01<00:00, 1647.72it/s]warmup should be done: 100%|█████████▉| 2988/3000 [00:01<00:00, 1651.18it/s]warmup should be done:  97%|█████████▋| 2899/3000 [00:01<00:00, 1595.72it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1599.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.65it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1591.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1599.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1588.55it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.18it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1697.40it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.37it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.28it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.55it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.11it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.58it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.74it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1703.25it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1653.13it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1678.68it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1687.78it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1640.71it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1649.45it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1620.88it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1629.46it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1654.72it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1707.66it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1691.68it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1681.24it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.30it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1646.36it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1609.74it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1614.34it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1655.95it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1709.06it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1645.53it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1693.03it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1684.97it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1649.45it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1623.74it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1626.86it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1647.09it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1686.67it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1691.37it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1652.50it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1627.31it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1629.19it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1477.97it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1517.26it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1644.29it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1654.88it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1684.72it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1685.43it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1613.62it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1615.30it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1542.76it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1576.19it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1656.35it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1644.45it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1681.71it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1680.38it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1615.57it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1617.04it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1585.14it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1616.60it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1655.84it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1641.06it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1684.53it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1674.67it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1625.92it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1626.34it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1616.56it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1647.31it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1657.15it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1642.87it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1683.79it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1668.11it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1617.01it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1617.66it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1638.62it/s]warmup should be done:  51%|█████▏    | 1542/3000 [00:00<00:00, 1666.48it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1657.07it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1645.17it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1684.02it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1665.29it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1629.21it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1628.91it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1654.68it/s]warmup should be done:  57%|█████▋    | 1713/3000 [00:01<00:00, 1679.46it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1656.27it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1645.61it/s]warmup should be done:  62%|██████▏   | 1860/3000 [00:01<00:00, 1683.64it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1664.36it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1630.92it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1631.08it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1665.75it/s]warmup should be done:  63%|██████▎   | 1885/3000 [00:01<00:00, 1689.43it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1652.36it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1643.78it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1684.31it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1662.02it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1621.83it/s]warmup should be done:  66%|██████▌   | 1977/3000 [00:01<00:00, 1623.17it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1671.64it/s]warmup should be done:  69%|██████▊   | 2057/3000 [00:01<00:00, 1696.69it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1644.03it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1651.12it/s]warmup should be done:  73%|███████▎  | 2198/3000 [00:01<00:00, 1683.04it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1668.99it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1632.04it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1631.45it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1675.81it/s]warmup should be done:  74%|███████▍  | 2228/3000 [00:01<00:00, 1699.48it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1645.53it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1653.03it/s]warmup should be done:  79%|███████▉  | 2367/3000 [00:01<00:00, 1682.36it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1676.91it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1634.68it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1634.65it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1680.61it/s]warmup should be done:  80%|███████▉  | 2399/3000 [00:01<00:00, 1701.73it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1654.16it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1644.53it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1683.08it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1683.40it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1636.47it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1636.49it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1684.39it/s]warmup should be done:  86%|████████▌ | 2571/3000 [00:01<00:00, 1704.41it/s]warmup should be done:  89%|████████▊ | 2656/3000 [00:01<00:00, 1652.96it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1642.81it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1684.33it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1688.18it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1630.36it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1631.46it/s]warmup should be done:  90%|████████▉ | 2693/3000 [00:01<00:00, 1686.61it/s]warmup should be done:  91%|█████████▏| 2743/3000 [00:01<00:00, 1708.08it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1644.67it/s]warmup should be done:  94%|█████████▍| 2822/3000 [00:01<00:00, 1652.30it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1683.87it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1686.80it/s]warmup should be done:  93%|█████████▎| 2797/3000 [00:01<00:00, 1637.46it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1636.57it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1686.15it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1708.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.70it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1649.99it/s]warmup should be done: 100%|█████████▉| 2989/3000 [00:01<00:00, 1654.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.43it/s]warmup should be done:  99%|█████████▉| 2963/3000 [00:01<00:00, 1643.58it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1639.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.93it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253ac582e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253ac540d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253af67e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253ac632b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253af68730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253ac531c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253ac55130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f253ac551f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 02:47:18.291555: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f207302ce10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:18.291621: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:18.299635: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:18.318266: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f20770291a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:18.318309: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:18.327741: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:18.595487: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2072834740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:18.595549: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:18.605086: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:18.759529: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f206a831150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:18.759593: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:18.769165: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:19.171299: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2072831070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:19.171376: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:19.172143: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2077028d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:19.172204: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:19.181739: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:19.181742: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:19.222863: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f206f0310b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:19.222927: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:19.227882: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2072795bd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:47:19.227924: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:47:19.231912: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:19.235221: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:47:25.527907: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:25.551718: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:25.764168: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:25.840311: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:25.888512: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:25.950516: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:26.039507: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:47:26.130599: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:48:19.333][ERROR][RK0][tid #139777949030144]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:48:19.333][ERROR][RK0][tid #139777949030144]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.342][ERROR][RK0][tid #139777949030144]: coll ps creation done
[HCTR][02:48:19.342][ERROR][RK0][tid #139777949030144]: replica 7 waits for coll ps creation barrier
[HCTR][02:48:19.384][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:48:19.384][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.389][ERROR][RK0][main]: coll ps creation done
[HCTR][02:48:19.389][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:48:19.631][ERROR][RK0][tid #139777756096256]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:48:19.632][ERROR][RK0][tid #139777756096256]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.637][ERROR][RK0][tid #139777756096256]: coll ps creation done
[HCTR][02:48:19.637][ERROR][RK0][tid #139777756096256]: replica 3 waits for coll ps creation barrier
[HCTR][02:48:19.653][ERROR][RK0][tid #139777672201984]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:48:19.653][ERROR][RK0][tid #139777672201984]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.658][ERROR][RK0][tid #139777672201984]: coll ps creation done
[HCTR][02:48:19.658][ERROR][RK0][tid #139777672201984]: replica 4 waits for coll ps creation barrier
[HCTR][02:48:19.663][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:48:19.664][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.670][ERROR][RK0][main]: coll ps creation done
[HCTR][02:48:19.670][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][02:48:19.693][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:48:19.693][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.698][ERROR][RK0][main]: coll ps creation done
[HCTR][02:48:19.698][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][02:48:19.841][ERROR][RK0][tid #139778217465600]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:48:19.842][ERROR][RK0][tid #139778217465600]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.846][ERROR][RK0][tid #139778217465600]: coll ps creation done
[HCTR][02:48:19.847][ERROR][RK0][tid #139778217465600]: replica 0 waits for coll ps creation barrier
[HCTR][02:48:19.910][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:48:19.910][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][02:48:19.915][ERROR][RK0][main]: coll ps creation done
[HCTR][02:48:19.915][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][02:48:19.915][ERROR][RK0][tid #139778217465600]: replica 0 preparing frequency
[HCTR][02:48:20.764][ERROR][RK0][tid #139778217465600]: replica 0 preparing frequency done
[HCTR][02:48:20.799][ERROR][RK0][tid #139778217465600]: replica 0 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][tid #139777949030144]: replica 7 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][tid #139777672201984]: replica 4 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][tid #139777756096256]: replica 3 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:48:20.799][ERROR][RK0][tid #139778217465600]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][tid #139777949030144]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][main]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][tid #139777672201984]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][tid #139777756096256]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][main]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][main]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][tid #139778217465600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][tid #139777949030144]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][tid #139777672201984]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][main]: Calling build_v2
[HCTR][02:48:20.799][ERROR][RK0][tid #139777756096256]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:48:20.799][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 02:48:20.803922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie
2022-12-12 02:48:20[.2022-12-12 02:48:20803965.: 803998E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 2022-12-12 02:48:20196v100x8, slow pcie.] 
804006assigning 0 to cpu: 
E[ 2022-12-12 02:48:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.:804057178: [] E2022-12-12 02:48:202022-12-12 02:48:20v100x8, slow pcie ..
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc804051804078:: : 196[[EE] 2022-12-12 02:48:20  assigning 0 to cpu./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:48:20
804107::.: 178212[804096E] ] :  2022-12-12 02:48:20v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.

 :804143[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196: [2022-12-12 02:48:20:] E2022-12-12 02:48:20.[178assigning 0 to cpu .8041942022-12-12 02:48:20] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc804198: .v100x8, slow pcie:: E804211
[178E : []  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 02:48:20v100x8, slow pcie2022-12-12 02:48:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[ .
.:2122022-12-12 02:48:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc8042632022-12-12 02:48:20804220[196] .:: .: 2022-12-12 02:48:20] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8804285213E804257E.assigning 0 to cpu
: ]  :  804319
Eremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  
: 2022-12-12 02:48:20:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.[178 :] :2022-12-12 02:48:208044152022-12-12 02:48:20] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212assigning 0 to cpu178.: .v100x8, slow pcie:] 
] 804452E804455
196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie:  : ] 

E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[assigning 0 to cpu2022-12-12 02:48:20 : [[[
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:48:202022-12-12 02:48:202022-12-12 02:48:20804548:] :...: 214remote time is 8.68421[212804565804564804567E] 
2022-12-12 02:48:20] : : [:  cpu time is 97.0588.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8EE2022-12-12 02:48:20E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
804624
  . :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc804671/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E:196:: :2022-12-12 02:48:20 212] 196E213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] assigning 0 to cpu]  ] 804721:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421: 212

:
E] 214 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.05882022-12-12 02:48:202022-12-12 02:48:20:
..213[804829804830] 2022-12-12 02:48:20[: [: remote time is 8.68421.2022-12-12 02:48:20E2022-12-12 02:48:20E
804853. 804856. : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 804860/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 02:48:20:E: : .213E 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc804906]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :: remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:cpu time is 97.0588213E
:212
]  212[] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 02:48:20build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[
214
8050212022-12-12 02:48:20] : .cpu time is 97.0588E805063[
 : [2022-12-12 02:48:20/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 02:48:20.: .805087805091214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : ] :EEcpu time is 97.0588214 
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588::
213213] ] remote time is 8.68421remote time is 8.68421

[2022-12-12 02:48:20.[8052502022-12-12 02:48:20: .E805254 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588
214] cpu time is 97.0588
[2022-12-12 02:49:37.815344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:49:37.855245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 02:49:37.976925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:49:37.976990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:49:37.977023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:49:37.977054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:49:37.977540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:49:37.977592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.978543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.979207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.992286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 02:49:37.992352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 02:49:37.992781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:49:37.992832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.993492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 02:49:37.993567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 02:49:37.993612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 02:49:37.993678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 02:49:37.994003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:49:37.994059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.994103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:49:37.994156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:49:372022-12-12 02:49:37..994337994344: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved5 solved

[[2022-12-12 02:49:372022-12-12 02:49:37..994453994457: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 4 initing device 4worker 0 thread 5 initing device 5

[2022-12-12 02:49:37.994926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-12 02:49:37
.994950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 02:49:37.994987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 02:49:37.995017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.997174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [7 solved2022-12-12 02:49:37
.997206: E[ 2022-12-12 02:49:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:997251202: ] E3 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 7 initing device 72022-12-12 02:49:37
.997311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 02:49:37.997714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:49:37.997745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 02:49:371815.] 997761Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:37.997821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  2542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  2821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  2862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  3552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  3622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  4120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  4611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  7155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  7344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  7461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  7599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  8086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  8141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38.  9098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:49:38. 57901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 02:49:38. 64631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:49:38. 64757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:49:38. 65681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38. 66472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38. 67500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:38. 67549: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[[[2022-12-12 02:49:382022-12-12 02:49:382022-12-12 02:49:38... 89744 89744 89744: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 02:49:38. 95427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:49:38. 95588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:49:38. 95671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:49:38. 95720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:49:38. 95801: E[ 2022-12-12 02:49:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 95797638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 02:49:38. 95890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:49:38. 96057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:49:38. 96101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[2022-12-12 02:49:382022-12-12 02:49:38.. 96366 96369: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::[198019802022-12-12 02:49:38] ] [.eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes2022-12-12 02:49:38 96434

.:  96444E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 5.00 Bytes] 
eager alloc mem 5.00 Bytes
[2022-12-12 02:49:38. 96642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.103100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.103641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.106167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38.106401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38.106450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38[.2022-12-12 02:49:38107208.: 107194E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] [
eager release cuda mem 52022-12-12 02:49:38
.107274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:49:38eager release cuda mem 5.
107324: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-12 02:49:3843.] [107353[WORKER[0] alloc host memory 38.15 MB2022-12-12 02:49:38: 2022-12-12 02:49:38
.E.107357 107384: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: E:2022-12-12 02:49:38E 638. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 107413/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 400000000: :638[
[E638] 2022-12-12 02:49:382022-12-12 02:49:38 ] eager release cuda mem 5../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000
107467107455:
: : 638EE]  [ eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:49:38/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:.:638107586638] : [] eager release cuda mem 625663E2022-12-12 02:49:38eager release cuda mem 5
 .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc107630:[: 6382022-12-12 02:49:38W] . eager release cuda mem 400000000107700/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
: :W[43 2022-12-12 02:49:38] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.WORKER[0] alloc host memory 38.15 MB:107747
43: ] EWORKER[0] alloc host memory 38.15 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:49:38.108592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.113557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.126503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.127079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:49:38.127852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38.127981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38.128227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38.128350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:38.128844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:38.128890: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:49:38.128949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:38.128993: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:49:38.129237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:38.129284: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:49:38.129371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:38.129418: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[[2022-12-12 02:49:382022-12-12 02:49:38..133321133325: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 02:49:38.133913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[1980[2022-12-12 02:49:38] 2022-12-12 02:49:38.eager alloc mem 25.25 KB.133944
133945: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-12 02:49:382022-12-12 02:49:38..134017134018: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 4.77 GBeager alloc mem 4.77 GB

[2022-12-12 02:49:38.134522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:49:38.134567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:49:38.153986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:49:38.154486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:49:38.154611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:49:38.154655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:49:38.154917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:49:38.155124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:49:38.155189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:49:38.155384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:49:38.155523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:49:38.155567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:49:38.155984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:49:38.156025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:39........967922967923967922967922967923967924967923967923: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 3 init p2p of link 2Device 1 init p2p of link 7Device 2 init p2p of link 1Device 4 init p2p of link 5Device 7 init p2p of link 4Device 0 init p2p of link 3Device 5 init p2p of link 6Device 6 init p2p of link 0







[[[[[2022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:39[[[2022-12-12 02:49:392022-12-12 02:49:39...2022-12-12 02:49:392022-12-12 02:49:392022-12-12 02:49:39..968468968468968468...968468968474: : : 968474968474968474: : EEE: : : EE   EEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::198019801980:::19801980] ] ] 198019801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB


eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 02:49:39.969528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.969557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:49:392022-12-12 02:49:39..969606969608[: [: [2022-12-12 02:49:39E[2022-12-12 02:49:39E2022-12-12 02:49:39. 2022-12-12 02:49:39. .969620/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.969622/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc969626: :969633: :: E638: E638E ] E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
:638:638638] 638] ] eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 02:49:39.982614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:49:39.982762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.982864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:49:39.983021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.983623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.983878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.987591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 02:49:39.987737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.987929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[[2022-12-12 02:49:392022-12-12 02:49:39..988081988068: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 7 init p2p of link 1eager alloc mem 611.00 KB

[2022-12-12 02:49:39.988277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.988308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 02:49:39.988465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.988593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39[.2022-12-12 02:49:39988652.: 988662E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1926Device 5 init p2p of link 4] 
Device 0 init p2p of link 6
[2022-12-12 02:49:39[.2022-12-12 02:49:39988832.: 988835E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 02:49:39.989024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 02:49:39.989047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.989317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.989629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.989730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.991917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:49:39.992028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.992265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:49:39.992383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:39.992871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:39.993242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40.  1399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:49:40.  1515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40.  2236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 02:49:40.  2353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40.  2395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40.  3239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40.  5395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:49:40.  5509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40.  5761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 02:49:40.  5881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40.  6300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40.  6543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:49:40.  6659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40.  6724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40.  7029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:49:40.  7163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40.  7437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40.  8010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 12618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:49:40. 12728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 13591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 13731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:49:40. 13848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 14711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 15047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:49:40. 15174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 15937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:49:40. 16025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 16050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 16526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 02:49:40. 16643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 16924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 17497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 19267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 02:49:40. 19383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 20242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 24520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 02:49:40. 24637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 25404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 25941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:49:40. 26056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:49:40. 26829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:49:40. 31313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 31817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 32092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03795 secs 
[2022-12-12 02:49:40. 32321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03731 secs 
[2022-12-12 02:49:40. 33077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 33449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03941 secs 
[2022-12-12 02:49:40. 34856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 35238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04242 secs 
[2022-12-12 02:49:40. 35262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 35625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.04065 secs 
[2022-12-12 02:49:40. 36269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 36639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03889 secs 
[2022-12-12 02:49:40. 37020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 37389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.03958 secs 
[2022-12-12 02:49:40. 38691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:49:40. 39058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 30000000 / 100000000 nodes ( 30.00 %) | cpu 60000000 / 100000000 nodes ( 60.00 %) | 4.77 GB | 2.06148 secs 
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][tid #139777756096256]: replica 3 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][tid #139778217465600]: replica 0 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][tid #139777949030144]: replica 7 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][tid #139777672201984]: replica 4 calling init per replica done, doing barrier
[HCTR][02:49:40.039][ERROR][RK0][tid #139777672201984]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][tid #139777756096256]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][tid #139777756096256]: init per replica done
[HCTR][02:49:40.039][ERROR][RK0][tid #139778217465600]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][tid #139777949030144]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:49:40.039][ERROR][RK0][tid #139777672201984]: init per replica done
[HCTR][02:49:40.039][ERROR][RK0][main]: init per replica done
[HCTR][02:49:40.039][ERROR][RK0][main]: init per replica done
[HCTR][02:49:40.039][ERROR][RK0][main]: init per replica done
[HCTR][02:49:40.039][ERROR][RK0][tid #139777949030144]: init per replica done
[HCTR][02:49:40.039][ERROR][RK0][main]: init per replica done
[HCTR][02:49:40.041][ERROR][RK0][tid #139778217465600]: init per replica done
