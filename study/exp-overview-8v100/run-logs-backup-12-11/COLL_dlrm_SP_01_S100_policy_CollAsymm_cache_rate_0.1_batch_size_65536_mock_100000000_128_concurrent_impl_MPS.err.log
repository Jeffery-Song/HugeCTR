2022-12-12 02:32:34.973750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:34.980139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:34.984901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:34.991849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:34.995887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.008053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.014019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.018672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.072789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.073169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.082215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.082232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.086314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.086402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.087583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.087848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.088879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.089258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.090200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.090707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.091664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.092088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.093032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.093555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.094291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.095438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.096306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.097281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.098237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.099205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.100112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.101039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.102740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.103871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.104795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.105707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.106618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.107558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.108509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.109429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.114179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.114650: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.115195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.116132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.117087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.118100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.119135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.120958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.122304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.122395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.123998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.124534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.125268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.126209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.127139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.128325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.129296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.129349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.131807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.131828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.131941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.134290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.134391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.134518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.136942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.137118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.137292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.139961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.140236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.140405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.141385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.142670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.143379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.143426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.144550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.145243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.145502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.146479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.146477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.147496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.148360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.148413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.149482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.149645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.150566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.151295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.151490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.152530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.153527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.153855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.154245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.156260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.156356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.158028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.159332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.159459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.160218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.161448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.161692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.168520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.184940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.196682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.197676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.198283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.198768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.198991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.199029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.199925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.200912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.201781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.202885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.202999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.203241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.204727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.206400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.206849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.207103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.207287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.209384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.210381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.210653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.210892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.211392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.212735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.214150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.214435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.214672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.214903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.216267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.217572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.217995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.218295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.218584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.220250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.220882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.221263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.221663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.221896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.224028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.224282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.224564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.224825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.225051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.227366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.227927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.228316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.228519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.228707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.230959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.231106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.231557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.231790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.232125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.234462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.234483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.234870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.234911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.235381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.237979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.238000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.238294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.238335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.238764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.241244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.241305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.241340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.241429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.241938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.242423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.245047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.245067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.245072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.245152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.245534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.246147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.248768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.248774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.248851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.248857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.249136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.249928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.252483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.252672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.252883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.253642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.253689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.254125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.256168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.256435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.256799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.256941: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.256973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.256991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.257473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.259589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.260125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.260470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.260650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.260655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.261297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.263292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.263861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.264492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.264540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.264677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.265107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.266754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.266814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.267546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.268212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.268276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.268513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.268997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.270875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.271057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.271845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.272393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.272547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.272723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.273034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.274836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.275015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.275988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.276525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.276664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.276872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.277271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.279234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.280022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.280719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.280960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.281249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.281250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.283061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.283788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.284666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.285283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.286650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.288153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.288763: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.288908: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.289393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.290396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.290466: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.291719: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.291836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.292759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.294184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.295331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.296497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.298994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.299118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.299217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.299302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.300393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.301556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.302515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.302825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.302929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.303140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.304528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.305829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.307022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.307204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.307222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.307493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.308511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.311073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.312233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.341444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.344916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.345507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.378343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.378710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.383048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.383265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.390127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.390360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.396273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.399722: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.401240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.408677: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:32:35.409301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.413453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.418318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.428899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.431938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:35.437590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.389381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.390015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.390667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.391142: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.391199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:32:36.408515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.409293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.409818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.410583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.411275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.411759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:32:36.457935: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.458131: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.500577: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:32:36.624301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.624943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.625910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.626553: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.626608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:32:36.644170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.645020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.645529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.646326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.646878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.647372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:32:36.669075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.670277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.670887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.671356: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.671409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:32:36.678329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.678888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.678980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.680005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.680086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.681019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.681042: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.681113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:32:36.681746: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.681792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:32:36.688700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.689325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.689838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.690739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.691306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.691785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:32:36.698235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.698486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.699113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.699434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.700038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.700320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.701106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.701577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.702309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.702685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.703190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:32:36.703419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:32:36.706515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.707123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.707681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.708144: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.708185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:32:36.710331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.710948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.711492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.711994: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.712046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:32:36.712091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.712863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.713383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.713843: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:32:36.713883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:32:36.725105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.725826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.726434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.727083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.727655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.728130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:32:36.729063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.729710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.730101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.730222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.731197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.731379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.732018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.732324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.732997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.733140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:32:36.733566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:32:36.734036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:32:36.749138: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.749342: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.749763: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.749919: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.749947: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.750067: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.751157: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:32:36.751709: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:32:36.751851: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:32:36.772833: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.773042: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.774904: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:32:36.778774: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.778945: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.779703: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.779828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.780715: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:32:36.781663: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:32:36.788816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.788999: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:32:36.790885: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][02:32:38.027][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.028][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.028][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.038][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.039][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.093][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:32:38.093][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 93it [00:01, 77.89it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 190it [00:01, 173.48it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 98it [00:01, 82.14it/s]warmup run: 287it [00:01, 279.11it/s]warmup run: 92it [00:01, 79.26it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 95it [00:01, 81.34it/s]warmup run: 184it [00:01, 165.47it/s]warmup run: 384it [00:01, 388.98it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 187it [00:01, 174.77it/s]warmup run: 77it [00:01, 67.36it/s]warmup run: 101it [00:01, 86.30it/s]warmup run: 190it [00:01, 176.10it/s]warmup run: 268it [00:01, 254.73it/s]warmup run: 481it [00:02, 496.33it/s]warmup run: 97it [00:01, 84.28it/s]warmup run: 282it [00:01, 279.80it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 173it [00:01, 166.81it/s]warmup run: 201it [00:01, 185.82it/s]warmup run: 283it [00:01, 277.70it/s]warmup run: 366it [00:01, 369.84it/s]warmup run: 578it [00:02, 594.77it/s]warmup run: 195it [00:01, 183.26it/s]warmup run: 380it [00:01, 393.36it/s]warmup run: 95it [00:01, 83.30it/s]warmup run: 273it [00:01, 281.17it/s]warmup run: 302it [00:01, 296.64it/s]warmup run: 376it [00:01, 382.83it/s]warmup run: 466it [00:02, 485.53it/s]warmup run: 675it [00:02, 680.87it/s]warmup run: 294it [00:01, 293.16it/s]warmup run: 481it [00:02, 509.04it/s]warmup run: 191it [00:01, 181.07it/s]warmup run: 373it [00:01, 398.99it/s]warmup run: 403it [00:01, 411.10it/s]warmup run: 469it [00:02, 485.27it/s]warmup run: 566it [00:02, 592.45it/s]warmup run: 773it [00:02, 754.39it/s]warmup run: 393it [00:01, 406.25it/s]warmup run: 583it [00:02, 616.22it/s]warmup run: 288it [00:01, 289.39it/s]warmup run: 473it [00:01, 512.07it/s]warmup run: 503it [00:02, 520.86it/s]warmup run: 564it [00:02, 583.10it/s]warmup run: 667it [00:02, 687.28it/s]warmup run: 871it [00:02, 813.13it/s]warmup run: 492it [00:01, 515.47it/s]warmup run: 686it [00:02, 710.59it/s]warmup run: 387it [00:01, 404.07it/s]warmup run: 575it [00:02, 619.29it/s]warmup run: 605it [00:02, 625.31it/s]warmup run: 659it [00:02, 667.29it/s]warmup run: 767it [00:02, 763.72it/s]warmup run: 968it [00:02, 855.56it/s]warmup run: 596it [00:02, 626.17it/s]warmup run: 788it [00:02, 786.44it/s]warmup run: 485it [00:01, 513.14it/s]warmup run: 677it [00:02, 712.44it/s]warmup run: 706it [00:02, 712.56it/s]warmup run: 753it [00:02, 734.73it/s]warmup run: 866it [00:02, 822.37it/s]warmup run: 1066it [00:02, 887.97it/s]warmup run: 699it [00:02, 718.68it/s]warmup run: 889it [00:02, 844.51it/s]warmup run: 584it [00:02, 614.65it/s]warmup run: 780it [00:02, 790.50it/s]warmup run: 807it [00:02, 785.85it/s]warmup run: 846it [00:02, 783.06it/s]warmup run: 966it [00:02, 868.49it/s]warmup run: 1163it [00:02, 910.22it/s]warmup run: 802it [00:02, 795.25it/s]warmup run: 990it [00:02, 888.98it/s]warmup run: 684it [00:02, 704.39it/s]warmup run: 881it [00:02, 846.37it/s]warmup run: 908it [00:02, 842.19it/s]warmup run: 941it [00:02, 828.28it/s]warmup run: 1066it [00:02, 902.73it/s]warmup run: 1261it [00:02, 929.88it/s]warmup run: 905it [00:02, 855.64it/s]warmup run: 1092it [00:02, 924.75it/s]warmup run: 785it [00:02, 780.71it/s]warmup run: 981it [00:02, 884.63it/s]warmup run: 1008it [00:02, 884.23it/s]warmup run: 1039it [00:02, 869.18it/s]warmup run: 1165it [00:02, 919.50it/s]warmup run: 1359it [00:02, 942.70it/s]warmup run: 1007it [00:02, 899.27it/s]warmup run: 1193it [00:02, 942.84it/s]warmup run: 885it [00:02, 836.39it/s]warmup run: 1080it [00:02, 913.69it/s]warmup run: 1109it [00:02, 917.99it/s]warmup run: 1138it [00:02, 902.48it/s]warmup run: 1457it [00:03, 953.04it/s]warmup run: 1263it [00:02, 925.95it/s]warmup run: 1109it [00:02, 931.45it/s]warmup run: 1295it [00:02, 963.38it/s]warmup run: 986it [00:02, 881.59it/s]warmup run: 1181it [00:02, 940.76it/s]warmup run: 1209it [00:02, 940.82it/s]warmup run: 1237it [00:02, 925.83it/s]warmup run: 1361it [00:02, 938.86it/s]warmup run: 1210it [00:02, 953.06it/s]warmup run: 1555it [00:03, 894.29it/s]warmup run: 1397it [00:02, 979.21it/s]warmup run: 1086it [00:02, 914.18it/s]warmup run: 1281it [00:02, 957.48it/s]warmup run: 1310it [00:02, 958.86it/s]warmup run: 1336it [00:02, 943.05it/s]warmup run: 1460it [00:03, 953.01it/s]warmup run: 1311it [00:02, 967.99it/s]warmup run: 1652it [00:03, 914.95it/s]warmup run: 1498it [00:03, 987.69it/s]warmup run: 1186it [00:02, 936.53it/s]warmup run: 1383it [00:02, 975.53it/s]warmup run: 1410it [00:02, 970.38it/s]warmup run: 1436it [00:03, 957.16it/s]warmup run: 1560it [00:03, 965.00it/s]warmup run: 1412it [00:02, 974.96it/s]warmup run: 1747it [00:03, 923.67it/s]warmup run: 1599it [00:03, 993.52it/s]warmup run: 1288it [00:02, 959.35it/s]warmup run: 1486it [00:02, 990.45it/s]warmup run: 1511it [00:03, 982.02it/s]warmup run: 1534it [00:03, 963.21it/s]warmup run: 1659it [00:03, 972.30it/s]warmup run: 1513it [00:03, 979.29it/s]warmup run: 1843it [00:03, 932.64it/s]warmup run: 1701it [00:03, 999.78it/s]warmup run: 1390it [00:02, 974.79it/s]warmup run: 1589it [00:03, 1001.55it/s]warmup run: 1612it [00:03, 989.26it/s]warmup run: 1633it [00:03, 968.49it/s]warmup run: 1758it [00:03, 971.52it/s]warmup run: 1613it [00:03, 982.83it/s]warmup run: 1941it [00:03, 945.65it/s]warmup run: 1804it [00:03, 1006.93it/s]warmup run: 1492it [00:02, 985.53it/s]warmup run: 1693it [00:03, 1010.63it/s]warmup run: 1714it [00:03, 995.35it/s]warmup run: 1732it [00:03, 971.94it/s]warmup run: 1856it [00:03, 970.14it/s]warmup run: 1713it [00:03, 986.05it/s]warmup run: 2050it [00:03, 986.86it/s]warmup run: 1907it [00:03, 1012.58it/s]warmup run: 1593it [00:03, 991.72it/s]warmup run: 1796it [00:03, 1014.17it/s]warmup run: 1815it [00:03, 994.60it/s]warmup run: 1830it [00:03, 969.05it/s]warmup run: 1954it [00:03, 968.73it/s]warmup run: 1813it [00:03, 983.30it/s]warmup run: 2170it [00:03, 1048.32it/s]warmup run: 2010it [00:03, 1017.31it/s]warmup run: 1695it [00:03, 998.72it/s]warmup run: 1899it [00:03, 1016.40it/s]warmup run: 1917it [00:03, 1000.07it/s]warmup run: 1928it [00:03, 943.16it/s]warmup run: 2064it [00:03, 1006.96it/s]warmup run: 1912it [00:03, 983.67it/s]warmup run: 2290it [00:03, 1091.22it/s]warmup run: 2132it [00:03, 1076.48it/s]warmup run: 1796it [00:03, 1001.42it/s]warmup run: 2002it [00:03, 1016.32it/s]warmup run: 2022it [00:03, 1013.67it/s]warmup run: 2023it [00:03, 938.92it/s]warmup run: 2185it [00:03, 1065.69it/s]warmup run: 2013it [00:03, 990.01it/s]warmup run: 2410it [00:03, 1122.05it/s]warmup run: 2254it [00:03, 1118.25it/s]warmup run: 1898it [00:03, 1005.09it/s]warmup run: 2125it [00:03, 1079.37it/s]warmup run: 2144it [00:03, 1074.06it/s]warmup run: 2134it [00:03, 987.86it/s]warmup run: 2307it [00:03, 1109.19it/s]warmup run: 2132it [00:03, 1047.74it/s]warmup run: 2530it [00:04, 1144.91it/s]warmup run: 2376it [00:03, 1148.38it/s]warmup run: 2000it [00:03, 1008.46it/s]warmup run: 2249it [00:03, 1125.10it/s]warmup run: 2266it [00:03, 1117.14it/s]warmup run: 2245it [00:03, 1021.35it/s]warmup run: 2428it [00:03, 1137.60it/s]warmup run: 2252it [00:03, 1091.23it/s]warmup run: 2649it [00:04, 1157.23it/s]warmup run: 2498it [00:03, 1168.24it/s]warmup run: 2119it [00:03, 1062.48it/s]warmup run: 2373it [00:03, 1157.53it/s]warmup run: 2389it [00:03, 1148.10it/s]warmup run: 2357it [00:03, 1048.43it/s]warmup run: 2548it [00:04, 1154.63it/s]warmup run: 2372it [00:03, 1122.10it/s]warmup run: 2769it [00:04, 1169.54it/s]warmup run: 2620it [00:04, 1182.65it/s]warmup run: 2239it [00:03, 1102.33it/s]warmup run: 2497it [00:03, 1179.62it/s]warmup run: 2511it [00:03, 1168.57it/s]warmup run: 2468it [00:04, 1066.04it/s]warmup run: 2667it [00:04, 1163.14it/s]warmup run: 2493it [00:03, 1146.24it/s]warmup run: 2889it [00:04, 1177.51it/s]warmup run: 2741it [00:04, 1188.71it/s]warmup run: 2359it [00:03, 1130.14it/s]warmup run: 2620it [00:03, 1194.52it/s]warmup run: 2632it [00:04, 1178.60it/s]warmup run: 2580it [00:04, 1079.32it/s]warmup run: 2787it [00:04, 1171.62it/s]warmup run: 2614it [00:04, 1163.20it/s]warmup run: 3000it [00:04, 667.92it/s] warmup run: 2863it [00:04, 1196.33it/s]warmup run: 2479it [00:03, 1148.49it/s]warmup run: 2741it [00:04, 1197.38it/s]warmup run: 2754it [00:04, 1188.83it/s]warmup run: 2690it [00:04, 1083.67it/s]warmup run: 2906it [00:04, 1176.86it/s]warmup run: 2733it [00:04, 1169.95it/s]warmup run: 2985it [00:04, 1202.93it/s]warmup run: 2595it [00:03, 1151.58it/s]warmup run: 3000it [00:04, 690.99it/s] warmup run: 2861it [00:04, 1193.41it/s]warmup run: 2877it [00:04, 1198.36it/s]warmup run: 3000it [00:04, 671.61it/s] warmup run: 2801it [00:04, 1088.78it/s]warmup run: 2854it [00:04, 1179.82it/s]warmup run: 2711it [00:04, 1152.21it/s]warmup run: 2981it [00:04, 1191.62it/s]warmup run: 3000it [00:04, 1206.87it/s]warmup run: 3000it [00:04, 690.28it/s] warmup run: 3000it [00:04, 694.12it/s] warmup run: 2913it [00:04, 1097.92it/s]warmup run: 2976it [00:04, 1189.13it/s]warmup run: 3000it [00:04, 690.76it/s] warmup run: 2829it [00:04, 1158.88it/s]warmup run: 3000it [00:04, 661.75it/s] warmup run: 2949it [00:04, 1168.79it/s]warmup run: 3000it [00:04, 690.78it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.23it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.03it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1595.74it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.94it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.58it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.26it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1530.16it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1583.69it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.18it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.16it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1634.32it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1594.52it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1642.05it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1631.61it/s]warmup should be done:  10%|█         | 309/3000 [00:00<00:01, 1536.14it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1637.20it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.69it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1642.91it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1636.95it/s]warmup should be done:  15%|█▌        | 464/3000 [00:00<00:01, 1541.74it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1637.14it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1589.84it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1633.29it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1602.86it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.09it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1651.06it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1635.68it/s]warmup should be done:  21%|██        | 619/3000 [00:00<00:01, 1543.34it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1623.30it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1570.44it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1616.92it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1594.20it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1636.25it/s]warmup should be done:  26%|██▌       | 774/3000 [00:00<00:01, 1544.19it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1660.96it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1647.36it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1625.31it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1577.60it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1603.96it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1606.43it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1634.31it/s]warmup should be done:  31%|███       | 929/3000 [00:00<00:01, 1541.78it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1648.65it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1654.89it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1623.13it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1580.64it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1592.29it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1598.48it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1654.67it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1630.00it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1654.48it/s]warmup should be done:  36%|███▌      | 1084/3000 [00:00<00:01, 1537.09it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1626.06it/s]warmup should be done:  37%|███▋      | 1118/3000 [00:00<00:01, 1583.95it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1609.31it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1589.96it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1630.54it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1651.31it/s]warmup should be done:  41%|████▏     | 1238/3000 [00:00<00:01, 1537.35it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1653.31it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1627.73it/s]warmup should be done:  43%|████▎     | 1278/3000 [00:00<00:01, 1588.61it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1616.16it/s]warmup should be done:  43%|████▎     | 1301/3000 [00:00<00:01, 1582.24it/s]warmup should be done:  46%|████▋     | 1392/3000 [00:00<00:01, 1537.70it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1630.97it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1655.72it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1652.11it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1627.78it/s]warmup should be done:  48%|████▊     | 1437/3000 [00:00<00:00, 1588.11it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1619.96it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1584.39it/s]warmup should be done:  52%|█████▏    | 1546/3000 [00:01<00:00, 1536.55it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1631.27it/s]warmup should be done:  55%|█████▌    | 1657/3000 [00:01<00:00, 1653.63it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1651.90it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1628.32it/s]warmup should be done:  53%|█████▎    | 1597/3000 [00:01<00:00, 1591.50it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1622.81it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1585.63it/s]warmup should be done:  57%|█████▋    | 1707/3000 [00:01<00:00, 1557.24it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1657.72it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1631.25it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1651.01it/s]warmup should be done:  60%|██████    | 1802/3000 [00:01<00:00, 1628.90it/s]warmup should be done:  59%|█████▊    | 1757/3000 [00:01<00:00, 1593.94it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1625.23it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1590.58it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1575.21it/s]warmup should be done:  64%|██████▍   | 1917/3000 [00:01<00:00, 1595.33it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1650.84it/s]warmup should be done:  66%|██████▌   | 1966/3000 [00:01<00:00, 1629.44it/s]warmup should be done:  66%|██████▋   | 1990/3000 [00:01<00:00, 1646.24it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1622.41it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1626.57it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1593.63it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1590.11it/s]warmup should be done:  69%|██████▉   | 2077/3000 [00:01<00:00, 1596.01it/s]warmup should be done:  71%|███████   | 2130/3000 [00:01<00:00, 1629.85it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1649.65it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1625.17it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1641.46it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1628.04it/s]warmup should be done:  70%|███████   | 2101/3000 [00:01<00:00, 1594.06it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1601.50it/s]warmup should be done:  75%|███████▍  | 2239/3000 [00:01<00:00, 1602.34it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1627.83it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1646.94it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1623.41it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1626.04it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1626.17it/s]warmup should be done:  75%|███████▌  | 2262/3000 [00:01<00:00, 1596.48it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1606.86it/s]warmup should be done:  80%|████████  | 2401/3000 [00:01<00:00, 1606.43it/s]warmup should be done:  83%|████████▎ | 2493/3000 [00:01<00:00, 1647.50it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1628.81it/s]warmup should be done:  82%|████████▏ | 2458/3000 [00:01<00:00, 1622.04it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1624.12it/s]warmup should be done:  82%|████████▏ | 2451/3000 [00:01<00:00, 1627.25it/s]warmup should be done:  81%|████████  | 2424/3000 [00:01<00:00, 1602.33it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1613.92it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1612.21it/s]warmup should be done:  89%|████████▊ | 2659/3000 [00:01<00:00, 1648.54it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1629.15it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1614.67it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1628.85it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1600.47it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1607.89it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1617.77it/s]warmup should be done:  91%|█████████ | 2727/3000 [00:01<00:00, 1616.89it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1629.25it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1650.05it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1619.88it/s]warmup should be done:  93%|█████████▎| 2779/3000 [00:01<00:00, 1629.92it/s]warmup should be done:  92%|█████████▏| 2748/3000 [00:01<00:00, 1607.72it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1614.85it/s]warmup should be done:  95%|█████████▍| 2848/3000 [00:01<00:00, 1622.86it/s]warmup should be done:  96%|█████████▋| 2891/3000 [00:01<00:00, 1622.28it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1652.48it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1635.60it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1627.16it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1633.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.77it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1612.18it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1619.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1603.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1578.96it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.80it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.09it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.61it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.18it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.09it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.67it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.77it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.23it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1703.00it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.59it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1688.93it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.01it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.49it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1671.62it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1677.61it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1666.91it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1680.04it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1706.20it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1674.35it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1689.23it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1672.94it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1681.29it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1661.05it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1669.44it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1674.37it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1704.80it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1680.25it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1678.11it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1672.20it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1679.93it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1660.52it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1682.95it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1684.96it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1703.21it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1679.52it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1682.42it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1678.16it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1670.38it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1657.74it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1670.82it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1692.29it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1678.92it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1703.41it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1678.47it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1672.04it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1675.68it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1654.89it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1658.92it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1694.14it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1686.93it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1702.44it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1671.81it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1673.47it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1674.90it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1650.68it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1664.64it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1693.41it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:00, 1704.66it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1690.05it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1671.77it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1671.20it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1677.36it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1647.96it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1661.24it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1695.15it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1704.91it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1671.05it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1676.93it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1668.90it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1681.33it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1634.54it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1640.41it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1697.27it/s]warmup should be done:  57%|█████▋    | 1710/3000 [00:01<00:00, 1705.15it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1671.37it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1678.00it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1673.89it/s]warmup should be done:  56%|█████▋    | 1693/3000 [00:01<00:00, 1676.64it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1628.78it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1631.44it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1706.36it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1698.31it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1671.81it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1678.24it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1678.58it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1674.64it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1623.08it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1625.46it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1707.00it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1696.77it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1680.82it/s]warmup should be done:  67%|██████▋   | 2015/3000 [00:01<00:00, 1671.43it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1678.41it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1672.59it/s]warmup should be done:  66%|██████▋   | 1988/3000 [00:01<00:00, 1616.17it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1618.59it/s]warmup should be done:  74%|███████▍  | 2223/3000 [00:01<00:00, 1705.88it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1692.28it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1682.63it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1677.86it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1670.42it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1669.17it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1613.39it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1614.13it/s]warmup should be done:  80%|███████▉  | 2394/3000 [00:01<00:00, 1705.39it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1684.48it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1679.14it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1671.78it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1682.97it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1668.40it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1624.26it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1628.52it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1706.48it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1684.42it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1680.69it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1668.42it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1680.13it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1669.90it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1631.30it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1637.38it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1707.19it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1685.19it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1681.41it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1681.59it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1662.40it/s]warmup should be done:  90%|████████▉ | 2699/3000 [00:01<00:00, 1669.89it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1622.91it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1627.36it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1705.74it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1680.61it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1674.93it/s]warmup should be done:  96%|█████████▌| 2866/3000 [00:01<00:00, 1668.21it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1658.20it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1652.87it/s]warmup should be done:  94%|█████████▎| 2807/3000 [00:01<00:00, 1619.00it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1621.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1704.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.44it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.96it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1618.78it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1620.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.00it/s]2022-12-12 02:34:14.287542: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6e9802a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:14.287604: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:14.301897: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8b37831090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:14.301946: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:14.303095: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6dfc02e270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:14.303144: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:14.325642: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8b2ff92940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:14.325713: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:14.982551: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6e4c02a030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:14.982620: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:15.047044: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6e88030e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:15.047121: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:15.079908: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6db402e310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:15.079982: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:15.080672: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8b378343c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:34:15.080723: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:34:16.521121: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:16.534727: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:16.552221: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:16.669002: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:17.289058: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:17.360403: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:17.389652: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:17.458552: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:34:19.342863: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:19.351886: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:19.425031: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:19.537680: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:20.187251: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:20.233972: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:20.263125: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:34:20.335952: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:34:48.397][ERROR][RK0][tid #140236042528512]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:34:48.397][ERROR][RK0][tid #140236042528512]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.404][ERROR][RK0][tid #140235983812352]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:34:48.404][ERROR][RK0][tid #140235983812352]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.407][ERROR][RK0][tid #140236042528512]: coll ps creation done
[HCTR][02:34:48.407][ERROR][RK0][tid #140236042528512]: replica 6 waits for coll ps creation barrier
[HCTR][02:34:48.413][ERROR][RK0][tid #140235983812352]: coll ps creation done
[HCTR][02:34:48.413][ERROR][RK0][tid #140235983812352]: replica 1 waits for coll ps creation barrier
[HCTR][02:34:48.451][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:34:48.452][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.456][ERROR][RK0][main]: coll ps creation done
[HCTR][02:34:48.456][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][02:34:48.463][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:34:48.463][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.468][ERROR][RK0][main]: coll ps creation done
[HCTR][02:34:48.468][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][02:34:48.477][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:34:48.477][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.482][ERROR][RK0][main]: coll ps creation done
[HCTR][02:34:48.482][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][02:34:48.489][ERROR][RK0][tid #140236050921216]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:34:48.489][ERROR][RK0][tid #140236050921216]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.494][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:34:48.494][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.497][ERROR][RK0][tid #140236050921216]: coll ps creation done
[HCTR][02:34:48.497][ERROR][RK0][tid #140236050921216]: replica 7 waits for coll ps creation barrier
[HCTR][02:34:48.502][ERROR][RK0][main]: coll ps creation done
[HCTR][02:34:48.502][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:34:48.533][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:34:48.533][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:34:48.541][ERROR][RK0][main]: coll ps creation done
[HCTR][02:34:48.541][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][02:34:48.541][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][02:34:49.391][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][02:34:49.435][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][tid #140235983812352]: replica 1 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][tid #140236050921216]: replica 7 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][tid #140236042528512]: replica 6 calling init per replica
[HCTR][02:34:49.435][ERROR][RK0][main]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][main]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][main]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][tid #140235983812352]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][main]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][tid #140236050921216]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][main]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][tid #140235983812352]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][tid #140236042528512]: Calling build_v2
[HCTR][02:34:49.435][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][tid #140236050921216]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:34:49.435][ERROR][RK0][tid #140236042528512]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 02:34:492022-12-12 02:34:492022-12-12 02:34:492022-12-12 02:34:492022-12-12 02:34:49[.2022-12-12 02:34:49..2022-12-12 02:34:49..435991.435996435991.435989435993: 435991: : 2022-12-12 02:34:49435994: : E: EE: .EE E  E436018  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE::136:136136: 136136] 136] ] 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] ] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS] :using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS

using concurrent impl MPS136



] using concurrent impl MPS
[2022-12-12 02:34:49.440369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 02:34:492022-12-12 02:34:49..440412440418: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178196] ] v100x8, slow pcieassigning 8 to cpu
[
2022-12-12 02:34:49[.2022-12-12 02:34:49440460.: 440473E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 02:34:49:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.178[:440494] 2022-12-12 02:34:49196: v100x8, slow pcie.] E
440503assigning 8 to cpu : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E:2022-12-12 02:34:49 212[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 02:34:49440541:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: 178
[440548E] 2022-12-12 02:34:49:  v100x8, slow pcie.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
[440576 :2022-12-12 02:34:492022-12-12 02:34:49: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196..E2022-12-12 02:34:49:] 440593440600 .178assigning 8 to cpu: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc440615] [
EE:: v100x8, slow pcie2022-12-12 02:34:49  212E
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  440645:[:[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [1782022-12-12 02:34:492132022-12-12 02:34:49
:E2022-12-12 02:34:49] .] .196 .[v100x8, slow pcie440707remote time is 8.68421440696] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc4407202022-12-12 02:34:49
: 
[: assigning 8 to cpu:: .E2022-12-12 02:34:49[E
178E440780 .2022-12-12 02:34:49 ]  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc440841./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E:: 440850: :
:2022-12-12 02:34:49 196EE178212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[]   ] ] 440916:2022-12-12 02:34:49assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 213.
::

E] 440965196214 remote time is 8.68421: [] [[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E2022-12-12 02:34:49assigning 8 to cpu2022-12-12 02:34:492022-12-12 02:34:49cpu time is 97.0588: [.
..
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:34:49441077441072441102] :.: [: : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196441122E2022-12-12 02:34:49EE
] :  .[  assigning 8 to cpuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc4411852022-12-12 02:34:49/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 :: .::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E441260196212:]  : ] [] 214remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu2022-12-12 02:34:49build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 
: 
.
cpu time is 97.0588212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[441350
] :2022-12-12 02:34:49[: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213.2022-12-12 02:34:49E
] 441403. [remote time is 8.68421: 441425[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:34:49
.E[: 2022-12-12 02:34:49:441457 2022-12-12 02:34:49E.212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. 441475] E:441507/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 214: :E
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE213 cpu time is 97.0588: ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-12 02:34:49:] :
.213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214441609[] 
] : 2022-12-12 02:34:49remote time is 8.68421cpu time is 97.0588E.
[
 4416782022-12-12 02:34:49[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .2022-12-12 02:34:49:E441722.213 441742: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: Eremote time is 8.68421:214E 
]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:
:2022-12-12 02:34:49213214.] ] 441863remote time is 8.68421cpu time is 97.0588: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[214] cpu time is 97.05882022-12-12 02:34:49
.441940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 02:36:08.757144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:36:08.797029: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:36:08.797103: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:36:08.798084: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 02:36:08.878770: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 02:36:09.281624: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 02:36:09.281715: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 02:36:16.208325: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 02:36:16.208420: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 02:36:17.962861: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 02:36:17.962954: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 02:36:17.965868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 02:36:17.965936: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 02:36:18.240066: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 02:36:18.268521: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 02:36:18.269932: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 02:36:18.290243: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 02:36:18.818138: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 02:37:54.139238: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 02:37:54.147794: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 02:37:54.151593: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 02:37:54.198668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:37:54.198763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:37:54.198800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:37:54.198834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:37:54.199363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:37:54.199422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.200343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.201014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.[2142432022-12-12 02:37:54: .E214271 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :4 solved202
] 2 solved
[2022-12-12 02:37:54.214388[: 2022-12-12 02:37:54E. 214397/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 4 initing device 4:
205] worker 0 thread 2 initing device 2
[2022-12-12 02:37:54[.2022-12-12 02:37:54214656.[: 2146712022-12-12 02:37:54E: . E214696/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE202: ] 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc7 solved] :
6 solved202
[] 2022-12-12 02:37:54[.3 solved2022-12-12 02:37:54214821
.: 214832E[:  2022-12-12 02:37:54E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc. :[214862/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205[2022-12-12 02:37:54: :] 2022-12-12 02:37:54.E205worker 0 thread 7 initing device 7.214890 ] 
214901: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 6 initing device 6: E:
E 205 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:worker 0 thread 3 initing device 3:1815
1815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 02:37:542022-12-12 02:37:54..215099215102: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 02:37:54.215405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 02:37:54.215427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:37:54.215461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 02:37:54.[2154832022-12-12 02:37:54: .E215493 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:37:54.215587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:37:542022-12-12 02:37:54..217935217946: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved5 solved

[[2022-12-12 02:37:542022-12-12 02:37:54..218054218057: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 5 initing device 5

[2022-12-12 02:37:54.218548: [E2022-12-12 02:37:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu218558:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:[18152022-12-12 02:37:54] .Building Coll Cache with ... num gpu device is 8218607
: [E2022-12-12 02:37:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu218633[:: 2022-12-12 02:37:541980[E.] 2022-12-12 02:37:54 218656eager alloc mem 381.47 MB./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 
218671:E: [1980 E2022-12-12 02:37:54] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu .eager alloc mem 381.47 MB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu218727
1980:: ] [1980Eeager alloc mem 381.47 MB2022-12-12 02:37:54]  
.eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu218777
:: 1980E]  [eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:37:54
:.1980218844] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.222938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.223032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.223082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.223119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.223176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.223230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.223302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.227306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.227359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:37:54.282691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 02:37:54.288096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:37:54.288244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:37:54.289072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.294554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:54.295597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:54.295651: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[[[[[[2022-12-12 02:37:542022-12-12 02:37:542022-12-12 02:37:542022-12-12 02:37:542022-12-12 02:37:542022-12-12 02:37:54...303322...303322303322: 303322303322303322: : E: : : EE EEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:::19801980] 198019801980] ] eager alloc mem 1023.00 Bytes] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes
eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes




[2022-12-12 02:37:54.308858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.309501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.309547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:37:54.309927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:37:54.310015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 02:37:54
.310021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:37:54.310109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:37:54eager release cuda mem 400000000.
310109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:37:54.310205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 02:37:54] .eager release cuda mem 400000000310201
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:37:54.310277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:37:54eager release cuda mem 1023.
310314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:37:54[.2022-12-12 02:37:54310373.: 310364E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1023
[2022-12-12 02:37:54.310466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:37:54.311183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 02:37:54.317083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.317614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.318126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.318636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.319143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.319655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.321552: [E2022-12-12 02:37:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc321570:: 638E]  eager release cuda mem 1023/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:54.321621: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:37:54:.1980321644] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:37:54.321682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:54.321725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:54.321760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:54.321807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:54.322428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:37:54.322595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:54.322639: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 02:37:54] .WORKER[0] alloc host memory 38.03 MB322651
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:37:542022-12-12 02:37:54..322699322705: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843[] ] 2022-12-12 02:37:54eager release cuda mem 625663WORKER[0] alloc host memory 38.11 MB.
[
3227372022-12-12 02:37:54: .E322761 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:37:54E:. 638322793/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] : [:eager release cuda mem 625663W2022-12-12 02:37:54638
 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc322830eager release cuda mem 625663:[: 
432022-12-12 02:37:54E] . [[WORKER[0] alloc host memory 38.14 MB322881/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:37:542022-12-12 02:37:54
: :..W638322917322903 ] : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cceager release cuda mem 625663WE:
  43/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :[:WORKER[0] alloc host memory 38.14 MB432022-12-12 02:37:541980
] .] WORKER[0] alloc host memory 38.10 MB323023eager alloc mem 611.00 KB
: 
W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.13 MB
[2022-12-12 02:37:54.324091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:54.324143: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.12 MB
[2022-12-12 02:37:54.347948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.348079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.348552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.348594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.76 GB
[2022-12-12 02:37:54.348680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.348723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:37:54.349308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.349724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.349929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.349973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:37:54.350108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.350198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:37:54.350326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.350381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:37:54.350716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.350761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB[
2022-12-12 02:37:54.350768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 25.25 KB2022-12-12 02:37:54
.350805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.350860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:37:54.351384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:37:54.351430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:56........ 17128 17128 17128 17127 17128 17128 17128 17128: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::192619261926192619261926] 19261926] ] ] ] ] Device 7 init p2p of link 4] ] Device 4 init p2p of link 5Device 1 init p2p of link 7Device 3 init p2p of link 2Device 0 init p2p of link 3Device 6 init p2p of link 0
Device 2 init p2p of link 1Device 5 init p2p of link 6






[[[[2022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:56....[ 17672[[[ 17672 17672 176722022-12-12 02:37:56: 2022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:56: : : .E...EEE 17700  17692 17692 17695   : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:EEE::: 1980   198019801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] ] :eager alloc mem 611.00 KB:::eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB1980
198019801980


] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-12 02:37:56. 18739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[[2022-12-12 02:37:562022-12-12 02:37:562022-12-12 02:37:56... 18768 18769 18770: [: : E2022-12-12 02:37:56EE . [ [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 18790[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:37:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:37:56:: 2022-12-12 02:37:56:.:.638E.638 18806638 18810]   18817] : ] : eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager release cuda mem 625663Eeager release cuda mem 625663E
:E
 
 638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::eager release cuda mem 625663:638638
638] ] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 02:37:56. 34551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:37:56. 34709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 34731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:37:56. 34886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 35421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 02:37:56. 35573: [E2022-12-12 02:37:56 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 35580:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 35688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 02:37:56. 35762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 35838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 36113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 02:37:56. 36284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 36339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 02:37:56. 36410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56.[ 364922022-12-12 02:37:56: .E 36486 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1926
] Device 2 init p2p of link 3
[2022-12-12 02:37:56. 36578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 02:37:56. 36683[: 2022-12-12 02:37:56E.  36692/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[eager release cuda mem 625663:2022-12-12 02:37:56
1980.]  36735eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 37138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 37362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56.[ 375922022-12-12 02:37:56: .E 37601 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 02:37:56. 51433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:37:56. 51558: E[ 2022-12-12 02:37:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: 515571980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:37:56. 51626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 02:37:56. 51693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 51749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 52108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:37:56. 52225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 52440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 52522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 52615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 52658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:37:56. 52708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:37:56. 52793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 52830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 53069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 53319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[[2022-12-12 02:37:562022-12-12 02:37:56.. 53425 53440: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 5 init p2p of link 7eager alloc mem 611.00 KB

[2022-12-12 02:37:56. 53579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 53614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 53654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 54295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 54405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 71440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 02:37:56. 71564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 72035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:37:56. 72151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 72425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 72943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:37:56. 72971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:37:56. 73012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 73056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 73086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 73899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 73931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 02:37:56. 73943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 02:37:56] .Device 0 init p2p of link 2 73965
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:37:56. 74076[: [2022-12-12 02:37:56E2022-12-12 02:37:56. . 74074/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 74086: :: E1980E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
:19261980] ] Device 2 init p2p of link 4eager alloc mem 611.00 KB

[2022-12-12 02:37:56. 74289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 74835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:37:56. 74961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:37:56. 74994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 02:37:56
. 75017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 75101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 75780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:37:56. 91333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 91380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 91817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9990271 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29871667 / 100000000 nodes ( 29.87 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.87673 secs 
[2022-12-12 02:37:56. 91933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9998891 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29863047 / 100000000 nodes ( 29.86 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.87636 secs 
[2022-12-12 02:37:56. 92532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 93304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 93467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9999199 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29862739 / 100000000 nodes ( 29.86 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.87839 secs 
[2022-12-12 02:37:56. 93905[: 2022-12-12 02:37:56E.  93917/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 40400000:
638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 94897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9968237 / 100000000 nodes ( 9.97 %~10.00 %) | remote 29893701 / 100000000 nodes ( 29.89 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.76 GB | 1.87945 secs 
[2022-12-12 02:37:56. 94946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 95001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:37:56. 95737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9993814 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29868124 / 100000000 nodes ( 29.87 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.87712 secs 
[2022-12-12 02:37:56. 95863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9994861 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29867077 / 100000000 nodes ( 29.87 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.87722 secs 
[2022-12-12 02:37:56. 98830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9999520 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29862418 / 100000000 nodes ( 29.86 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.89942 secs 
[2022-12-12 02:37:56. 98966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9987015 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29874923 / 100000000 nodes ( 29.87 %) | cpu 60138062 / 100000000 nodes ( 60.14 %) | 4.77 GB | 1.8835 secs 
[2022-12-12 02:37:56.101149: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.41 GB
[2022-12-12 02:37:57.426727: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.67 GB
[2022-12-12 02:37:57.427308: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.67 GB
[2022-12-12 02:37:57.428620: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.67 GB
[2022-12-12 02:37:58.897967: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.94 GB
[2022-12-12 02:37:58.898318: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.94 GB
[2022-12-12 02:37:58.899554: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.94 GB
[2022-12-12 02:38:00.215088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.15 GB
[2022-12-12 02:38:00.215329: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.15 GB
[2022-12-12 02:38:00.215783: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.15 GB
[2022-12-12 02:38:01.760841: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.37 GB
[2022-12-12 02:38:01.761009: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.37 GB
[2022-12-12 02:38:01.761364: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.37 GB
[2022-12-12 02:38:03.242962: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.82 GB
[2022-12-12 02:38:03.243162: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.82 GB
[2022-12-12 02:38:03.243519: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.82 GB
[2022-12-12 02:38:04.603594: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.02 GB
[2022-12-12 02:38:04.603862: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.02 GB
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][tid #140235983812352]: replica 1 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][tid #140236050921216]: replica 7 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][tid #140236042528512]: replica 6 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][tid #140236050921216]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][tid #140236042528512]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][tid #140235983812352]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:38:04.837][ERROR][RK0][main]: init per replica done
[HCTR][02:38:04.837][ERROR][RK0][tid #140236050921216]: init per replica done
[HCTR][02:38:04.837][ERROR][RK0][tid #140236042528512]: init per replica done
[HCTR][02:38:04.837][ERROR][RK0][main]: init per replica done
[HCTR][02:38:04.837][ERROR][RK0][tid #140235983812352]: init per replica done
[HCTR][02:38:04.837][ERROR][RK0][main]: init per replica done
[HCTR][02:38:04.837][ERROR][RK0][main]: init per replica done
[HCTR][02:38:04.840][ERROR][RK0][main]: init per replica done
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 3 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 3 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 3 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 3 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 5 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 5 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 5 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.843][ERROR][RK0][tid #140236050921216]: 5 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.843][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.843][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.843][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.843][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.843][ERROR][RK0][tid #140235983812352]: 1 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.843][ERROR][RK0][tid #140235983812352]: 1 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.843][ERROR][RK0][tid #140235983812352]: 1 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.843][ERROR][RK0][tid #140235983812352]: 1 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.843][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.843][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.843][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.843][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.846][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.846][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.846][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.846][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 7 allocated 3276800 at 0x7f8d2bd20000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 7 allocated 6553600 at 0x7f8d2c200000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 7 allocated 3276800 at 0x7f8d2c840000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 7 allocated 6553600 at 0x7f8d2cb60000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 0 allocated 3276800 at 0x7f74f2b20000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 0 allocated 6553600 at 0x7f74f3000000
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 0 allocated 3276800 at 0x7f70fe50e800
[HCTR][02:38:04.846][ERROR][RK0][tid #140236050921216]: 0 allocated 6553600 at 0x7f70fe82e800








