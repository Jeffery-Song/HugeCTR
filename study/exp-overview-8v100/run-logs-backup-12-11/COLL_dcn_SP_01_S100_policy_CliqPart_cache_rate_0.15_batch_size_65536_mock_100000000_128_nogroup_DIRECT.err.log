2022-12-12 04:04:39.748448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.758650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.765977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.770526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.782810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.788632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.795836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.808350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.861073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.864955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.866670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.868188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.868697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.869590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.870542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.871235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.872247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.872983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.874008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.874467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.875731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.876058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.877538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.877697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.879279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.879337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.881159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.881341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.882621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.883727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.884693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.885525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.887296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.888264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.889324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.890261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.891300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.892290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.893306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.894310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.899244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.899464: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:39.900383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.901466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.902456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.903527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.904475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.905528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.907066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.908412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.908706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.909731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.910480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.911633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.912529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.914456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.914570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.916631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.916727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.918835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.918948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.921002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.921195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.923181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.923506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.926194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.927705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.927727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.928832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.930829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.931139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.931427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.932047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.933847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.934323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.934704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.935325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.943370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.944991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.945333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.946063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.946829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.947653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.947886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.948905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.949299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.953018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.970104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.979581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.983722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.984090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.985409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.985444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.987240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.987836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.987882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.988429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.988600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.990120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.990614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.992430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.992657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.992709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.993452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.994568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.995645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.996177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.998445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.998605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:39.998720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.000233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.000878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.001528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.002710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.002748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.002982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.005189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.006217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.006639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.006731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.006822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.009059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.010535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.010661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.010710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.011998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.013694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.013773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.013865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.015178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.016444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.016586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.016716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.017984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.019270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.019405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.019492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.020796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.022528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.022564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.022698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.024753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.025231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.025333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.025378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.027907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.028231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.028256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.028318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.030885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.031124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.031270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.031326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.033680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.033950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.034083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.034174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.036666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.036883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.037018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.037066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.040033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.040997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.040997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.041008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.041785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.044000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.044191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.044234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.044539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.044648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.047139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.047382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.047473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.048143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.048205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.050634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.050772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.050905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.051371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.051479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.052513: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.053943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.054092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.054275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.054421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.054912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.055046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.057634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.057838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.058400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.058545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.059205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.059407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.061798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.062088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.062407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.062783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.063296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.063397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.063458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.066405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.066893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.067238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.067463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.068034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.068036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.068230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.071015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.071755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.072772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.073059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.074891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.075494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.075939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.076001: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.076383: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.076384: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.077956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.078376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.079102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.081165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.081679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.082224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.084328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.084646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.085192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.085490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.085633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.085659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.088705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.089333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.090109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.090530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.090768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.090870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.093572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.094261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.094857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.095421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.095619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.095712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.098321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.099057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.099558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.103010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.133592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.134143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.137686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.138841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.139667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.142219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.144807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.144936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.154979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.158477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.158599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.161082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.163595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.163652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.167508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.198417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.198594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.201663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.203631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.204143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.206212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.208856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.209276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.212093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.213277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.213582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.216358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.221497: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.226351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.230242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.241180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.244577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.245841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.247924: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.248858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.257349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.283721: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:04:40.292027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.342070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.343059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.348789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:40.349394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.227939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.228593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.229135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.229801: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.229860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:04:41.250231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.250885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.251629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.252576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.253124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.253607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:04:41.299812: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.300007: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.331808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:04:41.458795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.460033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.460576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.461036: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.461088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:04:41.481902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.482551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.483063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.483669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.484209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.484699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:04:41.501432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.502075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.502616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.503079: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.503146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:04:41.509808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.510686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.511277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.511770: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.511826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:04:41.520742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.521365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.521909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.522581: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.522635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:04:41.523351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.523989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.524714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.525314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.526146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.526645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:04:41.529698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.530449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.530978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.532509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.533036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.533512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:04:41.537386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.537978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.538495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.538966: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.539008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:04:41.540607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.541197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.541726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.542283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.543035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.543519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:04:41.556592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.557571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.558117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.558726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.559276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.560026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:04:41.569801: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.569994: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.571884: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:04:41.577664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.578275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.578594: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.578765: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.578811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.579322: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.579383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:04:41.579862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.580446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.580985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.581029: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 04:04:41.581447: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:04:41.581491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:04:41.589477: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.589652: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.591549: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:04:41.597899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.598568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.599078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.599336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.599979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.600216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.600954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.601090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.601847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:04:41.602071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.602603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:04:41.603069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:04:41.606249: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.606417: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.608303: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:04:41.608647: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.608777: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.609696: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:04:41.648804: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.648997: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.649948: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:04:41.650751: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.650892: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:04:41.652899: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:04:42.931][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.60s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 82.18it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 99it [00:01, 85.07it/s]warmup run: 96it [00:01, 82.70it/s]warmup run: 98it [00:01, 80.20it/s]warmup run: 101it [00:01, 87.44it/s]warmup run: 195it [00:01, 179.32it/s]warmup run: 98it [00:01, 83.91it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 199it [00:01, 185.19it/s]warmup run: 192it [00:01, 179.03it/s]warmup run: 198it [00:01, 176.58it/s]warmup run: 203it [00:01, 190.32it/s]warmup run: 296it [00:01, 290.36it/s]warmup run: 196it [00:01, 181.71it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 93it [00:01, 81.53it/s]warmup run: 300it [00:01, 296.51it/s]warmup run: 289it [00:01, 286.16it/s]warmup run: 295it [00:01, 279.84it/s]warmup run: 302it [00:01, 298.91it/s]warmup run: 293it [00:01, 288.02it/s]warmup run: 398it [00:01, 406.84it/s]warmup run: 101it [00:01, 88.57it/s]warmup run: 189it [00:01, 179.51it/s]warmup run: 402it [00:01, 413.40it/s]warmup run: 386it [00:01, 396.84it/s]warmup run: 393it [00:01, 389.13it/s]warmup run: 396it [00:01, 402.97it/s]warmup run: 392it [00:01, 401.06it/s]warmup run: 500it [00:02, 520.20it/s]warmup run: 202it [00:01, 191.41it/s]warmup run: 287it [00:01, 289.47it/s]warmup run: 504it [00:02, 527.15it/s]warmup run: 483it [00:02, 503.77it/s]warmup run: 491it [00:02, 496.73it/s]warmup run: 490it [00:02, 503.90it/s]warmup run: 491it [00:02, 511.00it/s]warmup run: 603it [00:02, 627.01it/s]warmup run: 303it [00:01, 303.80it/s]warmup run: 386it [00:01, 404.42it/s]warmup run: 606it [00:02, 631.28it/s]warmup run: 581it [00:02, 604.00it/s]warmup run: 590it [00:02, 597.84it/s]warmup run: 587it [00:02, 602.51it/s]warmup run: 589it [00:02, 609.92it/s]warmup run: 704it [00:02, 714.02it/s]warmup run: 403it [00:01, 417.75it/s]warmup run: 485it [00:01, 514.66it/s]warmup run: 705it [00:02, 714.40it/s]warmup run: 680it [00:02, 693.20it/s]warmup run: 686it [00:02, 680.29it/s]warmup run: 686it [00:02, 691.96it/s]warmup run: 686it [00:02, 693.18it/s]warmup run: 805it [00:02, 785.79it/s]warmup run: 502it [00:01, 525.85it/s]warmup run: 585it [00:02, 616.93it/s]warmup run: 806it [00:02, 787.30it/s]warmup run: 778it [00:02, 763.18it/s]warmup run: 782it [00:02, 747.50it/s]warmup run: 781it [00:02, 754.20it/s]warmup run: 783it [00:02, 760.28it/s]warmup run: 906it [00:02, 842.43it/s]warmup run: 602it [00:02, 625.88it/s]warmup run: 681it [00:02, 696.15it/s]warmup run: 908it [00:02, 848.01it/s]warmup run: 876it [00:02, 817.87it/s]warmup run: 883it [00:02, 813.77it/s]warmup run: 876it [00:02, 798.68it/s]warmup run: 879it [00:02, 812.22it/s]warmup run: 1008it [00:02, 890.04it/s]warmup run: 701it [00:02, 710.21it/s]warmup run: 778it [00:02, 762.71it/s]warmup run: 1009it [00:02, 891.91it/s]warmup run: 974it [00:02, 860.05it/s]warmup run: 985it [00:02, 869.16it/s]warmup run: 970it [00:02, 836.64it/s]warmup run: 976it [00:02, 853.71it/s]warmup run: 1112it [00:02, 930.57it/s]warmup run: 801it [00:02, 782.17it/s]warmup run: 874it [00:02, 812.43it/s]warmup run: 1112it [00:02, 930.15it/s]warmup run: 1072it [00:02, 892.89it/s]warmup run: 1088it [00:02, 912.88it/s]warmup run: 1065it [00:02, 866.23it/s]warmup run: 1077it [00:02, 896.07it/s]warmup run: 1217it [00:02, 964.45it/s]warmup run: 899it [00:02, 833.48it/s]warmup run: 972it [00:02, 855.79it/s]warmup run: 1215it [00:02, 957.32it/s]warmup run: 1172it [00:02, 922.23it/s]warmup run: 1188it [00:02, 928.91it/s]warmup run: 1161it [00:02, 891.76it/s]warmup run: 1178it [00:02, 927.72it/s]warmup run: 1321it [00:02, 985.83it/s]warmup run: 999it [00:02, 878.35it/s]warmup run: 1068it [00:02, 884.40it/s]warmup run: 1317it [00:02, 972.88it/s]warmup run: 1272it [00:02, 942.81it/s]warmup run: 1287it [00:02, 933.09it/s]warmup run: 1258it [00:02, 911.85it/s]warmup run: 1277it [00:02, 945.38it/s]warmup run: 1425it [00:02, 1000.79it/s]warmup run: 1100it [00:02, 913.64it/s]warmup run: 1170it [00:02, 920.95it/s]warmup run: 1420it [00:02, 988.91it/s]warmup run: 1373it [00:02, 961.02it/s]warmup run: 1385it [00:03, 937.24it/s]warmup run: 1353it [00:02, 922.00it/s]warmup run: 1377it [00:02, 960.61it/s]warmup run: 1529it [00:03, 1009.17it/s]warmup run: 1202it [00:02, 942.71it/s]warmup run: 1270it [00:02, 942.54it/s]warmup run: 1522it [00:03, 992.38it/s]warmup run: 1474it [00:03, 974.80it/s]warmup run: 1482it [00:03, 938.38it/s]warmup run: 1451it [00:03, 938.48it/s]warmup run: 1632it [00:03, 1013.02it/s]warmup run: 1304it [00:02, 963.99it/s]warmup run: 1476it [00:03, 960.90it/s]warmup run: 1371it [00:02, 961.83it/s]warmup run: 1625it [00:03, 1003.13it/s]warmup run: 1575it [00:03, 983.48it/s]warmup run: 1578it [00:03, 939.01it/s]warmup run: 1550it [00:03, 952.26it/s]warmup run: 1736it [00:03, 1020.76it/s]warmup run: 1407it [00:02, 982.13it/s]warmup run: 1575it [00:03, 962.85it/s]warmup run: 1472it [00:02, 974.50it/s]warmup run: 1728it [00:03, 1010.15it/s]warmup run: 1677it [00:03, 991.47it/s]warmup run: 1674it [00:03, 938.40it/s]warmup run: 1648it [00:03, 959.63it/s]warmup run: 1841it [00:03, 1028.70it/s]warmup run: 1508it [00:02, 988.65it/s]warmup run: 1673it [00:03, 961.20it/s]warmup run: 1573it [00:03, 983.43it/s]warmup run: 1831it [00:03, 1009.60it/s]warmup run: 1778it [00:03, 994.96it/s]warmup run: 1769it [00:03, 935.83it/s]warmup run: 1750it [00:03, 975.17it/s]warmup run: 1946it [00:03, 1033.80it/s]warmup run: 1609it [00:03, 988.96it/s]warmup run: 1771it [00:03, 958.71it/s]warmup run: 1673it [00:03, 935.77it/s]warmup run: 1933it [00:03, 1001.11it/s]warmup run: 1879it [00:03, 998.44it/s]warmup run: 1850it [00:03, 980.80it/s]warmup run: 1864it [00:03, 930.08it/s]warmup run: 2059it [00:03, 1060.91it/s]warmup run: 1710it [00:03, 992.54it/s]warmup run: 1868it [00:03, 957.41it/s]warmup run: 1768it [00:03, 916.71it/s]warmup run: 2039it [00:03, 1016.13it/s]warmup run: 1980it [00:03, 999.54it/s]warmup run: 1950it [00:03, 983.97it/s]warmup run: 1960it [00:03, 936.05it/s]warmup run: 2183it [00:03, 1111.94it/s]warmup run: 1812it [00:03, 998.41it/s]warmup run: 1965it [00:03, 955.89it/s]warmup run: 1867it [00:03, 937.11it/s]warmup run: 2160it [00:03, 1071.32it/s]warmup run: 2097it [00:03, 1050.03it/s]warmup run: 2057it [00:03, 1008.59it/s]warmup run: 2069it [00:03, 979.93it/s]warmup run: 2307it [00:03, 1148.62it/s]warmup run: 1914it [00:03, 1004.23it/s]warmup run: 2076it [00:03, 1000.19it/s]warmup run: 1967it [00:03, 953.38it/s]warmup run: 2281it [00:03, 1110.56it/s]warmup run: 2219it [00:03, 1100.57it/s]warmup run: 2175it [00:03, 1059.49it/s]warmup run: 2190it [00:03, 1046.28it/s]warmup run: 2431it [00:03, 1173.96it/s]warmup run: 2019it [00:03, 1015.52it/s]warmup run: 2197it [00:03, 1059.92it/s]warmup run: 2081it [00:03, 1007.22it/s]warmup run: 2402it [00:03, 1138.59it/s]warmup run: 2341it [00:03, 1135.76it/s]warmup run: 2294it [00:03, 1096.21it/s]warmup run: 2312it [00:03, 1095.93it/s]warmup run: 2555it [00:03, 1191.82it/s]warmup run: 2142it [00:03, 1078.79it/s]warmup run: 2317it [00:03, 1099.83it/s]warmup run: 2203it [00:03, 1068.06it/s]warmup run: 2523it [00:03, 1157.37it/s]warmup run: 2464it [00:03, 1161.30it/s]warmup run: 2413it [00:03, 1122.54it/s]warmup run: 2434it [00:04, 1131.97it/s]warmup run: 2679it [00:04, 1205.03it/s]warmup run: 2261it [00:03, 1111.63it/s]warmup run: 2437it [00:03, 1128.40it/s]warmup run: 2325it [00:03, 1112.03it/s]warmup run: 2644it [00:04, 1170.58it/s]warmup run: 2586it [00:04, 1178.60it/s]warmup run: 2532it [00:04, 1139.97it/s]warmup run: 2555it [00:04, 1155.04it/s]warmup run: 2801it [00:04, 1209.35it/s]warmup run: 2379it [00:03, 1131.95it/s]warmup run: 2558it [00:04, 1151.25it/s]warmup run: 2447it [00:03, 1141.78it/s]warmup run: 2763it [00:04, 1174.29it/s]warmup run: 2709it [00:04, 1191.79it/s]warmup run: 2651it [00:04, 1152.71it/s]warmup run: 2676it [00:04, 1169.41it/s]warmup run: 2924it [00:04, 1215.41it/s]warmup run: 2499it [00:03, 1150.30it/s]warmup run: 2679it [00:04, 1168.73it/s]warmup run: 2569it [00:04, 1162.76it/s]warmup run: 2883it [00:04, 1179.88it/s]warmup run: 3000it [00:04, 694.84it/s] warmup run: 2830it [00:04, 1196.55it/s]warmup run: 2768it [00:04, 1157.72it/s]warmup run: 2795it [00:04, 1174.54it/s]warmup run: 2619it [00:03, 1164.01it/s]warmup run: 2799it [00:04, 1176.22it/s]warmup run: 2690it [00:04, 1176.54it/s]warmup run: 3000it [00:04, 691.75it/s] warmup run: 2953it [00:04, 1203.95it/s]warmup run: 2888it [00:04, 1169.37it/s]warmup run: 2915it [00:04, 1181.43it/s]warmup run: 2739it [00:04, 1174.13it/s]warmup run: 2918it [00:04, 1179.63it/s]warmup run: 3000it [00:04, 686.83it/s] warmup run: 2808it [00:04, 1163.89it/s]warmup run: 3000it [00:04, 665.23it/s] warmup run: 3000it [00:04, 678.91it/s] warmup run: 3000it [00:04, 678.90it/s] warmup run: 2858it [00:04, 1178.13it/s]warmup run: 2930it [00:04, 1179.06it/s]warmup run: 2982it [00:04, 1195.76it/s]warmup run: 3000it [00:04, 695.19it/s] warmup run: 3000it [00:04, 683.59it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.68it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.80it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.49it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1627.63it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.42it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.25it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.69it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1602.37it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1695.02it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1701.34it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1650.81it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1636.26it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.28it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1687.29it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1633.22it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1694.29it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1691.89it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1688.14it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1653.80it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1638.98it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1699.21it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1646.32it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1691.89it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1646.62it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1691.71it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1690.47it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1638.55it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1652.31it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1655.95it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1690.89it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1644.00it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1692.09it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1690.76it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1636.43it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1657.45it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1690.89it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1659.13it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1641.69it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1688.68it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1682.56it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1634.92it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1690.31it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1688.54it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1652.72it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1659.48it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1687.63it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1638.94it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1677.07it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1630.52it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1656.29it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1684.41it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1683.51it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1683.72it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1645.20it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1635.85it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1654.35it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1655.80it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1686.26it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1687.09it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1684.56it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1627.06it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1636.42it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1642.01it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:01, 1640.83it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1655.04it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1687.51it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1688.42it/s]warmup should be done:  51%|█████     | 1526/3000 [00:00<00:00, 1683.73it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1636.43it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1631.32it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1588.35it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1631.56it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1654.21it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1686.60it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1686.65it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1635.27it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1681.03it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1616.63it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1574.51it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1625.94it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1684.30it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1635.39it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1647.38it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1679.12it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1679.06it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1606.15it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1621.24it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1563.68it/s]warmup should be done:  66%|██████▌   | 1976/3000 [00:01<00:00, 1634.55it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1677.67it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1680.84it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1641.97it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1666.53it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1599.78it/s]warmup should be done:  67%|██████▋   | 2010/3000 [00:01<00:00, 1619.22it/s]warmup should be done:  65%|██████▌   | 1951/3000 [00:01<00:00, 1555.54it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1638.85it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1678.82it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1685.08it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1635.39it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1661.71it/s]warmup should be done:  71%|███████▏  | 2139/3000 [00:01<00:00, 1597.64it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1617.26it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1553.27it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1642.67it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1685.31it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1677.62it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1630.87it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1655.35it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1595.55it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1616.06it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1550.39it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1641.85it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1686.32it/s]warmup should be done:  85%|████████▍ | 2538/3000 [00:01<00:00, 1678.70it/s]warmup should be done:  83%|████████▎ | 2484/3000 [00:01<00:00, 1623.39it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1597.52it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1613.99it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1613.25it/s]warmup should be done:  81%|████████  | 2419/3000 [00:01<00:00, 1549.70it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1640.35it/s]warmup should be done:  90%|█████████ | 2707/3000 [00:01<00:00, 1679.26it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1681.07it/s]warmup should be done:  88%|████████▊ | 2647/3000 [00:01<00:00, 1623.23it/s]warmup should be done:  88%|████████▊ | 2627/3000 [00:01<00:00, 1619.12it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1614.32it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1573.93it/s]warmup should be done:  90%|████████▉ | 2697/3000 [00:01<00:00, 1524.39it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1681.51it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1637.53it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1680.10it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1626.04it/s]warmup should be done:  93%|█████████▎| 2795/3000 [00:01<00:00, 1635.26it/s]warmup should be done:  94%|█████████▍| 2820/3000 [00:01<00:00, 1614.83it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1591.30it/s]warmup should be done:  95%|█████████▌| 2863/3000 [00:01<00:00, 1562.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.77it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1644.31it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1632.31it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1651.75it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1619.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.11it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1606.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.47it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.21it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1715.79it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.34it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.73it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.46it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1713.99it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1703.36it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.04it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.72it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1715.60it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1650.80it/s]warmup should be done:  12%|█▏        | 345/3000 [00:00<00:01, 1718.83it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1700.65it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1700.90it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1665.72it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1664.55it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1718.68it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1719.57it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1678.23it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1657.69it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1707.45it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1669.05it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1668.11it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1692.40it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1717.74it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1681.24it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1656.51it/s]warmup should be done:  23%|██▎       | 690/3000 [00:00<00:01, 1720.67it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1715.12it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1673.26it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1672.30it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1699.53it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1682.85it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1656.15it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1675.37it/s]warmup should be done:  29%|██▉       | 863/3000 [00:00<00:01, 1719.36it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1712.62it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1682.83it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1703.11it/s]warmup should be done:  29%|██▊       | 861/3000 [00:00<00:01, 1533.67it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1659.73it/s]warmup should be done:  34%|███▍      | 1035/3000 [00:00<00:01, 1718.75it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1680.41it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1673.16it/s]warmup should be done:  34%|███▍      | 1032/3000 [00:00<00:01, 1715.15it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1686.46it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1703.88it/s]warmup should be done:  34%|███▍      | 1031/3000 [00:00<00:01, 1585.54it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1663.06it/s]warmup should be done:  40%|████      | 1207/3000 [00:00<00:01, 1717.24it/s]warmup should be done:  40%|████      | 1204/3000 [00:00<00:01, 1713.95it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1677.28it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1689.08it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1665.16it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1695.74it/s]warmup should be done:  40%|████      | 1202/3000 [00:00<00:01, 1622.92it/s]warmup should be done:  46%|████▌     | 1380/3000 [00:00<00:00, 1719.68it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1660.39it/s]warmup should be done:  46%|████▌     | 1377/3000 [00:00<00:00, 1718.05it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1693.94it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1678.34it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1668.93it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:00, 1694.92it/s]warmup should be done:  46%|████▌     | 1375/3000 [00:00<00:00, 1652.82it/s]warmup should be done:  52%|█████▏    | 1552/3000 [00:00<00:00, 1718.92it/s]warmup should be done:  52%|█████▏    | 1550/3000 [00:00<00:00, 1718.90it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1659.31it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1694.39it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1675.90it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1669.64it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1697.41it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1671.93it/s]warmup should be done:  57%|█████▋    | 1724/3000 [00:01<00:00, 1718.48it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1661.73it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1694.55it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1676.82it/s]warmup should be done:  57%|█████▋    | 1722/3000 [00:01<00:00, 1714.00it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1672.96it/s]warmup should be done:  57%|█████▋    | 1710/3000 [00:01<00:00, 1698.19it/s]warmup should be done:  57%|█████▋    | 1719/3000 [00:01<00:00, 1684.65it/s]warmup should be done:  63%|██████▎   | 1897/3000 [00:01<00:00, 1721.26it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1663.85it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1678.63it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1692.52it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1676.60it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1700.20it/s]warmup should be done:  63%|██████▎   | 1894/3000 [00:01<00:00, 1700.71it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1694.71it/s]warmup should be done:  69%|██████▉   | 2070/3000 [00:01<00:00, 1723.12it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1662.37it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1679.36it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1691.58it/s]warmup should be done:  67%|██████▋   | 2018/3000 [00:01<00:00, 1676.99it/s]warmup should be done:  68%|██████▊   | 2053/3000 [00:01<00:00, 1703.75it/s]warmup should be done:  69%|██████▉   | 2067/3000 [00:01<00:00, 1708.14it/s]warmup should be done:  69%|██████▉   | 2064/3000 [00:01<00:00, 1703.73it/s]warmup should be done:  75%|███████▍  | 2243/3000 [00:01<00:00, 1720.62it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1677.85it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1689.73it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1676.19it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1650.47it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1704.05it/s]warmup should be done:  75%|███████▍  | 2240/3000 [00:01<00:00, 1712.35it/s]warmup should be done:  75%|███████▍  | 2236/3000 [00:01<00:00, 1707.94it/s]warmup should be done:  81%|████████  | 2416/3000 [00:01<00:00, 1719.96it/s]warmup should be done:  79%|███████▊  | 2361/3000 [00:01<00:00, 1679.04it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1687.84it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1677.88it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1655.34it/s]warmup should be done:  80%|███████▉  | 2395/3000 [00:01<00:00, 1703.04it/s]warmup should be done:  80%|████████  | 2412/3000 [00:01<00:00, 1713.58it/s]warmup should be done:  80%|████████  | 2408/3000 [00:01<00:00, 1709.88it/s]warmup should be done:  86%|████████▋ | 2588/3000 [00:01<00:00, 1718.43it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1675.67it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1690.27it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1677.48it/s]warmup should be done:  83%|████████▎ | 2501/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  86%|████████▌ | 2566/3000 [00:01<00:00, 1703.88it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1716.22it/s]warmup should be done:  86%|████████▌ | 2580/3000 [00:01<00:00, 1711.94it/s]warmup should be done:  92%|█████████▏| 2761/3000 [00:01<00:00, 1719.00it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1677.15it/s]warmup should be done:  90%|████████▉ | 2693/3000 [00:01<00:00, 1681.91it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1657.02it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1701.38it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1663.55it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1677.49it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1714.79it/s]warmup should be done:  98%|█████████▊| 2934/3000 [00:01<00:00, 1719.74it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1689.59it/s]warmup should be done:  96%|█████████▌| 2866/3000 [00:01<00:00, 1676.54it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1657.61it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1702.26it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1665.78it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1679.20it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1717.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1718.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1703.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.11it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1656.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.51it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e1887730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e17c90a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e17c72b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e17ba1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e1886b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e17bd190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e1889d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34e17c81c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 04:06:13.505458: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f301702d7b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:13.505519: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:13.515913: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:13.519466: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f301282f910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:13.519509: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:13.528002: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:13.670899: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3006798880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:13.670961: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:13.678867: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:13.786104: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f301a834280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:13.786167: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:13.796434: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:14.018277: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3022838060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:14.018348: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:14.028217: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:14.188369: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f302282c470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:14.188437: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:14.198075: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:14.201219: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3016830130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:14.201281: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:14.209359: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:14.243773: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f30128380f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:06:14.243836: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:06:14.253058: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:06:20.589488: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:20.646943: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:20.672989: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:20.907381: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:21.064654: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:21.076763: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:21.177733: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:06:21.275270: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:07:12.725][ERROR][RK0][tid #139845192111872]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:07:12.725][ERROR][RK0][tid #139845192111872]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:12.733][ERROR][RK0][tid #139845192111872]: coll ps creation done
[HCTR][04:07:12.733][ERROR][RK0][tid #139845192111872]: replica 5 waits for coll ps creation barrier
[HCTR][04:07:12.820][ERROR][RK0][tid #139844982392576]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:07:12.821][ERROR][RK0][tid #139844982392576]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:12.825][ERROR][RK0][tid #139844982392576]: coll ps creation done
[HCTR][04:07:12.826][ERROR][RK0][tid #139844982392576]: replica 7 waits for coll ps creation barrier
[HCTR][04:07:12.898][ERROR][RK0][tid #139845519263488]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:07:12.898][ERROR][RK0][tid #139845519263488]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:12.902][ERROR][RK0][tid #139845863200512]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:07:12.903][ERROR][RK0][tid #139845863200512]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:12.903][ERROR][RK0][tid #139845519263488]: coll ps creation done
[HCTR][04:07:12.903][ERROR][RK0][tid #139845519263488]: replica 2 waits for coll ps creation barrier
[HCTR][04:07:12.907][ERROR][RK0][tid #139845863200512]: coll ps creation done
[HCTR][04:07:12.907][ERROR][RK0][tid #139845863200512]: replica 3 waits for coll ps creation barrier
[HCTR][04:07:12.927][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:07:12.927][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:12.935][ERROR][RK0][main]: coll ps creation done
[HCTR][04:07:12.935][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][04:07:13.036][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:07:13.036][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:13.045][ERROR][RK0][main]: coll ps creation done
[HCTR][04:07:13.045][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][04:07:13.171][ERROR][RK0][tid #139845200504576]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:07:13.171][ERROR][RK0][tid #139845200504576]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:13.172][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:07:13.173][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:07:13.176][ERROR][RK0][tid #139845200504576]: coll ps creation done
[HCTR][04:07:13.176][ERROR][RK0][tid #139845200504576]: replica 1 waits for coll ps creation barrier
[HCTR][04:07:13.179][ERROR][RK0][main]: coll ps creation done
[HCTR][04:07:13.179][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][04:07:13.179][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][04:07:14.018][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][04:07:14.061][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][tid #139845519263488]: replica 2 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][tid #139844982392576]: replica 7 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][tid #139845200504576]: replica 1 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][tid #139845192111872]: replica 5 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][tid #139845863200512]: replica 3 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][04:07:14.061][ERROR][RK0][main]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][main]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][tid #139845519263488]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][tid #139844982392576]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][tid #139845200504576]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][tid #139845192111872]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][tid #139845519263488]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][tid #139845863200512]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][tid #139845200504576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][main]: Calling build_v2
[HCTR][04:07:14.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][tid #139844982392576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][tid #139845192111872]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][tid #139845863200512]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:07:14.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 04:07:14. 66159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie
2022-12-12 04:07:14[.2022-12-12 04:07:14 66204.:  66238E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 196v100x8, slow pcie] 
assigning 0 to cpu
[2022-12-12 04:07:14. 66289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:196] assigning 0 to cpu
2022-12-12 04:07:14. 66291: [E2022-12-12 04:07:14 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 66330:: 178E] [ v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:2022-12-12 04:07:14212.]  66359[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 2022-12-12 04:07:14
E.2022-12-12 04:07:14  66372./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [ 66336:E[: 212 2022-12-12 04:07:14E2022-12-12 04:07:14] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 66403/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 66384
196: :: ] E178Eassigning 0 to cpu ] [[ 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 04:07:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
.:2022-12-12 04:07:14213 66461178[.[] : ]  664332022-12-12 04:07:14[remote time is 8.68421Ev100x8, slow pcie: 2022-12-12 04:07:14.2022-12-12 04:07:14
 
E. 66496./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [ 66483:  66513[:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:07:14: E: 2022-12-12 04:07:14213:.E E2022-12-12 04:07:14.] 178 66547 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc . 66550remote time is 8.68421] : v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 66533: 
E
:196212: E [178] ] [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:07:14] assigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 04:07:14 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.v100x8, slow pcie

./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214 66680
 66686:196[] : : [178] [2022-12-12 04:07:14cpu time is 97.0588EE2022-12-12 04:07:14] assigning 0 to cpu2022-12-12 04:07:14.
  .v100x8, slow pcie
. 66758/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 66770
 66770: ::: : E214[196EE ] 2022-12-12 04:07:14[]   /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588.2022-12-12 04:07:14assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
 66858.
::213:  66875212196] E: ] ] remote time is 8.68421 Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[assigning 0 to cpu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
2022-12-12 04:07:14
:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196[2022-12-12 04:07:14: 66969] 2022-12-12 04:07:14.212: assigning 0 to cpu. 67001] E
[ 67015: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-12 04:07:14: E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E : 67057 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:] 2022-12-12 04:07:14E:2022-12-12 04:07:14214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. 213.] 
 67105/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  67106cpu time is 97.0588: :remote time is 8.68421: 
[E212
E2022-12-12 04:07:14 ]  .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 671872022-12-12 04:07:14:
:: .213212E 67219[] ]  : 2022-12-12 04:07:14remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.

:  67268213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 2022-12-12 04:07:14:E[remote time is 8.68421.214 2022-12-12 04:07:14
 67328] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: cpu time is 97.0588:[ 67340E
2132022-12-12 04:07:14:  ] .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 67382 :
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214[E:] 2022-12-12 04:07:14 213cpu time is 97.0588./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
 67458:remote time is 8.68421: 214
E]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:2022-12-12 04:07:14214.]  67526cpu time is 97.0588: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 04:08:30.723384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:08:30.763367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 04:08:30.890105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:08:30.890164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:08:31. 64528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:08:31. 64567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:08:31. 65079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:08:31. 65130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 66063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 66889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 79669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 04:08:31. 79736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[[2022-12-12 04:08:312022-12-12 04:08:31.. 80145 80158: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2021815] ] 5 solvedBuilding Coll Cache with ... num gpu device is 8

[[2022-12-12 04:08:312022-12-12 04:08:31.. 80220[[ 80204: 2022-12-12 04:08:312022-12-12 04:08:31: E..E  80224 80237 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:EE:205  202] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] worker 0 thread 5 initing device 5::
7 solved2021980
] ] eager alloc mem 381.47 MB
[1 solved2022-12-12 04:08:31
. 80326: E[ 2022-12-12 04:08:31/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.: 80341205: ] Eworker 0 thread 7 initing device 7 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 04:08:31. 80690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:08:31. 80743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 04:08:31
[.2022-12-12 04:08:31 80760.:  80768E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815:] 1815Building Coll Cache with ... num gpu device is 8] 
Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:08:31[.2022-12-12 04:08:31 80832.:  80836E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 82900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 83158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 83702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 83742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 84087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 04:08:31. 84178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 04:08:31. 84389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 04:08:31. 84482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 04:08:31. 84676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:08:31. 84759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 85020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:08:31. 85105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 86749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 87097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 87646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 87786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 88938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 89257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 04:08:31. 89329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 04:08:31. 89438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 89876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:08:31. 89942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 92572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 92691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 92800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31. 95565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:08:31.149309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:08:31.154980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:08:31.155102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.155933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.156584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:31.157663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:31.157718: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[[2022-12-12 04:08:312022-12-12 04:08:31..171174171174: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[[2022-12-12 04:08:312022-12-12 04:08:31..174428174428: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Bytes
eager alloc mem 5.00 Bytes
[2022-12-12 04:08:31.175207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:08:31.176886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:08:31.176984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.177032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:08:31.177128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.178493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.179505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:08:31.179764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.180924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:31.181048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:31.181693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:08:31.181781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.181974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:31.182020: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:08:31.182047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 04:08:31638.] 182095: eager release cuda mem 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:31.182157: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] [2022-12-12 04:08:31WORKER[0] alloc host memory 57.22 MB2022-12-12 04:08:31.
.182160182173: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 04:08:31.182290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.183281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.184496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:08:31.184851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.185362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.185881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:08:31.185966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.185999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 04:08:312022-12-12 04:08:31..186521186533: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 04:08:31.187032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 04:08:31eager release cuda mem 625663.
187049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.187085: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:08:31.187623: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 04:08:31:.638187634] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 04:08:31.187692: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:08:31.187725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 57.22 MB2022-12-12 04:08:31
.187763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:31.188809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:31.188855: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:08:31.189935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:08:31.190051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:08:31.191237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:08:31.192138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:31.193230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:31.193304: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:08:31.197532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:08:31.198162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:08:31.198206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.221244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:08:31.221879[: 2022-12-12 04:08:31E. 221869/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 25855:
1980] eager alloc mem 25.25 KB
[2022-12-12 04:08:31.221948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.222506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:08:31.222550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.225371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 04:08:312022-12-12 04:08:31..225985226001: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[2022-12-12 04:08:31.226070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.226613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:08:31.226655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.227201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:08:31.227819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:08:31.227863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.229642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:08:31.230296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:08:31.230370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:08:31.233521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:08:31.234131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:08:31.234175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[[[[[[[[2022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:33........980756980756980758980756980756980755980756980760: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] Device 7 init p2p of link 4] ] Device 2 init p2p of link 1Device 6 init p2p of link 0Device 1 init p2p of link 7Device 4 init p2p of link 5Device 5 init p2p of link 6
Device 3 init p2p of link 2Device 0 init p2p of link 3






[[[[[[2022-12-12 04:08:332022-12-12 04:08:33[2022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:332022-12-12 04:08:33..2022-12-12 04:08:33.[...981290981290.9812902022-12-12 04:08:33981290981293981297: : 981300: .: : : EE: E981327EEE  E :    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :::19801980:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu198019801980] ] 1980] :] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB1980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB

eager alloc mem 611.00 KB
] 



eager alloc mem 611.00 KB
[2022-12-12 04:08:33.982297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 04:08:33638.] 982313eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:33.982394[: 2022-12-12 04:08:33E. 982402[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: [2022-12-12 04:08:33[:E2022-12-12 04:08:33.2022-12-12 04:08:33638 .982413.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[982417: 982420eager release cuda mem 625663:2022-12-12 04:08:33: E: 
638.E E] 982442 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
E:638:638]  638] eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] eager release cuda mem 625663
:eager release cuda mem 625663
638
] eager release cuda mem 625663
[2022-12-12 04:08:33.995306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:08:33.995474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:33.995615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:08:33.995772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:33.996375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:33.996667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34.  2694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:08:34.  2859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  3095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 04:08:34.  3266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:08:341980.]   3270eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 04:08:34.  3334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 04:08:34.  3437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  3500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  3747: E[ 2022-12-12 04:08:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:  3747638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 04:08:34.  3934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  4024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:08:34.  4174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 04:08:34] .eager release cuda mem 625663  4192
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  4283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34.  4351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34.  4708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34.  5111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34.  7531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 04:08:34.  7652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  7841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:08:34.  7970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34.  8525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34.  8850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 17035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:08:34. 17166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 18108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 04:08:34eager release cuda mem 625663.
 18119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 04:08:34. 18262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 19181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 23385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:08:34. 23515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 23749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 04:08:34. 23877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 24156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 04:08:34. 24280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 24305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 24669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 25156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 25193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 04:08:34. 25335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 26213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 31478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:08:34. 31596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 31763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 04:08:34. 31880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 32493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 32782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 33352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:08:34. 33468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 33638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:08:34. 33762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 34355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 34661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 36720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 04:08:34. 36839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 37150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:08:34. 37281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 37735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 38178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 46143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:08:34. 46264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 46461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:08:34. 46596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:08:34. 47038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 47370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:08:34. 50842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 51184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 51533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.9708 secs 
[2022-12-12 04:08:34. 51768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96668 secs 
[2022-12-12 04:08:34. 53987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 54393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96447 secs 
[2022-12-12 04:08:34. 54451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 54835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97461 secs 
[2022-12-12 04:08:34. 56718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 57105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955[] 2022-12-12 04:08:34Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97236 secs .
 57117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 57511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97669 secs 
[2022-12-12 04:08:34. 58612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 58946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:08:34. 58996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97817 secs 
[2022-12-12 04:08:34. 59369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.99425 secs 
[HCTR][04:08:34.059][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][tid #139845192111872]: replica 5 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][tid #139844982392576]: replica 7 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][tid #139845519263488]: replica 2 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][tid #139845863200512]: replica 3 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][tid #139845200504576]: replica 1 calling init per replica done, doing barrier
[HCTR][04:08:34.059][ERROR][RK0][tid #139845519263488]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845200504576]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][tid #139844982392576]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845863200512]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845192111872]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845519263488]: init per replica done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845200504576]: init per replica done
[HCTR][04:08:34.059][ERROR][RK0][tid #139844982392576]: init per replica done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845863200512]: init per replica done
[HCTR][04:08:34.059][ERROR][RK0][tid #139845192111872]: init per replica done
[HCTR][04:08:34.059][ERROR][RK0][main]: init per replica done
[HCTR][04:08:34.059][ERROR][RK0][main]: init per replica done
[HCTR][04:08:34.062][ERROR][RK0][main]: init per replica done
