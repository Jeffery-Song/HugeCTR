2022-12-11 23:28:42.206726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.218969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.225291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.237094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.244289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.247787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.259471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.265569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.276129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.286823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.297595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.301237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.302186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.303141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.304115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.305217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.319283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.319834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.321193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.321753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.322276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.323119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.323316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.324446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.325466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.325568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.326713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.327554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.327599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.328726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.329389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.329571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.330766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.331123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.331617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.332800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.333024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.333511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.335084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.335258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.337577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.337609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.339289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.339866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.341929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.342011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.342666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.344143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.344207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.345747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.346685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.347098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.348074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.348739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.349223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.349913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.350838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.351147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.351569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.351583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.351836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.353152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.353812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.354658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.355049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.355199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.355951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.356484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.356980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.357901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.358486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.358740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.359319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.359847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.360084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.361521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.362088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.362228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.362671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.363302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.364658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.365068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.365248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.365331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.365694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.367578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.367998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.368107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.368362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.369779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.370076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.370084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.370208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.372082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.372083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.372117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.373584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.373591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.374588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.375087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.375600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.376093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.376593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.377087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.378363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.378950: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.378975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.379591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.380130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.380675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.380828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.381519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.381748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.382406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.382628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.383438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.383554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.384469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.384659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.385374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.385600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.386224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.386516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.387374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.387431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.388602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.388634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.388758: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.389162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.389806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.390004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.390845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.391334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.391583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.392440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.392840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.393133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.394305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.394609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.396489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.396676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.398207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.398253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.398310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.400227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.400376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.400390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.402380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.402427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.403075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.404308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.405181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.406317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.407281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.408366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.408380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.409660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.411034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.411700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.411817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.412953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.413442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.415618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.415778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.416458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.417254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.417807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.417846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.419282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.420207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.420214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.421539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.421898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.422248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.423752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.424599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.458392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.459333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.460003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.460175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.461512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.462875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.463950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.464604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.464783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.466485: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.466662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.467333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.468034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.469655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.469783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.471192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.473214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.474705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.475428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.475719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.475861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.477264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.478393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.479170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.480026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.480984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.481349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.483820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.484293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.485486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.486249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.486785: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.496523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.514970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.516979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.517741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.519482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.519631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.519733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.523192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.523601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.524744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.525294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.525461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.529005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.530227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.531568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.531759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.535093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.535324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.536591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.536867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.542821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.543087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.544202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.544414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.548342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.549029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.550207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.551566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.553681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.554242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.555546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.556673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.559204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.560784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.561288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.562568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.565496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.566714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.567223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.567424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.571768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.572781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.573961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.573978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.578768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.585624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.587429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.587517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.591633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.592143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.593221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.593381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.597381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.597895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.599676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.599848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.631342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.631744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.633186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.637056: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.637261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.638943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.640247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.646938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.673749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.677643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.678031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.679526: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.683857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.684448: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.686934: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:28:42.688948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.694386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.696345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.697023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.703164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.705515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.706073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.711576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:42.713081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.821601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.823283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.825276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.826992: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:43.827053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:28:43.846062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.846952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.847889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.848513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.849161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.849861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:28:43.861741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.862681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.863473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.864023: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:43.864079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:28:43.884213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.885565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.886906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.888341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.889671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.891207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:28:43.897270: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:43.897498: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:43.931763: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:28:43.952783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.953407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.953932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.954567: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:43.954628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:28:43.973733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.974384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.974899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.975505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.976021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.976496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:28:43.977859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:43.978030: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:43.981783: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:28:43.982410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.982997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.983641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:43.984478: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:43.984528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:28:44.002776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.004210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.005302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.007932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.009425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.010492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:28:44.051261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.051480: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.052401: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:28:44.055703: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.055863: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.057556: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:28:44.079230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.079851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.080395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.080865: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:44.080919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:28:44.088409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.089001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.089525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.089909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.090105: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:44.090157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:28:44.090835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.091405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.091871: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:44.091912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:28:44.098941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.100928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.101178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.102717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.102802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.104276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.104496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.105712: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:28:44.105771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:28:44.105976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.107098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:28:44.107643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.108870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.109850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.109958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.111584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.111710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.113082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.113239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.114813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:28:44.114879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.116021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.117059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:28:44.123518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.125964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.127082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.128239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.129333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:28:44.130400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:28:44.153241: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.153458: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.155372: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:28:44.160064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.160230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.161560: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.161750: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.162013: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:28:44.162711: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:28:44.174815: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.175000: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:28:44.176729: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][23:28:45.442][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.442][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.442][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.443][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.443][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.443][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.443][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:28:45.443][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 98it [00:01, 81.68it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 196it [00:01, 177.52it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 99it [00:01, 85.31it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 280it [00:01, 264.85it/s]warmup run: 95it [00:01, 79.53it/s]warmup run: 100it [00:01, 87.13it/s]warmup run: 199it [00:01, 185.67it/s]warmup run: 91it [00:01, 76.37it/s]warmup run: 101it [00:01, 87.24it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 101it [00:01, 84.91it/s]warmup run: 359it [00:01, 344.53it/s]warmup run: 190it [00:01, 172.69it/s]warmup run: 200it [00:01, 188.45it/s]warmup run: 296it [00:01, 291.96it/s]warmup run: 182it [00:01, 165.82it/s]warmup run: 99it [00:01, 86.04it/s]warmup run: 201it [00:01, 187.64it/s]warmup run: 199it [00:01, 181.07it/s]warmup run: 455it [00:02, 457.02it/s]warmup run: 286it [00:01, 276.77it/s]warmup run: 301it [00:01, 300.56it/s]warmup run: 389it [00:01, 394.45it/s]warmup run: 277it [00:01, 269.84it/s]warmup run: 198it [00:01, 186.12it/s]warmup run: 300it [00:01, 296.56it/s]warmup run: 298it [00:01, 288.68it/s]warmup run: 552it [00:02, 562.39it/s]warmup run: 387it [00:01, 392.89it/s]warmup run: 400it [00:01, 413.03it/s]warmup run: 482it [00:02, 494.34it/s]warmup run: 370it [00:01, 374.85it/s]warmup run: 297it [00:01, 295.61it/s]warmup run: 400it [00:01, 410.64it/s]warmup run: 397it [00:01, 400.64it/s]warmup run: 649it [00:02, 654.59it/s]warmup run: 489it [00:02, 508.25it/s]warmup run: 499it [00:01, 522.03it/s]warmup run: 574it [00:02, 584.25it/s]warmup run: 464it [00:02, 479.29it/s]warmup run: 500it [00:02, 521.39it/s]warmup run: 396it [00:01, 408.58it/s]warmup run: 497it [00:02, 511.57it/s]warmup run: 745it [00:02, 729.12it/s]warmup run: 590it [00:02, 612.78it/s]warmup run: 599it [00:02, 622.57it/s]warmup run: 671it [00:02, 672.11it/s]warmup run: 559it [00:02, 577.15it/s]warmup run: 601it [00:02, 624.52it/s]warmup run: 493it [00:01, 514.14it/s]warmup run: 597it [00:02, 613.63it/s]warmup run: 842it [00:02, 790.81it/s]warmup run: 690it [00:02, 701.34it/s]warmup run: 699it [00:02, 709.01it/s]warmup run: 768it [00:02, 745.93it/s]warmup run: 655it [00:02, 663.94it/s]warmup run: 702it [00:02, 713.89it/s]warmup run: 592it [00:02, 614.32it/s]warmup run: 698it [00:02, 703.43it/s]warmup run: 940it [00:02, 840.21it/s]warmup run: 793it [00:02, 781.40it/s]warmup run: 799it [00:02, 779.67it/s]warmup run: 865it [00:02, 803.04it/s]warmup run: 801it [00:02, 782.55it/s]warmup run: 750it [00:02, 733.56it/s]warmup run: 692it [00:02, 703.36it/s]warmup run: 795it [00:02, 768.50it/s]warmup run: 1042it [00:02, 888.55it/s]warmup run: 894it [00:02, 839.63it/s]warmup run: 899it [00:02, 836.89it/s]warmup run: 961it [00:02, 844.98it/s]warmup run: 901it [00:02, 837.91it/s]warmup run: 845it [00:02, 788.32it/s]warmup run: 793it [00:02, 778.91it/s]warmup run: 892it [00:02, 807.88it/s]warmup run: 1146it [00:02, 931.44it/s]warmup run: 996it [00:02, 887.08it/s]warmup run: 1000it [00:02, 882.09it/s]warmup run: 1057it [00:02, 875.63it/s]warmup run: 941it [00:02, 833.70it/s]warmup run: 893it [00:02, 836.84it/s]warmup run: 1000it [00:02, 877.03it/s]warmup run: 990it [00:02, 852.02it/s]warmup run: 1249it [00:02, 957.65it/s]warmup run: 1096it [00:02, 914.64it/s]warmup run: 1101it [00:02, 917.24it/s]warmup run: 1153it [00:02, 898.55it/s]warmup run: 1099it [00:02, 908.21it/s]warmup run: 994it [00:02, 883.49it/s]warmup run: 1037it [00:02, 866.82it/s]warmup run: 1086it [00:02, 878.78it/s]warmup run: 1350it [00:02, 970.62it/s]warmup run: 1196it [00:02, 918.00it/s]warmup run: 1202it [00:02, 942.43it/s]warmup run: 1249it [00:02, 914.81it/s]warmup run: 1093it [00:02, 911.96it/s]warmup run: 1198it [00:02, 929.26it/s]warmup run: 1132it [00:02, 866.49it/s]warmup run: 1182it [00:02, 900.39it/s]warmup run: 1450it [00:03, 972.12it/s]warmup run: 1302it [00:02, 958.43it/s]warmup run: 1347it [00:02, 932.28it/s]warmup run: 1294it [00:02, 917.50it/s]warmup run: 1195it [00:02, 941.57it/s]warmup run: 1297it [00:02, 946.28it/s]warmup run: 1224it [00:02, 863.80it/s]warmup run: 1279it [00:02, 917.74it/s]warmup run: 1550it [00:03, 972.63it/s]warmup run: 1403it [00:02, 971.93it/s]warmup run: 1443it [00:03, 938.35it/s]warmup run: 1390it [00:02, 919.64it/s]warmup run: 1295it [00:02, 957.06it/s]warmup run: 1399it [00:02, 965.75it/s]warmup run: 1324it [00:02, 902.11it/s]warmup run: 1378it [00:02, 937.13it/s]warmup run: 1650it [00:03, 979.14it/s]warmup run: 1503it [00:02, 979.66it/s]warmup run: 1539it [00:03, 933.21it/s]warmup run: 1485it [00:03, 920.20it/s]warmup run: 1499it [00:03, 975.49it/s]warmup run: 1395it [00:02, 962.61it/s]warmup run: 1426it [00:03, 934.60it/s]warmup run: 1477it [00:03, 952.45it/s]warmup run: 1751it [00:03, 986.11it/s]warmup run: 1604it [00:03, 985.90it/s]warmup run: 1579it [00:03, 921.13it/s]warmup run: 1634it [00:03, 921.70it/s]warmup run: 1600it [00:03, 985.22it/s]warmup run: 1494it [00:03, 967.02it/s]warmup run: 1527it [00:03, 954.38it/s]warmup run: 1576it [00:03, 963.33it/s]warmup run: 1853it [00:03, 995.91it/s]warmup run: 1705it [00:03, 991.65it/s]warmup run: 1674it [00:03, 928.04it/s]warmup run: 1728it [00:03, 907.98it/s]warmup run: 1702it [00:03, 993.97it/s]warmup run: 1593it [00:03, 964.31it/s]warmup run: 1674it [00:03, 968.07it/s]warmup run: 1624it [00:03, 932.06it/s]warmup run: 1954it [00:03, 999.81it/s]warmup run: 1806it [00:03, 992.97it/s]warmup run: 1768it [00:03, 930.70it/s]warmup run: 1820it [00:03, 898.17it/s]warmup run: 1803it [00:03, 997.46it/s]warmup run: 1691it [00:03, 961.33it/s]warmup run: 1773it [00:03, 971.52it/s]warmup run: 1720it [00:03, 938.77it/s]warmup run: 2062it [00:03, 1022.36it/s]warmup run: 1908it [00:03, 999.89it/s]warmup run: 1863it [00:03, 935.26it/s]warmup run: 1911it [00:03, 893.51it/s]warmup run: 1904it [00:03, 990.00it/s]warmup run: 1793it [00:03, 977.67it/s]warmup run: 1872it [00:03, 975.78it/s]warmup run: 1816it [00:03, 943.63it/s]warmup run: 2180it [00:03, 1068.02it/s]warmup run: 2012it [00:03, 1011.23it/s]warmup run: 1958it [00:03, 938.21it/s]warmup run: 2001it [00:03, 890.53it/s]warmup run: 2004it [00:03, 987.22it/s]warmup run: 1895it [00:03, 988.82it/s]warmup run: 1971it [00:03, 978.22it/s]warmup run: 1911it [00:03, 942.93it/s]warmup run: 2300it [00:03, 1106.51it/s]warmup run: 2133it [00:03, 1068.84it/s]warmup run: 2066it [00:03, 980.19it/s]warmup run: 2113it [00:03, 957.61it/s]warmup run: 2118it [00:03, 1032.33it/s]warmup run: 1997it [00:03, 995.66it/s]warmup run: 2082it [00:03, 1014.99it/s]warmup run: 2008it [00:03, 949.03it/s]warmup run: 2420it [00:03, 1134.37it/s]warmup run: 2254it [00:03, 1109.76it/s]warmup run: 2187it [00:03, 1046.97it/s]warmup run: 2226it [00:03, 1008.02it/s]warmup run: 2239it [00:03, 1082.99it/s]warmup run: 2115it [00:03, 1049.53it/s]warmup run: 2201it [00:03, 1065.81it/s]warmup run: 2121it [00:03, 1000.41it/s]warmup run: 2541it [00:04, 1155.02it/s]warmup run: 2375it [00:03, 1139.53it/s]warmup run: 2308it [00:03, 1094.16it/s]warmup run: 2339it [00:03, 1043.95it/s]warmup run: 2360it [00:03, 1119.16it/s]warmup run: 2234it [00:03, 1091.22it/s]warmup run: 2320it [00:03, 1101.22it/s]warmup run: 2239it [00:03, 1052.40it/s]warmup run: 2661it [00:04, 1168.11it/s]warmup run: 2496it [00:03, 1159.90it/s]warmup run: 2429it [00:03, 1127.81it/s]warmup run: 2452it [00:04, 1067.40it/s]warmup run: 2482it [00:03, 1148.24it/s]warmup run: 2354it [00:03, 1122.25it/s]warmup run: 2437it [00:03, 1119.18it/s]warmup run: 2353it [00:03, 1076.72it/s]warmup run: 2780it [00:04, 1172.88it/s]warmup run: 2617it [00:04, 1173.96it/s]warmup run: 2550it [00:04, 1151.99it/s]warmup run: 2565it [00:04, 1084.66it/s]warmup run: 2604it [00:04, 1166.85it/s]warmup run: 2473it [00:03, 1142.36it/s]warmup run: 2553it [00:04, 1130.37it/s]warmup run: 2473it [00:04, 1110.96it/s]warmup run: 2900it [00:04, 1180.95it/s]warmup run: 2738it [00:04, 1184.37it/s]warmup run: 2671it [00:04, 1168.43it/s]warmup run: 2678it [00:04, 1097.99it/s]warmup run: 2724it [00:04, 1176.04it/s]warmup run: 2592it [00:04, 1155.18it/s]warmup run: 2667it [00:04, 1132.37it/s]warmup run: 2592it [00:04, 1134.42it/s]warmup run: 3000it [00:04, 669.98it/s] warmup run: 2857it [00:04, 1183.70it/s]warmup run: 2791it [00:04, 1175.09it/s]warmup run: 2789it [00:04, 1100.14it/s]warmup run: 2844it [00:04, 1183.00it/s]warmup run: 2711it [00:04, 1164.04it/s]warmup run: 2785it [00:04, 1145.67it/s]warmup run: 2712it [00:04, 1152.89it/s]warmup run: 2978it [00:04, 1191.17it/s]warmup run: 2913it [00:04, 1186.76it/s]warmup run: 3000it [00:04, 693.17it/s] warmup run: 2902it [00:04, 1106.30it/s]warmup run: 2967it [00:04, 1194.82it/s]warmup run: 2829it [00:04, 1167.29it/s]warmup run: 2905it [00:04, 1160.74it/s]warmup run: 2831it [00:04, 1162.97it/s]warmup run: 3000it [00:04, 688.84it/s] warmup run: 3000it [00:04, 670.64it/s] warmup run: 3000it [00:04, 661.12it/s] warmup run: 2949it [00:04, 1174.93it/s]warmup run: 3000it [00:04, 673.15it/s] warmup run: 2950it [00:04, 1170.70it/s]warmup run: 3000it [00:04, 687.50it/s] warmup run: 3000it [00:04, 660.31it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.58it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.71it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1597.93it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.43it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.21it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.10it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.39it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.61it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1596.50it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.77it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1601.77it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.71it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.05it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.29it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1629.29it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1636.01it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1637.37it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1664.65it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1642.31it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1600.20it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1591.97it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1624.58it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1649.74it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1633.19it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1671.91it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1637.71it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1640.80it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1650.65it/s]warmup should be done:  21%|██▏       | 640/3000 [00:00<00:01, 1588.37it/s]warmup should be done:  21%|██▏       | 643/3000 [00:00<00:01, 1596.04it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1623.52it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1646.15it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1674.40it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1644.62it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1640.52it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1649.75it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1585.68it/s]warmup should be done:  27%|██▋       | 803/3000 [00:00<00:01, 1594.84it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1653.34it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1616.33it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1673.44it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1649.95it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1647.13it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1639.09it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1592.56it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1580.99it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1657.23it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1611.13it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1650.41it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1669.08it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1641.27it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1656.94it/s]warmup should be done:  37%|███▋      | 1117/3000 [00:00<00:01, 1575.02it/s]warmup should be done:  37%|███▋      | 1123/3000 [00:00<00:01, 1576.71it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1617.38it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1604.34it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1650.88it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1664.98it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1639.78it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1657.84it/s]warmup should be done:  42%|████▎     | 1275/3000 [00:00<00:01, 1574.89it/s]warmup should be done:  43%|████▎     | 1282/3000 [00:00<00:01, 1578.82it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1601.93it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1611.21it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1648.37it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1639.55it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1656.30it/s]warmup should be done:  48%|████▊     | 1433/3000 [00:00<00:00, 1574.94it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1652.38it/s]warmup should be done:  48%|████▊     | 1441/3000 [00:00<00:00, 1579.95it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1617.31it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1600.64it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1650.60it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1640.40it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1653.76it/s]warmup should be done:  53%|█████▎    | 1593/3000 [00:01<00:00, 1580.57it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1650.81it/s]warmup should be done:  53%|█████▎    | 1600/3000 [00:01<00:00, 1580.44it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1619.38it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1576.83it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1639.79it/s]warmup should be done:  61%|██████▏   | 1840/3000 [00:01<00:00, 1645.48it/s]warmup should be done:  59%|█████▊    | 1759/3000 [00:01<00:00, 1582.44it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1628.89it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1620.10it/s]warmup should be done:  58%|█████▊    | 1752/3000 [00:01<00:00, 1565.84it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1621.15it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1558.13it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1638.27it/s]warmup should be done:  64%|██████▍   | 1918/3000 [00:01<00:00, 1581.92it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1629.89it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1624.30it/s]warmup should be done:  64%|██████▎   | 1910/3000 [00:01<00:00, 1568.42it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1631.83it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1610.02it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1569.84it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1638.14it/s]warmup should be done:  69%|██████▉   | 2079/3000 [00:01<00:00, 1587.37it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1629.82it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1627.58it/s]warmup should be done:  69%|██████▉   | 2068/3000 [00:01<00:00, 1570.27it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1636.95it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1611.33it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1578.76it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1637.59it/s]warmup should be done:  75%|███████▍  | 2240/3000 [00:01<00:00, 1593.56it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1629.58it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1629.22it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1571.73it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1643.99it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1612.59it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1585.53it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1632.30it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1599.89it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1628.63it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1625.28it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1570.31it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1650.04it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1604.64it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1587.28it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1626.95it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1607.25it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1629.61it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1625.77it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1571.39it/s]warmup should be done:  89%|████████▉ | 2672/3000 [00:01<00:00, 1657.06it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1609.35it/s]warmup should be done:  86%|████████▌ | 2587/3000 [00:01<00:00, 1591.12it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1622.11it/s]warmup should be done:  91%|█████████ | 2726/3000 [00:01<00:00, 1607.01it/s]warmup should be done:  93%|█████████▎| 2787/3000 [00:01<00:00, 1628.73it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1626.65it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1574.35it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1653.37it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1610.86it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1592.83it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1611.61it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1624.79it/s]warmup should be done:  98%|█████████▊| 2952/3000 [00:01<00:00, 1633.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.75it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1634.56it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1581.50it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1617.24it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1598.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1636.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1577.25it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.25it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.90it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.39it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.29it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.05it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.75it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1582.99it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.43it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.90it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1649.13it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.60it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.14it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.58it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1642.47it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1692.83it/s]warmup should be done:  11%|█         | 318/3000 [00:00<00:01, 1573.28it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1656.91it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1699.09it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1679.34it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.19it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1644.69it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.34it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1698.57it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1597.56it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1710.55it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1664.67it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1653.21it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1678.67it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1648.99it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1688.54it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1705.08it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1611.85it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1669.43it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1715.86it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1681.76it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1692.06it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1650.76it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1656.46it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1708.75it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1617.03it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1668.45it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1718.17it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1680.92it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1710.26it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1689.30it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1648.99it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1639.79it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1602.37it/s]warmup should be done:  40%|████      | 1200/3000 [00:00<00:01, 1716.03it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1664.59it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1686.46it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1649.88it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1678.53it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1705.46it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1582.03it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1609.59it/s]warmup should be done:  46%|████▌     | 1373/3000 [00:00<00:00, 1719.08it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:00, 1667.19it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1680.83it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1687.93it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1646.66it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1701.84it/s]warmup should be done:  43%|████▎     | 1290/3000 [00:00<00:01, 1562.64it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1588.81it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1667.69it/s]warmup should be done:  52%|█████▏    | 1545/3000 [00:00<00:00, 1714.13it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1681.36it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1685.61it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1648.15it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1692.86it/s]warmup should be done:  48%|████▊     | 1448/3000 [00:00<00:00, 1566.97it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1593.03it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1675.07it/s]warmup should be done:  57%|█████▋    | 1717/3000 [00:01<00:00, 1715.43it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1682.76it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1650.53it/s]warmup should be done:  56%|█████▋    | 1693/3000 [00:01<00:00, 1687.34it/s]warmup should be done:  57%|█████▋    | 1711/3000 [00:01<00:00, 1686.34it/s]warmup should be done:  54%|█████▎    | 1607/3000 [00:01<00:00, 1570.60it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1602.17it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1681.06it/s]warmup should be done:  63%|██████▎   | 1890/3000 [00:01<00:00, 1717.83it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1684.77it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1687.88it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1664.24it/s]warmup should be done:  63%|██████▎   | 1880/3000 [00:01<00:00, 1679.81it/s]warmup should be done:  59%|█████▉    | 1765/3000 [00:01<00:00, 1559.54it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1583.31it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1683.53it/s]warmup should be done:  69%|██████▊   | 2062/3000 [00:01<00:00, 1718.35it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1684.76it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1686.34it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1676.19it/s]warmup should be done:  68%|██████▊   | 2048/3000 [00:01<00:00, 1676.83it/s]warmup should be done:  64%|██████▍   | 1921/3000 [00:01<00:00, 1550.50it/s]warmup should be done:  66%|██████▌   | 1966/3000 [00:01<00:00, 1572.97it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1684.93it/s]warmup should be done:  74%|███████▍  | 2234/3000 [00:01<00:00, 1717.68it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1683.32it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1684.82it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1682.59it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1673.80it/s]warmup should be done:  69%|██████▉   | 2079/3000 [00:01<00:00, 1557.88it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1573.84it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1688.12it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1717.95it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1683.52it/s]warmup should be done:  78%|███████▊  | 2340/3000 [00:01<00:00, 1693.20it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1679.42it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1671.24it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1552.31it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1574.18it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1689.98it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1719.39it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1683.33it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1696.91it/s]warmup should be done:  85%|████████▍ | 2538/3000 [00:01<00:00, 1681.13it/s]warmup should be done:  85%|████████▌ | 2552/3000 [00:01<00:00, 1670.14it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1557.20it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1579.28it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1719.53it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1690.27it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1683.04it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1701.90it/s]warmup should be done:  90%|█████████ | 2707/3000 [00:01<00:00, 1679.11it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1670.11it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1550.93it/s]warmup should be done:  87%|████████▋ | 2600/3000 [00:01<00:00, 1568.23it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1689.59it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1708.35it/s]warmup should be done:  96%|█████████▌| 2870/3000 [00:01<00:00, 1682.04it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1704.96it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1680.30it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1668.76it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1553.50it/s]warmup should be done:  92%|█████████▏| 2758/3000 [00:01<00:00, 1569.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1711.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.60it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1552.11it/s]warmup should be done:  97%|█████████▋| 2917/3000 [00:01<00:00, 1573.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1570.62it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a683740d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a68365190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a686a8730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a683731c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a686a7e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a686aad30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a683740d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f6a683672b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 23:30:16.453648: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6592834440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:16.453709: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:16.453795: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6592830650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:16.453853: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:16.462622: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:16.463481: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:16.650133: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6592834410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:16.650190: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:16.660567: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:16.979077: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f659a8342b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:16.979148: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:16.987303: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:17.316553: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f659302dbd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:17.316626: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:17.326130: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:17.338750: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f659a82c450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:17.338811: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:17.349135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:17.378757: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f658e833bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:17.378825: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:17.379228: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f659a7992c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:30:17.379286: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:30:17.388132: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:17.388860: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:30:23.635595: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:23.659309: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:23.823535: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:23.992748: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:24.047291: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:24.280110: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:24.353319: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:30:24.458340: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:31:23.726][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:31:23.726][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:23.735][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:23.735][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][23:31:23.860][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:31:23.860][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:23.868][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:23.869][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][23:31:23.936][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:31:23.936][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:23.945][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:23.946][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][23:31:24.008][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:31:24.008][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:24.010][ERROR][RK0][tid #140074561816320]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:31:24.010][ERROR][RK0][tid #140074561816320]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:24.016][ERROR][RK0][tid #140074561816320]: coll ps creation done
[HCTR][23:31:24.016][ERROR][RK0][tid #140074561816320]: replica 3 waits for coll ps creation barrier
[HCTR][23:31:24.016][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:24.016][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:31:24.019][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:31:24.019][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:24.026][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:24.026][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][23:31:24.067][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:31:24.068][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:24.075][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:24.075][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:31:24.111][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:31:24.111][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][23:31:24.118][ERROR][RK0][main]: coll ps creation done
[HCTR][23:31:24.118][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][23:31:24.118][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][23:31:25.028][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:31:25.060][ERROR][RK0][tid #140074561816320]: replica 3 calling init per replica
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:31:25.060][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][tid #140074561816320]: Calling build_v2
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.061][ERROR][RK0][main]: Calling build_v2
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][tid #140074561816320]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:31:25.061][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 23:31:252022-12-11 23:31:252022-12-11 23:31:252022-12-11 23:31:25.2022-12-11 23:31:25.2022-12-11 23:31:25.2022-12-11 23:31:25. 61197. 61208. 61208.[ 61209:  61207:  61208:  61214: E: E: E: E E2022-12-11 23:31:25 E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 61260:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:: 136:136:136] 136E] 136] 136] using concurrent impl MPS]  using concurrent impl MPS] using concurrent impl MPS] using concurrent impl MPS
using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc
using concurrent impl MPS
using concurrent impl MPS

:

136] using concurrent impl MPS
[2022-12-11 23:31:25. 65485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:31:25. 65524: E[ 2022-12-11 23:31:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 65527196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 23:31:252022-12-11 23:31:25.. 65577 65574: : E[E 2022-12-11 23:31:25 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 65598:196: 178] [E] assigning 8 to cpu2022-12-11 23:31:25 v100x8, slow pcie
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 65618:: 212E[]  2022-12-11 23:31:25build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
: 65656[[178: 2022-12-11 23:31:252022-12-11 23:31:25[] E..2022-12-11 23:31:25v100x8, slow pcie  65671 65684.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :  65691:2022-12-11 23:31:25[EE: 196.2022-12-11 23:31:25  E]  65721[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc assigning 8 to cpu: 2022-12-11 23:31:25 65741::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
E.: 178212:2022-12-11 23:31:25  65768E] ] 213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  v100x8, slow pcie[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]  65815:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 23:31:25
remote time is 8.68421[: 178 :.
2022-12-11 23:31:25E] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196 65888. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[v100x8, slow pcie2022-12-11 23:31:25:] :  65943:2022-12-11 23:31:25
.178assigning 8 to cpuE: 178. 65955] 
[ E]  65972: v100x8, slow pcie2022-12-11 23:31:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc v100x8, slow pcie: E
.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E [ 66028212[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:31:25: [] 2022-12-11 23:31:25196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.E2022-12-11 23:31:25build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] :213 66081 .
 66090assigning 8 to cpu214] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 66103: 
] [remote time is 8.68421E:: Ecpu time is 97.05882022-12-11 23:31:25
 196E 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 66197:assigning 8 to cpu2022-12-11 23:31:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 23:31:25: 212
.:196.E]  66250196]  66255 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] assigning 8 to cpu: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
Eassigning 8 to cpu
E2022-12-11 23:31:25: 
[ .213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:31:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 66344] :[.:: remote time is 8.68421214[2022-12-11 23:31:25 66380212E
] 2022-12-11 23:31:25.: ]  cpu time is 97.0588[. 66415Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 23:31:25 66428:  
:.: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212 66474E :[] :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-11 23:31:25build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] .
 :212remote time is 8.68421 66549/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212] [
: :[] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 23:31:25E2142022-12-11 23:31:25build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
. ] .
 66628/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[ 66660: :
2022-12-11 23:31:25[: E213.2022-12-11 23:31:25E ]  66706. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:  66725/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
E: :213 E[214] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 23:31:25] remote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.cpu time is 97.0588
213: 66819
] 213[: remote time is 8.68421] 2022-12-11 23:31:25E
remote time is 8.68421. [
 66876/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:31:25: :[.E2142022-12-11 23:31:25 66925 ] .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588 66955E:
:  214E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  :cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214
:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 23:32:42.109791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:32:42.149880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 23:32:42.149951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 2999999
[2022-12-11 23:32:42.259539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:32:42.259629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:32:42.373798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:32:42.373838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:32:42.374297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.375264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.376113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.389012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:32:42.389069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 23:32:42.389256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 23:32:42.389311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:32:42.389446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.389685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.390977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.391090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.391625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[[2022-12-11 23:32:422022-12-11 23:32:42..391680391656: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205202] ] worker 0 thread 6 initing device 65 solved

[2022-12-11 23:32:42.391796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 23:32:42.392144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.392212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.392885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.393417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 23:32:42.393474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-11 23:32:42worker 0 thread 2 initing device 2.
393487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.393873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.395405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.395617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.396937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.397692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.397760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.398964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 23:32:422022-12-11 23:32:42..399220399220: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved4 solved

[[2022-12-11 23:32:422022-12-11 23:32:42..399307399307: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 4 initing device 4worker 0 thread 1 initing device 1

[[2022-12-11 23:32:422022-12-11 23:32:42..399815399815: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 23:32:42.401634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.401702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.403268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.403412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:32:42.452355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:32:42.452730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:32:42.457841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.457928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.457975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.458761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.459496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.460478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.460570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.461244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.461285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[2022-12-11 23:32:422022-12-11 23:32:42..472972472972: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[[2022-12-11 23:32:422022-12-11 23:32:42..473365473366: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-11 23:32:42.476262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:32:42.476587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:32:42.478675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.478740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-11 23:32:422022-12-11 23:32:42..478765478782: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-11 23:32:42.478854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.478899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.479637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.480495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.481346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.481520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.482306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.482393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.482465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.482547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.482747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.482811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.482851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.483062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.483103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:32:42.483213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.483254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:32:42.483613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.484410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:32:42.484750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:32:42.487509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.488476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.488559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.489073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 23:32:42.489230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.489283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:32:42.489382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:32:42.489625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.489693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.489734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.491760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.492394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.493357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.493442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.494112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.494151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:32:42.494209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.494272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.494312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.496631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.497057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.498010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.498092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.498765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.498802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[2022-12-11 23:32:422022-12-11 23:32:42..512201512201: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-11 23:32:42.512553[: 2022-12-11 23:32:42E. 512561/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes:
1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:32:42.517736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.517798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.517838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.517877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:32:42.517941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 23:32:42.517982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:32:42.518830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.519605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:32:42.520200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.520437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.521142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.521226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.521386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.521469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:32:42.521896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.521934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:32:42.522139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:32:42.522179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[2022-12-11 23:32:42[[2022-12-11 23:32:42[2022-12-11 23:32:42[.[2022-12-11 23:32:422022-12-11 23:32:42.2022-12-11 23:32:42.2022-12-11 23:32:427785502022-12-11 23:32:42..778561.778565.: .778562778566: 778578: 778583E778589: : E: E:  : EE E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:1980:] :19801980] 1980] 1980eager alloc mem 611.00 KB1980] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB




[[[2022-12-11 23:32:422022-12-11 23:32:42[[[2022-12-11 23:32:42..[2022-12-11 23:32:422022-12-11 23:32:42[2022-12-11 23:32:42.7797567797562022-12-11 23:32:42..2022-12-11 23:32:42.779764: : .779765779765.779767: EE779777: : 779782: E  : EE: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] ] :638638:638] eager release cuda mem 625663eager release cuda mem 625663638] ] 638] eager release cuda mem 625663

] eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663
eager release cuda mem 625663

eager release cuda mem 625663


[[2022-12-11 23:32:422022-12-11 23:32:42..780056780060: [: E[[2022-12-11 23:32:42E[ [2022-12-11 23:32:422022-12-11 23:32:42[. 2022-12-11 23:32:42/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:32:42..2022-12-11 23:32:42780073/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:.780081780081.: :7800861980780091: : 780095E1980: ] : EE:  ] Eeager alloc mem 611.00 KBE  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::19801980:] 19801980] ] 1980eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

eager alloc mem 611.00 KB


[2022-12-11 23:32:42.780989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.781048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:32:42eager release cuda mem 625663.
781066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:32:42eager alloc mem 611.00 KB[[.
[2022-12-11 23:32:42[[2022-12-11 23:32:427810902022-12-11 23:32:42.2022-12-11 23:32:422022-12-11 23:32:42.: .[781100..781103E7811082022-12-11 23:32:42: 781126781112:  : .E: : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE781142 EE : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] : 638::638eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 1980638] 
] :eager release cuda mem 625663] ] eager release cuda mem 625663eager release cuda mem 625663638
eager alloc mem 611.00 KBeager release cuda mem 625663

] 

[eager release cuda mem 6256632022-12-11 23:32:42
.781383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:32:42eager alloc mem 611.00 KB.[
781417[2022-12-11 23:32:42: 2022-12-11 23:32:42[.E.2022-12-11 23:32:42[781425 781428.2022-12-11 23:32:42: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 781437.E:E: 781450 1980 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E:eager alloc mem 611.00 KB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980
1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] 1980:eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 1980

eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 23:32:42.781848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.781914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.782144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 23:32:42] .eager release cuda mem 625663782159
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.782234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:32:421980.] 782249eager alloc mem 611.00 KB: [
E2022-12-11 23:32:42 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu782276:: 1980E]  [eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 23:32:42
:[2022-12-11 23:32:42.6382022-12-11 23:32:42.782311] .782315: eager release cuda mem 625663782322: E
: E [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:32:42 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638782359:638[] : 638] 2022-12-11 23:32:42eager release cuda mem 625663E] eager release cuda mem 625663.
 eager release cuda mem 625663
782403/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663[:
2022-12-11 23:32:421980[.[] 2022-12-11 23:32:427824922022-12-11 23:32:42eager alloc mem 611.00 KB.: .
782500E782511:  : E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 2022-12-11 23:32:42: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:782546] :1980: eager alloc mem 611.00 KB1980] E
] eager alloc mem 611.00 KB eager alloc mem 611.00 KB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.782663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.782730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:32:422022-12-11 23:32:42..783048783049: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-11 23:32:422022-12-11 23:32:42..783191783192: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] [eager alloc mem 611.00 KBeager alloc mem 611.00 KB2022-12-11 23:32:42

.783265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:32:422022-12-11 23:32:42..783354783357[: : 2022-12-11 23:32:42E[E. 2022-12-11 23:32:42 783369/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :783378:E638: 1980 [] E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:32:42eager release cuda mem 625663 eager alloc mem 611.00 KB:.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
638783413:] : 638[eager release cuda mem 625663E] 2022-12-11 23:32:42
 [eager release cuda mem 625663./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:32:42
783477:.: 638783503E] [:  eager release cuda mem 6256632022-12-11 23:32:42E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
[. :2022-12-11 23:32:42783557/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638.: :] 783582E1980eager release cuda mem 625663:  [] 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:32:42eager alloc mem 611.00 KB :.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980783636:] : 1980[eager alloc mem 611.00 KBE] 2022-12-11 23:32:42
 eager alloc mem 611.00 KB./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
783707:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.784014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.784046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.784081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.784116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.784190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.784259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.784417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.784468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[2022-12-11 23:32:426382022-12-11 23:32:42.] .784484eager release cuda mem 625663784485: 
: E[E 2022-12-11 23:32:42 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-11 23:32:42784512[:1980.: 2022-12-11 23:32:42638] 784533E.] eager alloc mem 611.00 KB:  784555eager release cuda mem 625663
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 
 :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638eager release cuda mem 625663:] 
1980eager release cuda mem 625663[] 
2022-12-11 23:32:42eager alloc mem 611.00 KB.
784676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 23:32:42.784721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:32:421980.] 784739eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.784828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:32:422022-12-11 23:32:42..784893784893: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 23:32:42.785009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.785036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.785077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:32:42.785347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.785414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:32:42eager alloc mem 611.00 KB.
785429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:32:42eager release cuda mem 625663.
785452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.785489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:32:42eager release cuda mem 625663.
785508[[: 2022-12-11 23:32:422022-12-11 23:32:42[E..2022-12-11 23:32:42 785526785527./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : 785540:EE: 1980  E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager alloc mem 611.00 KB::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
6381980:] ] 638eager release cuda mem 625663eager alloc mem 611.00 KB] 

eager release cuda mem 12399996[
2022-12-11 23:32:42.785708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:32:422022-12-11 23:32:42..785747785750: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] [] eager alloc mem 611.00 KB2022-12-11 23:32:42eager release cuda mem 12399996
[.
2022-12-11 23:32:42785811.: 785826E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 23:32:42[.2022-12-11 23:32:42785934.: 785939E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 12399996638
] eager release cuda mem 12399996
[2022-12-11 23:32:42.786170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.786206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:32:42.786391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42[.2022-12-11 23:32:42786432.: 786438E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 12399996
[2022-12-11 23:32:42.786492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:32:42.786588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:32:42.786627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:32:42.789428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.397227 secs 
[2022-12-11 23:32:42.789847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.400165 secs 
[2022-12-11 23:32:42.790264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.390459 secs 
[2022-12-11 23:32:42.790678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.401239 secs 
[2022-12-11 23:32:42.791152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.399014 secs 
[2022-12-11 23:32:42.791560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.397692 secs 
[2022-12-11 23:32:42.791969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.417679 secs 
[2022-12-11 23:32:42.792550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.392745 secs 
[2022-12-11 23:32:42.796789: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.75 GB
[2022-12-11 23:32:44.482421: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.01 GB
[2022-12-11 23:32:44.484054: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.01 GB
[2022-12-11 23:32:44.484943: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.01 GB
[2022-12-11 23:32:45.910699: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.28 GB
[2022-12-11 23:32:45.911118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.28 GB
[2022-12-11 23:32:45.912450: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.28 GB
[2022-12-11 23:32:47.174551: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.49 GB
[2022-12-11 23:32:47.174678: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.49 GB
[2022-12-11 23:32:47.174982: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.49 GB
[2022-12-11 23:32:48.481139: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.71 GB
[2022-12-11 23:32:48.481334: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.71 GB
[2022-12-11 23:32:48.481647: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.71 GB
[2022-12-11 23:32:49.559440: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.16 GB
[2022-12-11 23:32:49.560663: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.16 GB
[2022-12-11 23:32:49.562210: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.16 GB
[2022-12-11 23:32:50.968632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.36 GB
[2022-12-11 23:32:50.969233: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.36 GB
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][tid #140074561816320]: replica 3 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][23:32:52.348][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][tid #140074561816320]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:32:52.349][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.349][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.349][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.349][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.349][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.349][ERROR][RK0][tid #140074561816320]: init per replica done
[HCTR][23:32:52.352][ERROR][RK0][main]: init per replica done
[HCTR][23:32:52.387][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f49f8238400
[HCTR][23:32:52.387][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f49f8558400
[HCTR][23:32:52.387][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f49f8b98400
[HCTR][23:32:52.387][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f49f8eb8400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074763142912]: 2 allocated 3276800 at 0x7f4a0a238400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074763142912]: 2 allocated 6553600 at 0x7f4a0a558400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074628925184]: 7 allocated 3276800 at 0x7f4920238400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074763142912]: 2 allocated 3276800 at 0x7f4a0ab98400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074628925184]: 7 allocated 6553600 at 0x7f4920558400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074763142912]: 2 allocated 6553600 at 0x7f4a0aeb8400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074628925184]: 7 allocated 3276800 at 0x7f4920b98400
[HCTR][23:32:52.387][ERROR][RK0][tid #140074628925184]: 7 allocated 6553600 at 0x7f4920eb8400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074704426752]: 6 allocated 3276800 at 0x7f4938238400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074561816320]: 3 allocated 3276800 at 0x7f4974238400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074704426752]: 6 allocated 6553600 at 0x7f4938558400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074561816320]: 3 allocated 6553600 at 0x7f4974558400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074704426752]: 6 allocated 3276800 at 0x7f4938b98400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074561816320]: 3 allocated 3276800 at 0x7f4974b98400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074704426752]: 6 allocated 6553600 at 0x7f4938eb8400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074561816320]: 3 allocated 6553600 at 0x7f4974eb8400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074972862208]: 4 allocated 3276800 at 0x7f49ac238400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074972862208]: 4 allocated 6553600 at 0x7f49ac558400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074972862208]: 4 allocated 3276800 at 0x7f49acb98400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074972862208]: 4 allocated 6553600 at 0x7f49aceb8400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074838644480]: 1 allocated 3276800 at 0x7f4990238400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074838644480]: 1 allocated 6553600 at 0x7f4990558400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074838644480]: 1 allocated 3276800 at 0x7f4990b98400
[HCTR][23:32:52.388][ERROR][RK0][tid #140074838644480]: 1 allocated 6553600 at 0x7f4990eb8400
[HCTR][23:32:52.390][ERROR][RK0][tid #140074763142912]: 0 allocated 3276800 at 0x7f48e8320000
[HCTR][23:32:52.390][ERROR][RK0][tid #140074763142912]: 0 allocated 6553600 at 0x7f48e8640000
[HCTR][23:32:52.390][ERROR][RK0][tid #140074763142912]: 0 allocated 3276800 at 0x7f48e8c80000
[HCTR][23:32:52.390][ERROR][RK0][tid #140074763142912]: 0 allocated 6553600 at 0x7f48e8fa0000
