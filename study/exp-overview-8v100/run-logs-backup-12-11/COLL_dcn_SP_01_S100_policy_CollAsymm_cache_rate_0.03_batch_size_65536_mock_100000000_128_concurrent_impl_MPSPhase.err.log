2022-12-11 23:42:08.289609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.301354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.306878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.318744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.330219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.333865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.342172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.346552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.362381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.364109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.365060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.365415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.366683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.367195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.368268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.368972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.369868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.370588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.371531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.372489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.373455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.374789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.375770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.376137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.377708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.378171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.379924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.381559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.383748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.383965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.386082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.386185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.386369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.388382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.388381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.388559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.390656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.390812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.390826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.392907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.392962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.393606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.395434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.395784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.396371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.397342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.397701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.398307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.398813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.400091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.400139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.400988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.401473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.402774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.402912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.403732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.404081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.404914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.405200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.406016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.406802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.407125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.407745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.408333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.408784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.409430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.409852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.410442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.411088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.411405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.412126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.412778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.412929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.414254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.414302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.415389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.415437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.416512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.417031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.417529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.418789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.419453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.420023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.420558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.421112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.421652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.422209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.422740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.424116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.424674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.425196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.425689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.426192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.426680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.427203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.427585: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.427699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.428199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.428702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.429210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.429705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.430209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.430697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.431214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.431708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.433542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.434154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.434688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.435245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.435798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.436349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.436631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.437046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.437596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.437915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.438516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.442351: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.448562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.448742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.450040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.450531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.451165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.451184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.459020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.460691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.462118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.486881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.488346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.488857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.488913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.489367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.489403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.489438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.491716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.493352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.494083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.494163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.494185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.494228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.494311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.497031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.499302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.499971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.500035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.500166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.500253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.502029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.503653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.504400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.504449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.504577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.504664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.506914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.508254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.509000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.509138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.509178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.509314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.511653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.513321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.514134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.514184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.514265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.514360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.516309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.518024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.518950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.518989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.519124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.519175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.521128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.522590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.523524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.523710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.523750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.523792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.525742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.527410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.528341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.528481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.528520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.528562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.530556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.532142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.533366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.533414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.533450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.533535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.535397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.536834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.538014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.538102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.538186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.538229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.540794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.542302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.543255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.543304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.543349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.543447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.545066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.546507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.548132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.548236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.548320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.548409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.551047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.552694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.553898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.553947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.554026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.554145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.556600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.557834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.558154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.558300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.558381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.558653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.589967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.591195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.592326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.592417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.592422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.592975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.593993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.595185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.597227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.597677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.597801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.598001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.599075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.600923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.603334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.603583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.603773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.603962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.604947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.606288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.607774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.607962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.608266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.608425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.609417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.611248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.612681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.612833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.613214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.613361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.614486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.616400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.618173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.618284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.618628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.618831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.619936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.621456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.623476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.623723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.623948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.623984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.627486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.627601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.628128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.628242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.628603: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.630024: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.634410: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.634845: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.635017: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.635608: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:42:08.638037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.639312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.640586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.642334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.643553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.644037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.644352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.644546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.645059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.645405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.649070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.649746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.649786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.650113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.653833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.654380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.654421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:08.654930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.738453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.739081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.739618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.740087: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:09.740146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:42:09.757640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.758437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.759185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.759762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.760282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.760747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:42:09.806454: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:09.806647: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:09.850887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.851530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.852062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.852527: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:09.852581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:42:09.856491: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:42:09.869844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.870465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.871085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.871672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.872386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.872870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:42:09.918868: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:09.919066: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:09.921051: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 23:42:09.993813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.994601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.995297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:09.995837: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:09.995896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:42:10.010434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.012099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.012656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.012815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.013540: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:10.013620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:42:10.013897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.014452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.014829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.015427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.016358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.016571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.017535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.017698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:42:10.018079: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:10.018134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:42:10.018488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.019090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.019656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.020131: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:10.020179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:42:10.025310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.025945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.026466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.026933: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:10.026987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:42:10.032054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.032673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.033189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.033752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.034277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.034744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:42:10.036506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.037127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.037511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.037641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.038715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.038829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.039703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.039853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.040947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:42:10.041045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.041594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.042067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:42:10.043930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.044691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.045244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.045816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.046330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.046799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:42:10.058473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.059100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.059640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.060678: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:42:10.060751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:42:10.077917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.078587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.079101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.079695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.080213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:42:10.080677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:42:10.088117: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.088329: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.090240: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:42:10.093020: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.093169: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.094360: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.094538: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.094898: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:42:10.096423: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:42:10.101289: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.101444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.103325: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:42:10.107555: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.107685: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.110692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:42:10.127742: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.127942: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:42:10.129622: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][23:42:11.407][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.408][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.408][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.408][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.408][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.408][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.408][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:42:11.409][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 84.85it/s]warmup run: 96it [00:01, 80.60it/s]warmup run: 1it [00:01,  1.63s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 98it [00:01, 84.15it/s]warmup run: 91it [00:01, 78.38it/s]warmup run: 198it [00:01, 183.78it/s]warmup run: 192it [00:01, 174.91it/s]warmup run: 95it [00:01, 76.30it/s]warmup run: 100it [00:01, 84.63it/s]warmup run: 197it [00:01, 183.26it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 182it [00:01, 169.66it/s]warmup run: 297it [00:01, 292.68it/s]warmup run: 289it [00:01, 280.24it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 189it [00:01, 165.38it/s]warmup run: 201it [00:01, 184.56it/s]warmup run: 295it [00:01, 290.60it/s]warmup run: 95it [00:01, 82.28it/s]warmup run: 275it [00:01, 272.74it/s]warmup run: 396it [00:01, 404.86it/s]warmup run: 386it [00:01, 389.72it/s]warmup run: 99it [00:01, 86.44it/s]warmup run: 284it [00:01, 266.00it/s]warmup run: 302it [00:01, 294.95it/s]warmup run: 394it [00:01, 403.44it/s]warmup run: 189it [00:01, 176.81it/s]warmup run: 372it [00:01, 385.31it/s]warmup run: 495it [00:02, 513.69it/s]warmup run: 483it [00:02, 496.52it/s]warmup run: 199it [00:01, 187.84it/s]warmup run: 379it [00:02, 371.70it/s]warmup run: 405it [00:01, 412.43it/s]warmup run: 493it [00:02, 512.79it/s]warmup run: 283it [00:01, 280.62it/s]warmup run: 469it [00:02, 494.50it/s]warmup run: 595it [00:02, 616.07it/s]warmup run: 581it [00:02, 596.89it/s]warmup run: 299it [00:01, 298.73it/s]warmup run: 474it [00:02, 475.73it/s]warmup run: 505it [00:02, 520.95it/s]warmup run: 591it [00:02, 611.30it/s]warmup run: 377it [00:01, 387.76it/s]warmup run: 567it [00:02, 596.48it/s]warmup run: 695it [00:02, 704.47it/s]warmup run: 680it [00:02, 685.78it/s]warmup run: 570it [00:02, 574.67it/s]warmup run: 399it [00:01, 412.74it/s]warmup run: 603it [00:02, 616.82it/s]warmup run: 689it [00:02, 695.60it/s]warmup run: 471it [00:02, 491.86it/s]warmup run: 667it [00:02, 689.87it/s]warmup run: 795it [00:02, 777.50it/s]warmup run: 778it [00:02, 758.34it/s]warmup run: 499it [00:01, 523.75it/s]warmup run: 667it [00:02, 663.42it/s]warmup run: 704it [00:02, 707.35it/s]warmup run: 786it [00:02, 763.71it/s]warmup run: 567it [00:02, 589.86it/s]warmup run: 766it [00:02, 763.48it/s]warmup run: 894it [00:02, 832.08it/s]warmup run: 876it [00:02, 814.91it/s]warmup run: 600it [00:02, 626.15it/s]warmup run: 763it [00:02, 735.24it/s]warmup run: 803it [00:02, 776.54it/s]warmup run: 885it [00:02, 821.02it/s]warmup run: 663it [00:02, 674.95it/s]warmup run: 866it [00:02, 824.50it/s]warmup run: 974it [00:02, 858.25it/s]warmup run: 699it [00:02, 709.54it/s]warmup run: 859it [00:02, 791.69it/s]warmup run: 992it [00:02, 856.41it/s]warmup run: 901it [00:02, 827.79it/s]warmup run: 985it [00:02, 867.79it/s]warmup run: 756it [00:02, 736.39it/s]warmup run: 966it [00:02, 870.72it/s]warmup run: 1072it [00:02, 891.37it/s]warmup run: 955it [00:02, 836.81it/s]warmup run: 797it [00:02, 775.60it/s]warmup run: 1091it [00:02, 893.17it/s]warmup run: 999it [00:02, 864.93it/s]warmup run: 1084it [00:02, 900.62it/s]warmup run: 1065it [00:02, 902.50it/s]warmup run: 1170it [00:02, 914.73it/s]warmup run: 1051it [00:02, 870.59it/s]warmup run: 895it [00:02, 829.05it/s]warmup run: 1192it [00:02, 925.00it/s]warmup run: 849it [00:02, 743.16it/s]warmup run: 1184it [00:02, 926.95it/s]warmup run: 1097it [00:02, 886.90it/s]warmup run: 1164it [00:02, 926.29it/s]warmup run: 1268it [00:02, 929.18it/s]warmup run: 996it [00:02, 877.05it/s]warmup run: 1147it [00:02, 889.01it/s]warmup run: 1293it [00:02, 947.66it/s]warmup run: 947it [00:02, 803.13it/s]warmup run: 1285it [00:02, 950.80it/s]warmup run: 1194it [00:02, 903.03it/s]warmup run: 1263it [00:02, 942.94it/s]warmup run: 1367it [00:02, 944.99it/s]warmup run: 1097it [00:02, 913.85it/s]warmup run: 1393it [00:02, 962.14it/s]warmup run: 1242it [00:02, 902.54it/s]warmup run: 1046it [00:02, 852.55it/s]warmup run: 1388it [00:02, 972.20it/s]warmup run: 1290it [00:02, 914.78it/s]warmup run: 1363it [00:02, 957.84it/s]warmup run: 1466it [00:03, 956.30it/s]warmup run: 1198it [00:02, 939.26it/s]warmup run: 1493it [00:03, 972.03it/s]warmup run: 1337it [00:03, 909.17it/s]warmup run: 1145it [00:02, 889.63it/s]warmup run: 1490it [00:03, 985.25it/s]warmup run: 1387it [00:02, 929.86it/s]warmup run: 1462it [00:03, 963.99it/s]warmup run: 1299it [00:02, 957.40it/s]warmup run: 1564it [00:03, 959.00it/s]warmup run: 1594it [00:03, 982.23it/s]warmup run: 1431it [00:03, 917.57it/s]warmup run: 1244it [00:02, 915.38it/s]warmup run: 1591it [00:03, 982.06it/s]warmup run: 1485it [00:03, 942.23it/s]warmup run: 1561it [00:03, 969.10it/s]warmup run: 1400it [00:02, 970.94it/s]warmup run: 1662it [00:03, 961.77it/s]warmup run: 1698it [00:03, 997.67it/s]warmup run: 1527it [00:03, 928.09it/s]warmup run: 1343it [00:02, 934.74it/s]warmup run: 1583it [00:03, 952.92it/s]warmup run: 1691it [00:03, 975.80it/s]warmup run: 1660it [00:03, 968.27it/s]warmup run: 1500it [00:02, 975.30it/s]warmup run: 1802it [00:03, 1008.60it/s]warmup run: 1761it [00:03, 967.79it/s]warmup run: 1631it [00:03, 959.54it/s]warmup run: 1445it [00:03, 958.44it/s]warmup run: 1681it [00:03, 960.39it/s]warmup run: 1791it [00:03, 982.38it/s]warmup run: 1758it [00:03, 953.99it/s]warmup run: 1600it [00:03, 981.39it/s]warmup run: 1905it [00:03, 1014.11it/s]warmup run: 1866it [00:03, 989.89it/s]warmup run: 1734it [00:03, 979.52it/s]warmup run: 1546it [00:03, 973.36it/s]warmup run: 1779it [00:03, 963.77it/s]warmup run: 1893it [00:03, 990.89it/s]warmup run: 1700it [00:03, 985.68it/s]warmup run: 1855it [00:03, 944.41it/s]warmup run: 2010it [00:03, 1022.35it/s]warmup run: 1970it [00:03, 1003.35it/s]warmup run: 1833it [00:03, 981.44it/s]warmup run: 1646it [00:03, 979.90it/s]warmup run: 1995it [00:03, 998.27it/s]warmup run: 1877it [00:03, 959.93it/s]warmup run: 1952it [00:03, 950.51it/s]warmup run: 1800it [00:03, 986.89it/s]warmup run: 2084it [00:03, 1043.43it/s]warmup run: 2133it [00:03, 1082.57it/s]warmup run: 1933it [00:03, 986.83it/s]warmup run: 1746it [00:03, 984.45it/s]warmup run: 2114it [00:03, 1053.26it/s]warmup run: 1974it [00:03, 955.78it/s]warmup run: 2204it [00:03, 1089.49it/s]warmup run: 1901it [00:03, 992.72it/s]warmup run: 2058it [00:03, 981.51it/s]warmup run: 2256it [00:03, 1124.34it/s]warmup run: 2039it [00:03, 1007.85it/s]warmup run: 1847it [00:03, 990.37it/s]warmup run: 2235it [00:03, 1097.84it/s]warmup run: 2085it [00:03, 999.45it/s]warmup run: 2174it [00:03, 1033.69it/s]warmup run: 2323it [00:03, 1118.24it/s]warmup run: 2003it [00:03, 998.26it/s]warmup run: 2379it [00:03, 1153.90it/s]warmup run: 2158it [00:03, 1059.86it/s]warmup run: 1948it [00:03, 995.79it/s]warmup run: 2356it [00:03, 1129.13it/s]warmup run: 2202it [00:03, 1048.55it/s]warmup run: 2278it [00:03, 1035.06it/s]warmup run: 2444it [00:03, 1143.03it/s]warmup run: 2122it [00:03, 1053.99it/s]warmup run: 2502it [00:03, 1174.76it/s]warmup run: 2276it [00:03, 1095.63it/s]warmup run: 2057it [00:03, 1022.12it/s]warmup run: 2476it [00:03, 1149.42it/s]warmup run: 2320it [00:03, 1087.54it/s]warmup run: 2566it [00:04, 1164.76it/s]warmup run: 2387it [00:03, 1047.87it/s]warmup run: 2241it [00:03, 1093.00it/s]warmup run: 2625it [00:04, 1189.69it/s]warmup run: 2397it [00:04, 1127.69it/s]warmup run: 2175it [00:03, 1068.30it/s]warmup run: 2597it [00:04, 1164.78it/s]warmup run: 2439it [00:03, 1116.05it/s]warmup run: 2688it [00:04, 1180.03it/s]warmup run: 2359it [00:03, 1116.71it/s]warmup run: 2748it [00:04, 1200.08it/s]warmup run: 2518it [00:04, 1149.71it/s]warmup run: 2294it [00:03, 1102.19it/s]warmup run: 2492it [00:04, 1019.18it/s]warmup run: 2718it [00:04, 1175.72it/s]warmup run: 2557it [00:04, 1132.33it/s]warmup run: 2808it [00:04, 1185.84it/s]warmup run: 2869it [00:04, 1202.87it/s]warmup run: 2478it [00:03, 1136.87it/s]warmup run: 2639it [00:04, 1166.25it/s]warmup run: 2412it [00:03, 1124.60it/s]warmup run: 2610it [00:04, 1064.63it/s]warmup run: 2838it [00:04, 1180.06it/s]warmup run: 2674it [00:04, 1142.99it/s]warmup run: 2930it [00:04, 1193.54it/s]warmup run: 2992it [00:04, 1209.53it/s]warmup run: 2596it [00:04, 1149.14it/s]warmup run: 2760it [00:04, 1174.20it/s]warmup run: 2529it [00:04, 1137.16it/s]warmup run: 2728it [00:04, 1096.08it/s]warmup run: 3000it [00:04, 689.62it/s] warmup run: 3000it [00:04, 675.17it/s] warmup run: 2958it [00:04, 1183.69it/s]warmup run: 2792it [00:04, 1151.78it/s]warmup run: 2714it [00:04, 1157.46it/s]warmup run: 2647it [00:04, 1148.50it/s]warmup run: 2881it [00:04, 1182.19it/s]warmup run: 2844it [00:04, 1112.72it/s]warmup run: 3000it [00:04, 684.97it/s] warmup run: 2909it [00:04, 1155.44it/s]warmup run: 2831it [00:04, 1159.76it/s]warmup run: 2765it [00:04, 1156.50it/s]warmup run: 3000it [00:04, 658.79it/s] warmup run: 2961it [00:04, 1127.00it/s]warmup run: 3000it [00:04, 668.34it/s] warmup run: 3000it [00:04, 674.01it/s] warmup run: 2949it [00:04, 1165.08it/s]warmup run: 2884it [00:04, 1165.21it/s]warmup run: 3000it [00:04, 689.02it/s] warmup run: 3000it [00:04, 675.31it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.50it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.22it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.95it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1612.85it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.16it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.81it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.30it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.07it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1617.23it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1607.35it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.65it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1615.11it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.45it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1631.21it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1633.78it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1611.26it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1651.62it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1634.34it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1655.07it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1602.38it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1604.76it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1621.28it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1591.20it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1579.27it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1634.09it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1653.78it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1650.47it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1599.34it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1597.47it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1598.43it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1614.26it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1563.77it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1633.24it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1606.97it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1646.43it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1648.28it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1594.64it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1607.94it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1628.81it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1554.54it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1640.21it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1616.34it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1645.74it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1641.73it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1611.54it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1593.67it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1636.06it/s]warmup should be done:  32%|███▏      | 966/3000 [00:00<00:01, 1548.55it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1620.34it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1638.26it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1646.02it/s]warmup should be done:  38%|███▊      | 1127/3000 [00:00<00:01, 1595.09it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1614.37it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1634.14it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1619.26it/s]warmup should be done:  37%|███▋      | 1121/3000 [00:00<00:01, 1543.08it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1641.32it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1624.87it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1641.46it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1621.64it/s]warmup should be done:  43%|████▎     | 1290/3000 [00:00<00:01, 1603.12it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1632.21it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1615.94it/s]warmup should be done:  43%|████▎     | 1276/3000 [00:00<00:01, 1542.33it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1628.94it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1642.39it/s]warmup should be done:  48%|████▊     | 1452/3000 [00:00<00:00, 1608.19it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1626.79it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1630.80it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1631.37it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1619.53it/s]warmup should be done:  48%|████▊     | 1431/3000 [00:00<00:01, 1541.48it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1644.07it/s]warmup should be done:  54%|█████▍    | 1626/3000 [00:01<00:00, 1630.93it/s]warmup should be done:  54%|█████▍    | 1614/3000 [00:01<00:00, 1611.22it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1628.65it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1630.08it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1623.98it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1627.86it/s]warmup should be done:  53%|█████▎    | 1586/3000 [00:01<00:00, 1540.18it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1644.67it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1631.43it/s]warmup should be done:  59%|█████▉    | 1776/3000 [00:01<00:00, 1613.31it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1629.70it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1628.14it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1629.90it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1615.64it/s]warmup should be done:  58%|█████▊    | 1741/3000 [00:01<00:00, 1539.14it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1643.36it/s]warmup should be done:  65%|██████▌   | 1954/3000 [00:01<00:00, 1631.78it/s]warmup should be done:  65%|██████▍   | 1938/3000 [00:01<00:00, 1613.17it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1634.70it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1623.19it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1635.24it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1613.09it/s]warmup should be done:  63%|██████▎   | 1895/3000 [00:01<00:00, 1538.05it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1633.81it/s]warmup should be done:  71%|███████▏  | 2140/3000 [00:01<00:00, 1636.60it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1639.98it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1608.53it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1638.38it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1620.03it/s]warmup should be done:  71%|███████▏  | 2140/3000 [00:01<00:00, 1611.64it/s]warmup should be done:  68%|██████▊   | 2049/3000 [00:01<00:00, 1538.25it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1634.98it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1633.30it/s]warmup should be done:  76%|███████▋  | 2288/3000 [00:01<00:00, 1644.51it/s]warmup should be done:  75%|███████▌  | 2261/3000 [00:01<00:00, 1605.54it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  77%|███████▋  | 2305/3000 [00:01<00:00, 1618.73it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1611.09it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1538.16it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1634.28it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1644.34it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1626.36it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1599.80it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1639.94it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1615.07it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1608.11it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1536.49it/s]warmup should be done:  87%|████████▋ | 2610/3000 [00:01<00:00, 1634.12it/s]warmup should be done:  87%|████████▋ | 2619/3000 [00:01<00:00, 1646.17it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1625.17it/s]warmup should be done:  86%|████████▌ | 2582/3000 [00:01<00:00, 1598.20it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1642.68it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1613.98it/s]warmup should be done:  88%|████████▊ | 2625/3000 [00:01<00:00, 1608.25it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1535.92it/s]warmup should be done:  92%|█████████▏| 2774/3000 [00:01<00:00, 1634.95it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1647.33it/s]warmup should be done:  93%|█████████▎| 2795/3000 [00:01<00:00, 1627.43it/s]warmup should be done:  91%|█████████▏| 2742/3000 [00:01<00:00, 1598.11it/s]warmup should be done:  93%|█████████▎| 2795/3000 [00:01<00:00, 1644.25it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1614.13it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1607.29it/s]warmup should be done:  89%|████████▉ | 2665/3000 [00:01<00:00, 1536.65it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1641.03it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1655.17it/s]warmup should be done:  99%|█████████▊| 2960/3000 [00:01<00:00, 1634.01it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1603.59it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1650.43it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1622.46it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1612.58it/s]warmup should be done:  94%|█████████▍| 2820/3000 [00:01<00:00, 1538.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1603.79it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1544.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1547.14it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1579.27it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.61it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.83it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.02it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.97it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.09it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1680.21it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.01it/s]warmup should be done:  11%|█         | 316/3000 [00:00<00:01, 1578.53it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.12it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1682.27it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1688.52it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1661.53it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1656.78it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1680.88it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1660.44it/s]warmup should be done:  16%|█▌        | 475/3000 [00:00<00:01, 1582.79it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1645.64it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1685.13it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1663.82it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1691.49it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1661.48it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1656.42it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1673.34it/s]warmup should be done:  21%|██        | 635/3000 [00:00<00:01, 1585.87it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1649.91it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1668.30it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1687.37it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1667.24it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1689.90it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1673.23it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1673.83it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1652.55it/s]warmup should be done:  26%|██▋       | 794/3000 [00:00<00:01, 1584.79it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1671.65it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1689.56it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1670.02it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1687.53it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1675.00it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1661.33it/s]warmup should be done:  32%|███▏      | 954/3000 [00:00<00:01, 1589.91it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1676.93it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1688.73it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1647.89it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1690.13it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1667.95it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1672.30it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1672.69it/s]warmup should be done:  37%|███▋      | 1122/3000 [00:00<00:01, 1619.28it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1678.06it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1686.24it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1646.24it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1689.60it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1665.54it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1668.92it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1678.73it/s]warmup should be done:  43%|████▎     | 1288/3000 [00:00<00:01, 1630.94it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1680.39it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1689.82it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1692.63it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1645.14it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1671.74it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1670.72it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1685.55it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1646.44it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1680.98it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1690.87it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1646.61it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1692.62it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1679.72it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1670.01it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1688.95it/s]warmup should be done:  54%|█████▍    | 1625/3000 [00:01<00:00, 1655.44it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1682.71it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1690.81it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1649.01it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1687.90it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1693.23it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1691.63it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1670.49it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1661.57it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1683.91it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1691.94it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1694.31it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1693.44it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1649.47it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1671.77it/s]warmup should be done:  62%|██████▏   | 1863/3000 [00:01<00:00, 1695.25it/s]warmup should be done:  65%|██████▌   | 1962/3000 [00:01<00:00, 1667.64it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1694.58it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1691.03it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1696.42it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1647.03it/s]warmup should be done:  67%|██████▋   | 2019/3000 [00:01<00:00, 1670.36it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1670.89it/s]warmup should be done:  68%|██████▊   | 2033/3000 [00:01<00:00, 1668.73it/s]warmup should be done:  71%|███████   | 2131/3000 [00:01<00:00, 1674.29it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1697.26it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1647.21it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1693.45it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1689.94it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1674.27it/s]warmup should be done:  73%|███████▎  | 2189/3000 [00:01<00:00, 1670.29it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1676.46it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1680.12it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1700.02it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1648.87it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1692.55it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1690.13it/s]warmup should be done:  79%|███████▊  | 2359/3000 [00:01<00:00, 1683.90it/s]warmup should be done:  79%|███████▊  | 2358/3000 [00:01<00:00, 1674.39it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1683.46it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1684.10it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1701.87it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1650.03it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1692.52it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1691.03it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1691.15it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1678.50it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1690.80it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1683.91it/s]warmup should be done:  90%|█████████ | 2707/3000 [00:01<00:00, 1702.78it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1693.73it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1691.20it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1645.94it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1695.54it/s]warmup should be done:  90%|████████▉ | 2696/3000 [00:01<00:00, 1680.11it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1695.34it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1685.30it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1702.47it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1693.10it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1646.99it/s]warmup should be done:  96%|█████████▌| 2885/3000 [00:01<00:00, 1689.88it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1697.93it/s]warmup should be done:  96%|█████████▌| 2865/3000 [00:01<00:00, 1680.66it/s]warmup should be done:  96%|█████████▌| 2885/3000 [00:01<00:00, 1693.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.19it/s]warmup should be done:  99%|█████████▉| 2979/3000 [00:01<00:00, 1688.09it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1646.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.13it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe556c51f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe556c40d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe55792e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe556d42b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe556c5190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe55793730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe55795d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fbe556d31c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 23:43:42.069225: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb98282cc40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.069293: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.078690: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.107415: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb98b02d7e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.107470: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.108433: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb98302d740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.108489: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.116276: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.118548: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.266934: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb983031d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.266990: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.274896: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.828996: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb987029320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.829070: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.829531: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb98b02d6d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.829595: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.837881: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.839909: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.848591: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb986834210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.848648: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.856534: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:42.872211: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb98af91f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:43:42.872273: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:43:42.879933: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:43:49.301489: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.302344: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.340751: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.372869: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.604186: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.604243: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.672078: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:43:49.681402: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:44:49.381][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:44:49.381][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.387][ERROR][RK0][main]: coll ps creation done
[HCTR][23:44:49.387][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][23:44:49.546][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:44:49.546][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.552][ERROR][RK0][main]: coll ps creation done
[HCTR][23:44:49.552][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:44:49.569][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:44:49.569][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.574][ERROR][RK0][main]: coll ps creation done
[HCTR][23:44:49.574][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:44:49.616][ERROR][RK0][tid #140435850761984]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:44:49.616][ERROR][RK0][tid #140435850761984]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.624][ERROR][RK0][tid #140435850761984]: coll ps creation done
[HCTR][23:44:49.624][ERROR][RK0][tid #140435850761984]: replica 0 waits for coll ps creation barrier
[HCTR][23:44:49.727][ERROR][RK0][tid #140435221636864]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:44:49.727][ERROR][RK0][tid #140435221636864]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.732][ERROR][RK0][tid #140435221636864]: coll ps creation done
[HCTR][23:44:49.732][ERROR][RK0][tid #140435221636864]: replica 2 waits for coll ps creation barrier
[HCTR][23:44:49.803][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:44:49.803][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.807][ERROR][RK0][main]: coll ps creation done
[HCTR][23:44:49.807][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][23:44:49.923][ERROR][RK0][tid #140434961594112]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:44:49.923][ERROR][RK0][tid #140434961594112]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.927][ERROR][RK0][tid #140434961594112]: coll ps creation done
[HCTR][23:44:49.927][ERROR][RK0][tid #140434961594112]: replica 6 waits for coll ps creation barrier
[HCTR][23:44:49.947][ERROR][RK0][tid #140435380999936]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:44:49.947][ERROR][RK0][tid #140435380999936]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:44:49.954][ERROR][RK0][tid #140435380999936]: coll ps creation done
[HCTR][23:44:49.954][ERROR][RK0][tid #140435380999936]: replica 7 waits for coll ps creation barrier
[HCTR][23:44:49.954][ERROR][RK0][tid #140435850761984]: replica 0 preparing frequency
[HCTR][23:44:50.842][ERROR][RK0][tid #140435850761984]: replica 0 preparing frequency done
[HCTR][23:44:50.880][ERROR][RK0][tid #140435850761984]: replica 0 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][tid #140435221636864]: replica 2 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][tid #140434961594112]: replica 6 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][tid #140435380999936]: replica 7 calling init per replica
[HCTR][23:44:50.880][ERROR][RK0][tid #140435850761984]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][main]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][main]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][main]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][main]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][tid #140435221636864]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][tid #140434961594112]: Calling build_v2
[HCTR][23:44:50.881][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.880][ERROR][RK0][tid #140435380999936]: Calling build_v2
[HCTR][23:44:50.880][ERROR][RK0][tid #140435850761984]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.881][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.881][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.881][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.881][ERROR][RK0][tid #140435221636864]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.881][ERROR][RK0][tid #140434961594112]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:44:50.881][ERROR][RK0][tid #140435380999936]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 23:44:502022-12-11 23:44:502022-12-11 23:44:502022-12-11 23:44:502022-12-11 23:44:50.2022-12-11 23:44:50.2022-12-11 23:44:50...881089.881084.881084881084881084: 881104: [881104: : : E: E: EEE E E   2022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::881172136:136:136136136: ] 136] 136] ] ] Eusing concurrent impl MPSPhase] using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase 
using concurrent impl MPSPhase
using concurrent impl MPSPhase


/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc

:136] using concurrent impl MPSPhase
[2022-12-11 23:44:50.885417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:44:50.885455: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:44:50:.196885461] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:44:50.[8855082022-12-11 23:44:50: .E885520 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:[ 1782022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .[:v100x8, slow pcie8855402022-12-11 23:44:50196
: .] E885554assigning 8 to cpu[ : 
2022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[.: 2022-12-11 23:44:50885590212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: ] :885598Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178:  
] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie :[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196[2022-12-11 23:44:50:[] [2022-12-11 23:44:50.1782022-12-11 23:44:50assigning 8 to cpu2022-12-11 23:44:50.885651] .
.[885651: v100x8, slow pcie8856648856682022-12-11 23:44:50: E
: : .E EE[885698 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[  2022-12-11 23:44:50: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:44:50:2022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:.212.::885755 178885762] 885756213196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] ] E:v100x8, slow pcieE
Eremote time is 8.68421assigning 8 to cpu 178
  

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 23:44:50[v100x8, slow pcie::2022-12-11 23:44:50196.2022-12-11 23:44:50
212178[.] 885912.] [] 2022-12-11 23:44:50885922assigning 8 to cpu: 885930build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 23:44:50v100x8, slow pcie.: 
E: 
.
885971E E885992: [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ [: E2022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:44:50E .:213.:. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc886078196] 886084214886093/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: ] remote time is 8.68421: ] : :212Eassigning 8 to cpu
Ecpu time is 97.0588E196]  
 
[ ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:44:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu
::.:
213[196[886255212] 2022-12-11 23:44:50] 2022-12-11 23:44:50: ] remote time is 8.68421.[assigning 8 to cpu.Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
8863132022-12-11 23:44:50
886321 
: .[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[8863602022-12-11 23:44:502022-12-11 23:44:50[E: : ..2022-12-11 23:44:50 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE886426886399./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : : : 886421:cpu time is 97.0588212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE: 213
] :  E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc remote time is 8.68421
] ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212214:[
[] ] 2132022-12-11 23:44:502022-12-11 23:44:50build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588] [..

remote time is 8.684212022-12-11 23:44:50886602886609
.: : [886636E[E2022-12-11 23:44:50:  2022-12-11 23:44:50 .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc886690 :886701:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213: 214E:] E]  213remote time is 8.68421 cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:[remote time is 8.68421:2132022-12-11 23:44:50
214] .] [remote time is 8.68421886835cpu time is 97.05882022-12-11 23:44:50
: 
.E[886865 2022-12-11 23:44:50: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:886913 214: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:cpu time is 97.0588 214
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 23:46:09.710453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:46:09.750397: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:46:09.750487: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:46:09.751681: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 23:46:09.826919: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 23:46:10.210074: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 23:46:10.210163: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 23:46:17. 19240: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 23:46:17. 19338: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 23:46:18.736417: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 23:46:18.736510: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 23:46:18.739296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 23:46:18.739357: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 23:46:18.985095: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 23:46:19. 13762: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 23:46:19. 15209: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 23:46:19. 36615: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 23:46:19.558704: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 23:46:19.560937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 23:46:19.563921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 23:46:19.566811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 23:46:19.569681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 23:46:19.572537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 23:46:19.575373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 23:46:19.578193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 23:46:19.581028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 23:46:47.959354: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 23:46:47.967261: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 23:46:47.968767: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 23:46:48. 13877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:46:48. 13978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:46:48. 14011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:46:48. 14043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:46:48. 14634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:46:48. 14689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 15884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 16544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 29410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 23:46:48. 29492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:46:48. 29692[: 2022-12-11 23:46:48E.  29718/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:4 solved202
] 1 solved
[2022-12-11 23:46:48. 29786[: 2022-12-11 23:46:48E.  29795/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 4 initing device 4:
205] worker 0 thread 1 initing device 1
[2022-12-11 23:46:48. 29949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:46:48. 30007: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 23:46:482022-12-11 23:46:48.. 30238 30239: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 23:46:482022-12-11 23:46:48.. 30314 30314: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 23:46:48. 31733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:46:48. 31792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3[
2022-12-11 23:46:48. 31802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 23:46:48. 31878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 23:46:48. 32109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 32161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 32207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 32242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:46:48. 32287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 32348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:46:48. 32403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48.[ 340242022-12-11 23:46:48: .E 34044 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :6 solved202
] 2 solved[
2022-12-11 23:46:48. 34133: [E2022-12-11 23:46:48 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 34148:: 205E]  worker 0 thread 6 initing device 6/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 2 initing device 2
[2022-12-11 23:46:48[.2022-12-11 23:46:48 34624.:  34629E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815:] 1815Building Coll Cache with ... num gpu device is 8] 
Building Coll Cache with ... num gpu device is 8
[[2022-12-11 23:46:482022-12-11 23:46:48.. 34721 34724: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 23:46:48. 35199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 36199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 36255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 36329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 36396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 39240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 39294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 40609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 40676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 41881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 41992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:46:48. 92517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:46:48. 98098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:46:48. 98229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:46:48. 99067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48. 99686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.100695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.100744: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.42 MB
[2022-12-11 23:46:48.103958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:46:48.104745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.104790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[[2022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:48......116131116132116131116132116132116132: : : : [: : EEEE2022-12-11 23:46:48EE    .  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu116258/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::: ::1980198019801980E19801980] ] ] ]  ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes


:

1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:46:48.122772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:46:48.122852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:46:48.122946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:46:48.123021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:46:48.123043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:46:48.[1231252022-12-11 23:46:48: .E123120 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1024
[2022-12-11 23:46:48.123225: [E2022-12-11 23:46:48 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc123221:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-11 23:46:48.123285[: 2022-12-11 23:46:48E. 123310/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[[2022-12-11 23:46:482022-12-11 23:46:48..123363123379: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-11 23:46:48.123455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:46:48.123654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.124580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.125662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.126165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.126685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.127205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.127726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:46:48.128387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.128750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.129087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.129141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.129182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.129218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.129263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.129364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.129410: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:46:48.129725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.129770: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.41 MB
[2022-12-11 23:46:48.130066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.130099: E[ 2022-12-11 23:46:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:130112638: ] Weager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.43 MB
[[2022-12-11 23:46:482022-12-11 23:46:48..130153130156: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[::2022-12-11 23:46:4863843.] ] 130181eager release cuda mem 625663WORKER[0] alloc host memory 11.43 MB: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
[2022-12-11 23:46:482022-12-11 23:46:48..130224130231: : EW[  2022-12-11 23:46:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.::13025163843: ] ] Weager release cuda mem 625663WORKER[0] alloc host memory 11.44 MB 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.43 MB
[2022-12-11 23:46:48.130311: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:46:48.133032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:46:48.133789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.133833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:46:48.137998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:46:48.138238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:46:48.138403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:46:48[.2022-12-11 23:46:48138606.: 138621[E: 2022-12-11 23:46:48 E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 138634:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 1980[:E] 2022-12-11 23:46:48638 eager alloc mem 25.25 KB.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
138683eager release cuda mem 25855:: 
1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] [eager alloc mem 25.25 KB2022-12-11 23:46:48
.138789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:46:48.138853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.138897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:46:48.139021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.139066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:46:48.139322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.139365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:46:48.139386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.139430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:46:48.139470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:46:48.139516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[[[[[[[2022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:482022-12-11 23:46:48........596946596946596945596945596945596945596946596946: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 6 init p2p of link 0Device 1 init p2p of link 7Device 2 init p2p of link 1Device 3 init p2p of link 2Device 4 init p2p of link 5Device 0 init p2p of link 3Device 7 init p2p of link 4Device 5 init p2p of link 6







[[2022-12-11 23:46:482022-12-11 23:46:48.[.5974672022-12-11 23:46:48[[597468[[: .2022-12-11 23:46:482022-12-11 23:46:48: 2022-12-11 23:46:482022-12-11 23:46:48E597476[..E.. : 2022-12-11 23:46:48597481597483 597485597485/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE.: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : : 597511EE:EE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:   1980  ] :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980 ::eager alloc mem 611.00 KB::
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980
19801980eager alloc mem 611.00 KB:] ] ] ] 
1980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] 



eager alloc mem 611.00 KB
[2022-12-11 23:46:48.598474: E[ 2022-12-11 23:46:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:598486638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 23:46:48.598522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
[2022-12-11 23:46:48[2022-12-11 23:46:48.[[2022-12-11 23:46:48.5985462022-12-11 23:46:482022-12-11 23:46:48.598552: ..598553: E598565598567: E : : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] ::638] eager release cuda mem 625663638638] eager release cuda mem 625663
] ] eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 23:46:48.611684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:46:48.611739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:46:48.611836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.611892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.612081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:46:48.612171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 23:46:48.612246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.612347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.612587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:46:48.612630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.612711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.612754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.612871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:46:48.613030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.613060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.613084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 23:46:48.613136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:46:48eager release cuda mem 625663.
613146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:46:48.613249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.613310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.613531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.613823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.614048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.614104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.625189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 23:46:48.625307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.625445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:46:48.625559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.625614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 23:46:48.625649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:46:48.625737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.625767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.625819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:46:48.625939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.626108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.626348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.626418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:46:48.626536: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:46:48:.1980626548] : eager alloc mem 611.00 KBE
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 23:46:48:2022-12-11 23:46:48.638.626563] 626574: eager release cuda mem 625663: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 7eager release cuda mem 625663

[2022-12-11 23:46:48.626714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:46:481980.] 626728eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.626775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 23:46:48.626892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.627330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.627536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.627675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.641265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:46:48.641378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.641792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 23:46:48.641900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.[6421782022-12-11 23:46:48: .E642176 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631926
] Device 0 init p2p of link 2
[2022-12-11 23:46:48.642273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:46:48.642323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.642392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.642613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:46:48.642660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:46:481926.] 642686Device 6 init p2p of link 7: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 23:46:48] .eager release cuda mem 625663642728
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.642798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.643021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:46:48.643114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 23:46:48.643149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.643202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:46:482022-12-11 23:46:48..643513643526: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 3eager release cuda mem 625663

[2022-12-11 23:46:48.643595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.643647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:46:48.643947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.644421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:46:48.656815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:46:48.657006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:46:48.658218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:46:48.658525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2996877 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8989243 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.623809 secs 
[2022-12-11 23:46:48[.2022-12-11 23:46:48658608[.: 2022-12-11 23:46:48658607E.:  658618E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1955 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638Asymm Coll cache (policy: coll_cache_asymm_link) | local 2998586 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8987534 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.628303 secs :] 
638eager release cuda mem 12399996] 
eager release cuda mem 12399996
[2022-12-11 23:46:48.658761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:46:48.659107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2994394 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8991726 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.644431 secs 
[2022-12-11 23:46:48.659225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:46:48.659361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:46:48.659692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2995563 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8990557 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.629705 secs 
[2022-12-11 23:46:48.659730: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.75 GB
[2022-12-11 23:46:48.660221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2996263 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8989857 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.627943 secs 
[2022-12-11 23:46:48.660307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2991488 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8994632 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.627923 secs 
[2022-12-11 23:46:48.661723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2999719 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8986401 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.44 GB | 0.63142 secs 
[2022-12-11 23:46:48.661819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2999350 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8986770 / 100000000 nodes ( 8.99 %) | cpu 88013880 / 100000000 nodes ( 88.01 %) | 1.43 GB | 0.627108 secs 
[2022-12-11 23:46:50. 96242: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.02 GB
[2022-12-11 23:46:50. 96942: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.02 GB
[2022-12-11 23:46:50. 97727: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.02 GB
[2022-12-11 23:46:51.441473: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.28 GB
[2022-12-11 23:46:51.442201: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.28 GB
[2022-12-11 23:46:51.444119: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.28 GB
[2022-12-11 23:46:52.797069: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.49 GB
[2022-12-11 23:46:52.797398: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.49 GB
[2022-12-11 23:46:52.798053: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.49 GB
[2022-12-11 23:46:54.380419: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.71 GB
[2022-12-11 23:46:54.381078: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.71 GB
[2022-12-11 23:46:54.381687: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 10.71 GB
[2022-12-11 23:46:54.382927: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 10.71 GB
[2022-12-11 23:46:54.383510: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.71 GB
[2022-12-11 23:46:55.824440: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.91 GB
[2022-12-11 23:46:55.824585: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.91 GB
[HCTR][23:46:55.824][ERROR][RK0][tid #140434961594112]: replica 6 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][tid #140435221636864]: replica 2 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][tid #140435850761984]: replica 0 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][tid #140435380999936]: replica 7 calling init per replica done, doing barrier
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][tid #140435380999936]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][tid #140435850761984]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][tid #140434961594112]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][tid #140435221636864]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:46:55.824][ERROR][RK0][main]: init per replica done
[HCTR][23:46:55.824][ERROR][RK0][tid #140435380999936]: init per replica done
[HCTR][23:46:55.824][ERROR][RK0][main]: init per replica done
[HCTR][23:46:55.824][ERROR][RK0][main]: init per replica done
[HCTR][23:46:55.824][ERROR][RK0][main]: init per replica done
[HCTR][23:46:55.824][ERROR][RK0][tid #140434961594112]: init per replica done
[HCTR][23:46:55.824][ERROR][RK0][tid #140435221636864]: init per replica done
[HCTR][23:46:55.827][ERROR][RK0][tid #140435850761984]: init per replica done
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 5 allocated 3276800 at 0x7f9cd0238400
[HCTR][23:46:55.863][ERROR][RK0][tid #140435221636864]: 2 allocated 3276800 at 0x7f9e00238400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 5 allocated 6553600 at 0x7f9cd0558400
[HCTR][23:46:55.863][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f9dcc238400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 5 allocated 3276800 at 0x7f9cd0b98400
[HCTR][23:46:55.863][ERROR][RK0][tid #140435221636864]: 2 allocated 6553600 at 0x7f9e00558400
[HCTR][23:46:55.863][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f9dcc558400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 5 allocated 6553600 at 0x7f9cd0eb8400
[HCTR][23:46:55.863][ERROR][RK0][tid #140435221636864]: 2 allocated 3276800 at 0x7f9e00b98400
[HCTR][23:46:55.863][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f9dccb98400
[HCTR][23:46:55.863][ERROR][RK0][tid #140435221636864]: 2 allocated 6553600 at 0x7f9e00eb8400
[HCTR][23:46:55.863][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f9dcceb8400
[HCTR][23:46:55.863][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f9ddc238400
[HCTR][23:46:55.863][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f9ddc558400
[HCTR][23:46:55.863][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f9ddcb98400
[HCTR][23:46:55.863][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f9ddceb8400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 3 allocated 3276800 at 0x7f9d94238400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 3 allocated 6553600 at 0x7f9d94558400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 3 allocated 3276800 at 0x7f9d94b98400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 3 allocated 6553600 at 0x7f9d94eb8400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 6 allocated 3276800 at 0x7f9cf0238400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 6 allocated 6553600 at 0x7f9cf0558400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 6 allocated 3276800 at 0x7f9cf0b98400
[HCTR][23:46:55.863][ERROR][RK0][tid #140434961594112]: 6 allocated 6553600 at 0x7f9cf0eb8400
[HCTR][23:46:55.863][ERROR][RK0][tid #140435515217664]: 1 allocated 3276800 at 0x7f9e00238400
[HCTR][23:46:55.864][ERROR][RK0][tid #140435515217664]: 1 allocated 6553600 at 0x7f9e00558400
[HCTR][23:46:55.864][ERROR][RK0][tid #140435515217664]: 1 allocated 3276800 at 0x7f9e00b98400
[HCTR][23:46:55.864][ERROR][RK0][tid #140435515217664]: 1 allocated 6553600 at 0x7f9e00eb8400
[HCTR][23:46:55.866][ERROR][RK0][tid #140435850761984]: 0 allocated 3276800 at 0x7f9dd0320000
[HCTR][23:46:55.866][ERROR][RK0][tid #140435850761984]: 0 allocated 6553600 at 0x7f9dd0640000
[HCTR][23:46:55.866][ERROR][RK0][tid #140435850761984]: 0 allocated 3276800 at 0x7f9dd0c80000
[HCTR][23:46:55.866][ERROR][RK0][tid #140435850761984]: 0 allocated 6553600 at 0x7f9dd0fa0000
