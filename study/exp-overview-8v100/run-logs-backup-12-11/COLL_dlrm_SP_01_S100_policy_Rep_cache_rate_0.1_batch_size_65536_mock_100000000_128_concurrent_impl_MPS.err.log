2022-12-12 02:27:30.364437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.375529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.380101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.386126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.397180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.406279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.417322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.424393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.483761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.484031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.485150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.485385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.486694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.486712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.488455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.488506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.490081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.490182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.491507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.491774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.492859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.493336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.494225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.494871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.495635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.496993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.497952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.498925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.499930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.500848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.501856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.502871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.504607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.505581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.506821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.508301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.508688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.509588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.510186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.511079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.511627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.512946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.513529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.515015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.515691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.515977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.517464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.517695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.519030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.519394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.520697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.521424: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.521489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.524390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.525447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.526012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.527739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.528309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.530173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.530263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.530753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.530821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.533327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.533407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.533438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.533686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.536218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.536351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.536355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.536403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.536785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.538632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.539014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.539049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.539635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.541261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.541769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.541866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.542390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.542969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.544215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.544815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.544954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.545556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.546077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.547490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.548168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.548851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.548989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.550342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.550946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.551621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.551671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.553587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.553954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.554277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.556032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.556431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.557549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.558650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.559164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.559994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.569592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.569598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.570896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.570965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.572686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.572727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.579876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.594182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.600444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.601495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.604662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.611821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.616597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.622762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.627739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.631923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.632175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.632221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.632263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.632458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.632583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.633044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.637132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.637255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.637407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.637448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.637561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.637654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.638332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.641855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.642025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.642121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.642164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.642449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.642675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.643297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.647125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.647337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.647382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.647474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.647630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.647732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.648807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.651727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.651988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.652084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.652607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.652659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.652969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.653581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.656892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.657076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.657263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.657665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.657706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.657937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.658652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.661175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.661483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.661740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.662027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.662071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.662490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.663211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.666297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.666521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.666800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.666959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.667046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.667204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.670732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.671031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.671274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.671480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.671571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.671676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.672338: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.674933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.675145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.675339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.675346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.675347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.675426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.678739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.679011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.679454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.679502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.679599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.679600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.681275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.684114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.684257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.684625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.684704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.684844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.684909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.685170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.687488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.688168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.689066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.689185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.689612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.689711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.689802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.692222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.693056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.693913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.694439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.694482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.694575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.696672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.697599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.698402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.699020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.699168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.699307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.701904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.702179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.703039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.703823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.703899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.703985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.706693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.706829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.707666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.708532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.708640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.708677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.711254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.712230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.712716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.712776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.712889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.714782: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.715401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.716189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.716573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.716668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.716917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.723434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.723566: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.723589: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.724350: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.724352: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.724354: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:27:30.725126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.728248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.733202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.733316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.734128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.734256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.734265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.766785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.766890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.766896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.766944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.766984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.800151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.800237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.800253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.800273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:30.800289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.830761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.831421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.832293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.832770: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:31.832827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:27:31.851037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.851711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.852634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.853618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.854347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:31.854826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:27:31.901344: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:31.901563: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:31.935754: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:27:32.025688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.026321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.027258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.027737: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.027791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:27:32.046084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.046749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.047281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.047854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.048882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.049375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:27:32.095294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.096129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.096849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.096960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.097973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.097975: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.098039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:27:32.103878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.103996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.105328: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.105331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.105384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:27:32.106051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.106528: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.106571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:27:32.110985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.111630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.112151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.112845: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.112894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:27:32.119868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.120253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.120742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.121252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.121341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.121659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.122990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.123016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.123034: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.123081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:27:32.123511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.125002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.125092: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:27:32.125144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:27:32.125305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.125480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.126784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.127069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.127083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.128139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.128535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.128638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.129655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.129847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:27:32.130022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.130652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.130882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:27:32.131247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:27:32.131367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.131940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.132453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.133008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.133544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.134009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:27:32.141487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.142111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.142186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.143211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.143247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.144197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.144325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.145166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.145230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.146138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:27:32.146153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:27:32.146685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:27:32.157516: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.157718: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.158798: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:27:32.176827: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.177054: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.177261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.177416: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.178040: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:27:32.178458: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:27:32.178982: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.179205: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.181008: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:27:32.193032: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.193270: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.193687: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.193838: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.194449: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:27:32.194790: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:27:32.228029: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.228250: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:27:32.229183: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][02:27:33.491][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.491][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.491][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.491][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.491][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.492][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.534][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:27:33.535][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 100it [00:01, 84.19it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 101it [00:01, 85.97it/s]warmup run: 200it [00:01, 182.60it/s]warmup run: 94it [00:01, 81.12it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 98it [00:01, 83.85it/s]warmup run: 95it [00:01, 80.80it/s]warmup run: 204it [00:01, 188.48it/s]warmup run: 300it [00:01, 291.40it/s]warmup run: 192it [00:01, 179.86it/s]warmup run: 95it [00:01, 81.73it/s]warmup run: 96it [00:01, 82.42it/s]warmup run: 196it [00:01, 181.72it/s]warmup run: 191it [00:01, 176.26it/s]warmup run: 306it [00:01, 300.15it/s]warmup run: 399it [00:01, 402.63it/s]warmup run: 290it [00:01, 288.46it/s]warmup run: 192it [00:01, 179.09it/s]warmup run: 193it [00:01, 179.50it/s]warmup run: 294it [00:01, 289.35it/s]warmup run: 289it [00:01, 283.97it/s]warmup run: 407it [00:01, 413.80it/s]warmup run: 499it [00:02, 513.31it/s]warmup run: 388it [00:01, 400.13it/s]warmup run: 288it [00:01, 284.48it/s]warmup run: 291it [00:01, 287.66it/s]warmup run: 393it [00:01, 402.09it/s]warmup run: 388it [00:01, 396.98it/s]warmup run: 507it [00:02, 522.80it/s]warmup run: 601it [00:02, 618.49it/s]warmup run: 486it [00:02, 508.44it/s]warmup run: 387it [00:01, 398.68it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 391it [00:01, 402.35it/s]warmup run: 492it [00:02, 511.71it/s]warmup run: 485it [00:02, 503.90it/s]warmup run: 606it [00:02, 620.84it/s]warmup run: 699it [00:02, 700.63it/s]warmup run: 584it [00:02, 607.39it/s]warmup run: 486it [00:02, 509.00it/s]warmup run: 101it [00:01, 88.75it/s]warmup run: 489it [00:02, 509.83it/s]warmup run: 593it [00:02, 616.05it/s]warmup run: 585it [00:02, 607.76it/s]warmup run: 703it [00:02, 699.83it/s]warmup run: 800it [00:02, 775.28it/s]warmup run: 683it [00:02, 695.47it/s]warmup run: 587it [00:02, 614.35it/s]warmup run: 200it [00:01, 189.50it/s]warmup run: 588it [00:02, 610.38it/s]warmup run: 695it [00:02, 708.06it/s]warmup run: 686it [00:02, 699.34it/s]warmup run: 804it [00:02, 775.37it/s]warmup run: 900it [00:02, 833.41it/s]warmup run: 782it [00:02, 768.55it/s]warmup run: 688it [00:02, 705.29it/s]warmup run: 300it [00:01, 301.06it/s]warmup run: 687it [00:02, 697.38it/s]warmup run: 786it [00:02, 773.36it/s]warmup run: 797it [00:02, 783.44it/s]warmup run: 905it [00:02, 836.23it/s]warmup run: 1000it [00:02, 876.50it/s]warmup run: 881it [00:02, 825.10it/s]warmup run: 790it [00:02, 781.55it/s]warmup run: 401it [00:01, 416.98it/s]warmup run: 785it [00:02, 766.30it/s]warmup run: 886it [00:02, 831.14it/s]warmup run: 899it [00:02, 843.80it/s]warmup run: 1006it [00:02, 881.99it/s]warmup run: 1100it [00:02, 910.02it/s]warmup run: 979it [00:02, 867.14it/s]warmup run: 891it [00:02, 839.31it/s]warmup run: 502it [00:01, 528.90it/s]warmup run: 882it [00:02, 816.23it/s]warmup run: 987it [00:02, 877.73it/s]warmup run: 1002it [00:02, 892.65it/s]warmup run: 1107it [00:02, 916.37it/s]warmup run: 1199it [00:02, 932.74it/s]warmup run: 1078it [00:02, 900.24it/s]warmup run: 991it [00:02, 881.58it/s]warmup run: 604it [00:02, 633.18it/s]warmup run: 978it [00:02, 854.54it/s]warmup run: 1086it [00:02, 908.41it/s]warmup run: 1103it [00:02, 921.30it/s]warmup run: 1207it [00:02, 937.54it/s]warmup run: 1300it [00:02, 953.78it/s]warmup run: 1176it [00:02, 921.60it/s]warmup run: 1090it [00:02, 911.07it/s]warmup run: 707it [00:02, 724.86it/s]warmup run: 1075it [00:02, 884.55it/s]warmup run: 1185it [00:02, 922.56it/s]warmup run: 1203it [00:02, 936.46it/s]warmup run: 1310it [00:02, 961.97it/s]warmup run: 1400it [00:02, 958.56it/s]warmup run: 1274it [00:02, 938.09it/s]warmup run: 1189it [00:02, 928.30it/s]warmup run: 810it [00:02, 799.51it/s]warmup run: 1171it [00:02, 889.10it/s]warmup run: 1285it [00:02, 942.91it/s]warmup run: 1303it [00:02, 945.06it/s]warmup run: 1413it [00:02, 981.11it/s]warmup run: 1500it [00:03, 968.04it/s]warmup run: 1374it [00:02, 953.79it/s]warmup run: 1289it [00:02, 946.70it/s]warmup run: 911it [00:02, 852.64it/s]warmup run: 1266it [00:02, 902.01it/s]warmup run: 1385it [00:02, 959.39it/s]warmup run: 1402it [00:02, 951.82it/s]warmup run: 1516it [00:03, 992.43it/s]warmup run: 1600it [00:03, 975.24it/s]warmup run: 1473it [00:03, 963.06it/s]warmup run: 1391it [00:02, 966.61it/s]warmup run: 1011it [00:02, 889.60it/s]warmup run: 1360it [00:02, 909.72it/s]warmup run: 1484it [00:03, 965.81it/s]warmup run: 1500it [00:03, 959.36it/s]warmup run: 1618it [00:03, 998.17it/s]warmup run: 1701it [00:03, 983.27it/s]warmup run: 1572it [00:03, 969.49it/s]warmup run: 1494it [00:03, 984.14it/s]warmup run: 1111it [00:02, 917.10it/s]warmup run: 1454it [00:03, 917.69it/s]warmup run: 1583it [00:03, 966.66it/s]warmup run: 1598it [00:03, 963.90it/s]warmup run: 1721it [00:03, 1004.76it/s]warmup run: 1801it [00:03, 983.35it/s]warmup run: 1671it [00:03, 972.73it/s]warmup run: 1595it [00:03, 984.43it/s]warmup run: 1211it [00:02, 936.81it/s]warmup run: 1548it [00:03, 921.85it/s]warmup run: 1682it [00:03, 971.18it/s]warmup run: 1696it [00:03, 967.36it/s]warmup run: 1823it [00:03, 1004.36it/s]warmup run: 1901it [00:03, 987.05it/s]warmup run: 1770it [00:03, 977.40it/s]warmup run: 1697it [00:03, 994.84it/s]warmup run: 1311it [00:02, 946.07it/s]warmup run: 1643it [00:03, 929.94it/s]warmup run: 1783it [00:03, 979.57it/s]warmup run: 1794it [00:03, 969.91it/s]warmup run: 1925it [00:03, 1003.50it/s]warmup run: 2002it [00:03, 992.76it/s]warmup run: 1869it [00:03, 979.25it/s]warmup run: 1800it [00:03, 1003.61it/s]warmup run: 1410it [00:02, 954.53it/s]warmup run: 1737it [00:03, 932.13it/s]warmup run: 1883it [00:03, 985.19it/s]warmup run: 1892it [00:03, 965.96it/s]warmup run: 2030it [00:03, 1016.94it/s]warmup run: 2118it [00:03, 1041.97it/s]warmup run: 1968it [00:03, 981.78it/s]warmup run: 1902it [00:03, 1005.40it/s]warmup run: 1510it [00:02, 967.67it/s]warmup run: 1832it [00:03, 936.20it/s]warmup run: 1983it [00:03, 989.56it/s]warmup run: 1990it [00:03, 966.74it/s]warmup run: 2149it [00:03, 1066.12it/s]warmup run: 2234it [00:03, 1075.03it/s]warmup run: 2080it [00:03, 1021.67it/s]warmup run: 2004it [00:03, 1005.37it/s]warmup run: 1611it [00:03, 978.08it/s]warmup run: 1929it [00:03, 944.89it/s]warmup run: 2096it [00:03, 1029.62it/s]warmup run: 2109it [00:03, 1032.78it/s]warmup run: 2268it [00:03, 1100.61it/s]warmup run: 2350it [00:03, 1098.42it/s]warmup run: 2199it [00:03, 1070.28it/s]warmup run: 2120it [00:03, 1050.59it/s]warmup run: 1712it [00:03, 985.59it/s]warmup run: 2032it [00:03, 969.68it/s]warmup run: 2214it [00:03, 1072.05it/s]warmup run: 2232it [00:03, 1089.59it/s]warmup run: 2387it [00:03, 1126.55it/s]warmup run: 2460it [00:03, 1094.41it/s]warmup run: 2318it [00:03, 1105.08it/s]warmup run: 2237it [00:03, 1086.05it/s]warmup run: 1812it [00:03, 989.04it/s]warmup run: 2154it [00:03, 1043.10it/s]warmup run: 2331it [00:03, 1100.00it/s]warmup run: 2355it [00:03, 1130.03it/s]warmup run: 2506it [00:03, 1144.42it/s]warmup run: 2576it [00:04, 1113.70it/s]warmup run: 2437it [00:03, 1129.63it/s]warmup run: 2356it [00:03, 1114.86it/s]warmup run: 1914it [00:03, 995.87it/s]warmup run: 2276it [00:03, 1093.54it/s]warmup run: 2448it [00:03, 1119.85it/s]warmup run: 2477it [00:03, 1156.71it/s]warmup run: 2625it [00:04, 1156.78it/s]warmup run: 2690it [00:04, 1120.01it/s]warmup run: 2552it [00:04, 1135.58it/s]warmup run: 2475it [00:03, 1136.30it/s]warmup run: 2017it [00:03, 1005.00it/s]warmup run: 2398it [00:03, 1130.32it/s]warmup run: 2566it [00:04, 1136.73it/s]warmup run: 2599it [00:04, 1175.34it/s]warmup run: 2743it [00:04, 1161.63it/s]warmup run: 2808it [00:04, 1136.92it/s]warmup run: 2670it [00:04, 1146.88it/s]warmup run: 2596it [00:04, 1157.15it/s]warmup run: 2135it [00:03, 1056.47it/s]warmup run: 2520it [00:04, 1155.24it/s]warmup run: 2684it [00:04, 1148.25it/s]warmup run: 2722it [00:04, 1189.34it/s]warmup run: 2863it [00:04, 1171.21it/s]warmup run: 2927it [00:04, 1152.04it/s]warmup run: 2789it [00:04, 1159.49it/s]warmup run: 2716it [00:04, 1169.12it/s]warmup run: 2253it [00:03, 1092.64it/s]warmup run: 2640it [00:04, 1167.20it/s]warmup run: 3000it [00:04, 676.43it/s] warmup run: 2802it [00:04, 1157.24it/s]warmup run: 2843it [00:04, 1195.12it/s]warmup run: 2982it [00:04, 1176.71it/s]warmup run: 2908it [00:04, 1168.20it/s]warmup run: 3000it [00:04, 686.43it/s] warmup run: 2837it [00:04, 1178.57it/s]warmup run: 2371it [00:03, 1116.15it/s]warmup run: 2762it [00:04, 1180.48it/s]warmup run: 2920it [00:04, 1161.62it/s]warmup run: 2966it [00:04, 1204.22it/s]warmup run: 3000it [00:04, 681.34it/s] warmup run: 3000it [00:04, 684.79it/s] warmup run: 2958it [00:04, 1185.12it/s]warmup run: 2489it [00:03, 1134.09it/s]warmup run: 3000it [00:04, 678.21it/s] warmup run: 3000it [00:04, 686.30it/s] warmup run: 2881it [00:04, 1175.85it/s]warmup run: 2608it [00:03, 1149.50it/s]warmup run: 3000it [00:04, 674.83it/s] warmup run: 2724it [00:04, 1076.00it/s]warmup run: 2833it [00:04, 1076.48it/s]warmup run: 2951it [00:04, 1104.66it/s]warmup run: 3000it [00:04, 686.42it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1587.51it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.52it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1605.39it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1594.00it/s]warmup should be done:   5%|         | 157/3000 [00:00<00:01, 1561.68it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1642.65it/s]warmup should be done:   5%|         | 156/3000 [00:00<00:01, 1553.54it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1650.31it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1653.46it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1619.10it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1607.29it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1618.02it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1605.63it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1648.21it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1657.83it/s]warmup should be done:  11%|         | 319/3000 [00:00<00:01, 1593.12it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1618.28it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1607.24it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1618.59it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1658.03it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1614.04it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1598.99it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1639.19it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1556.34it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1617.20it/s]warmup should be done:  21%|       | 644/3000 [00:00<00:01, 1606.34it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1620.29it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1659.42it/s]warmup should be done:  21%|       | 643/3000 [00:00<00:01, 1607.86it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1608.28it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1628.26it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1585.48it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1619.95it/s]warmup should be done:  27%|       | 805/3000 [00:00<00:01, 1605.96it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1658.65it/s]warmup should be done:  27%|       | 807/3000 [00:00<00:01, 1617.35it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1617.83it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1604.96it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1619.80it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1601.79it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1619.80it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1657.82it/s]warmup should be done:  32%|      | 966/3000 [00:00<00:01, 1602.03it/s]warmup should be done:  32%|      | 969/3000 [00:00<00:01, 1617.03it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1610.50it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1596.73it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1609.91it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1605.03it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1627.30it/s]warmup should be done:  38%|      | 1127/3000 [00:00<00:01, 1601.89it/s]warmup should be done:  38%|      | 1133/3000 [00:00<00:01, 1621.59it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1651.52it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1609.45it/s]warmup should be done:  38%|      | 1130/3000 [00:00<00:01, 1595.35it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1608.45it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1613.29it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1631.04it/s]warmup should be done:  43%|     | 1288/3000 [00:00<00:01, 1601.95it/s]warmup should be done:  43%|     | 1297/3000 [00:00<00:01, 1625.10it/s]warmup should be done:  43%|     | 1297/3000 [00:00<00:01, 1610.18it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1643.37it/s]warmup should be done:  43%|     | 1290/3000 [00:00<00:01, 1594.34it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1612.79it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1620.32it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1633.21it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1626.47it/s]warmup should be done:  48%|     | 1449/3000 [00:00<00:00, 1601.55it/s]warmup should be done:  49%|     | 1459/3000 [00:00<00:00, 1611.10it/s]warmup should be done:  48%|     | 1450/3000 [00:00<00:00, 1593.41it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1637.05it/s]warmup should be done:  49%|     | 1472/3000 [00:00<00:00, 1615.53it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1610.82it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1634.08it/s]warmup should be done:  54%|    | 1623/3000 [00:01<00:00, 1626.52it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1598.81it/s]warmup should be done:  54%|    | 1621/3000 [00:01<00:00, 1611.51it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1592.94it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1631.98it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1616.80it/s]warmup should be done:  55%|    | 1642/3000 [00:01<00:00, 1616.85it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1635.33it/s]warmup should be done:  60%|    | 1786/3000 [00:01<00:00, 1625.71it/s]warmup should be done:  59%|    | 1770/3000 [00:01<00:00, 1598.65it/s]warmup should be done:  59%|    | 1783/3000 [00:01<00:00, 1611.44it/s]warmup should be done:  59%|    | 1770/3000 [00:01<00:00, 1593.46it/s]warmup should be done:  60%|    | 1797/3000 [00:01<00:00, 1618.80it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1629.38it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1617.29it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1635.34it/s]warmup should be done:  65%|   | 1949/3000 [00:01<00:00, 1625.74it/s]warmup should be done:  64%|   | 1931/3000 [00:01<00:00, 1600.02it/s]warmup should be done:  65%|   | 1945/3000 [00:01<00:00, 1609.55it/s]warmup should be done:  64%|   | 1930/3000 [00:01<00:00, 1593.28it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1617.26it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1625.60it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1614.74it/s]warmup should be done:  71%|   | 2123/3000 [00:01<00:00, 1637.36it/s]warmup should be done:  70%|   | 2093/3000 [00:01<00:00, 1605.92it/s]warmup should be done:  70%|   | 2112/3000 [00:01<00:00, 1620.50it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1607.58it/s]warmup should be done:  70%|   | 2090/3000 [00:01<00:00, 1592.43it/s]warmup should be done:  72%|  | 2149/3000 [00:01<00:00, 1626.42it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1612.41it/s]warmup should be done:  71%|   | 2128/3000 [00:01<00:00, 1611.38it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1635.53it/s]warmup should be done:  75%|  | 2255/3000 [00:01<00:00, 1609.10it/s]warmup should be done:  76%|  | 2275/3000 [00:01<00:00, 1614.02it/s]warmup should be done:  76%|  | 2267/3000 [00:01<00:00, 1603.69it/s]warmup should be done:  75%|  | 2250/3000 [00:01<00:00, 1589.19it/s]warmup should be done:  77%|  | 2312/3000 [00:01<00:00, 1625.44it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1606.52it/s]warmup should be done:  76%|  | 2290/3000 [00:01<00:00, 1607.46it/s]warmup should be done:  82%| | 2451/3000 [00:01<00:00, 1635.07it/s]warmup should be done:  81%|  | 2418/3000 [00:01<00:00, 1614.91it/s]warmup should be done:  81%|  | 2437/3000 [00:01<00:00, 1613.78it/s]warmup should be done:  81%|  | 2428/3000 [00:01<00:00, 1604.39it/s]warmup should be done:  80%|  | 2410/3000 [00:01<00:00, 1590.11it/s]warmup should be done:  82%| | 2475/3000 [00:01<00:00, 1622.11it/s]warmup should be done:  81%| | 2444/3000 [00:01<00:00, 1605.66it/s]warmup should be done:  82%| | 2451/3000 [00:01<00:00, 1605.92it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1634.40it/s]warmup should be done:  86%| | 2581/3000 [00:01<00:00, 1618.85it/s]warmup should be done:  87%| | 2601/3000 [00:01<00:00, 1618.69it/s]warmup should be done:  86%| | 2589/3000 [00:01<00:00, 1606.02it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1590.56it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1622.28it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1604.38it/s]warmup should be done:  87%| | 2612/3000 [00:01<00:00, 1601.50it/s]warmup should be done:  93%|| 2779/3000 [00:01<00:00, 1633.74it/s]warmup should be done:  91%|| 2744/3000 [00:01<00:00, 1620.09it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1623.24it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1608.32it/s]warmup should be done:  91%| | 2730/3000 [00:01<00:00, 1591.75it/s]warmup should be done:  93%|| 2801/3000 [00:01<00:00, 1622.61it/s]warmup should be done:  92%|| 2766/3000 [00:01<00:00, 1603.81it/s]warmup should be done:  92%|| 2773/3000 [00:01<00:00, 1598.85it/s]warmup should be done:  98%|| 2944/3000 [00:01<00:00, 1638.26it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1625.35it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1629.11it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1613.64it/s]warmup should be done:  96%|| 2892/3000 [00:01<00:00, 1597.64it/s]warmup should be done:  99%|| 2965/3000 [00:01<00:00, 1626.50it/s]warmup should be done:  98%|| 2928/3000 [00:01<00:00, 1608.62it/s]warmup should be done:  98%|| 2935/3000 [00:01<00:00, 1602.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1630.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1619.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1614.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.00it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1607.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1597.23it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1679.09it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1664.66it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.90it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1683.77it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1682.66it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.37it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.58it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1649.90it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1683.29it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1665.41it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1669.05it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.84it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1657.97it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1681.28it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1679.59it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1633.12it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1670.77it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1687.13it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1660.88it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1669.99it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1644.94it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1676.67it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1680.59it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1645.45it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1673.22it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1662.29it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1645.05it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1677.37it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1685.33it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1678.89it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1663.88it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1648.16it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1673.25it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1680.76it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1660.17it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1688.12it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1641.61it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1664.10it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1673.29it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1649.64it/s]warmup should be done:  34%|      | 1006/3000 [00:00<00:01, 1670.50it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1680.84it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1643.61it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1665.37it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1661.24it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1689.02it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1673.59it/s]warmup should be done:  33%|      | 997/3000 [00:00<00:01, 1654.62it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1665.82it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1669.42it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1646.38it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1661.69it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1686.90it/s]warmup should be done:  39%|      | 1180/3000 [00:00<00:01, 1672.74it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1657.74it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1667.30it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1667.31it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1663.24it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1669.65it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1643.79it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1688.22it/s]warmup should be done:  45%|     | 1348/3000 [00:00<00:00, 1673.72it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1655.45it/s]warmup should be done:  45%|     | 1348/3000 [00:00<00:00, 1662.03it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1667.51it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1663.36it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1645.50it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1666.35it/s]warmup should be done:  51%|     | 1516/3000 [00:00<00:00, 1672.56it/s]warmup should be done:  51%|     | 1525/3000 [00:00<00:00, 1683.30it/s]warmup should be done:  50%|     | 1496/3000 [00:00<00:00, 1656.15it/s]warmup should be done:  50%|     | 1515/3000 [00:00<00:00, 1656.88it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1664.88it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1669.15it/s]warmup should be done:  55%|    | 1652/3000 [00:01<00:00, 1648.16it/s]warmup should be done:  56%|    | 1684/3000 [00:01<00:00, 1673.79it/s]warmup should be done:  56%|    | 1695/3000 [00:01<00:00, 1686.24it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1660.80it/s]warmup should be done:  55%|    | 1663/3000 [00:01<00:00, 1658.98it/s]warmup should be done:  56%|    | 1681/3000 [00:01<00:00, 1654.69it/s]warmup should be done:  61%|    | 1836/3000 [00:01<00:00, 1665.79it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1670.91it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1648.47it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1678.72it/s]warmup should be done:  62%|   | 1865/3000 [00:01<00:00, 1688.28it/s]warmup should be done:  61%|    | 1830/3000 [00:01<00:00, 1659.92it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1657.24it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1653.79it/s]warmup should be done:  67%|   | 2003/3000 [00:01<00:00, 1664.49it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1646.30it/s]warmup should be done:  67%|   | 2010/3000 [00:01<00:00, 1670.53it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1681.05it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1688.52it/s]warmup should be done:  67%|   | 1996/3000 [00:01<00:00, 1656.95it/s]warmup should be done:  67%|   | 2009/3000 [00:01<00:00, 1653.52it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1650.66it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1646.94it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1663.92it/s]warmup should be done:  73%|  | 2178/3000 [00:01<00:00, 1670.26it/s]warmup should be done:  73%|  | 2203/3000 [00:01<00:00, 1687.88it/s]warmup should be done:  73%|  | 2192/3000 [00:01<00:00, 1681.43it/s]warmup should be done:  72%|  | 2162/3000 [00:01<00:00, 1656.58it/s]warmup should be done:  72%|  | 2175/3000 [00:01<00:00, 1648.86it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1647.88it/s]warmup should be done:  78%|  | 2337/3000 [00:01<00:00, 1664.49it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1648.29it/s]warmup should be done:  78%|  | 2346/3000 [00:01<00:00, 1670.32it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1688.94it/s]warmup should be done:  79%|  | 2361/3000 [00:01<00:00, 1680.96it/s]warmup should be done:  78%|  | 2329/3000 [00:01<00:00, 1658.23it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1650.24it/s]warmup should be done:  78%|  | 2346/3000 [00:01<00:00, 1653.06it/s]warmup should be done:  83%| | 2504/3000 [00:01<00:00, 1664.26it/s]warmup should be done:  83%| | 2478/3000 [00:01<00:00, 1647.51it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1669.54it/s]warmup should be done:  85%| | 2543/3000 [00:01<00:00, 1691.01it/s]warmup should be done:  84%| | 2530/3000 [00:01<00:00, 1681.39it/s]warmup should be done:  83%| | 2495/3000 [00:01<00:00, 1658.28it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1652.10it/s]warmup should be done:  84%| | 2513/3000 [00:01<00:00, 1657.84it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1662.39it/s]warmup should be done:  88%| | 2643/3000 [00:01<00:00, 1644.57it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1669.23it/s]warmup should be done:  90%| | 2713/3000 [00:01<00:00, 1692.82it/s]warmup should be done:  89%| | 2661/3000 [00:01<00:00, 1656.38it/s]warmup should be done:  90%| | 2699/3000 [00:01<00:00, 1678.16it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1651.96it/s]warmup should be done:  89%| | 2680/3000 [00:01<00:00, 1659.54it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1645.88it/s]warmup should be done:  95%|| 2838/3000 [00:01<00:00, 1661.87it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1669.25it/s]warmup should be done:  96%|| 2883/3000 [00:01<00:00, 1691.28it/s]warmup should be done:  94%|| 2827/3000 [00:01<00:00, 1656.10it/s]warmup should be done:  96%|| 2867/3000 [00:01<00:00, 1672.87it/s]warmup should be done:  95%|| 2839/3000 [00:01<00:00, 1649.43it/s]warmup should be done:  95%|| 2846/3000 [00:01<00:00, 1659.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.16it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1668.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.86it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.31it/s]warmup should be done:  99%|| 2974/3000 [00:01<00:00, 1647.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.70it/s]warmup should be done: 100%|| 2994/3000 [00:01<00:00, 1657.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1654.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.48it/s]2022-12-12 02:29:08.107724: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f958b830e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:08.107784: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.148671: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f958382d380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.148734: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.149062: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9587830ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.149112: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.813109: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78dc0305b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.813177: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.852701: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f957ff93380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.852770: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.853083: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f958b82cc90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.853131: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.909597: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f957f833ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.909673: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:09.915247: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f780402e040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:29:09.915304: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:29:10.374424: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:11.445041: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:11.449084: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:12.120250: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:12.186911: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:12.195552: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:12.224361: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:12.237760: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:29:13.268016: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:14.374407: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:14.414146: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:15.083867: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:15.132682: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:15.134522: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:15.135026: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:29:15.202286: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:29:43.349][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:29:43.349][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.357][ERROR][RK0][main]: coll ps creation done
[HCTR][02:29:43.358][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][02:29:43.374][ERROR][RK0][tid #140280518919936]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:29:43.374][ERROR][RK0][tid #140280518919936]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.379][ERROR][RK0][tid #140280518919936]: coll ps creation done
[HCTR][02:29:43.379][ERROR][RK0][tid #140280518919936]: replica 2 waits for coll ps creation barrier
[HCTR][02:29:43.419][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:29:43.419][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.424][ERROR][RK0][main]: coll ps creation done
[HCTR][02:29:43.424][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][02:29:43.484][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:29:43.484][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.491][ERROR][RK0][main]: coll ps creation done
[HCTR][02:29:43.492][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:29:43.515][ERROR][RK0][tid #140280586028800]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:29:43.515][ERROR][RK0][tid #140280586028800]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.523][ERROR][RK0][tid #140280586028800]: coll ps creation done
[HCTR][02:29:43.523][ERROR][RK0][tid #140280586028800]: replica 1 waits for coll ps creation barrier
[HCTR][02:29:43.548][ERROR][RK0][tid #140280518919936]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:29:43.548][ERROR][RK0][tid #140280518919936]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.552][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:29:43.552][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.553][ERROR][RK0][tid #140280518919936]: coll ps creation done
[HCTR][02:29:43.553][ERROR][RK0][tid #140280518919936]: replica 7 waits for coll ps creation barrier
[HCTR][02:29:43.560][ERROR][RK0][main]: coll ps creation done
[HCTR][02:29:43.560][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][02:29:43.578][ERROR][RK0][tid #140280510527232]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:29:43.579][ERROR][RK0][tid #140280510527232]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][02:29:43.586][ERROR][RK0][tid #140280510527232]: coll ps creation done
[HCTR][02:29:43.586][ERROR][RK0][tid #140280510527232]: replica 0 waits for coll ps creation barrier
[HCTR][02:29:43.586][ERROR][RK0][tid #140280510527232]: replica 0 preparing frequency
[HCTR][02:29:44.531][ERROR][RK0][tid #140280510527232]: replica 0 preparing frequency done
[HCTR][02:29:44.576][ERROR][RK0][tid #140280510527232]: replica 0 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][tid #140280518919936]: replica 2 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][tid #140280586028800]: replica 1 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][tid #140280518919936]: replica 7 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][02:29:44.576][ERROR][RK0][tid #140280510527232]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][main]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][main]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][main]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][tid #140280518919936]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][tid #140280586028800]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][tid #140280518919936]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][tid #140280510527232]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][main]: Calling build_v2
[HCTR][02:29:44.576][ERROR][RK0][tid #140280518919936]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][tid #140280518919936]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][tid #140280586028800]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:29:44.576][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 02:29:44[2022-12-12 02:29:442022-12-12 02:29:442022-12-12 02:29:442022-12-12 02:29:44.2022-12-12 02:29:442022-12-12 02:29:44....576274..2022-12-12 02:29:44576292576296576294576297: 576295576291.: : : : E: : 576314EEEE EE:     /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ::::136::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136136] 136136:] ] ] ] using concurrent impl MPS] ] 136using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS] 





using concurrent impl MPS
[2022-12-12 02:29:44.580569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:29:44.580607: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:29:44:.196580614] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:29:44.580662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [assigning 8 to cpu2022-12-12 02:29:44
[.2022-12-12 02:29:44580665.: 580694E:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:29:44 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178580706:[] : 2122022-12-12 02:29:44v100x8, slow pcieE] .
 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[580736/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[2022-12-12 02:29:44: :2022-12-12 02:29:44.E178.580754 ] 580769[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie: 2022-12-12 02:29:44E:[
E. 2122022-12-12 02:29:44 580796/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :[2022-12-12 02:29:44build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8580805:E1782022-12-12 02:29:44.
: 196 [[] .580838E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:29:442022-12-12 02:29:44v100x8, slow pcie580850:  assigning 8 to cpu:..
: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
213580893580924E[ :] : :  2022-12-12 02:29:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178remote time is 8.68421EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[] 
  :5809991962022-12-12 02:29:44v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178: ] .
:2022-12-12 02:29:44:] Eassigning 8 to cpu581059178[.213v100x8, slow pcie 
: ] 2022-12-12 02:29:44581101] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie.: remote time is 8.68421:[ 
581150E
[1962022-12-12 02:29:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  [2022-12-12 02:29:44] [.:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:29:44.assigning 8 to cpu2022-12-12 02:29:44581214212 :.581227
.: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214581249: 581258Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:] : E: [ 
196cpu time is 97.0588E E2022-12-12 02:29:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .:assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:29:44:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc581371196
:.212:: ] 196581418] 214Eassigning 8 to cpu] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]  [
assigning 8 to cpuE
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:29:44
 
:.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2125815252022-12-12 02:29:44:[] : .213[2022-12-12 02:29:44build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E581567] 2022-12-12 02:29:44.
 : remote time is 8.68421.581586/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[
581604: : 2022-12-12 02:29:44: E[212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E 2022-12-12 02:29:44] :581665 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:581693
] E:212: remote time is 8.68421 212] E[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-12 02:29:44:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.213
2022-12-12 02:29:44:581794] [.214[: remote time is 8.684212022-12-12 02:29:44581825] 2022-12-12 02:29:44E
.: cpu time is 97.0588. 581867E[
581881/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  2022-12-12 02:29:44: :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E213 :581936 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:] E:
213cpu time is 97.0588 213] 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.684212022-12-12 02:29:44:remote time is 8.68421
.214
582043[] : 2022-12-12 02:29:44[cpu time is 97.0588E.2022-12-12 02:29:44
 582090./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 582102:E: 214 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 02:31:03.330730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:31:03.371232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 02:31:03.371291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 10000000
[2022-12-12 02:31:03.482640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:31:03.482731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:31:03.506728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:31:03.506766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:31:03.507236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.508199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.508996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[[2022-12-12 02:31:032022-12-12 02:31:032022-12-12 02:31:03...522265522276522275: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:::202202202] ] ] 3 solved6 solved7 solved


[2022-12-12 02:31:03[[.2022-12-12 02:31:032022-12-12 02:31:03522460..: 522464522467E: :  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205::] 205205worker 0 thread 3 initing device 3] ] 
worker 0 thread 6 initing device 6worker 0 thread 7 initing device 7

[[2022-12-12 02:31:032022-12-12 02:31:03..522753522759: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved5 solved

[[2022-12-12 02:31:032022-12-12 02:31:03..522866522869: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 5 initing device 5

[2022-12-12 02:31:03.522966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 02:31:032022-12-12 02:31:03..523001523002: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 02:31:03.523307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 02:31:031980.] 523321eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.525191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 02:31:03.525246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 02:31:03.525481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 02:31:03.525532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 02:31:03.525626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.525905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.526887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.527526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.527594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.527642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.527720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.530298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.530665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.531411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.531475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.532057: [E2022-12-12 02:31:03 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu532081:: 1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.532198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.534889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.535280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:31:03.589218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 02:31:03.589582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:31:03.594535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:31:03.594601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 02:31:03.594643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:31:03.595421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.595880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.596883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.596968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:31:03.597637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:31:03.597677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:31:03.614426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 02:31:03.614731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:31:03.618614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 02:31:03.618906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:31:03.619620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:31:03.619686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 02:31:03.619728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:31:03.620478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.620908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.621900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.621983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:31:03.622653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:31:03.622693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:31:03.623682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:31:03.623743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 02:31:03.623784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:31:03.624490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[[[2022-12-12 02:31:032022-12-12 02:31:032022-12-12 02:31:032022-12-12 02:31:03....624556624556624556624556: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[2022-12-12 02:31:03.624817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 02:31:03[2022-12-12 02:31:03.2022-12-12 02:31:03.624946.[624948: 6249522022-12-12 02:31:03: E: .E E624962 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:1980: 1980] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 1024.00 Bytes] :eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes1980

] eager alloc mem 1024.00 Bytes
[2022-12-12 02:31:03.630393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.631478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.632282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:31:03.632357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 02:31:03.632366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 02:31:03638.] 632404eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:31:03[.2022-12-12 02:31:03632445.: 632436E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[:] 2022-12-12 02:31:03638eager release cuda mem 2.] 
632476eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[2022-12-12 02:31:036382022-12-12 02:31:03.[] .6325342022-12-12 02:31:03eager release cuda mem 625663632522: .
: E632553E [:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:31:03E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:. :638632585/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] : :] eager release cuda mem 400000000E638eager release cuda mem 1024
 ] [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 22022-12-12 02:31:03:
.638632647[] : [2022-12-12 02:31:03eager release cuda mem 1024E2022-12-12 02:31:03.
 .632687/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu632701: :: E[1980E 2022-12-12 02:31:03]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:632745
:638: 638] E] eager release cuda mem 2 eager release cuda mem 400000000
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 2
[2022-12-12 02:31:03.632838: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:31:03:.638632851] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:31:03.633441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:31:03.633483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:31:03.634143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.634651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.641191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.641703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.642224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:31:03.643426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.643470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.643732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.643777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.643813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:03.644426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.644465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.644509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:31:03.644545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:31:03.644716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.644769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.644797: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:31:03:.1980644807] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:03.644849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:31:03.644901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:31:03.645181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:31:03[.2022-12-12 02:31:03645215.: 645219E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 25855] 
eager alloc mem 4.77 GB
[2022-12-12 02:31:03.645274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:31:03.645477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:31:03[.2022-12-12 02:31:03645517.: 645518E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 4.77 GB] 
eager release cuda mem 25855
[2022-12-12 02:31:03.[6455722022-12-12 02:31:03: .E645578 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 258551980
] eager alloc mem 4.77 GB
[2022-12-12 02:31:03.645630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:04........616211616211616213616211616211616227616228616227: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB






[[[[2022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:04[....2022-12-12 02:31:04617343617347[617343[[617346.: : 2022-12-12 02:31:04: 2022-12-12 02:31:042022-12-12 02:31:04: 617357EE.E..E:   617368 617371617372 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ::E:EE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638 638  638:] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638eager release cuda mem 625663eager release cuda mem 625663:eager release cuda mem 625663::eager release cuda mem 625663] 

638
638638
eager release cuda mem 625663] ] ] 
eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[[2022-12-12 02:31:042022-12-12 02:31:04.[.6176442022-12-12 02:31:04617648: [.: E2022-12-12 02:31:04617654E [.:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:31:04[617665E[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:.2022-12-12 02:31:04:  2022-12-12 02:31:042022-12-12 02:31:04:1980617678.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..1980] : 617687 :617690617691] eager alloc mem 611.00 KBE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980: : eager alloc mem 611.00 KB
 E:] EE
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980eager alloc mem 611.00 KB  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:eager alloc mem 611.00 KB::] 1980
19801980eager alloc mem 611.00 KB] ] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 02:31:04.618547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 02:31:04
.618568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.618598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.618625: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:31:04:.1980618636[] : 2022-12-12 02:31:04eager alloc mem 611.00 KBE.[
 6186482022-12-12 02:31:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: .[[[:2022-12-12 02:31:04E6186602022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:04638. : ...] 618670/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE618670618670618670eager release cuda mem 625663: : : : : 
E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEEE ] :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
] :[::1980eager release cuda mem 6256636382022-12-12 02:31:04638638] 
] .] ] eager alloc mem 611.00 KBeager release cuda mem 625663618814eager release cuda mem 625663eager release cuda mem 625663

: 

E[ 2022-12-12 02:31:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6189131980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[:[2022-12-12 02:31:042022-12-12 02:31:0419802022-12-12 02:31:04..] .618952618958eager alloc mem 611.00 KB618962: : 
: EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 02:31:04.619398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.619471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.619555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:31:042022-12-12 02:31:04..619628619628: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663[

2022-12-12 02:31:04.619671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.619725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 02:31:04.[6197472022-12-12 02:31:04: .E619751 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663[[[
2022-12-12 02:31:042022-12-12 02:31:042022-12-12 02:31:04...619793619794619797: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::638638[638] ] 2022-12-12 02:31:04] eager release cuda mem 625663eager release cuda mem 625663.eager release cuda mem 625663

619856
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:31:042022-12-12 02:31:04.[.6199412022-12-12 02:31:04619944: .: E619951E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] :] eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB
] 
eager alloc mem 611.00 KB
[2022-12-12 02:31:04.620219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.620286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.620410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.[6204742022-12-12 02:31:04: .E620481 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] [eager alloc mem 611.00 KB2022-12-12 02:31:04
.620518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.620561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.620590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.620648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.620715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:31:04[2022-12-12 02:31:04.2022-12-12 02:31:04.620738.620739: 620744: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:638] 638] eager release cuda mem 625663] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-12 02:31:04[.[2022-12-12 02:31:046208602022-12-12 02:31:04.: .620864E620867:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1980eager alloc mem 611.00 KB1980] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 02:31:04.621033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.621102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.621253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.621309: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:31:04:.638621322] : [eager release cuda mem 625663E2022-12-12 02:31:04
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu621342:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 02:31:04.621399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.621431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.621464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.621532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.[621649[2022-12-12 02:31:04: 2022-12-12 02:31:04.E.621658 621660: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E:E 638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663:638
638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 02:31:04.621756: [E[2022-12-12 02:31:04 2022-12-12 02:31:04./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.621767:621769: 1980: E] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 02:31:04.621850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.621919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.622107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.622146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04[.2022-12-12 02:31:04622177.: 622180E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:[] 6382022-12-12 02:31:04eager alloc mem 611.00 KB] .
eager release cuda mem 625663622212
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:31:042022-12-12 02:31:04..622276622278: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-12 02:31:04.622373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:31:04.622535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] [2022-12-12 02:31:04eager release cuda mem 6256632022-12-12 02:31:04.
.622555622557: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 02:31:04.622624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:31:042022-12-12 02:31:04[..2022-12-12 02:31:04622660622662.: : 622665EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc19801980:] ] 638eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 

eager release cuda mem 625663
[2022-12-12 02:31:04.622797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:31:04.622959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:31:04eager release cuda mem 625663.
622978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:31:04eager release cuda mem 625663.
623004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:31:04eager release cuda mem 40400000.
623028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 404000002022-12-12 02:31:04
.623052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.623093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:31:04.623118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.623163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:31:04.623281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10029 secs 
[2022-12-12 02:31:04.623375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:31:04.623415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:31:04.623501[: 2022-12-12 02:31:04E. 623508/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 02:31:04.623557: [E2022-12-12 02:31:04 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc623564:: 638E]  eager release cuda mem 40400000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 40400000
[2022-12-12 02:31:04.623930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10094 secs 
[2022-12-12 02:31:04.624266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09837 secs 
[2022-12-12 02:31:04.624405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09878 secs 
[2022-12-12 02:31:04.624642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10134 secs 
[2022-12-12 02:31:04.625304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10234 secs 
[2022-12-12 02:31:04.625770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10246 secs 
[2022-12-12 02:31:04.626779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.11955 secs 
[2022-12-12 02:31:04.629105: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.41 GB
[2022-12-12 02:31:06. 76327: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.67 GB
[2022-12-12 02:31:06. 77173: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.67 GB
[2022-12-12 02:31:06. 82033: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.67 GB
[2022-12-12 02:31:07.391095: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.93 GB
[2022-12-12 02:31:07.391696: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.93 GB
[2022-12-12 02:31:07.393283: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.93 GB
[2022-12-12 02:31:08.480683: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.15 GB
[2022-12-12 02:31:08.480905: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.15 GB
[2022-12-12 02:31:08.481274: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.15 GB
[2022-12-12 02:31:09.868811: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.36 GB
[2022-12-12 02:31:09.868954: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.36 GB
[2022-12-12 02:31:09.869244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.36 GB
[2022-12-12 02:31:11.155482: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 12.82 GB
[2022-12-12 02:31:11.156156: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 12.82 GB
[2022-12-12 02:31:11.156832: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 12.82 GB
[2022-12-12 02:31:12.210626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.02 GB
[2022-12-12 02:31:12.211305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.02 GB
[HCTR][02:31:13.344][ERROR][RK0][tid #140280518919936]: replica 2 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][tid #140280586028800]: replica 1 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][tid #140280510527232]: replica 0 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][tid #140280518919936]: replica 7 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][02:31:13.344][ERROR][RK0][tid #140280518919936]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][tid #140280586028800]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][tid #140280518919936]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][tid #140280510527232]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:31:13.344][ERROR][RK0][tid #140280518919936]: init per replica done
[HCTR][02:31:13.344][ERROR][RK0][main]: init per replica done
[HCTR][02:31:13.344][ERROR][RK0][main]: init per replica done
[HCTR][02:31:13.344][ERROR][RK0][tid #140280586028800]: init per replica done
[HCTR][02:31:13.344][ERROR][RK0][main]: init per replica done
[HCTR][02:31:13.344][ERROR][RK0][tid #140280518919936]: init per replica done
[HCTR][02:31:13.344][ERROR][RK0][main]: init per replica done
[HCTR][02:31:13.347][ERROR][RK0][tid #140280510527232]: init per replica done
[HCTR][02:31:13.350][ERROR][RK0][tid #140280586028800]: 1 allocated 3276800 at 0x7f9777d20000
[HCTR][02:31:13.350][ERROR][RK0][tid #140280586028800]: 1 allocated 6553600 at 0x7f9778200000
[HCTR][02:31:13.350][ERROR][RK0][tid #140280586028800]: 1 allocated 3276800 at 0x7f9778840000
[HCTR][02:31:13.350][ERROR][RK0][tid #140280586028800]: 1 allocated 6553600 at 0x7f9778b60000
[HCTR][02:31:13.350][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f9771d20000
[HCTR][02:31:13.350][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f9772200000
[HCTR][02:31:13.350][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f9772840000
[HCTR][02:31:13.350][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f9772b60000
[HCTR][02:31:13.350][ERROR][RK0][tid #140280518919936]: 7 allocated 3276800 at 0x7f9777d20000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 7 allocated 6553600 at 0x7f9778200000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 2 allocated 3276800 at 0x7f9777d20000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 7 allocated 3276800 at 0x7f9778840000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 2 allocated 6553600 at 0x7f9778200000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 7 allocated 6553600 at 0x7f9778b60000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 2 allocated 3276800 at 0x7f9778840000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280518919936]: 2 allocated 6553600 at 0x7f9778b60000
[HCTR][02:31:13.351][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f9773d20000
[HCTR][02:31:13.350][ERROR][RK0][tid #140280577636096]: 3 allocated 3276800 at 0x7f9777d20000
[HCTR][02:31:13.351][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f9774200000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280577636096]: 3 allocated 6553600 at 0x7f9778200000
[HCTR][02:31:13.351][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f9774840000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280577636096]: 3 allocated 3276800 at 0x7f9778840000
[HCTR][02:31:13.351][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f9774b60000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280577636096]: 3 allocated 6553600 at 0x7f9778b60000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280703461120]: 5 allocated 3276800 at 0x7f9765d20000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280703461120]: 5 allocated 6553600 at 0x7f9766200000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280703461120]: 5 allocated 3276800 at 0x7f9766840000
[HCTR][02:31:13.351][ERROR][RK0][tid #140280703461120]: 5 allocated 6553600 at 0x7f9766b60000
[HCTR][02:31:13.353][ERROR][RK0][tid #140280510527232]: 0 allocated 3276800 at 0x7f977a520000
[HCTR][02:31:13.354][ERROR][RK0][tid #140280510527232]: 0 allocated 6553600 at 0x7f977aa00000
[HCTR][02:31:13.354][ERROR][RK0][tid #140280510527232]: 0 allocated 3276800 at 0x7f977b70e800
[HCTR][02:31:13.354][ERROR][RK0][tid #140280510527232]: 0 allocated 6553600 at 0x7f977ba2e800








