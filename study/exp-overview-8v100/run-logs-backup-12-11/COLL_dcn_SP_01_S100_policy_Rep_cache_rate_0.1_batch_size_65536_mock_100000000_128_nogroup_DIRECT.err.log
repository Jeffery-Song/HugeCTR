2022-12-12 02:57:59.209776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.215697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.222122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.229199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.235500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.248995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.256611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.269780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.322909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.325517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.327173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.329068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.331311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.332650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.333081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.335166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.335365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.337615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.337649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.340032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.340083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.341995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.342334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.344107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.344632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.346163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.346812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.348295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.348969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.350388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.352207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.353738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.356162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.358921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.361042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.362387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.363447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.364551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.365578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.367374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.373136: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.377726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.379277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.380791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.381036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.381846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.382226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.382940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.384252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.384421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.385072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.386533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.386773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.387324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.389038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.389360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.391285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.391662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.394085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.395846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.396179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.398722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.400679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.401082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.403650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.403663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.403947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.406245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.406467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.406946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.408989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.409505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.409912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.410871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.417034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.419087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.420104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.421520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.422309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.422406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.423813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.424235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.425827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.426354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.426768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.428653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.438598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.442137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.461653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.463507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.463573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.464036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.467071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.467207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.467490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.468736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.470345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.470878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.471680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.473238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.473369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.475365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.476354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.477050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.477686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.479637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.479694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.481769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.482988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.483382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.484029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.484120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.486476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.487880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.488274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.488558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.488605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.491000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.493259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.493301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.493344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.495044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.496775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.497008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.499585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.499751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.499938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.502562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.502645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.502774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.506533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.506679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.506721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.509762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.509833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.509876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.513287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.513289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.513350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.516338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.516373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.516470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.518776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.519347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.519353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.519382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.522268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.522775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.523114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.523162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.525344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.526930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.528239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.528839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.529594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.530512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.531837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.532514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.532541: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.533272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.533924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.535205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.535812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.536836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.537002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.537582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.538956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.539743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.541043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.541081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.541215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.541679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.541803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.543850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.545882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.547600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.547741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.547770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.548472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.548628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.550105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.551500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.553188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.553534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.553597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.554096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.554271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.556439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.556971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.558474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.558794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.558834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.559252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.562794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.563303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.564435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.564526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.564690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.565125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.569701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.569774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.569817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.570466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.571376: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.572232: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.574721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.574856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.574875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.575241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.578991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.578992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.579105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.579342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.580517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.580826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.583725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.583773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.584223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.584537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.586453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.586568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.589461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.589702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.589874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.590181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.592331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.592548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.596244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.596487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.596754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.598349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.601252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.601541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.601947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.604703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.607829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.608251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.608873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.610082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.612786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.622644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.651076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.653323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.655819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.656544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.657175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.659089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.662141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.663911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.664116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.667176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.698219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.698424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.698602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.701556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.705632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.705952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.706094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.707211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.714222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.714560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.714653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.719311: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.719724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.720011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.720045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.723867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.724439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.727612: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.728423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.736390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.742125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.742618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.743186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.744536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.748968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.749569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.749858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.750824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.815459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.817792: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.824879: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:57:59.826663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.833531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.833531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.840624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.840723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:57:59.848729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.777361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.778031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.778744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.779233: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:00.779289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:58:00.797451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.798077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.799575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.800154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.800682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:00.802127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:58:00.844359: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:00.844560: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:00.880453: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:58:01.035883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.036859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.037401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.037871: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.037926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:58:01.055718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.056352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.056860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.057444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.058174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.058652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:58:01.067941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.068584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.069208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.069723: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.069781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:58:01.087501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.088335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.088854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.089447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.089961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.090448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:58:01.098104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.098720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.099251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.099938: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.099989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:58:01.116766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.117611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.118116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.118692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.119219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.119701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:58:01.144852: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.145069: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.146908: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:58:01.162968: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.163188: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.165054: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:58:01.165873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.166492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.167015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.167571: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.167630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:58:01.168445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.169033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.169571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.170032: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.170073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:58:01.172083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.172671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.173184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.173654: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.173696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:58:01.177568: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.177761: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.179742: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 02:58:01.185117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.185749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.186251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.186836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.187374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.187779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.187862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:58:01.188433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.188944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.189537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.190049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.190524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:58:01.190631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.191255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.191812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.192390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.192910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.193600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:58:01.198129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.198734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.199287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.199772: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:58:01.199819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:58:01.216899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.217577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.218082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.218690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.219221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:58:01.219697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:58:01.230525: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.230718: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.232498: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:58:01.235888: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.236078: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.236868: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.237024: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.237075: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:58:01.238816: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:58:01.262868: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.263068: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:58:01.264911: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][02:58:02.527][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.527][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.527][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.527][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.527][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.527][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.550][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:58:02.550][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  1.84it/s]warmup run: 91it [00:00, 188.28it/s]warmup run: 182it [00:00, 352.78it/s]warmup run: 271it [00:00, 483.89it/s]warmup run: 359it [00:00, 586.20it/s]warmup run: 454it [00:01, 682.76it/s]warmup run: 550it [00:01, 759.21it/s]warmup run: 644it [00:01, 810.42it/s]warmup run: 740it [00:01, 853.35it/s]warmup run: 835it [00:01, 880.87it/s]warmup run: 930it [00:01, 900.43it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1025it [00:01, 912.74it/s]warmup run: 98it [00:01, 82.35it/s]warmup run: 1it [00:01,  1.61s/it]warmup run: 99it [00:01, 83.89it/s]warmup run: 102it [00:01, 87.35it/s]warmup run: 95it [00:01, 80.79it/s]warmup run: 99it [00:01, 84.27it/s]warmup run: 1124it [00:01, 933.02it/s]warmup run: 198it [00:01, 180.76it/s]warmup run: 101it [00:01, 82.00it/s]warmup run: 200it [00:01, 184.02it/s]warmup run: 205it [00:01, 190.18it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 192it [00:01, 177.32it/s]warmup run: 196it [00:01, 180.54it/s]warmup run: 1226it [00:01, 956.73it/s]warmup run: 295it [00:01, 285.63it/s]warmup run: 201it [00:01, 177.60it/s]warmup run: 301it [00:01, 294.19it/s]warmup run: 306it [00:01, 300.45it/s]warmup run: 100it [00:01, 88.12it/s]warmup run: 289it [00:01, 283.72it/s]warmup run: 294it [00:01, 287.72it/s]warmup run: 1329it [00:01, 976.76it/s]warmup run: 395it [00:01, 398.97it/s]warmup run: 302it [00:01, 285.31it/s]warmup run: 401it [00:01, 406.72it/s]warmup run: 408it [00:01, 415.92it/s]warmup run: 201it [00:01, 191.27it/s]warmup run: 386it [00:01, 393.58it/s]warmup run: 391it [00:01, 397.47it/s]warmup run: 1432it [00:02, 990.35it/s]warmup run: 495it [00:02, 509.75it/s]warmup run: 403it [00:02, 397.97it/s]warmup run: 501it [00:02, 516.65it/s]warmup run: 509it [00:02, 526.80it/s]warmup run: 301it [00:01, 302.52it/s]warmup run: 483it [00:02, 501.11it/s]warmup run: 489it [00:02, 506.13it/s]warmup run: 1535it [00:02, 999.99it/s]warmup run: 503it [00:02, 507.43it/s]warmup run: 597it [00:02, 615.54it/s]warmup run: 603it [00:02, 621.40it/s]warmup run: 613it [00:02, 633.79it/s]warmup run: 401it [00:01, 417.23it/s]warmup run: 582it [00:02, 602.99it/s]warmup run: 589it [00:02, 609.14it/s]warmup run: 1638it [00:02, 1006.90it/s]warmup run: 698it [00:02, 706.16it/s]warmup run: 605it [00:02, 612.59it/s]warmup run: 704it [00:02, 710.72it/s]warmup run: 713it [00:02, 718.07it/s]warmup run: 501it [00:01, 527.40it/s]warmup run: 679it [00:02, 687.09it/s]warmup run: 688it [00:02, 696.41it/s]warmup run: 1739it [00:02, 1000.42it/s]warmup run: 798it [00:02, 777.23it/s]warmup run: 702it [00:02, 692.18it/s]warmup run: 804it [00:02, 781.42it/s]warmup run: 812it [00:02, 777.61it/s]warmup run: 601it [00:02, 627.74it/s]warmup run: 776it [00:02, 755.74it/s]warmup run: 786it [00:02, 765.22it/s]warmup run: 1841it [00:02, 1005.37it/s]warmup run: 898it [00:02, 833.13it/s]warmup run: 905it [00:02, 839.96it/s]warmup run: 799it [00:02, 742.93it/s]warmup run: 701it [00:02, 714.59it/s]warmup run: 910it [00:02, 818.93it/s]warmup run: 873it [00:02, 810.35it/s]warmup run: 887it [00:02, 827.77it/s]warmup run: 1944it [00:02, 1010.71it/s]warmup run: 997it [00:02, 874.35it/s]warmup run: 1006it [00:02, 885.44it/s]warmup run: 895it [00:02, 796.64it/s]warmup run: 802it [00:02, 788.24it/s]warmup run: 969it [00:02, 850.68it/s]warmup run: 1007it [00:02, 853.04it/s]warmup run: 988it [00:02, 874.95it/s]warmup run: 2054it [00:02, 1037.04it/s]warmup run: 1096it [00:02, 902.98it/s]warmup run: 1110it [00:02, 926.52it/s]warmup run: 990it [00:02, 831.75it/s]warmup run: 903it [00:02, 845.86it/s]warmup run: 1065it [00:02, 876.59it/s]warmup run: 1103it [00:02, 882.23it/s]warmup run: 1088it [00:02, 908.87it/s]warmup run: 2175it [00:02, 1086.95it/s]warmup run: 1195it [00:02, 925.87it/s]warmup run: 1214it [00:02, 956.23it/s]warmup run: 1084it [00:02, 859.63it/s]warmup run: 1005it [00:02, 891.54it/s]warmup run: 1199it [00:02, 903.63it/s]warmup run: 1161it [00:02, 896.60it/s]warmup run: 1190it [00:02, 939.71it/s]warmup run: 2296it [00:02, 1123.14it/s]warmup run: 1294it [00:02, 940.40it/s]warmup run: 1316it [00:02, 974.19it/s]warmup run: 1183it [00:02, 894.80it/s]warmup run: 1107it [00:02, 926.35it/s]warmup run: 1295it [00:02, 914.38it/s]warmup run: 1256it [00:02, 905.85it/s]warmup run: 1291it [00:02, 959.31it/s]warmup run: 2417it [00:02, 1147.95it/s]warmup run: 1393it [00:02, 954.34it/s]warmup run: 1419it [00:02, 990.39it/s]warmup run: 1283it [00:02, 922.49it/s]warmup run: 1210it [00:02, 954.56it/s]warmup run: 1391it [00:02, 922.90it/s]warmup run: 1394it [00:02, 978.21it/s]warmup run: 1351it [00:02, 916.33it/s]warmup run: 2539it [00:03, 1167.05it/s]warmup run: 1492it [00:03, 963.90it/s]warmup run: 1522it [00:03, 999.47it/s]warmup run: 1384it [00:03, 947.28it/s]warmup run: 1311it [00:02, 969.45it/s]warmup run: 1486it [00:03, 930.20it/s]warmup run: 1497it [00:03, 991.78it/s]warmup run: 1446it [00:03, 922.58it/s]warmup run: 2663it [00:03, 1186.61it/s]warmup run: 1592it [00:03, 974.16it/s]warmup run: 1626it [00:03, 1008.65it/s]warmup run: 1488it [00:03, 971.69it/s]warmup run: 1414it [00:02, 986.33it/s]warmup run: 1582it [00:03, 937.97it/s]warmup run: 1599it [00:03, 997.36it/s]warmup run: 1541it [00:03, 925.69it/s]warmup run: 2785it [00:03, 1195.66it/s]warmup run: 1692it [00:03, 980.83it/s]warmup run: 1730it [00:03, 1015.21it/s]warmup run: 1590it [00:03, 984.69it/s]warmup run: 1678it [00:03, 944.36it/s]warmup run: 1516it [00:02, 968.82it/s]warmup run: 1701it [00:03, 999.05it/s]warmup run: 1635it [00:03, 850.42it/s]warmup run: 1793it [00:03, 986.82it/s]warmup run: 1834it [00:03, 1020.21it/s]warmup run: 1695it [00:03, 1001.42it/s]warmup run: 2905it [00:03, 1104.28it/s]warmup run: 1774it [00:03, 947.79it/s]warmup run: 1803it [00:03, 1004.41it/s]warmup run: 1615it [00:03, 947.72it/s]warmup run: 1730it [00:03, 876.04it/s]warmup run: 3000it [00:03, 863.66it/s] warmup run: 1893it [00:03, 988.87it/s]warmup run: 1937it [00:03, 1018.21it/s]warmup run: 1798it [00:03, 1006.96it/s]warmup run: 1870it [00:03, 949.96it/s]warmup run: 1905it [00:03, 1003.61it/s]warmup run: 1715it [00:03, 962.52it/s]warmup run: 1832it [00:03, 915.56it/s]warmup run: 1993it [00:03, 990.29it/s]warmup run: 2045it [00:03, 1034.19it/s]warmup run: 1900it [00:03, 1000.15it/s]warmup run: 1966it [00:03, 951.30it/s]warmup run: 2008it [00:03, 1010.91it/s]warmup run: 1817it [00:03, 977.59it/s]warmup run: 1935it [00:03, 946.05it/s]warmup run: 2111it [00:03, 1045.81it/s]warmup run: 2169it [00:03, 1093.72it/s]warmup run: 2001it [00:03, 998.86it/s] warmup run: 2078it [00:03, 999.87it/s]warmup run: 2132it [00:03, 1078.29it/s]warmup run: 1919it [00:03, 988.46it/s]warmup run: 2045it [00:03, 990.54it/s]warmup run: 2232it [00:03, 1093.59it/s]warmup run: 2293it [00:03, 1136.27it/s]warmup run: 2125it [00:03, 1069.86it/s]warmup run: 2199it [00:03, 1060.44it/s]warmup run: 2256it [00:03, 1125.85it/s]warmup run: 2024it [00:03, 1005.00it/s]warmup run: 2168it [00:03, 1060.20it/s]warmup run: 2353it [00:03, 1126.06it/s]warmup run: 2417it [00:03, 1166.00it/s]warmup run: 2250it [00:03, 1121.86it/s]warmup run: 2319it [00:03, 1100.83it/s]warmup run: 2380it [00:03, 1159.75it/s]warmup run: 2145it [00:03, 1064.87it/s]warmup run: 2291it [00:03, 1109.99it/s]warmup run: 2473it [00:03, 1147.26it/s]warmup run: 2541it [00:03, 1186.79it/s]warmup run: 2375it [00:03, 1158.97it/s]warmup run: 2439it [00:03, 1129.29it/s]warmup run: 2504it [00:03, 1182.69it/s]warmup run: 2266it [00:03, 1106.18it/s]warmup run: 2413it [00:03, 1140.55it/s]warmup run: 2593it [00:04, 1161.34it/s]warmup run: 2665it [00:04, 1201.96it/s]warmup run: 2500it [00:04, 1184.44it/s]warmup run: 2559it [00:04, 1149.22it/s]warmup run: 2628it [00:04, 1198.86it/s]warmup run: 2387it [00:03, 1135.60it/s]warmup run: 2535it [00:04, 1163.36it/s]warmup run: 2713it [00:04, 1171.87it/s]warmup run: 2788it [00:04, 1207.67it/s]warmup run: 2625it [00:04, 1202.28it/s]warmup run: 2679it [00:04, 1164.26it/s]warmup run: 2752it [00:04, 1205.26it/s]warmup run: 2508it [00:03, 1156.29it/s]warmup run: 2657it [00:04, 1179.96it/s]warmup run: 2832it [00:04, 1175.66it/s]warmup run: 2912it [00:04, 1214.83it/s]warmup run: 2750it [00:04, 1213.92it/s]warmup run: 2797it [00:04, 1168.83it/s]warmup run: 2876it [00:04, 1213.64it/s]warmup run: 2629it [00:04, 1171.31it/s]warmup run: 2778it [00:04, 1187.18it/s]warmup run: 3000it [00:04, 692.56it/s] warmup run: 2952it [00:04, 1181.69it/s]warmup run: 2873it [00:04, 1216.92it/s]warmup run: 2916it [00:04, 1174.68it/s]warmup run: 3000it [00:04, 679.58it/s] warmup run: 3000it [00:04, 1219.55it/s]warmup run: 3000it [00:04, 689.39it/s] warmup run: 2750it [00:04, 1180.58it/s]warmup run: 2900it [00:04, 1194.44it/s]warmup run: 2997it [00:04, 1222.39it/s]warmup run: 3000it [00:04, 678.42it/s] warmup run: 3000it [00:04, 673.97it/s] warmup run: 2869it [00:04, 1182.37it/s]warmup run: 3000it [00:04, 670.87it/s] warmup run: 2990it [00:04, 1189.24it/s]warmup run: 3000it [00:04, 694.64it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1636.12it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.61it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.58it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.44it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1633.16it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.07it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.20it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1681.12it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1683.35it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1659.35it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1657.50it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1696.21it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1693.80it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.78it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1684.09it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.25it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1657.77it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1683.44it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.32it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1658.93it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1694.07it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1690.54it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1679.73it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1626.58it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1685.92it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1656.39it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1661.58it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1693.64it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1659.60it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1678.16it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1690.87it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1616.82it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1684.16it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1654.49it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1659.06it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1661.97it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1691.53it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1676.83it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1679.80it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1627.98it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1659.01it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1653.23it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1680.39it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1676.44it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1661.38it/s]warmup should be done:  34%|      | 1020/3000 [00:00<00:01, 1679.05it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1672.42it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1639.33it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1648.68it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1651.38it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1672.67it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1657.69it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1674.73it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1679.02it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1665.03it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1632.78it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1672.98it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1649.30it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1675.80it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1658.99it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1648.31it/s]warmup should be done:  45%|     | 1357/3000 [00:00<00:00, 1682.25it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1664.59it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1616.27it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1676.58it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1647.89it/s]warmup should be done:  50%|     | 1515/3000 [00:00<00:00, 1671.00it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1659.53it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1645.03it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1683.18it/s]warmup should be done:  51%|     | 1520/3000 [00:00<00:00, 1663.12it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1617.77it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1648.24it/s]warmup should be done:  56%|    | 1687/3000 [00:01<00:00, 1678.18it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1662.02it/s]warmup should be done:  56%|    | 1683/3000 [00:01<00:00, 1670.40it/s]warmup should be done:  56%|    | 1695/3000 [00:01<00:00, 1684.45it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1642.53it/s]warmup should be done:  56%|    | 1687/3000 [00:01<00:00, 1662.77it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1622.98it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1649.52it/s]warmup should be done:  62%|   | 1855/3000 [00:01<00:00, 1677.18it/s]warmup should be done:  61%|    | 1835/3000 [00:01<00:00, 1661.84it/s]warmup should be done:  62%|   | 1851/3000 [00:01<00:00, 1671.43it/s]warmup should be done:  62%|   | 1865/3000 [00:01<00:00, 1686.34it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1640.94it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1662.93it/s]warmup should be done:  60%|    | 1809/3000 [00:01<00:00, 1624.38it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1648.98it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1674.39it/s]warmup should be done:  67%|   | 2002/3000 [00:01<00:00, 1661.08it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1685.64it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1638.24it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1661.30it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1654.46it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1619.13it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1648.45it/s]warmup should be done:  73%|  | 2203/3000 [00:01<00:00, 1685.45it/s]warmup should be done:  72%|  | 2169/3000 [00:01<00:00, 1658.36it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1665.48it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1636.63it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1658.25it/s]warmup should be done:  73%|  | 2187/3000 [00:01<00:00, 1649.19it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1622.18it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1649.16it/s]warmup should be done:  79%|  | 2372/3000 [00:01<00:00, 1683.02it/s]warmup should be done:  77%|  | 2318/3000 [00:01<00:00, 1636.47it/s]warmup should be done:  79%|  | 2358/3000 [00:01<00:00, 1656.77it/s]warmup should be done:  78%|  | 2335/3000 [00:01<00:00, 1641.77it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1651.19it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1644.28it/s]warmup should be done:  77%|  | 2298/3000 [00:01<00:00, 1623.48it/s]warmup should be done:  85%| | 2541/3000 [00:01<00:00, 1684.83it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1639.41it/s]warmup should be done:  83%| | 2482/3000 [00:01<00:00, 1635.60it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1654.74it/s]warmup should be done:  83%| | 2500/3000 [00:01<00:00, 1642.68it/s]warmup should be done:  84%| | 2518/3000 [00:01<00:00, 1648.74it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1645.74it/s]warmup should be done:  82%| | 2461/3000 [00:01<00:00, 1623.01it/s]warmup should be done:  88%| | 2651/3000 [00:01<00:00, 1644.96it/s]warmup should be done:  90%| | 2710/3000 [00:01<00:00, 1684.56it/s]warmup should be done:  88%| | 2649/3000 [00:01<00:00, 1644.25it/s]warmup should be done:  90%| | 2690/3000 [00:01<00:00, 1654.71it/s]warmup should be done:  89%| | 2667/3000 [00:01<00:00, 1648.91it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1637.76it/s]warmup should be done:  89%| | 2682/3000 [00:01<00:00, 1646.12it/s]warmup should be done:  88%| | 2625/3000 [00:01<00:00, 1626.00it/s]warmup should be done:  96%|| 2880/3000 [00:01<00:00, 1689.12it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1653.52it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1647.43it/s]warmup should be done:  95%|| 2857/3000 [00:01<00:00, 1656.26it/s]warmup should be done:  94%|| 2834/3000 [00:01<00:00, 1653.35it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1649.62it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1638.90it/s]warmup should be done:  93%|| 2789/3000 [00:01<00:00, 1627.67it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1668.76it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.60it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.34it/s]warmup should be done: 100%|| 2988/3000 [00:01<00:00, 1663.53it/s]warmup should be done:  99%|| 2982/3000 [00:01<00:00, 1651.10it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1652.28it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1636.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.17it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.19it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1695.35it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1723.36it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1712.53it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.88it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1682.86it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.82it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1720.03it/s]warmup should be done:  12%|        | 347/3000 [00:00<00:01, 1732.42it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1702.79it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1704.64it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1668.60it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1680.48it/s]warmup should be done:  11%|        | 344/3000 [00:00<00:01, 1709.32it/s]warmup should be done:  12%|        | 346/3000 [00:00<00:01, 1719.67it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1676.17it/s]warmup should be done:  17%|        | 522/3000 [00:00<00:01, 1736.51it/s]warmup should be done:  17%|        | 514/3000 [00:00<00:01, 1709.08it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1682.50it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1700.74it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1706.79it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1681.68it/s]warmup should be done:  17%|        | 519/3000 [00:00<00:01, 1721.04it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1678.32it/s]warmup should be done:  23%|       | 696/3000 [00:00<00:01, 1737.42it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1687.25it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1704.45it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1700.99it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1685.53it/s]warmup should be done:  23%|       | 687/3000 [00:00<00:01, 1708.64it/s]warmup should be done:  23%|       | 693/3000 [00:00<00:01, 1724.83it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1681.55it/s]warmup should be done:  29%|       | 870/3000 [00:00<00:01, 1737.43it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1689.82it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1703.26it/s]warmup should be done:  29%|       | 859/3000 [00:00<00:01, 1711.29it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1687.52it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1687.65it/s]warmup should be done:  29%|       | 867/3000 [00:00<00:01, 1727.47it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1700.44it/s]warmup should be done:  35%|      | 1045/3000 [00:00<00:01, 1738.34it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1689.16it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1704.17it/s]warmup should be done:  34%|      | 1031/3000 [00:00<00:01, 1711.44it/s]warmup should be done:  35%|      | 1040/3000 [00:00<00:01, 1727.66it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1695.69it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1685.94it/s]warmup should be done:  34%|      | 1027/3000 [00:00<00:01, 1697.70it/s]warmup should be done:  41%|      | 1219/3000 [00:00<00:01, 1737.41it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1685.03it/s]warmup should be done:  40%|      | 1190/3000 [00:00<00:01, 1703.27it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1700.61it/s]warmup should be done:  40%|      | 1213/3000 [00:00<00:01, 1723.94it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1682.56it/s]warmup should be done:  40%|      | 1203/3000 [00:00<00:01, 1707.50it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1696.23it/s]warmup should be done:  46%|     | 1394/3000 [00:00<00:00, 1740.96it/s]warmup should be done:  45%|     | 1363/3000 [00:00<00:00, 1711.69it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1688.28it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1703.31it/s]warmup should be done:  46%|     | 1387/3000 [00:00<00:00, 1728.19it/s]warmup should be done:  45%|     | 1357/3000 [00:00<00:00, 1693.52it/s]warmup should be done:  46%|     | 1375/3000 [00:00<00:00, 1710.01it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1698.19it/s]warmup should be done:  52%|    | 1569/3000 [00:00<00:00, 1738.96it/s]warmup should be done:  51%|     | 1536/3000 [00:00<00:00, 1715.55it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1690.31it/s]warmup should be done:  51%|    | 1539/3000 [00:00<00:00, 1702.51it/s]warmup should be done:  51%|     | 1530/3000 [00:00<00:00, 1703.80it/s]warmup should be done:  52%|    | 1560/3000 [00:00<00:00, 1724.06it/s]warmup should be done:  51%|    | 1538/3000 [00:00<00:00, 1697.64it/s]warmup should be done:  52%|    | 1547/3000 [00:00<00:00, 1708.85it/s]warmup should be done:  57%|    | 1708/3000 [00:01<00:00, 1716.72it/s]warmup should be done:  56%|    | 1693/3000 [00:01<00:00, 1692.56it/s]warmup should be done:  57%|    | 1710/3000 [00:01<00:00, 1702.53it/s]warmup should be done:  57%|    | 1703/3000 [00:01<00:00, 1710.82it/s]warmup should be done:  58%|    | 1743/3000 [00:01<00:00, 1728.62it/s]warmup should be done:  57%|    | 1718/3000 [00:01<00:00, 1705.85it/s]warmup should be done:  58%|    | 1733/3000 [00:01<00:00, 1719.42it/s]warmup should be done:  57%|    | 1708/3000 [00:01<00:00, 1685.38it/s]warmup should be done:  62%|   | 1863/3000 [00:01<00:00, 1693.44it/s]warmup should be done:  63%|   | 1880/3000 [00:01<00:00, 1715.13it/s]warmup should be done:  63%|   | 1881/3000 [00:01<00:00, 1704.14it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1715.80it/s]warmup should be done:  63%|   | 1890/3000 [00:01<00:00, 1707.40it/s]warmup should be done:  64%|   | 1905/3000 [00:01<00:00, 1717.14it/s]warmup should be done:  64%|   | 1916/3000 [00:01<00:00, 1721.55it/s]warmup should be done:  63%|   | 1877/3000 [00:01<00:00, 1686.17it/s]warmup should be done:  68%|   | 2033/3000 [00:01<00:00, 1693.87it/s]warmup should be done:  68%|   | 2052/3000 [00:01<00:00, 1713.15it/s]warmup should be done:  68%|   | 2049/3000 [00:01<00:00, 1719.34it/s]warmup should be done:  68%|   | 2053/3000 [00:01<00:00, 1706.14it/s]warmup should be done:  69%|   | 2061/3000 [00:01<00:00, 1706.74it/s]warmup should be done:  69%|   | 2077/3000 [00:01<00:00, 1716.23it/s]warmup should be done:  70%|   | 2089/3000 [00:01<00:00, 1717.48it/s]warmup should be done:  68%|   | 2046/3000 [00:01<00:00, 1686.18it/s]warmup should be done:  73%|  | 2203/3000 [00:01<00:00, 1692.82it/s]warmup should be done:  74%|  | 2224/3000 [00:01<00:00, 1713.01it/s]warmup should be done:  74%|  | 2222/3000 [00:01<00:00, 1721.32it/s]warmup should be done:  74%|  | 2224/3000 [00:01<00:00, 1704.21it/s]warmup should be done:  75%|  | 2249/3000 [00:01<00:00, 1714.35it/s]warmup should be done:  74%|  | 2215/3000 [00:01<00:00, 1686.55it/s]warmup should be done:  74%|  | 2232/3000 [00:01<00:00, 1700.43it/s]warmup should be done:  75%|  | 2261/3000 [00:01<00:00, 1714.49it/s]warmup should be done:  79%|  | 2373/3000 [00:01<00:00, 1694.29it/s]warmup should be done:  80%|  | 2395/3000 [00:01<00:00, 1723.14it/s]warmup should be done:  80%|  | 2397/3000 [00:01<00:00, 1716.24it/s]warmup should be done:  80%|  | 2395/3000 [00:01<00:00, 1701.61it/s]warmup should be done:  81%|  | 2421/3000 [00:01<00:00, 1712.77it/s]warmup should be done:  80%|  | 2385/3000 [00:01<00:00, 1690.38it/s]warmup should be done:  81%|  | 2433/3000 [00:01<00:00, 1711.46it/s]warmup should be done:  80%|  | 2403/3000 [00:01<00:00, 1695.57it/s]warmup should be done:  85%| | 2543/3000 [00:01<00:00, 1695.02it/s]warmup should be done:  86%| | 2569/3000 [00:01<00:00, 1725.40it/s]warmup should be done:  86%| | 2571/3000 [00:01<00:00, 1721.15it/s]warmup should be done:  86%| | 2566/3000 [00:01<00:00, 1703.03it/s]warmup should be done:  86%| | 2593/3000 [00:01<00:00, 1712.43it/s]warmup should be done:  85%| | 2556/3000 [00:01<00:00, 1694.04it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1709.13it/s]warmup should be done:  86%| | 2573/3000 [00:01<00:00, 1691.43it/s]warmup should be done:  90%| | 2713/3000 [00:01<00:00, 1694.78it/s]warmup should be done:  92%|| 2745/3000 [00:01<00:00, 1725.27it/s]warmup should be done:  91%| | 2737/3000 [00:01<00:00, 1704.48it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1711.55it/s]warmup should be done:  91%| | 2727/3000 [00:01<00:00, 1695.95it/s]warmup should be done:  93%|| 2776/3000 [00:01<00:00, 1707.97it/s]warmup should be done:  91%|| 2743/3000 [00:01<00:00, 1688.58it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1695.97it/s]warmup should be done:  96%|| 2883/3000 [00:01<00:00, 1692.58it/s]warmup should be done:  97%|| 2919/3000 [00:01<00:00, 1727.43it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1704.67it/s]warmup should be done:  97%|| 2897/3000 [00:01<00:00, 1692.34it/s]warmup should be done:  98%|| 2937/3000 [00:01<00:00, 1705.46it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1706.27it/s]warmup should be done:  97%|| 2912/3000 [00:01<00:00, 1686.28it/s]warmup should be done:  97%|| 2912/3000 [00:01<00:00, 1696.50it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1721.50it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1716.10it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1710.61it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1703.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1699.92it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1697.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1693.89it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1689.79it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f44736622b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f44739a7730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f44736641f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f44739a9d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4473667190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f44739a6e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4473673040>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f44736721c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 02:59:31.550811: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f9e82c240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:31.550874: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:31.560412: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:31.915807: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f9f02d810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:31.915873: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:31.925241: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:32.166414: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f9f02d880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:32.166477: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:32.175978: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:32.517449: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f8f031ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:32.517522: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:32.524949: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f9a82c230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:32.525013: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:32.527249: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:32.535211: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:32.537977: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f968308b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:32.538040: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:32.546808: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:32.558581: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3fa2833f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:32.558649: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:32.562641: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3f9e799540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:59:32.562694: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:59:32.566093: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:32.570112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:59:38.876684: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.050371: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.153232: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.188053: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.647682: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.649731: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.684524: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:59:39.688225: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:00:32.843][ERROR][RK0][tid #139911512454912]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:00:32.843][ERROR][RK0][tid #139911512454912]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:32.852][ERROR][RK0][tid #139911512454912]: coll ps creation done
[HCTR][03:00:32.852][ERROR][RK0][tid #139911512454912]: replica 5 waits for coll ps creation barrier
[HCTR][03:00:33.012][ERROR][RK0][tid #139912443586304]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:00:33.012][ERROR][RK0][tid #139912443586304]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.018][ERROR][RK0][tid #139912443586304]: coll ps creation done
[HCTR][03:00:33.018][ERROR][RK0][tid #139912443586304]: replica 7 waits for coll ps creation barrier
[HCTR][03:00:33.239][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:00:33.239][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.244][ERROR][RK0][main]: coll ps creation done
[HCTR][03:00:33.244][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][03:00:33.291][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:00:33.291][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.298][ERROR][RK0][main]: coll ps creation done
[HCTR][03:00:33.298][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][03:00:33.325][ERROR][RK0][tid #139911998969600]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:00:33.326][ERROR][RK0][tid #139911998969600]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.331][ERROR][RK0][tid #139911998969600]: coll ps creation done
[HCTR][03:00:33.331][ERROR][RK0][tid #139911998969600]: replica 0 waits for coll ps creation barrier
[HCTR][03:00:33.336][ERROR][RK0][tid #139911436953344]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:00:33.336][ERROR][RK0][tid #139911436953344]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.340][ERROR][RK0][tid #139911436953344]: coll ps creation done
[HCTR][03:00:33.340][ERROR][RK0][tid #139911436953344]: replica 6 waits for coll ps creation barrier
[HCTR][03:00:33.373][ERROR][RK0][tid #139911504062208]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:00:33.373][ERROR][RK0][tid #139911504062208]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.376][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:00:33.376][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:00:33.377][ERROR][RK0][tid #139911504062208]: coll ps creation done
[HCTR][03:00:33.377][ERROR][RK0][tid #139911504062208]: replica 3 waits for coll ps creation barrier
[HCTR][03:00:33.381][ERROR][RK0][main]: coll ps creation done
[HCTR][03:00:33.381][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][03:00:33.381][ERROR][RK0][tid #139911998969600]: replica 0 preparing frequency
[HCTR][03:00:34.209][ERROR][RK0][tid #139911998969600]: replica 0 preparing frequency done
[HCTR][03:00:34.245][ERROR][RK0][tid #139911998969600]: replica 0 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][tid #139911436953344]: replica 6 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][tid #139911504062208]: replica 3 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][tid #139911436953344]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][tid #139912443586304]: replica 7 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][tid #139911512454912]: replica 5 calling init per replica
[HCTR][03:00:34.245][ERROR][RK0][tid #139911998969600]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][main]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][tid #139911504062208]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][tid #139911436953344]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][main]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][tid #139912443586304]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][main]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][tid #139911998969600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][tid #139911512454912]: Calling build_v2
[HCTR][03:00:34.245][ERROR][RK0][tid #139911504062208]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][tid #139912443586304]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:00:34.245][ERROR][RK0][tid #139911512454912]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 03:00:34.250127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 03:00:34.250203: E 2022-12-12 03:00:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:250176196: [] Eassigning 0 to cpu 
2022-12-12 03:00:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:250222[178: ] E2022-12-12 03:00:34v100x8, slow pcie .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc250264[:: [2022-12-12 03:00:34178[E.] 2022-12-12 03:00:342022-12-12 03:00:34 250314v100x8, slow pcie../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
250323250310:E[: : 178[ EE] 2022-12-12 03:00:342022-12-12 03:00:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  v100x8, slow pcie..:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
250387250365212::: : 2022-12-12 03:00:34] [196178E[E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 03:00:34] ]   250419
.assigning 0 to cpuv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2504722022-12-12 03:00:34

::[E: .1961782022-12-12 03:00:34[ E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc250468] ] .2022-12-12 03:00:34 :: assigning 0 to cpuv100x8, slow pcie250561.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E

: 2505832022-12-12 03:00:34:] [ E: .196v100x8, slow pcie2022-12-12 03:00:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E250629[] 
.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : 2022-12-12 03:00:34assigning 0 to cpu250687178:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.
: ] 2132022-12-12 03:00:34: 250717Ev100x8, slow pcie] .196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [ 
remote time is 8.68421250761] :E2022-12-12 03:00:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[: assigning 0 to cpu212 .:2022-12-12 03:00:34[E
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc250852196.2022-12-12 03:00:34 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:: ] 250870./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212:Eassigning 0 to cpu: 250900] [196 
[E: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 03:00:34] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:00:34 E
.assigning 0 to cpu:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 251004
212251015[:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] : 2022-12-12 03:00:341962022-12-12 03:00:34:Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[.] .214 
 2022-12-12 03:00:34251077assigning 0 to cpu251081] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: [
cpu time is 97.0588E::251125E2022-12-12 03:00:34
 212213:  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc251197:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212remote time is 8.68421 2022-12-12 03:00:34:: 
] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.213Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[
:[251283]  2022-12-12 03:00:342122022-12-12 03:00:34: remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[] .E
:2513352022-12-12 03:00:34build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8251338 213: .[
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E2513692022-12-12 03:00:34E:remote time is 8.68421 : [. 212
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 03:00:34[.251418/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 2022-12-12 03:00:34251461: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E214
] 251502:E ] remote time is 8.68421: 213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.0588
E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 03:00:34
 remote time is 8.68421[:214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 03:00:34213] 251572[:.251607] cpu time is 97.0588remote time is 8.68421: 2022-12-12 03:00:34214: 

E.] E 251650cpu time is 97.0588[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
2022-12-12 03:00:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E.:213 251703214] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] remote time is 8.68421:Ecpu time is 97.0588
214 
] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.05882022-12-12 03:00:34:
.214251817] : cpu time is 97.0588E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 03:01:51.170390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:01:51.212837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 03:01:51.212903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 10000000
[2022-12-12 03:01:51.332520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:01:51.332606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:01:51.488290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:01:51.488414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:01:51.488989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.490322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.491536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.503444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 03:01:51.503498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 03:01:51.503739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 03:01:51.[5037932022-12-12 03:01:51: .E503785 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 1 initing device 1202
] 2 solved
[2022-12-12 03:01:51.503858: E[ [2022-12-12 03:01:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 03:01:51.:.503867205503857: ] : Eworker 0 thread 2 initing device 2E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1980202] ] eager alloc mem 381.47 MB3 solved

[2022-12-12 03:01:51.503944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 03:01:51.504186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.504248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.504307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.505399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 03:01:51.505456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[205] 2022-12-12 03:01:51worker 0 thread 5 initing device 5.
505442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 03:01:51.505537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 03:01:51.505845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.505943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.507695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.507871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.507918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.508057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.509111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.509424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 03:01:51.509478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 03:01:51.509611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.509840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.512161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.512257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.512364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.512580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.513092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.513594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.514094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.517193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:01:51.575230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 03:01:51.575618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 03:01:51.581121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:01:51.581214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:01:51.581262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:01:51.582082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.582642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.583674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:51.583768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:01:51.584454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:01:51.584497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[2022-12-12 03:01:512022-12-12 03:01:512022-12-12 03:01:512022-12-12 03:01:512022-12-12 03:01:512022-12-12 03:01:51......598293598281598294598296598296598301: : : : : : EEEEEE      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::198019801980198019801980] ] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes





[[2022-12-12 03:01:512022-12-12 03:01:51[[..[2022-12-12 03:01:51[2022-12-12 03:01:515987575987592022-12-12 03:01:51.2022-12-12 03:01:51.: : .598762.598763EE598767: 598769:   : E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ] :1980:1980eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes1980] 1980] 

] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes


[2022-12-12 03:01:51.603277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 03:01:51.603639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 03:01:51.605750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:01:51.605830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:01:51.605876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 03:01:51] .eager release cuda mem 400000000605876
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:01:51.605952: [E2022-12-12 03:01:51 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc605947:: 638E]  eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-12 03:01:51.606018: E[ [2022-12-12 03:01:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:01:51.:.606028638606017: ] : Eeager release cuda mem 400000000E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 1024

[[2022-12-12 03:01:51[2022-12-12 03:01:51.2022-12-12 03:01:51.606117.606119: 606106: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:638] 638] eager release cuda mem 400000000] eager release cuda mem 2
eager release cuda mem 1024

[[[2022-12-12 03:01:512022-12-12 03:01:512022-12-12 03:01:51...606199606215606218: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::638638638] ] ] eager release cuda mem 1024eager release cuda mem 400000000eager release cuda mem 2


[2022-12-12 03:01:51[.2022-12-12 03:01:51606308.: 606312E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 400000000
[2022-12-12 03:01:51.606376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:01:51.608027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.608626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.609223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.609729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.610235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.610492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:01:51.610564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:01:51.610607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:01:51.610850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.612376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.612778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:01:51.613029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 03:01:51eager alloc mem 611.00 KB.
613054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.613156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.613200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.613263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.613382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:51.613474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:01:51.613670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:51.[6140442022-12-12 03:01:51: .E614052 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 03:01:51.614139: [E[[2022-12-12 03:01:51 2022-12-12 03:01:512022-12-12 03:01:51./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..614148:614153614157: 638: : E] EE eager release cuda mem 25855  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-12 03:01:51::638.19801980] [614198] ] eager release cuda mem 6256632022-12-12 03:01:51: eager alloc mem 25.25 KBeager alloc mem 25.25 KB
.E

614229 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:01:51E:. 638614257/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] : :eager release cuda mem 625663E1980
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 4.77 GB:
[6382022-12-12 03:01:51] .eager release cuda mem 625663614310
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:01:51.614360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:01:51.614384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:01:51.614670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:51.614759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 03:01:512022-12-12 03:01:51..614904614908: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[2022-12-12 03:01:51[.2022-12-12 03:01:51614972.: 614974[E: 2022-12-12 03:01:51 E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 614986:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 1980:E] 1980 eager alloc mem 4.77 GB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
eager alloc mem 4.77 GB:2022-12-12 03:01:51
638.] 615027eager release cuda mem 25855: [
E2022-12-12 03:01:51 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc615052:: 638[E] 2022-12-12 03:01:51 eager release cuda mem 25855./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
615076:: 638E]  eager release cuda mem 25855/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
:2022-12-12 03:01:511980.] 615111eager alloc mem 4.77 GB: 
E[ 2022-12-12 03:01:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6151461980: ] Eeager alloc mem 4.77 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:01:51.615431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:01:51.615491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[2022-12-12 03:01:522022-12-12 03:01:522022-12-12 03:01:522022-12-12 03:01:52[2022-12-12 03:01:522022-12-12 03:01:522022-12-12 03:01:52....2022-12-12 03:01:52...590043590061590036590036590036.590036590040: : : : : 590075: : EEEEE: EE     E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980198019801980:19801980] ] ] ] ] 1980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB




eager alloc mem 611.00 KB


[2022-12-12 03:01:52[[.[[2022-12-12 03:01:522022-12-12 03:01:525911462022-12-12 03:01:52[2022-12-12 03:01:52..: .2022-12-12 03:01:52.591151591152E591153.591153[: :  : 591164: 2022-12-12 03:01:52EE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: E.  2022-12-12 03:01:52: E 591195/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ::591208] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E638638: eager release cuda mem 625663638:638 ] ] E
] 638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663eager release cuda mem 625663 eager release cuda mem 625663] eager release cuda mem 625663:

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
eager release cuda mem 625663
638:
] 638[eager release cuda mem 625663] 2022-12-12 03:01:52
eager release cuda mem 625663.[
591399[2022-12-12 03:01:52: 2022-12-12 03:01:52[.[E.2022-12-12 03:01:525914292022-12-12 03:01:52[ 591432.: .2022-12-12 03:01:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 591441E591442.:E:  : 5914531980 [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:01:52 : E[eager alloc mem 611.00 KB:./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 2022-12-12 03:01:52
1980591484:] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.] : 1980eager alloc mem 611.00 KB1980:591511eager alloc mem 611.00 KBE] 
] 1980: 
 eager alloc mem 611.00 KBeager alloc mem 611.00 KB] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

eager alloc mem 611.00 KB :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 03:01:52.592289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.592336: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:01:52:.638592349[] : 2022-12-12 03:01:52[eager release cuda mem 625663E.[2022-12-12 03:01:52
 592360[2022-12-12 03:01:52./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 2022-12-12 03:01:52.592367:E.592371: 638 592379: E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[: E eager release cuda mem 625663:[[2022-12-12 03:01:52E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
19802022-12-12 03:01:522022-12-12 03:01:52. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] ..592418/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638eager alloc mem 611.00 KB592436[592436: :638] 
: 2022-12-12 03:01:52: E638] eager release cuda mem 625663E.E ] eager release cuda mem 625663
 592515 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
:E:6381980 638] ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager release cuda mem 625663eager alloc mem 611.00 KB2022-12-12 03:01:52:eager release cuda mem 625663[

.[1980
2022-12-12 03:01:525926362022-12-12 03:01:52] .: .eager alloc mem 611.00 KB592657E592667
:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:] :2022-12-12 03:01:521980eager alloc mem 611.00 KB1980.] 
[] 592725eager alloc mem 611.00 KB2022-12-12 03:01:52eager alloc mem 611.00 KB: 
.
E592754 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.593274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.593341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.593408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.593436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.593477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 03:01:52
.[5934952022-12-12 03:01:52: .E593503 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-12 03:01:52] :2022-12-12 03:01:52.eager release cuda mem 6256631980.593525
] 593531: eager alloc mem 611.00 KB: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663[

2022-12-12 03:01:52[.2022-12-12 03:01:52[593588.2022-12-12 03:01:52: 593601.E: 593613[[ E: 2022-12-12 03:01:522022-12-12 03:01:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E..:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 593649593650638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : ] 1980:EEeager release cuda mem 625663] 638  
eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager release cuda mem 625663::
19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 03:01:52.593805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.593840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.594088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.594156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.594229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 03:01:522022-12-12 03:01:52..594293594296: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 03:01:52.594399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.594499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
[2022-12-12 03:01:522022-12-12 03:01:52..594520594521: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638[] ] 2022-12-12 03:01:52eager release cuda mem 625663[eager release cuda mem 625663.
2022-12-12 03:01:52
594561.: 594571E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[638[2022-12-12 03:01:52:2022-12-12 03:01:52] 2022-12-12 03:01:52.1980.eager release cuda mem 625663.594629] 594639
594644: eager alloc mem 611.00 KB: : E
EE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::63819801980] ] ] eager release cuda mem 625663eager alloc mem 611.00 KB[eager alloc mem 611.00 KB

2022-12-12 03:01:52
.594740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.594806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.594917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.594985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.595152[: 2022-12-12 03:01:52E. 595159/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 03:01:52.595241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.595486[: 2022-12-12 03:01:52E. 595493/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: :2022-12-12 03:01:52E638. ] 595508/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663: :
E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 03:01:52
.595584[: 2022-12-12 03:01:52E. 595596/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 03:01:52
.595735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.595811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.595917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.595983: [E2022-12-12 03:01:52 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu595990:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 03:01:52.596080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.596285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52[.2022-12-12 03:01:52596351.: [596353E2022-12-12 03:01:52:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu596364 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980E[:]  2022-12-12 03:01:52638eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.] 
:596397eager release cuda mem 625663638: 
] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.[5964822022-12-12 03:01:52: .E596488[ : 2022-12-12 03:01:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE.: 5964981980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ] :Eeager alloc mem 611.00 KB1980 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.596556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.596594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:01:52.596625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.596709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:01:52.596762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.596800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:01:52.596828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.596864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:01:52.597057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09281 secs 
[2022-12-12 03:01:52.597152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.597187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:01:52.597263: E[ [2022-12-12 03:01:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:01:52.:.597274638597276: ] : Eeager release cuda mem 625663E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] [] eager release cuda mem 6256632022-12-12 03:01:52eager release cuda mem 625663
.
597326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:01:52[638.2022-12-12 03:01:52] 597352.eager release cuda mem 40400000: 597356
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 40400000] 
eager release cuda mem 40400000
[2022-12-12 03:01:52.597423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09158 secs 
[2022-12-12 03:01:52.597464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:01:52.597512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:01:52.597598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09167 secs 
[2022-12-12 03:01:52.598317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09414 secs 
[2022-12-12 03:01:52.598852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09455 secs 
[2022-12-12 03:01:52.598971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09511 secs 
[2022-12-12 03:01:52.599424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.08959 secs 
[2022-12-12 03:01:52.599858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.11089 secs 
[HCTR][03:01:52.600][ERROR][RK0][tid #139911504062208]: replica 3 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][tid #139911436953344]: replica 6 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][tid #139912443586304]: replica 7 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][tid #139911998969600]: replica 0 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][tid #139911512454912]: replica 5 calling init per replica done, doing barrier
[HCTR][03:01:52.600][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911998969600]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911436953344]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][tid #139912443586304]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911512454912]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911504062208]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:01:52.600][ERROR][RK0][main]: init per replica done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911436953344]: init per replica done
[HCTR][03:01:52.600][ERROR][RK0][tid #139912443586304]: init per replica done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911512454912]: init per replica done
[HCTR][03:01:52.600][ERROR][RK0][main]: init per replica done
[HCTR][03:01:52.600][ERROR][RK0][main]: init per replica done
[HCTR][03:01:52.600][ERROR][RK0][tid #139911504062208]: init per replica done
[HCTR][03:01:52.602][ERROR][RK0][tid #139911998969600]: init per replica done
