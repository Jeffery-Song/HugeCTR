2022-12-11 19:51:22.064663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.070269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.083336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.089922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.095189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.101001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.113714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.127895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.178655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.178858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.180219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.180527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.181954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.182243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.183575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.183656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.184988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.185039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.186197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.186468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.188065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.188253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.189518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.189881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.190908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.192120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.193020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.194338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.196096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.196860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.197193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.198426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.198512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.200035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.200059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.201807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.202564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.203143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.204254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.204993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.205887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.206713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.207460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.208749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.209192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.211620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.212597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.213559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.214874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.214895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.215454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.217045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.217111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.218970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.219015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.220623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.220670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.221193: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.222325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.222367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.224310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.224355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.226930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.227044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.227562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.229761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.230037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.230192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.230411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.232643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.232963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.233140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.233336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.234574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.235164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.235499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.235721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.235941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.237900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.238220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.238479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.238612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.240852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.241079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.241421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.241500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.243564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.243833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.244024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.244396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.246348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.246575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.246678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.247216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.249311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.249394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.249854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.251410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.252055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.253130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.254361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.255199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.255632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.255720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.257042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.270311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.272597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.272738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.273548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.273823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.273997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.275690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.275845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.276914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.293815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.295155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.296965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.308482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.311808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.312088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.312515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.312685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.312762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.312801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.312975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.317225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.317366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.317567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.317663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.317752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.317853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.318777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.323205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.323307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.323561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.323626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.323723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.323861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.324627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.327725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.327899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.328094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.328248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.328284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.328383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.329271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.332501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.332680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.332909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.333006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.333050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.333096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.334081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.337650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.337897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.338031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.338165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.338247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.338291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.339662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.342556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.342936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.343019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.343286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.343336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.343467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.344443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.347340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.347712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.347842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.348063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.348120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.348159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.349405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.352167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.352539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.352655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.353056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.353147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.353182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.354285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.357137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.357446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.357688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.357888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.357930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.358068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.361646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.361782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.362066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.362193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.362287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.362324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.363678: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.365830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.366291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.366344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.366389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.366472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.366701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.369826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.370282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.370355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.370392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.370608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.370850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.372683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.374044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.374449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.374665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.374750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.374961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.375223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.377235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.379092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.379392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.379496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.379525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.379613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.379903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.381701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.384204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.384488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.384700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.384871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.384890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.384995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.387974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.388657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.389517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.389694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.389883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.389921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.392902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.393489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.394118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.394449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.394506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.394582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.397557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.398253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.398919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.399047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.399163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.399289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.402342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.403786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.404007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.404100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.404198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.406494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.407584: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.407724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.407947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.407989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.408128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.410612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.411877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.411977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.412017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.412200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.416106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.416279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.416301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.416368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.416424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.419071: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.419894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.422122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.424098: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.424099: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.424100: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.424100: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:51:22.427982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.430666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.432861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.433237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.433301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.433339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.463923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.466070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.466073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.466104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.466147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.471020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.471073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.471111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:22.471243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.535328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.535957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.536482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.537032: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.537089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:51:23.555337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.555987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.556494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.557076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.557586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.558140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:51:23.601725: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.601935: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.647787: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:51:23.710672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.711546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.712081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.712998: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.713055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:51:23.730112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.731080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.731611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.732193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.732860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.733516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:51:23.773761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.774810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.775359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.775823: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.775889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:51:23.794315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.794960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.795884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.796473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.797079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.797577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:51:23.802114: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.802296: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.803282: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:51:23.810601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.811440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.811993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.812557: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.812610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:51:23.823927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.824000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.824968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.825049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.826173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.826326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.826935: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.826990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:51:23.827495: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.827545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:51:23.829115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.829714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.830226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.830818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.831382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.831860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:51:23.832063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.832634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.833171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.833630: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.833674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:51:23.844288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.844920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.845280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.845496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.846308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.846668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.847069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.847215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.848013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.848642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.848698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.849222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:51:23.849873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.849907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.850634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.850859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:51:23.850900: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:51:23.850945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:51:23.851599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.852124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.852678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.853204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.853668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:51:23.869260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.869938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.870446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.871030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.871562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:51:23.872042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:51:23.875502: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.875710: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.878304: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 19:51:23.893531: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.893726: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.894524: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.894586: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:51:23.894660: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.894713: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.894840: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.896528: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:51:23.896675: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:51:23.897319: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.897434: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.908182: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 19:51:23.916833: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.917011: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:51:23.918633: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.192][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:51:25.193][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.63it/s]warmup run: 1it [00:00,  2.56it/s]warmup run: 1it [00:00,  2.55it/s]warmup run: 1it [00:00,  2.54it/s]warmup run: 64it [00:00, 173.51it/s]warmup run: 68it [00:00, 181.10it/s]warmup run: 60it [00:00, 159.11it/s]warmup run: 59it [00:00, 155.55it/s]warmup run: 118it [00:00, 275.77it/s]warmup run: 123it [00:00, 282.46it/s]warmup run: 107it [00:00, 243.65it/s]warmup run: 111it [00:00, 255.06it/s]warmup run: 172it [00:00, 350.77it/s]warmup run: 179it [00:00, 360.32it/s]warmup run: 156it [00:00, 311.82it/s]warmup run: 166it [00:00, 337.40it/s]warmup run: 226it [00:00, 405.38it/s]warmup run: 236it [00:00, 420.56it/s]warmup run: 205it [00:00, 360.80it/s]warmup run: 221it [00:00, 398.00it/s]warmup run: 279it [00:00, 440.27it/s]warmup run: 293it [00:00, 463.07it/s]warmup run: 253it [00:00, 394.67it/s]warmup run: 276it [00:00, 441.33it/s]warmup run: 334it [00:00, 470.85it/s]warmup run: 350it [00:00, 493.61it/s]warmup run: 302it [00:00, 421.53it/s]warmup run: 329it [00:00, 466.91it/s]warmup run: 387it [00:01, 487.17it/s]warmup run: 407it [00:01, 515.07it/s]warmup run: 354it [00:01, 450.13it/s]warmup run: 381it [00:01, 473.17it/s]warmup run: 440it [00:01, 496.07it/s]warmup run: 464it [00:01, 528.93it/s]warmup run: 406it [00:01, 470.37it/s]warmup run: 432it [00:01, 477.57it/s]warmup run: 493it [00:01, 504.97it/s]warmup run: 459it [00:01, 485.30it/s]warmup run: 520it [00:01, 518.19it/s]warmup run: 483it [00:01, 481.40it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 548it [00:01, 517.22it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 511it [00:01, 494.82it/s]warmup run: 574it [00:01, 514.09it/s]warmup run: 533it [00:01, 485.53it/s]warmup run: 85it [00:01, 79.12it/s]warmup run: 604it [00:01, 527.50it/s]warmup run: 98it [00:01, 90.91it/s]warmup run: 79it [00:01, 73.20it/s]warmup run: 81it [00:01, 75.26it/s]warmup run: 565it [00:01, 506.24it/s]warmup run: 583it [00:01, 486.91it/s]warmup run: 627it [00:01, 508.25it/s]warmup run: 178it [00:01, 179.17it/s]warmup run: 660it [00:01, 534.63it/s]warmup run: 194it [00:01, 192.67it/s]warmup run: 160it [00:01, 159.47it/s]warmup run: 167it [00:01, 167.39it/s]warmup run: 619it [00:01, 516.06it/s]warmup run: 634it [00:01, 493.66it/s]warmup run: 679it [00:01, 503.16it/s]warmup run: 270it [00:01, 285.54it/s]warmup run: 716it [00:01, 539.79it/s]warmup run: 288it [00:01, 299.64it/s]warmup run: 263it [00:01, 280.99it/s]warmup run: 251it [00:01, 266.75it/s]warmup run: 672it [00:01, 507.46it/s]warmup run: 686it [00:01, 499.85it/s]warmup run: 738it [00:01, 525.31it/s]warmup run: 361it [00:01, 391.55it/s]warmup run: 383it [00:01, 409.74it/s]warmup run: 771it [00:01, 541.21it/s]warmup run: 341it [00:01, 374.22it/s]warmup run: 356it [00:01, 390.96it/s]warmup run: 738it [00:01, 505.60it/s]warmup run: 724it [00:01, 493.75it/s]warmup run: 797it [00:01, 541.59it/s]warmup run: 451it [00:01, 490.56it/s]warmup run: 477it [00:01, 512.58it/s]warmup run: 432it [00:01, 478.74it/s]warmup run: 826it [00:01, 542.34it/s]warmup run: 448it [00:01, 494.15it/s]warmup run: 792it [00:01, 514.49it/s]warmup run: 774it [00:01, 486.69it/s]warmup run: 855it [00:01, 552.80it/s]warmup run: 541it [00:01, 579.37it/s]warmup run: 573it [00:01, 609.43it/s]warmup run: 528it [00:01, 583.30it/s]warmup run: 542it [00:01, 590.72it/s]warmup run: 881it [00:01, 541.46it/s]warmup run: 846it [00:02, 520.66it/s]warmup run: 825it [00:02, 491.67it/s]warmup run: 911it [00:02, 525.60it/s]warmup run: 636it [00:02, 667.07it/s]warmup run: 628it [00:02, 681.96it/s]warmup run: 671it [00:02, 695.99it/s]warmup run: 637it [00:02, 675.06it/s]warmup run: 936it [00:02, 537.58it/s]warmup run: 901it [00:02, 526.85it/s]warmup run: 877it [00:02, 499.48it/s]warmup run: 965it [00:02, 512.03it/s]warmup run: 734it [00:02, 744.70it/s]warmup run: 728it [00:02, 760.97it/s]warmup run: 770it [00:02, 768.54it/s]warmup run: 733it [00:02, 746.09it/s]warmup run: 990it [00:02, 534.08it/s]warmup run: 957it [00:02, 534.99it/s]warmup run: 929it [00:02, 504.88it/s]warmup run: 1017it [00:02, 501.30it/s]warmup run: 830it [00:02, 800.90it/s]warmup run: 826it [00:02, 817.59it/s]warmup run: 867it [00:02, 821.42it/s]warmup run: 827it [00:02, 796.74it/s]warmup run: 1044it [00:02, 522.00it/s]warmup run: 1011it [00:02, 529.03it/s]warmup run: 983it [00:02, 514.81it/s]warmup run: 1068it [00:02, 498.24it/s]warmup run: 923it [00:02, 827.77it/s]warmup run: 926it [00:02, 865.82it/s]warmup run: 963it [00:02, 857.36it/s]warmup run: 920it [00:02, 828.82it/s]warmup run: 1097it [00:02, 519.52it/s]warmup run: 1064it [00:02, 527.21it/s]warmup run: 1039it [00:02, 527.81it/s]warmup run: 1118it [00:02, 496.58it/s]warmup run: 1016it [00:02, 854.59it/s]warmup run: 1026it [00:02, 902.88it/s]warmup run: 1059it [00:02, 883.32it/s]warmup run: 1016it [00:02, 865.25it/s]warmup run: 1150it [00:02, 515.44it/s]warmup run: 1118it [00:02, 529.18it/s]warmup run: 1096it [00:02, 537.82it/s]warmup run: 1168it [00:02, 495.67it/s]warmup run: 1115it [00:02, 892.35it/s]warmup run: 1125it [00:02, 927.69it/s]warmup run: 1113it [00:02, 893.50it/s]warmup run: 1155it [00:02, 899.96it/s]warmup run: 1202it [00:02, 507.97it/s]warmup run: 1150it [00:02, 537.15it/s]warmup run: 1171it [00:02, 519.16it/s]warmup run: 1218it [00:02, 493.52it/s]warmup run: 1215it [00:02, 923.31it/s]warmup run: 1224it [00:02, 943.25it/s]warmup run: 1209it [00:02, 912.75it/s]warmup run: 1250it [00:02, 910.97it/s]warmup run: 1253it [00:02, 508.10it/s]warmup run: 1204it [00:02, 534.37it/s]warmup run: 1223it [00:02, 508.81it/s]warmup run: 1268it [00:02, 492.26it/s]warmup run: 1315it [00:02, 943.05it/s]warmup run: 1305it [00:02, 925.87it/s]warmup run: 1322it [00:02, 945.26it/s]warmup run: 1348it [00:02, 928.28it/s]warmup run: 1307it [00:02, 515.40it/s]warmup run: 1258it [00:02, 528.74it/s]warmup run: 1274it [00:02, 503.43it/s]warmup run: 1318it [00:02, 491.33it/s]warmup run: 1401it [00:02, 858.81it/s]warmup run: 1359it [00:02, 509.78it/s]warmup run: 1311it [00:02, 521.15it/s]warmup run: 1444it [00:02, 822.38it/s]warmup run: 1325it [00:02, 493.34it/s]warmup run: 1412it [00:02, 788.05it/s]warmup run: 1420it [00:02, 785.49it/s]warmup run: 1369it [00:02, 494.13it/s]warmup run: 1411it [00:03, 507.11it/s]warmup run: 1365it [00:03, 524.18it/s]warmup run: 1375it [00:03, 482.45it/s]warmup run: 1420it [00:03, 496.22it/s]warmup run: 1531it [00:03, 727.02it/s]warmup run: 1490it [00:03, 702.35it/s]warmup run: 1497it [00:03, 697.35it/s]warmup run: 1505it [00:03, 705.34it/s]warmup run: 1462it [00:03, 503.12it/s]warmup run: 1423it [00:03, 538.16it/s]warmup run: 1426it [00:03, 489.48it/s]warmup run: 1470it [00:03, 497.28it/s]warmup run: 1513it [00:03, 503.10it/s]warmup run: 1479it [00:03, 543.90it/s]warmup run: 1609it [00:03, 656.98it/s]warmup run: 1581it [00:03, 659.49it/s]warmup run: 1477it [00:03, 493.36it/s]warmup run: 1573it [00:03, 641.52it/s]warmup run: 1567it [00:03, 626.76it/s]warmup run: 1526it [00:03, 515.34it/s]warmup run: 1564it [00:03, 504.29it/s]warmup run: 1536it [00:03, 551.47it/s]warmup run: 1527it [00:03, 492.23it/s]warmup run: 1679it [00:03, 629.87it/s]warmup run: 1651it [00:03, 633.06it/s]warmup run: 1585it [00:03, 535.50it/s]warmup run: 1642it [00:03, 619.90it/s]warmup run: 1635it [00:03, 591.78it/s]warmup run: 1616it [00:03, 507.55it/s]warmup run: 1594it [00:03, 557.82it/s]warmup run: 1578it [00:03, 495.28it/s]warmup run: 1639it [00:03, 526.62it/s]warmup run: 1745it [00:03, 603.57it/s]warmup run: 1707it [00:03, 606.85it/s]warmup run: 1717it [00:03, 613.91it/s]warmup run: 1698it [00:03, 566.06it/s]warmup run: 1667it [00:03, 507.31it/s]warmup run: 1650it [00:03, 558.24it/s]warmup run: 1633it [00:03, 511.38it/s]warmup run: 1692it [00:03, 522.33it/s]warmup run: 1780it [00:03, 598.04it/s]warmup run: 1770it [00:03, 579.64it/s]warmup run: 1807it [00:03, 564.99it/s]warmup run: 1719it [00:03, 509.41it/s]warmup run: 1707it [00:03, 561.21it/s]warmup run: 1757it [00:03, 530.33it/s]warmup run: 1690it [00:03, 528.69it/s]warmup run: 1745it [00:03, 512.98it/s]warmup run: 1841it [00:03, 586.26it/s]warmup run: 1865it [00:03, 555.43it/s]warmup run: 1772it [00:03, 512.96it/s]warmup run: 1830it [00:03, 553.47it/s]warmup run: 1764it [00:03, 550.36it/s]warmup run: 1747it [00:03, 539.29it/s]warmup run: 1812it [00:03, 512.98it/s]warmup run: 1797it [00:03, 496.73it/s]warmup run: 1825it [00:03, 516.68it/s]warmup run: 1901it [00:03, 559.96it/s]warmup run: 1922it [00:03, 547.78it/s]warmup run: 1820it [00:03, 546.03it/s]warmup run: 1887it [00:03, 529.40it/s]warmup run: 1801it [00:03, 534.07it/s]warmup run: 1865it [00:03, 482.86it/s]warmup run: 1851it [00:03, 506.17it/s]warmup run: 1882it [00:03, 530.87it/s]warmup run: 1978it [00:03, 547.56it/s]warmup run: 1958it [00:03, 538.03it/s]warmup run: 1875it [00:03, 543.74it/s]warmup run: 1855it [00:03, 530.91it/s]warmup run: 1941it [00:03, 513.13it/s]warmup run: 1914it [00:04, 471.42it/s]warmup run: 1902it [00:04, 492.77it/s]warmup run: 1937it [00:04, 535.89it/s]warmup run: 2034it [00:04, 537.78it/s]warmup run: 1930it [00:04, 535.70it/s]warmup run: 2013it [00:04, 524.50it/s]warmup run: 1911it [00:04, 537.64it/s]warmup run: 1993it [00:04, 511.53it/s]warmup run: 1966it [00:04, 481.64it/s]warmup run: 1953it [00:04, 496.01it/s]warmup run: 1991it [00:04, 531.76it/s]warmup run: 2090it [00:04, 541.85it/s]warmup run: 1984it [00:04, 530.28it/s]warmup run: 1966it [00:04, 540.94it/s]warmup run: 2066it [00:04, 514.94it/s]warmup run: 2045it [00:04, 508.87it/s]warmup run: 2019it [00:04, 492.42it/s]warmup run: 2005it [00:04, 502.76it/s]warmup run: 2045it [00:04, 516.51it/s]warmup run: 2147it [00:04, 549.49it/s]warmup run: 2041it [00:04, 541.51it/s]warmup run: 2021it [00:04, 540.54it/s]warmup run: 2099it [00:04, 517.13it/s]warmup run: 2118it [00:04, 502.36it/s]warmup run: 2072it [00:04, 501.42it/s]warmup run: 2061it [00:04, 517.29it/s]warmup run: 2098it [00:04, 519.55it/s]warmup run: 2205it [00:04, 555.68it/s]warmup run: 2096it [00:04, 538.59it/s]warmup run: 2076it [00:04, 537.14it/s]warmup run: 2170it [00:04, 506.23it/s]warmup run: 2151it [00:04, 507.51it/s]warmup run: 2125it [00:04, 508.29it/s]warmup run: 2118it [00:04, 532.54it/s]warmup run: 2261it [00:04, 554.50it/s]warmup run: 2151it [00:04, 512.98it/s]warmup run: 2150it [00:04, 537.43it/s]warmup run: 2130it [00:04, 524.56it/s]warmup run: 2222it [00:04, 508.47it/s]warmup run: 2202it [00:04, 500.47it/s]warmup run: 2180it [00:04, 517.70it/s]warmup run: 2172it [00:04, 531.02it/s]warmup run: 2317it [00:04, 548.59it/s]warmup run: 2203it [00:04, 508.72it/s]warmup run: 2204it [00:04, 528.94it/s]warmup run: 2184it [00:04, 527.96it/s]warmup run: 2280it [00:04, 527.73it/s]warmup run: 2253it [00:04, 492.84it/s]warmup run: 2236it [00:04, 527.61it/s]warmup run: 2226it [00:04, 529.58it/s]warmup run: 2254it [00:04, 508.04it/s]warmup run: 2372it [00:04, 545.37it/s]warmup run: 2257it [00:04, 520.42it/s]warmup run: 2237it [00:04, 526.19it/s]warmup run: 2333it [00:04, 524.86it/s]warmup run: 2303it [00:04, 488.32it/s]warmup run: 2290it [00:04, 529.67it/s]warmup run: 2281it [00:04, 534.54it/s]warmup run: 2305it [00:04, 506.39it/s]warmup run: 2427it [00:04, 543.68it/s]warmup run: 2310it [00:04, 512.21it/s]warmup run: 2290it [00:04, 519.41it/s]warmup run: 2386it [00:04, 505.09it/s]warmup run: 2356it [00:04, 498.73it/s]warmup run: 2344it [00:04, 524.22it/s]warmup run: 2335it [00:04, 529.53it/s]warmup run: 2360it [00:04, 518.93it/s]warmup run: 2482it [00:04, 533.91it/s]warmup run: 2344it [00:04, 524.93it/s]warmup run: 2362it [00:04, 500.93it/s]warmup run: 2439it [00:04, 510.43it/s]warmup run: 2410it [00:04, 508.59it/s]warmup run: 2391it [00:04, 537.47it/s]warmup run: 2397it [00:04, 516.34it/s]warmup run: 2415it [00:04, 527.71it/s]warmup run: 2536it [00:04, 519.80it/s]warmup run: 2399it [00:05, 531.92it/s]warmup run: 2496it [00:05, 527.57it/s]warmup run: 2413it [00:05, 489.84it/s]warmup run: 2463it [00:05, 511.99it/s]warmup run: 2446it [00:05, 539.96it/s]warmup run: 2449it [00:05, 510.52it/s]warmup run: 2469it [00:05, 530.64it/s]warmup run: 2589it [00:05, 520.13it/s]warmup run: 2453it [00:05, 529.97it/s]warmup run: 2553it [00:05, 538.45it/s]warmup run: 2467it [00:05, 501.93it/s]warmup run: 2519it [00:05, 525.04it/s]warmup run: 2501it [00:05, 533.08it/s]warmup run: 2501it [00:05, 482.35it/s]warmup run: 2523it [00:05, 528.31it/s]warmup run: 2642it [00:05, 521.08it/s]warmup run: 2507it [00:05, 528.06it/s]warmup run: 2611it [00:05, 549.79it/s]warmup run: 2519it [00:05, 505.19it/s]warmup run: 2577it [00:05, 539.07it/s]warmup run: 2555it [00:05, 531.39it/s]warmup run: 2580it [00:05, 539.26it/s]warmup run: 2550it [00:05, 469.66it/s]warmup run: 2560it [00:05, 523.27it/s]warmup run: 2668it [00:05, 554.86it/s]warmup run: 2573it [00:05, 514.01it/s]warmup run: 2634it [00:05, 545.90it/s]warmup run: 2609it [00:05, 523.26it/s]warmup run: 2606it [00:05, 493.21it/s]warmup run: 2616it [00:05, 532.33it/s]warmup run: 2628it [00:05, 522.99it/s]warmup run: 2660it [00:05, 504.65it/s]warmup run: 2695it [00:05, 332.14it/s]warmup run: 2637it [00:05, 348.58it/s]warmup run: 2745it [00:05, 366.76it/s]warmup run: 2689it [00:05, 346.39it/s]warmup run: 2662it [00:05, 334.43it/s]warmup run: 2692it [00:05, 390.04it/s]warmup run: 2724it [00:05, 311.68it/s]warmup run: 2794it [00:05, 394.36it/s]warmup run: 2670it [00:05, 342.96it/s]warmup run: 2681it [00:05, 333.90it/s]warmup run: 2740it [00:05, 380.42it/s]warmup run: 2715it [00:05, 374.51it/s]warmup run: 2711it [00:05, 330.78it/s]warmup run: 2746it [00:05, 424.74it/s]warmup run: 2776it [00:05, 351.44it/s]warmup run: 2843it [00:05, 415.97it/s]warmup run: 2727it [00:05, 390.10it/s]warmup run: 2737it [00:05, 380.82it/s]warmup run: 2790it [00:05, 407.06it/s]warmup run: 2767it [00:05, 407.43it/s]warmup run: 2764it [00:05, 372.68it/s]warmup run: 2800it [00:05, 453.24it/s]warmup run: 2832it [00:05, 394.73it/s]warmup run: 2900it [00:05, 454.64it/s]warmup run: 2784it [00:05, 431.92it/s]warmup run: 2793it [00:05, 421.40it/s]warmup run: 2844it [00:05, 438.92it/s]warmup run: 2819it [00:05, 434.44it/s]warmup run: 2816it [00:05, 405.54it/s]warmup run: 2855it [00:05, 476.52it/s]warmup run: 2888it [00:05, 431.65it/s]warmup run: 2958it [00:06, 486.95it/s]warmup run: 2841it [00:06, 465.75it/s]warmup run: 2850it [00:06, 456.89it/s]warmup run: 2900it [00:06, 469.24it/s]warmup run: 2869it [00:06, 450.92it/s]warmup run: 2870it [00:06, 438.24it/s]warmup run: 2907it [00:06, 486.35it/s]warmup run: 3000it [00:06, 493.13it/s]warmup run: 2939it [00:06, 449.97it/s]warmup run: 2895it [00:06, 482.99it/s]warmup run: 2904it [00:06, 478.49it/s]warmup run: 2955it [00:06, 489.62it/s]warmup run: 2920it [00:06, 465.38it/s]warmup run: 2927it [00:06, 472.04it/s]warmup run: 2961it [00:06, 500.98it/s]warmup run: 2990it [00:06, 463.40it/s]warmup run: 2948it [00:06, 484.58it/s]warmup run: 3000it [00:06, 482.36it/s]warmup run: 3000it [00:06, 482.27it/s]warmup run: 2956it [00:06, 489.06it/s]warmup run: 3000it [00:06, 479.34it/s]warmup run: 2973it [00:06, 483.03it/s]warmup run: 2979it [00:06, 478.18it/s]warmup run: 3000it [00:06, 475.35it/s]warmup run: 3000it [00:06, 474.90it/s]warmup run: 3000it [00:06, 474.59it/s]warmup run: 3000it [00:06, 474.46it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.03it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.56it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.94it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.45it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.55it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.40it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.89it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.19it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.50it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1710.34it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.40it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1658.94it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.97it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1669.74it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1662.63it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1618.06it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1659.52it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1711.74it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1661.66it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1649.76it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.71it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1661.55it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1660.09it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1636.65it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1661.93it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1660.44it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1691.93it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1708.79it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.64it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1661.29it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1659.39it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1640.50it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1688.39it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1656.70it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1653.12it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1659.27it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1645.23it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1698.59it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1658.87it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1619.24it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1656.40it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1685.88it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1656.02it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1650.82it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1654.09it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1654.97it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1685.72it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1588.55it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1656.33it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1684.34it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1661.15it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1651.34it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1651.85it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1653.51it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1681.92it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1599.23it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:00, 1666.02it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1657.75it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1681.78it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:01, 1661.69it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1645.36it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:01, 1644.39it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1682.67it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1607.85it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1668.83it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1653.76it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1671.87it/s]warmup should be done:  51%|█████     | 1526/3000 [00:00<00:00, 1680.55it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1646.08it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1641.96it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1684.41it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1588.74it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1669.67it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1676.67it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1652.19it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1679.12it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1645.44it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1684.13it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1638.44it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1594.30it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1671.80it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1679.53it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1654.51it/s]warmup should be done:  62%|██████▏   | 1863/3000 [00:01<00:00, 1679.03it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1640.17it/s]warmup should be done:  62%|██████▎   | 1875/3000 [00:01<00:00, 1690.13it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1639.20it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1597.62it/s]warmup should be done:  67%|██████▋   | 2007/3000 [00:01<00:00, 1675.23it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1681.17it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1657.11it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1674.34it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1642.42it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1694.37it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1637.62it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1606.27it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1678.85it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1682.06it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1658.82it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1644.37it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1693.70it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1667.88it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1634.82it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1607.86it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1682.46it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1682.96it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1659.93it/s]warmup should be done:  77%|███████▋  | 2321/3000 [00:01<00:00, 1640.00it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1663.77it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1634.68it/s]warmup should be done:  80%|███████▉  | 2386/3000 [00:01<00:00, 1686.78it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1612.78it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1685.73it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1683.36it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1657.24it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1643.77it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1661.85it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1631.67it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1684.50it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1612.63it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1684.10it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1687.51it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1653.87it/s]warmup should be done:  88%|████████▊ | 2653/3000 [00:01<00:00, 1645.63it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1685.42it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1660.02it/s]warmup should be done:  88%|████████▊ | 2647/3000 [00:01<00:00, 1628.67it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1609.40it/s]warmup should be done:  95%|█████████▌| 2859/3000 [00:01<00:00, 1684.13it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1689.73it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1652.87it/s]warmup should be done:  94%|█████████▍| 2819/3000 [00:01<00:00, 1647.97it/s]warmup should be done:  96%|█████████▋| 2894/3000 [00:01<00:00, 1687.73it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1633.84it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1656.65it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1611.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.71it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1652.78it/s]warmup should be done: 100%|█████████▉| 2985/3000 [00:01<00:00, 1650.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.05it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1635.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.12it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1618.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.05it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1728.96it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1727.76it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.25it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.41it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.26it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1734.10it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.36it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.20it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1727.01it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.16it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1696.72it/s]warmup should be done:  12%|█▏        | 349/3000 [00:00<00:01, 1737.65it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1695.88it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1706.77it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1661.35it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1671.21it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1726.22it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1672.55it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1699.01it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1697.67it/s]warmup should be done:  17%|█▋        | 524/3000 [00:00<00:01, 1740.80it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1716.45it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1676.53it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1691.18it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1698.04it/s]warmup should be done:  23%|██▎       | 692/3000 [00:00<00:01, 1722.90it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1670.99it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1694.64it/s]warmup should be done:  23%|██▎       | 699/3000 [00:00<00:01, 1739.26it/s]warmup should be done:  23%|██▎       | 693/3000 [00:00<00:01, 1723.16it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1684.46it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1677.89it/s]warmup should be done:  29%|██▉       | 867/3000 [00:00<00:01, 1730.30it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1672.12it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1696.31it/s]warmup should be done:  29%|██▉       | 867/3000 [00:00<00:01, 1728.24it/s]warmup should be done:  29%|██▉       | 873/3000 [00:00<00:01, 1732.96it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1683.79it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1685.01it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1685.25it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1676.04it/s]warmup should be done:  35%|███▍      | 1043/3000 [00:00<00:01, 1737.15it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1701.11it/s]warmup should be done:  35%|███▍      | 1043/3000 [00:00<00:01, 1736.94it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1694.27it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1688.62it/s]warmup should be done:  35%|███▍      | 1047/3000 [00:00<00:01, 1728.17it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1691.33it/s]warmup should be done:  41%|████      | 1218/3000 [00:00<00:01, 1741.23it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1701.31it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1670.16it/s]warmup should be done:  41%|████      | 1219/3000 [00:00<00:01, 1741.38it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1700.61it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1690.60it/s]warmup should be done:  41%|████      | 1220/3000 [00:00<00:01, 1724.42it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1694.64it/s]warmup should be done:  46%|████▋     | 1395/3000 [00:00<00:00, 1748.10it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1707.44it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1674.17it/s]warmup should be done:  47%|████▋     | 1396/3000 [00:00<00:00, 1749.74it/s]warmup should be done:  46%|████▌     | 1369/3000 [00:00<00:00, 1712.07it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1691.11it/s]warmup should be done:  46%|████▋     | 1393/3000 [00:00<00:00, 1725.03it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:00, 1701.83it/s]warmup should be done:  52%|█████▏    | 1571/3000 [00:00<00:00, 1750.94it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1708.31it/s]warmup should be done:  52%|█████▏    | 1572/3000 [00:00<00:00, 1752.21it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1674.53it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1718.54it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1690.83it/s]warmup should be done:  52%|█████▏    | 1566/3000 [00:00<00:00, 1719.23it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1702.17it/s]warmup should be done:  58%|█████▊    | 1747/3000 [00:01<00:00, 1752.53it/s]warmup should be done:  58%|█████▊    | 1748/3000 [00:01<00:00, 1753.95it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1712.96it/s]warmup should be done:  57%|█████▋    | 1716/3000 [00:01<00:00, 1721.56it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1671.57it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1691.41it/s]warmup should be done:  58%|█████▊    | 1738/3000 [00:01<00:00, 1714.96it/s]warmup should be done:  57%|█████▋    | 1710/3000 [00:01<00:00, 1702.37it/s]warmup should be done:  64%|██████▍   | 1923/3000 [00:01<00:00, 1753.21it/s]warmup should be done:  64%|██████▍   | 1925/3000 [00:01<00:00, 1756.15it/s]warmup should be done:  63%|██████▎   | 1887/3000 [00:01<00:00, 1722.12it/s]warmup should be done:  63%|██████▎   | 1890/3000 [00:01<00:00, 1724.82it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1674.87it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1691.56it/s]warmup should be done:  64%|██████▎   | 1910/3000 [00:01<00:00, 1714.49it/s]warmup should be done:  63%|██████▎   | 1882/3000 [00:01<00:00, 1706.97it/s]warmup should be done:  70%|██████▉   | 2099/3000 [00:01<00:00, 1754.62it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1758.98it/s]warmup should be done:  69%|██████▊   | 2062/3000 [00:01<00:00, 1728.42it/s]warmup should be done:  69%|██████▉   | 2064/3000 [00:01<00:00, 1729.28it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1669.81it/s]warmup should be done:  68%|██████▊   | 2037/3000 [00:01<00:00, 1690.40it/s]warmup should be done:  69%|██████▉   | 2082/3000 [00:01<00:00, 1714.60it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1710.85it/s]warmup should be done:  76%|███████▌  | 2276/3000 [00:01<00:00, 1756.63it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1760.64it/s]warmup should be done:  75%|███████▍  | 2236/3000 [00:01<00:00, 1730.90it/s]warmup should be done:  75%|███████▍  | 2239/3000 [00:01<00:00, 1732.75it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1672.25it/s]warmup should be done:  74%|███████▎  | 2207/3000 [00:01<00:00, 1690.23it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1713.70it/s]warmup should be done:  74%|███████▍  | 2227/3000 [00:01<00:00, 1702.21it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1756.19it/s]warmup should be done:  82%|████████▏ | 2456/3000 [00:01<00:00, 1761.12it/s]warmup should be done:  80%|████████  | 2410/3000 [00:01<00:00, 1731.62it/s]warmup should be done:  80%|████████  | 2414/3000 [00:01<00:00, 1735.05it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1675.37it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1690.19it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1711.17it/s]warmup should be done:  80%|████████  | 2400/3000 [00:01<00:00, 1710.24it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1756.80it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1731.08it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1760.29it/s]warmup should be done:  86%|████████▋ | 2589/3000 [00:01<00:00, 1737.22it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1678.23it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1691.15it/s]warmup should be done:  87%|████████▋ | 2598/3000 [00:01<00:00, 1709.65it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1717.34it/s]warmup should be done:  94%|█████████▎| 2805/3000 [00:01<00:00, 1758.29it/s]warmup should be done:  92%|█████████▏| 2758/3000 [00:01<00:00, 1731.75it/s]warmup should be done:  92%|█████████▏| 2764/3000 [00:01<00:00, 1740.06it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1758.27it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1680.04it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1691.93it/s]warmup should be done:  92%|█████████▏| 2770/3000 [00:01<00:00, 1710.68it/s]warmup should be done:  92%|█████████▏| 2749/3000 [00:01<00:00, 1724.65it/s]warmup should be done:  99%|█████████▉| 2981/3000 [00:01<00:00, 1756.47it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1733.12it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1742.07it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1749.29it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1692.04it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1672.30it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1711.45it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1727.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1747.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1746.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1721.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1718.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1717.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1706.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.18it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff2dc0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff2dd1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff39a790>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff2dc160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff2d4310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff2db2e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff2d4250>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fcbff399b80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 19:52:29.143957: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc7332ec680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.144019: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.153562: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.165933: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc736829770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.165984: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.175246: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.187640: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc73a82e040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.187691: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.196474: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.714093: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc73e82e750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.714157: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.723672: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.747038: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc736b818b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.747094: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.755669: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc736829f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.755723: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.756011: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc72e831720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.756056: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.756181: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.759992: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc73eb8a1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:52:29.760045: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:52:29.765295: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.765297: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:29.769683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:52:31.853751: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:31.925849: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:31.926018: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:32.170597: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:32.202112: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:32.205170: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:32.260564: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:52:32.266136: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:52:53.723][ERROR][RK0][tid #140494050932480]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:52:53.723][ERROR][RK0][tid #140494050932480]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.736][ERROR][RK0][tid #140494050932480]: coll ps creation done
[HCTR][19:52:53.736][ERROR][RK0][tid #140494050932480]: replica 4 waits for coll ps creation barrier
[HCTR][19:52:53.799][ERROR][RK0][tid #140495116281600]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:52:53.799][ERROR][RK0][tid #140495116281600]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.804][ERROR][RK0][tid #140495116281600]: coll ps creation done
[HCTR][19:52:53.804][ERROR][RK0][tid #140495116281600]: replica 5 waits for coll ps creation barrier
[HCTR][19:52:53.878][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:52:53.878][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.883][ERROR][RK0][main]: coll ps creation done
[HCTR][19:52:53.883][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][19:52:53.899][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:52:53.899][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.904][ERROR][RK0][main]: coll ps creation done
[HCTR][19:52:53.904][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][19:52:53.921][ERROR][RK0][tid #140494243866368]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:52:53.921][ERROR][RK0][tid #140494243866368]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.926][ERROR][RK0][tid #140494243866368]: coll ps creation done
[HCTR][19:52:53.927][ERROR][RK0][tid #140494243866368]: replica 3 waits for coll ps creation barrier
[HCTR][19:52:53.957][ERROR][RK0][tid #140494101255936]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:52:53.957][ERROR][RK0][tid #140494101255936]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.965][ERROR][RK0][tid #140494101255936]: coll ps creation done
[HCTR][19:52:53.965][ERROR][RK0][tid #140494101255936]: replica 2 waits for coll ps creation barrier
[HCTR][19:52:53.985][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:52:53.985][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:53.993][ERROR][RK0][main]: coll ps creation done
[HCTR][19:52:53.993][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][19:52:54.024][ERROR][RK0][tid #140494201935616]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:52:54.024][ERROR][RK0][tid #140494201935616]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][19:52:54.029][ERROR][RK0][tid #140494201935616]: coll ps creation done
[HCTR][19:52:54.029][ERROR][RK0][tid #140494201935616]: replica 1 waits for coll ps creation barrier
[HCTR][19:52:54.029][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][19:53:01.049][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][19:53:01.084][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][tid #140494101255936]: replica 2 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][tid #140494201935616]: replica 1 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][tid #140494243866368]: replica 3 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][tid #140494050932480]: replica 4 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][tid #140495116281600]: replica 5 calling init per replica
[HCTR][19:53:01.084][ERROR][RK0][main]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][tid #140494101255936]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][tid #140494201935616]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][tid #140494243866368]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][tid #140494050932480]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][main]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][main]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][tid #140495116281600]: Calling build_v2
[HCTR][19:53:01.084][ERROR][RK0][tid #140494101255936]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][tid #140494201935616]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][tid #140494243866368]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][tid #140494050932480]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:53:01.084][ERROR][RK0][tid #140495116281600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-11 19:53:01[[2022-12-11 19:53:012022-12-11 19:53:012022-12-11 19:53:01.2022-12-11 19:53:01..2022-12-11 19:53:01.2022-12-11 19:53:01 848282022-12-11 19:53:01. 84828 84828. 84829.: . 84844: :  84854:  84854E 84843: EE: E:  : E  E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136136:136:] :136] ] 136] 136using concurrent impl MPS136] using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS] 
] using concurrent impl MPS

using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS



[2022-12-11 19:53:01. 89217: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:53:01. 89256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 19:53:01196.]  89264assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:53:01[.2022-12-11 19:53:01 89311.: [ 89309E2022-12-11 19:53:01:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 89325 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196E:]  [178assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:53:01] 
:.v100x8, slow pcie212 89355
] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[
 2022-12-11 19:53:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.:2022-12-11 19:53:01 89399178.[[: ]  894012022-12-11 19:53:012022-12-11 19:53:01Ev100x8, slow pcie: .. 
E 89418 89421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : : :[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE1962022-12-11 19:53:012022-12-11 19:53:01:  ] ..178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[ 89457 89452] ::
2022-12-11 19:53:01: : v100x8, slow pcie212213[.EE
] ] 2022-12-11 19:53:01 89499  [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421[.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:53:01

2022-12-11 19:53:01 89541E::..[:  [196178 89581 895882022-12-11 19:53:01E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:53:01] ] : : . :.assigning 8 to cpuv100x8, slow pcieEE 89642/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178 89647

  : :] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E178v100x8, slow pcieE::2022-12-11 19:53:01 ] [
 212196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-11 19:53:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] []  89762:
.:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 19:53:01assigning 8 to cpu: 214 89793213[
.
E] : ] 2022-12-11 19:53:01 89830 [cpu time is 97.0588Eremote time is 8.68421.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:53:01
 
 89883E:[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:  1962022-12-11 19:53:01 89926:2022-12-11 19:53:01E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .: 212. :assigning 8 to cpu 89979E]  89971/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196
:  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E196assigning 8 to cpu : ] [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[2022-12-11 19:53:01:] :
2022-12-11 19:53:01.214remote time is 8.68421212. 90102] 
]  90114: cpu time is 97.0588[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [E
[2022-12-11 19:53:01
E2022-12-11 19:53:01 2022-12-11 19:53:01. ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[. 90177/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 90184:2022-12-11 19:53:01 90191: :: 213.: E212E]  90233E ]  remote time is 8.68421:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
: :[212214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-11 19:53:01[] ] :] .2022-12-11 19:53:01build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 90338.

] 
:  90359remote time is 8.68421E[: [
 2022-12-11 19:53:01E2022-12-11 19:53:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[ .: 904412022-12-11 19:53:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 90448214: .:: ] E 90471213Ecpu time is 97.0588 : ]  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213] [:] remote time is 8.684212022-12-11 19:53:01214remote time is 8.68421
.] 
 90608[cpu time is 97.0588: 2022-12-11 19:53:01[
E.2022-12-11 19:53:01  90648./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  90657:E: 214 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 19:54:44.758462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:54:45.109019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 19:54:45.109107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 35310982
[2022-12-11 19:54:46.529636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:54:46.529751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:54:46.529796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:54:46.529839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:54:46.530256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.534369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.538436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.662800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 19:54:46.662882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 19:54:46.663282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.664624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 19:54:46.664682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 19:54:46.665023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.665074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 19:54:46.665132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 19:54:46.665477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.666673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 19:54:46.666745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:54:46.667120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.667222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 19:54:46.667275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 19:54:46.667621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.669725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:54:46.669778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:54:46.670113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.670309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:54:46.670369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:54:46.670736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.691912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.697488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.697669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.701332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.701726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.701812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.705630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.733029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.733308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.733531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.733617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.737306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.737388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:46.741200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:54:47.225882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:54:47.226295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:54:47.226344: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[2022-12-11 19:54:47.244104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:54:47.244222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:54:47.244268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:54:47.248630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.249558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.257052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.258605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.264365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.264428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[[[[[2022-12-11 19:54:472022-12-11 19:54:472022-12-11 19:54:472022-12-11 19:54:472022-12-11 19:54:47.....469856469856469849469856469856: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[2022-12-11 19:54:47.[[470326[[2022-12-11 19:54:472022-12-11 19:54:47: 2022-12-11 19:54:472022-12-11 19:54:47..E..470331470333 470332470332: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : EE:EE  1980 ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::
:1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[

2022-12-11 19:54:47.470456: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683[[] 2022-12-11 19:54:472022-12-11 19:54:47[using empty feat=27[..2022-12-11 19:54:47
2022-12-11 19:54:47470482470485..: : 470490470492WW: :   WW/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu16831683::] ] 16831683using empty feat=27using empty feat=27] ] 

using empty feat=27using empty feat=27

[2022-12-11 19:54:47.487838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:54:47.487913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 19:54:47.488186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:54:47.488249[: 2022-12-11 19:54:47W. 488253/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1683 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuusing empty feat=27:
1980] eager alloc mem 1024.00 Bytes
[2022-12-11 19:54:47.488319: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1683] using empty feat=27
[2022-12-11 19:54:47.488855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:54:47.488926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:54:47.488966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:54:47.489016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:54:47.489083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 19:54:47eager release cuda mem 2.
489085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:54:47.489137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340[
2022-12-11 19:54:47.489157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2[
2022-12-11 19:54:47.489168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10242022-12-11 19:54:47
.489209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:54:47.489235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-11 19:54:472022-12-11 19:54:47..489261489278: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 3531098340

[2022-12-11 19:54:47.489346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:54:47.489389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:54:47.493404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.498309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.502701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.506881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.510812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.511918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.512252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.512519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.512576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.512687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.519815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.520137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.520252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.520421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.520481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.520513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.520590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.521014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.521072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.521138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.525012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 19:54:47.525078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:54:47[.2022-12-11 19:54:47525118.: 525108E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 3531098340] 
eager release cuda mem 1024
[2022-12-11 19:54:47.525195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 19:54:47.525234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:54:47.525835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.525881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:54:47.526270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.526313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:54:47.526712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.526759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 16.84 GB2022-12-11 19:54:47
.526773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.526823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 19:54:47eager alloc mem 16.84 GB.
526836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.526886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:54:47.529451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.533604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:54:47.803143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 19:54:47eager alloc mem 5.26 MB.
803174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:47.813413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 19:54:47638.] 813430eager release cuda mem 5518079: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:47.813893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.813965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:54:47.818357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.818420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[2022-12-11 19:54:47.818444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:54:47.818492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.84 GB
[[[[[[[[2022-12-11 19:54:502022-12-11 19:54:502022-12-11 19:54:502022-12-11 19:54:502022-12-11 19:54:502022-12-11 19:54:502022-12-11 19:54:502022-12-11 19:54:50........769763769763769764769763769768769763769764769775: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB







[2022-12-11 19:54:50.779242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.779597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.780778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.780946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.781011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.781095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.781171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.781246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.781329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.781417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.789897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.789960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.790458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.790747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.791265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.791668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.791739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.791848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.791916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.792176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.799396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.799664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.799706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.800012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.800108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.800170: E[ 2022-12-11 19:54:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:800187638: [] E2022-12-11 19:54:50eager release cuda mem 5518079 .
800204/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 5.26 MB:
638[] 2022-12-11 19:54:50eager release cuda mem 5518079.
800255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.800305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.801010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.801330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.801473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.801783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.801849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.801919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.806427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.806738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.808338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.808639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.809402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.809657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[[2022-12-11 19:54:502022-12-11 19:54:50..809695809702: : EE[  2022-12-11 19:54:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.::8097356381980: ] ] Eeager release cuda mem 5518079[eager alloc mem 5.26 MB 
2022-12-11 19:54:50
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:809794638: ] E[eager release cuda mem 5518079 2022-12-11 19:54:50
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:809835638: ] Eeager release cuda mem 5518079 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.810214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.810814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.811068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.811191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.811454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.813106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.813415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.815012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.815350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.817429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.817727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.818167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.818485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.818552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.818655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.818705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.818760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.819361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.819498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.819611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.819679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.819774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.821133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.821885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.822208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.824085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.824392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.825496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.825810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.826992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.827119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.827171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.827232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.827358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.827554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.827837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.827977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.828143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:54:50.828582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.828901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.829623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.829913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.16664 secs 
[2022-12-11 19:54:50.830240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.16262 secs 
[2022-12-11 19:54:50.830752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.831587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.832170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.833338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.16623 secs 
[2022-12-11 19:54:50.833442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.834991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.835446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.835484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.17047 secs 
[2022-12-11 19:54:50.835556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.835678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:54:50.836307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.837313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.838403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.838468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:54:50.839149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.16904 secs 
[2022-12-11 19:54:50.840978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.31073 secs 
[2022-12-11 19:54:50.841534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.17606 secs 
[2022-12-11 19:54:50.841759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 0 / 882774585 nodes ( 0.00 %) | cpu 847463603 / 882774585 nodes ( 96.00 %) | 16.84 GB | 4.17103 secs 
[2022-12-11 19:54:50.843471: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.23 GB
[2022-12-11 19:54:52.309578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.49 GB
[2022-12-11 19:54:52.310305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.49 GB
[2022-12-11 19:54:52.311619: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.49 GB
[2022-12-11 19:54:53.786700: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.75 GB
[2022-12-11 19:54:53.786932: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.75 GB
[2022-12-11 19:54:53.787722: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.75 GB
[2022-12-11 19:54:55.177632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.97 GB
[2022-12-11 19:54:55.177999: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.97 GB
[2022-12-11 19:54:55.178455: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.97 GB
[2022-12-11 19:54:56.546884: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.18 GB
[2022-12-11 19:54:56.547286: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.18 GB
[2022-12-11 19:54:56.549492: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.18 GB
[2022-12-11 19:54:57.735319: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.64 GB
[2022-12-11 19:54:57.736283: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.64 GB
[2022-12-11 19:54:57.761531: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.64 GB
[2022-12-11 19:54:59.656828: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.84 GB
[2022-12-11 19:54:59.658558: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.84 GB
[HCTR][19:55:01.013][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][tid #140494101255936]: replica 2 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][tid #140495116281600]: replica 5 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][tid #140494243866368]: replica 3 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][tid #140494050932480]: replica 4 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][tid #140494201935616]: replica 1 calling init per replica done, doing barrier
[HCTR][19:55:01.013][ERROR][RK0][tid #140494201935616]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494101255936]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494243866368]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494050932480]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][main]: init per replica done
[HCTR][19:55:01.013][ERROR][RK0][tid #140495116281600]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494201935616]: init per replica done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494101255936]: init per replica done
[HCTR][19:55:01.013][ERROR][RK0][main]: init per replica done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494243866368]: init per replica done
[HCTR][19:55:01.013][ERROR][RK0][tid #140494050932480]: init per replica done
[HCTR][19:55:01.013][ERROR][RK0][tid #140495116281600]: init per replica done
[HCTR][19:55:01.034][ERROR][RK0][main]: init per replica done
