2022-12-12 04:10:40.360138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.367526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.372906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.376595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.382021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.400443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.408736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.412894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.464079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.466088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.467122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.468220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.469240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.470282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.471353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.472564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.474374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.475462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.475532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.477320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.477328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.478826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.478933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.480253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.480556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.481755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.482252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.483398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.483838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.485016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.485530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.486929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.487223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.488715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.489741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.490767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.491841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.492891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.493847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.494783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.500234: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.504321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.506189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.508089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.508182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.510119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.510367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.510576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.511141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.513297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.513484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.513527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.513657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.514077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.517089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.517341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.517414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.517501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.517794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.521091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.521343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.521380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.521429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.522778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.525198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.525449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.525535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.525681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.527204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.527780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.529404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.529488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.529716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.531199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.531969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.533218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.533259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.533303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.533445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.535081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.535741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.536984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.537283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.537429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.539140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.539704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.540423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.541073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.542368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.542868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.543093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.545055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.545437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.545479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.547264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.547466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.547642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.549540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.549643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.558962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.560008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.560569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.561768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.574592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.581488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.584483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.587592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.594568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.599176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.599427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.599461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.599504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.599549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.599587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.603061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.603639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.603763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.603886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.603932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.604016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.604078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.608196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.609634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.609712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.609806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.609859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.609902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.610010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.613381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.614224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.614302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.614346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.614389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.614554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.614604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.618578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.619232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.619314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.619358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.619407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.619578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.619762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.623972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.624636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.624679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.624816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.624869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.624965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.625468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.628657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.629230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.629369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.629486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.629657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.629836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.630532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.633537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.634152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.634208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.634289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.634375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.634802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.635854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.638492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.639165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.639319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.639390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.639573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.639874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.640741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.643514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.644280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.644471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.644515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.644979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.645481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.645907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.648416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.649110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.649273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.649454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.649629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.650139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.650434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.653374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.654056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.654143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.654322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.654373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.654966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.655227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.657805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.658802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.658885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.659012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.659233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.659892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.660133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.662616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.663206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.663333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.663375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.663606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.664273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.666554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.667121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.667213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.667255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.667440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.667996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.669175: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.670429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.671778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.671882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.672521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.672612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.672676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.673016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.675101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.675163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.676706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.676793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.676869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.677893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.679164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.679232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.679673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.680609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.680674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.680835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.681960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.683266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.683338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.683816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.684868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.684919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.685036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.686365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.687447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.687522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.688067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.689308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.689517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.689570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.690698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.691765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.691886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.693508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.693578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.693671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.694968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.696067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.696809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.697510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.697670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.697708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.698942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.699961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.700917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.701813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.701897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.702033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.703118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.706459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.708263: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.708419: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.708975: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.709008: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.709101: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.711297: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:10:40.719506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.719506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.720168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.720418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.720548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.722269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.731452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.731494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.731564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.731644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.731673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.731730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.765549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.765678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.765678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.765731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.765764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:40.765835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.856450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.857072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.857594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.858059: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:41.858118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:10:41.875570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.876230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.876969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.877683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.878407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:41.878875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:10:41.925187: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:41.925394: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:41.972454: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:10:42.059315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.059929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.060467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.060926: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.060990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:10:42.078359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.078999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.079533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.080118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.080644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.081117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:10:42.134094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.134732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.135279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.135749: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.135805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:10:42.158128: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.158202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.158222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.158250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.158251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.158318: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.160084: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:10:42.160570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.160650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.160691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.160724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.162529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.162687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.162703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.162746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.164445: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.164497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:10:42.164750: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.164797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:10:42.164866: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.164913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:10:42.164911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.165859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.166343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:10:42.180185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.180796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.181327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.181585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.181585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.182059: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.182118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:10:42.182231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.183424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.183521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.184008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.184887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.185129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.185574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.186599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.186805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.187234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.188167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.188654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.188999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.189826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:10:42.190114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:10:42.190242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:10:42.199559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.200186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.200693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.201268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.201770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.202239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:10:42.226483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.227288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.227869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.228344: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:10:42.228403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:10:42.235820: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.236026: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.236757: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.236955: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.237876: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:10:42.237898: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.238065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.238689: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:10:42.240053: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 04:10:42.246049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.246772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.247316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.247896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.248294: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.248418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:10:42.248439: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.248885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:10:42.250285: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:10:42.252147: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.252278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.254139: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:10:42.295847: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.296069: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:10:42.297877: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][04:10:43.560][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.564][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.564][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.564][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.564][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.564][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.611][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:10:43.612][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 99it [00:01, 83.68it/s]warmup run: 95it [00:01, 79.73it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 94it [00:01, 80.34it/s]warmup run: 96it [00:01, 81.64it/s]warmup run: 199it [00:01, 182.64it/s]warmup run: 189it [00:01, 171.97it/s]warmup run: 100it [00:01, 86.01it/s]warmup run: 193it [00:01, 179.53it/s]warmup run: 192it [00:01, 176.92it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 300it [00:01, 292.90it/s]warmup run: 285it [00:01, 276.62it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 200it [00:01, 186.22it/s]warmup run: 293it [00:01, 290.12it/s]warmup run: 289it [00:01, 283.27it/s]warmup run: 97it [00:01, 84.84it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 400it [00:01, 406.02it/s]warmup run: 381it [00:01, 385.36it/s]warmup run: 99it [00:01, 84.15it/s]warmup run: 301it [00:01, 297.60it/s]warmup run: 393it [00:01, 404.39it/s]warmup run: 386it [00:01, 393.64it/s]warmup run: 194it [00:01, 183.30it/s]warmup run: 99it [00:01, 87.29it/s]warmup run: 501it [00:02, 517.61it/s]warmup run: 475it [00:02, 487.33it/s]warmup run: 199it [00:01, 183.58it/s]warmup run: 401it [00:01, 410.87it/s]warmup run: 493it [00:02, 515.59it/s]warmup run: 483it [00:02, 500.83it/s]warmup run: 293it [00:01, 293.57it/s]warmup run: 198it [00:01, 188.41it/s]warmup run: 603it [00:02, 622.75it/s]warmup run: 573it [00:02, 590.31it/s]warmup run: 300it [00:01, 294.46it/s]warmup run: 494it [00:02, 508.08it/s]warmup run: 595it [00:02, 621.11it/s]warmup run: 582it [00:02, 603.25it/s]warmup run: 392it [00:01, 407.19it/s]warmup run: 296it [00:01, 297.47it/s]warmup run: 703it [00:02, 708.97it/s]warmup run: 669it [00:02, 674.16it/s]warmup run: 401it [00:01, 409.14it/s]warmup run: 590it [00:02, 602.37it/s]warmup run: 691it [00:02, 696.28it/s]warmup run: 681it [00:02, 692.17it/s]warmup run: 490it [00:01, 515.52it/s]warmup run: 393it [00:01, 408.07it/s]warmup run: 803it [00:02, 779.23it/s]warmup run: 764it [00:02, 741.06it/s]warmup run: 502it [00:02, 520.73it/s]warmup run: 683it [00:02, 677.22it/s]warmup run: 791it [00:02, 770.58it/s]warmup run: 780it [00:02, 765.18it/s]warmup run: 590it [00:02, 617.91it/s]warmup run: 490it [00:01, 515.01it/s]warmup run: 903it [00:02, 835.11it/s]warmup run: 861it [00:02, 799.38it/s]warmup run: 604it [00:02, 624.87it/s]warmup run: 778it [00:02, 744.41it/s]warmup run: 890it [00:02, 827.10it/s]warmup run: 878it [00:02, 820.57it/s]warmup run: 689it [00:02, 704.22it/s]warmup run: 592it [00:02, 622.49it/s]warmup run: 1004it [00:02, 880.54it/s]warmup run: 956it [00:02, 833.74it/s]warmup run: 707it [00:02, 716.93it/s]warmup run: 873it [00:02, 795.96it/s]warmup run: 988it [00:02, 868.24it/s]warmup run: 979it [00:02, 870.84it/s]warmup run: 790it [00:02, 779.96it/s]warmup run: 695it [00:02, 717.68it/s]warmup run: 1105it [00:02, 915.78it/s]warmup run: 1052it [00:02, 866.61it/s]warmup run: 810it [00:02, 793.37it/s]warmup run: 968it [00:02, 837.22it/s]warmup run: 1091it [00:02, 913.26it/s]warmup run: 1077it [00:02, 898.24it/s]warmup run: 891it [00:02, 839.36it/s]warmup run: 797it [00:02, 792.98it/s]warmup run: 1207it [00:02, 943.92it/s]warmup run: 1149it [00:02, 894.52it/s]warmup run: 912it [00:02, 852.46it/s]warmup run: 1063it [00:02, 868.10it/s]warmup run: 1193it [00:02, 941.11it/s]warmup run: 1177it [00:02, 926.07it/s]warmup run: 993it [00:02, 886.46it/s]warmup run: 898it [00:02, 848.88it/s]warmup run: 1308it [00:02, 958.09it/s]warmup run: 1245it [00:02, 912.98it/s]warmup run: 1014it [00:02, 896.88it/s]warmup run: 1159it [00:02, 892.94it/s]warmup run: 1293it [00:02, 957.90it/s]warmup run: 1276it [00:02, 943.60it/s]warmup run: 1096it [00:02, 924.30it/s]warmup run: 999it [00:02, 890.53it/s]warmup run: 1408it [00:02, 963.51it/s]warmup run: 1341it [00:02, 926.16it/s]warmup run: 1117it [00:02, 932.83it/s]warmup run: 1256it [00:02, 912.39it/s]warmup run: 1393it [00:02, 955.76it/s]warmup run: 1377it [00:02, 960.48it/s]warmup run: 1197it [00:02, 943.12it/s]warmup run: 1099it [00:02, 918.13it/s]warmup run: 1437it [00:03, 932.68it/s]warmup run: 1508it [00:03, 961.68it/s]warmup run: 1221it [00:02, 961.33it/s]warmup run: 1357it [00:02, 939.28it/s]warmup run: 1494it [00:03, 970.45it/s]warmup run: 1477it [00:03, 970.31it/s]warmup run: 1297it [00:02, 956.53it/s]warmup run: 1199it [00:02, 932.79it/s]warmup run: 1533it [00:03, 938.35it/s]warmup run: 1607it [00:03, 964.30it/s]warmup run: 1323it [00:02, 976.21it/s]warmup run: 1459it [00:03, 961.12it/s]warmup run: 1595it [00:03, 979.89it/s]warmup run: 1577it [00:03, 973.16it/s]warmup run: 1397it [00:02, 966.21it/s]warmup run: 1298it [00:02, 940.46it/s]warmup run: 1629it [00:03, 941.66it/s]warmup run: 1705it [00:03, 961.81it/s]warmup run: 1426it [00:02, 990.19it/s]warmup run: 1561it [00:03, 977.61it/s]warmup run: 1695it [00:03, 980.34it/s]warmup run: 1676it [00:03, 976.17it/s]warmup run: 1497it [00:02, 969.86it/s]warmup run: 1396it [00:02, 948.70it/s]warmup run: 1725it [00:03, 942.08it/s]warmup run: 1803it [00:03, 960.89it/s]warmup run: 1529it [00:03, 1001.27it/s]warmup run: 1663it [00:03, 989.44it/s]warmup run: 1795it [00:03, 982.23it/s]warmup run: 1778it [00:03, 986.77it/s]warmup run: 1597it [00:03, 976.78it/s]warmup run: 1494it [00:02, 955.38it/s]warmup run: 1821it [00:03, 945.35it/s]warmup run: 1900it [00:03, 959.50it/s]warmup run: 1632it [00:03, 1006.63it/s]warmup run: 1765it [00:03, 997.08it/s]warmup run: 1894it [00:03, 981.40it/s]warmup run: 1880it [00:03, 993.59it/s]warmup run: 1697it [00:03, 980.31it/s]warmup run: 1592it [00:03, 959.01it/s]warmup run: 1917it [00:03, 947.89it/s]warmup run: 1997it [00:03, 962.21it/s]warmup run: 1735it [00:03, 1010.95it/s]warmup run: 1867it [00:03, 1001.07it/s]warmup run: 1994it [00:03, 984.23it/s]warmup run: 1981it [00:03, 997.87it/s]warmup run: 1798it [00:03, 987.44it/s]warmup run: 1690it [00:03, 957.05it/s]warmup run: 2013it [00:03, 949.14it/s]warmup run: 2117it [00:03, 1030.14it/s]warmup run: 1838it [00:03, 1013.29it/s]warmup run: 1969it [00:03, 1004.13it/s]warmup run: 2114it [00:03, 1047.23it/s]warmup run: 2094it [00:03, 1035.24it/s]warmup run: 1900it [00:03, 996.77it/s]warmup run: 1787it [00:03, 956.97it/s]warmup run: 2131it [00:03, 1016.73it/s]warmup run: 2238it [00:03, 1081.64it/s]warmup run: 1941it [00:03, 1016.42it/s]warmup run: 2083it [00:03, 1042.08it/s]warmup run: 2237it [00:03, 1099.95it/s]warmup run: 2212it [00:03, 1077.52it/s]warmup run: 2002it [00:03, 1003.03it/s]warmup run: 1884it [00:03, 956.70it/s]warmup run: 2249it [00:03, 1064.39it/s]warmup run: 2359it [00:03, 1118.12it/s]warmup run: 2050it [00:03, 1038.00it/s]warmup run: 2203it [00:03, 1087.82it/s]warmup run: 2360it [00:03, 1137.24it/s]warmup run: 2330it [00:03, 1107.66it/s]warmup run: 2123it [00:03, 1061.84it/s]warmup run: 1981it [00:03, 957.35it/s]warmup run: 2367it [00:03, 1098.76it/s]warmup run: 2480it [00:03, 1142.69it/s]warmup run: 2171it [00:03, 1087.33it/s]warmup run: 2323it [00:03, 1120.87it/s]warmup run: 2483it [00:03, 1164.11it/s]warmup run: 2449it [00:03, 1130.76it/s]warmup run: 2244it [00:03, 1103.90it/s]warmup run: 2096it [00:03, 1013.61it/s]warmup run: 2486it [00:04, 1123.53it/s]warmup run: 2600it [00:04, 1159.56it/s]warmup run: 2291it [00:03, 1119.45it/s]warmup run: 2443it [00:03, 1143.93it/s]warmup run: 2606it [00:04, 1183.11it/s]warmup run: 2568it [00:04, 1147.39it/s]warmup run: 2365it [00:03, 1134.92it/s]warmup run: 2217it [00:03, 1070.76it/s]warmup run: 2604it [00:04, 1139.56it/s]warmup run: 2720it [00:04, 1171.41it/s]warmup run: 2411it [00:03, 1141.69it/s]warmup run: 2562it [00:04, 1157.61it/s]warmup run: 2729it [00:04, 1195.82it/s]warmup run: 2687it [00:04, 1159.92it/s]warmup run: 2486it [00:03, 1154.99it/s]warmup run: 2338it [00:03, 1110.91it/s]warmup run: 2722it [00:04, 1150.86it/s]warmup run: 2839it [00:04, 1175.27it/s]warmup run: 2532it [00:03, 1161.87it/s]warmup run: 2682it [00:04, 1167.65it/s]warmup run: 2851it [00:04, 1200.91it/s]warmup run: 2805it [00:04, 1163.76it/s]warmup run: 2607it [00:04, 1169.64it/s]warmup run: 2459it [00:03, 1138.39it/s]warmup run: 2840it [00:04, 1159.00it/s]warmup run: 2959it [00:04, 1181.26it/s]warmup run: 2651it [00:04, 1168.92it/s]warmup run: 2802it [00:04, 1175.78it/s]warmup run: 2974it [00:04, 1209.14it/s]warmup run: 2923it [00:04, 1166.50it/s]warmup run: 3000it [00:04, 680.26it/s] warmup run: 2727it [00:04, 1177.90it/s]warmup run: 3000it [00:04, 686.15it/s] warmup run: 2577it [00:03, 1148.55it/s]warmup run: 2957it [00:04, 1159.63it/s]warmup run: 2770it [00:04, 1173.39it/s]warmup run: 3000it [00:04, 678.91it/s] warmup run: 2922it [00:04, 1180.84it/s]warmup run: 3000it [00:04, 664.17it/s] warmup run: 2845it [00:04, 1177.69it/s]warmup run: 2694it [00:04, 1152.71it/s]warmup run: 3000it [00:04, 681.86it/s] warmup run: 2888it [00:04, 1172.97it/s]warmup run: 2967it [00:04, 1188.42it/s]warmup run: 2810it [00:04, 1150.69it/s]warmup run: 3000it [00:04, 691.61it/s] warmup run: 3000it [00:04, 690.10it/s] warmup run: 2926it [00:04, 1152.26it/s]warmup run: 3000it [00:04, 686.68it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.71it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.36it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.41it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.36it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.49it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.73it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.36it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.10it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1670.73it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.93it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1638.14it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1623.69it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.71it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.94it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1620.51it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1616.43it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.54it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1624.94it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.00it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1662.91it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1648.70it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.08it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1614.67it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1614.13it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1624.09it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1669.74it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1661.22it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1649.70it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1661.64it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1630.64it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1610.22it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1610.62it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1667.16it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1621.75it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1647.92it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1660.73it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1658.97it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1627.94it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1607.81it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1533.11it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1627.52it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1666.34it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1645.81it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1657.24it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1656.84it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1624.48it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1604.85it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1386.82it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1661.75it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1652.92it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1637.08it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1650.53it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1619.08it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1598.70it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1589.96it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1460.86it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:01, 1661.43it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1651.82it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1619.08it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1652.65it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1630.67it/s]warmup should be done:  43%|████▎     | 1293/3000 [00:00<00:01, 1596.77it/s]warmup should be done:  43%|████▎     | 1301/3000 [00:00<00:01, 1587.57it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1516.71it/s]warmup should be done:  50%|█████     | 1505/3000 [00:00<00:00, 1661.60it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1652.18it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1619.09it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1651.86it/s]warmup should be done:  48%|████▊     | 1454/3000 [00:00<00:00, 1597.78it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1627.21it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1588.93it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1554.90it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1653.83it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1657.40it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1617.07it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1650.77it/s]warmup should be done:  54%|█████▍    | 1615/3000 [00:01<00:00, 1599.06it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1625.81it/s]warmup should be done:  54%|█████▍    | 1620/3000 [00:01<00:00, 1587.54it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1567.88it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1653.13it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1616.19it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1652.56it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1650.76it/s]warmup should be done:  59%|█████▉    | 1775/3000 [00:01<00:00, 1598.27it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1624.03it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1586.86it/s]warmup should be done:  59%|█████▉    | 1783/3000 [00:01<00:00, 1580.63it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1652.07it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1616.17it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1651.71it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1648.53it/s]warmup should be done:  64%|██████▍   | 1935/3000 [00:01<00:00, 1596.74it/s]warmup should be done:  65%|██████▍   | 1938/3000 [00:01<00:00, 1587.25it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1593.93it/s]warmup should be done:  65%|██████▍   | 1942/3000 [00:01<00:00, 1567.85it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1615.87it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1652.02it/s]warmup should be done:  70%|██████▉   | 2095/3000 [00:01<00:00, 1596.69it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1649.39it/s]warmup should be done:  72%|███████▏  | 2169/3000 [00:01<00:00, 1646.07it/s]warmup should be done:  70%|██████▉   | 2097/3000 [00:01<00:00, 1586.48it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1567.70it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1558.71it/s]warmup should be done:  76%|███████▌  | 2280/3000 [00:01<00:00, 1615.12it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1652.77it/s]warmup should be done:  75%|███████▌  | 2255/3000 [00:01<00:00, 1596.82it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1644.84it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1649.64it/s]warmup should be done:  75%|███████▌  | 2256/3000 [00:01<00:00, 1586.17it/s]warmup should be done:  76%|███████▋  | 2291/3000 [00:01<00:00, 1551.85it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1552.76it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1613.76it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1649.96it/s]warmup should be done:  80%|████████  | 2415/3000 [00:01<00:00, 1594.98it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1646.22it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1638.89it/s]warmup should be done:  80%|████████  | 2415/3000 [00:01<00:00, 1585.57it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1538.56it/s]warmup should be done:  80%|████████  | 2413/3000 [00:01<00:00, 1545.66it/s]warmup should be done:  87%|████████▋ | 2604/3000 [00:01<00:00, 1613.84it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1650.27it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1595.45it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1646.80it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1635.42it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1584.09it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1529.75it/s]warmup should be done:  86%|████████▌ | 2568/3000 [00:01<00:00, 1542.95it/s]warmup should be done:  92%|█████████▏| 2766/3000 [00:01<00:00, 1613.52it/s]warmup should be done:  94%|█████████▍| 2826/3000 [00:01<00:00, 1651.66it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1596.21it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1635.15it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1632.99it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1584.11it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1523.17it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1541.45it/s]warmup should be done:  98%|█████████▊| 2930/3000 [00:01<00:00, 1620.21it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1655.06it/s]warmup should be done:  97%|█████████▋| 2896/3000 [00:01<00:00, 1600.21it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1635.95it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1643.26it/s]warmup should be done:  96%|█████████▋| 2894/3000 [00:01<00:00, 1591.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1649.95it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1524.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.27it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1547.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1600.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1584.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1546.57it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.16it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.79it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1696.66it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.18it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.65it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.92it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.65it/s]warmup should be done:   4%|▍         | 113/3000 [00:00<00:02, 1126.92it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.08it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.73it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1695.24it/s]warmup should be done:   9%|▉         | 276/3000 [00:00<00:01, 1420.67it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1688.82it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.64it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1630.30it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1617.37it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1690.41it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1674.12it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1697.34it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1672.82it/s]warmup should be done:  15%|█▍        | 444/3000 [00:00<00:01, 1534.96it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.86it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1620.11it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1681.04it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1698.32it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1693.41it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1680.59it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1676.19it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1637.65it/s]warmup should be done:  20%|██        | 612/3000 [00:00<00:01, 1588.45it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1624.04it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1680.10it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1695.06it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1686.56it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1693.50it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1637.78it/s]warmup should be done:  26%|██▌       | 775/3000 [00:00<00:01, 1602.11it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1677.72it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1679.83it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1596.43it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1695.01it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1691.27it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1635.64it/s]warmup should be done:  31%|███▏      | 944/3000 [00:00<00:01, 1629.10it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1673.96it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1676.68it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1680.45it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1581.74it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1636.94it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1688.88it/s]warmup should be done:  37%|███▋      | 1113/3000 [00:00<00:01, 1645.94it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1670.08it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1676.43it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1669.56it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1666.11it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1594.98it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1691.81it/s]warmup should be done:  43%|████▎     | 1281/3000 [00:00<00:01, 1656.33it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1633.25it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1671.11it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1685.44it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1670.59it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1663.88it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1598.99it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1692.24it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1635.25it/s]warmup should be done:  48%|████▊     | 1451/3000 [00:00<00:00, 1667.06it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1671.08it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1689.41it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1668.29it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1660.81it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1585.78it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1692.72it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1637.16it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1668.74it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1673.29it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1693.28it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1669.03it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1660.56it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1598.66it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1693.08it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1637.27it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1667.55it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1675.85it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1697.18it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1672.88it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1660.91it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1600.56it/s]warmup should be done:  65%|██████▌   | 1953/3000 [00:01<00:00, 1667.79it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1690.73it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1634.98it/s]warmup should be done:  67%|██████▋   | 2019/3000 [00:01<00:00, 1677.70it/s]warmup should be done:  68%|██████▊   | 2037/3000 [00:01<00:00, 1699.00it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1680.25it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1659.11it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1610.43it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1635.26it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1691.09it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1668.79it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1677.59it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1699.48it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1684.85it/s]warmup should be done:  73%|███████▎  | 2190/3000 [00:01<00:00, 1657.11it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1625.49it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1637.92it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1692.54it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1670.77it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1677.88it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1701.26it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1685.39it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1658.95it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1636.39it/s]warmup should be done:  85%|████████▍ | 2549/3000 [00:01<00:00, 1695.78it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1638.65it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1667.58it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1679.03it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1703.74it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1686.75it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1658.91it/s]warmup should be done:  82%|████████▏ | 2448/3000 [00:01<00:00, 1641.24it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1638.93it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1697.37it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1678.24it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1706.78it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1660.78it/s]warmup should be done:  90%|█████████ | 2703/3000 [00:01<00:00, 1686.01it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1659.60it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1632.83it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1641.21it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1696.57it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1676.69it/s]warmup should be done:  96%|█████████▋| 2893/3000 [00:01<00:00, 1706.21it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1658.94it/s]warmup should be done:  96%|█████████▌| 2872/3000 [00:01<00:00, 1683.87it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1658.21it/s]warmup should be done:  93%|█████████▎| 2779/3000 [00:01<00:00, 1638.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.09it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1643.23it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1659.21it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1641.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.54it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e066190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e3acd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e0660d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e3a9b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e0742b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e3acbb0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e0660d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa83e0761c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 04:12:13.819437: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa36b02ce10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:13.819499: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:13.829053: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.266607: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa36b02e000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.266670: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.276112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.317377: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa37282bdb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.317437: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.326422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.781644: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa36e834130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.781707: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.789625: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa372830940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.789680: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.790202: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.791071: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa36b028ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.791125: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.799783: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.800589: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.819575: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa372833da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.819641: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.821509: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa36ef91770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:12:14.821561: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:12:14.829321: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:14.829347: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:12:21.195689: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.222147: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.404669: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.690679: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.731398: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.799238: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.867653: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:12:21.944399: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:13:13.289][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:13:13.289][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.295][ERROR][RK0][main]: coll ps creation done
[HCTR][04:13:13.295][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][04:13:13.598][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:13:13.599][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.603][ERROR][RK0][main]: coll ps creation done
[HCTR][04:13:13.603][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][04:13:13.649][ERROR][RK0][tid #140340136769280]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:13:13.649][ERROR][RK0][tid #140340136769280]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.656][ERROR][RK0][tid #140340136769280]: coll ps creation done
[HCTR][04:13:13.656][ERROR][RK0][tid #140340136769280]: replica 7 waits for coll ps creation barrier
[HCTR][04:13:13.687][ERROR][RK0][tid #140340136769280]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:13:13.687][ERROR][RK0][tid #140340136769280]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.692][ERROR][RK0][tid #140340136769280]: coll ps creation done
[HCTR][04:13:13.692][ERROR][RK0][tid #140340136769280]: replica 3 waits for coll ps creation barrier
[HCTR][04:13:13.693][ERROR][RK0][tid #140340396812032]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:13:13.693][ERROR][RK0][tid #140340396812032]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.698][ERROR][RK0][tid #140340396812032]: coll ps creation done
[HCTR][04:13:13.698][ERROR][RK0][tid #140340396812032]: replica 2 waits for coll ps creation barrier
[HCTR][04:13:13.742][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:13:13.742][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.750][ERROR][RK0][main]: coll ps creation done
[HCTR][04:13:13.750][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][04:13:13.793][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:13:13.793][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.801][ERROR][RK0][main]: coll ps creation done
[HCTR][04:13:13.801][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][04:13:13.840][ERROR][RK0][tid #140340128376576]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:13:13.840][ERROR][RK0][tid #140340128376576]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:13:13.847][ERROR][RK0][tid #140340128376576]: coll ps creation done
[HCTR][04:13:13.847][ERROR][RK0][tid #140340128376576]: replica 0 waits for coll ps creation barrier
[HCTR][04:13:13.847][ERROR][RK0][tid #140340128376576]: replica 0 preparing frequency
[HCTR][04:13:14.662][ERROR][RK0][tid #140340128376576]: replica 0 preparing frequency done
[HCTR][04:13:14.722][ERROR][RK0][tid #140340396812032]: replica 2 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][tid #140340136769280]: replica 7 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][tid #140340128376576]: replica 0 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][tid #140340136769280]: replica 3 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][04:13:14.722][ERROR][RK0][tid #140340396812032]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][tid #140340136769280]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][tid #140340128376576]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][main]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][tid #140340396812032]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][tid #140340136769280]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][main]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][main]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][tid #140340136769280]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][main]: Calling build_v2
[HCTR][04:13:14.722][ERROR][RK0][tid #140340136769280]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][tid #140340128376576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:13:14.722][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 04:13:14[2022-12-12 04:13:142022-12-12 04:13:142022-12-12 04:13:14.2022-12-12 04:13:142022-12-12 04:13:14...2022-12-12 04:13:14722916..722924722925722928.: 722925[722937: : : 722943E: : EEE:  EE  2022-12-12 04:13:14  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::723009::136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136136: 136136] 136136] ] E] ] using concurrent impl MPS] ] using concurrent impl MPSusing concurrent impl MPS using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS

/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc



:136] using concurrent impl MPS
[2022-12-12 04:13:14.727386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 04:13:14.727427: [E2022-12-12 04:13:14 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc727430:: 196E]  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:178] v100x8, slow pcie
[2022-12-12 04:13:14.727481: E[ 2022-12-12 04:13:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:727484196[: ] 2022-12-12 04:13:14Eassigning 8 to cpu. 
[727511/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:13:14: :.E178727530 ] [: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 04:13:14E2022-12-12 04:13:14:
. .[212727575/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[7275842022-12-12 04:13:14] : :2022-12-12 04:13:14[: .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E178.2022-12-12 04:13:14E727620
 ] 727638. : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie: 727670/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:[
[E: : 1782022-12-12 04:13:142022-12-12 04:13:14[ E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ..2022-12-12 04:13:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] :v100x8, slow pcie727734.:727728/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178
: 727766196: :
] E: [] [178Ev100x8, slow pcie E2022-12-12 04:13:14assigning 8 to cpu2022-12-12 04:13:14]  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .
.v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[727870727898
:213:2022-12-12 04:13:14: : 178[] [196.EE] 2022-12-12 04:13:14remote time is 8.684212022-12-12 04:13:14] 727946  .
.v100x8, slow pcieassigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc727990727993[

E::: : 2022-12-12 04:13:14 196213E[E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ]  [2022-12-12 04:13:14 728071:assigning 8 to cpuremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:13:14./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc728112: 196

:.:: E] 196728138[212E assigning 8 to cpu] : 2022-12-12 04:13:14]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[assigning 8 to cpuE.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 04:13:14
 [728217
:214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:13:14: 196] 728260:.[E[] cpu time is 97.0588: 2127283212022-12-12 04:13:14 2022-12-12 04:13:14assigning 8 to cpu
E] : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E728348:728353/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 : 214: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] E212[: cpu time is 97.0588 ] 2022-12-12 04:13:14212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] ::2022-12-12 04:13:14
728466build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213212.: 
] ] [728491Eremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 04:13:14[:  E
 
.2022-12-12 04:13:14/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[728547[.::2022-12-12 04:13:14: 2022-12-12 04:13:14728567213212.E.: ] ] 728601 728614Eremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  

E:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [213 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:13:14[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213:.2022-12-12 04:13:14remote time is 8.68421:] 214728702.
213remote time is 8.68421] : 728712] 
[cpu time is 97.0588E: remote time is 8.684212022-12-12 04:13:14
 [E
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:13:14 728788:.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2147288112022-12-12 04:13:14E:] : . 213cpu time is 97.0588E728844/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
 : :remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE214
: ] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[] :
2022-12-12 04:13:14cpu time is 97.0588214.
] cpu time is 97.0588
728957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 04:14:31.877373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:14:31.917506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 04:14:32. 45778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:14:32. 45837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:14:32.112166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:14:32.112204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:14:32.112706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:14:32.112757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.113734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.114477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.127374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 04:14:32.127433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 04:14:32.127648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved[
2022-12-12 04:14:32.127685: [E2022-12-12 04:14:32 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc127713:: 202E]  5 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] [worker 0 thread 1 initing device 12022-12-12 04:14:32
.127754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-12 04:14:32worker 0 thread 5 initing device 5.[
1277662022-12-12 04:14:32: .E127779 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :6 solved[202
2022-12-12 04:14:32] .4 solved127839[
: 2022-12-12 04:14:32E. [127856/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:14:32: :.E1815127871 ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccBuilding Coll Cache with ... num gpu device is 8E:
 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 6 initing device 6205
[] 2022-12-12 04:14:32worker 0 thread 4 initing device 4.
127928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.128151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:14:32.128191: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:14:32:.1815128205] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.128245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.128304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 04:14:322022-12-12 04:14:32..128343128347: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151980] ] Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB

[2022-12-12 04:14:32.128410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.129766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 04:14:32.129816: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 04:14:32:.205129808] : worker 0 thread 7 initing device 7E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 04:14:32.129907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 04:14:32.130270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:14:32.130315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.130358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:14:32.130411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.132379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.132496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.132544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.132601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.132722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.134195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.134691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.136862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.136917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.136964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.137079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.137194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.138171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.139081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:14:32.196085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:14:32.201526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:14:32.201657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:14:32.202483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.203137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:32.204204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:32.204253: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:14:32.221892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [[2022-12-12 04:14:32[[eager alloc mem 5.00 Bytes2022-12-12 04:14:32.2022-12-12 04:14:322022-12-12 04:14:32
.221948..221948: 221948221948: E: : E EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::1980] 19801980] eager alloc mem 5.00 Bytes] ] eager alloc mem 5.00 Bytes
eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 04:14:32.223524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:14:32.228245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:14:32.228343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:14:32.228406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:14:32.228492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:14:32.228665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:14:32.228749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:14:32.228765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:14:32.[2288462022-12-12 04:14:32: .E228838 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 5
[2022-12-12 04:14:32.228938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:14:32.228975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:14:32.229046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:14:32.229125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:14:32.229154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.230095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.231116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.231651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.232239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.232996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.238826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:32.239321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:32.239769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:32.239852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:32.239898: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 04:14:32WORKER[0] alloc host memory 57.22 MB.[
2399002022-12-12 04:14:32: .E239920 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] [eager alloc mem 611.00 KB2022-12-12 04:14:32
.239968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:32.240345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:32.240390: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:14:32.240799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:32.240844: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:14:32[[2022-12-12 04:14:32.2022-12-12 04:14:32.240960.240980[: 240979: 2022-12-12 04:14:32E: E. E 241010/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E638:638 ] 638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5] eager release cuda mem 625663:
eager release cuda mem 625663
638
] eager release cuda mem 625663
[[2022-12-12 04:14:322022-12-12 04:14:32[..2022-12-12 04:14:32241152241155[.: : 2022-12-12 04:14:32241160WW.:   241167W/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:  ::E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc4343 :] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43WORKER[0] alloc host memory 57.22 MBWORKER[0] alloc host memory 57.22 MB:] 

638WORKER[0] alloc host memory 57.22 MB] 
eager release cuda mem 400000000
[2022-12-12 04:14:32.242656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:14:32.243304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:32.244019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.244350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:32.244399: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 04:14:32.244651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.244698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:14:32.278877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.279198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.279508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.279555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:14:32.279750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.279805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.279846: E[ 2022-12-12 04:14:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2798431980: ] Eeager alloc mem 7.16 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.280204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.280355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.280398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:14:32.280466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.280508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:14:32.280577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.280809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.280851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:14:32.281186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.281230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:14:32.283105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:14:32.283727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:14:32.283769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[[[[[[[[2022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:35........ 25118 25118 25119 25119 25119 25119 25119 25118: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 5 init p2p of link 6Device 4 init p2p of link 5Device 1 init p2p of link 7Device 6 init p2p of link 0Device 3 init p2p of link 2Device 0 init p2p of link 3Device 7 init p2p of link 4Device 2 init p2p of link 1







[[[[[2022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:352022-12-12 04:14:35.[.... 25719[[2022-12-12 04:14:35 25719 25719 25719 25726: 2022-12-12 04:14:352022-12-12 04:14:35.: : : : E.. 25738EEEE  25747 25748:     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980: : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] EE ::::eager alloc mem 611.00 KB  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980198019801980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ] ] ] ::1980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB19801980] 



] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 04:14:35. 26710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 26759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 04:14:352022-12-12 04:14:35[..2022-12-12 04:14:35 26868 26869[.: : 2022-12-12 04:14:35[[ 26875EE.2022-12-12 04:14:352022-12-12 04:14:35:    26883..E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  26895 26896 ::E: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638 EE:] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  638eager release cuda mem 625663eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 

638::eager release cuda mem 625663] 638638
eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 04:14:35. 39868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:14:35. 40035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 40164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:14:35. 40328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 40942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 41234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 47242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:14:35. 47413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 47601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 04:14:35. 47780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 47812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 04:14:35. 47984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 48079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 04:14:35. 48246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 48262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:14:35. 48329[: 2022-12-12 04:14:35E.  48325/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 7 init p2p of link 1
[2022-12-12 04:14:35. 48421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 48542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:14:351980.]  48556eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 48896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 49135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 49335[: 2022-12-12 04:14:35E.  49343/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 04:14:35. 52132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 04:14:35. 52258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 52408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:14:35. 52530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 53134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 53409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 60919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 04:14:35. 61070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 61958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 62374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:14:35. 62521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 63447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 67406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:14:35. 67539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 68107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 04:14:35. 68237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 68327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 68910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 04:14:35. 69034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 69091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 04:14:35Device 2 init p2p of link 0.
 69123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 69239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 69813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 70119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 74729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:14:35. 74847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 75757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 77042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 04:14:35. 77168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 77437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:14:35. 77561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 77794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:14:35. 77932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 78063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 78458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 78826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 80322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:14:35. 80464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 81200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 04:14:35. 81328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 81356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 82224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 89794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:14:35. 89927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 90708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 91423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:14:35. 91542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:14:35. 92319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:14:35. 93905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35. 94646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96641 secs 
[2022-12-12 04:14:35. 95426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35. 95456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35. 95961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96556 secs 
[2022-12-12 04:14:35. 96661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.96826 secs 
[2022-12-12 04:14:35.100724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35.101140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35.101937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35.102066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97415 secs 
[2022-12-12 04:14:35.102205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97387 secs 
[2022-12-12 04:14:35.103123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97492 secs [
2022-12-12 04:14:35.103153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35.103216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:14:35.105620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97531 secs 
[2022-12-12 04:14:35.106760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.99401 secs 
[2022-12-12 04:14:35.108269: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.48 GB
[2022-12-12 04:14:36.568801: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.74 GB
[2022-12-12 04:14:36.569450: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.74 GB
[2022-12-12 04:14:36.569972: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.74 GB
[2022-12-12 04:14:37.860681: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.00 GB
[2022-12-12 04:14:37.861168: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.00 GB
[2022-12-12 04:14:37.863089: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.00 GB
[2022-12-12 04:14:39.117980: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.22 GB
[2022-12-12 04:14:39.118118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.22 GB
[2022-12-12 04:14:39.118423: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.22 GB
[2022-12-12 04:14:40.582000: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.43 GB
[2022-12-12 04:14:40.582160: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.43 GB
[2022-12-12 04:14:40.582494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.43 GB
[2022-12-12 04:14:42. 73163: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.89 GB
[2022-12-12 04:14:42. 73335: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.89 GB
[2022-12-12 04:14:42. 73719: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.89 GB
[2022-12-12 04:14:43.343916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.09 GB
[2022-12-12 04:14:43.344194: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.09 GB
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][tid #140340396812032]: replica 2 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][tid #140340136769280]: replica 3 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][tid #140340136769280]: replica 7 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][tid #140340128376576]: replica 0 calling init per replica done, doing barrier
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340396812032]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340128376576]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340136769280]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340136769280]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:14:43.785][ERROR][RK0][main]: init per replica done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340396812032]: init per replica done
[HCTR][04:14:43.785][ERROR][RK0][main]: init per replica done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340136769280]: init per replica done
[HCTR][04:14:43.785][ERROR][RK0][main]: init per replica done
[HCTR][04:14:43.785][ERROR][RK0][tid #140340136769280]: init per replica done
[HCTR][04:14:43.785][ERROR][RK0][main]: init per replica done
[HCTR][04:14:43.788][ERROR][RK0][tid #140340128376576]: init per replica done
[HCTR][04:14:43.824][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f85c0238400
[HCTR][04:14:43.824][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f85c0558400
[HCTR][04:14:43.824][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f85c0b98400
[HCTR][04:14:43.824][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f85c0eb8400
[HCTR][04:14:43.824][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f865c238400
[HCTR][04:14:43.824][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f865c558400
[HCTR][04:14:43.824][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f865cb98400
[HCTR][04:14:43.824][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f865ceb8400
[HCTR][04:14:43.824][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f85a0238400
[HCTR][04:14:43.824][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f85a0558400
[HCTR][04:14:43.824][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f85a0b98400
[HCTR][04:14:43.824][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f85a0eb8400
[HCTR][04:14:43.824][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f8550238400
[HCTR][04:14:43.824][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f8628238400
[HCTR][04:14:43.824][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f8550558400
[HCTR][04:14:43.824][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f8628558400
[HCTR][04:14:43.824][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f8550b98400
[HCTR][04:14:43.824][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f8628b98400
[HCTR][04:14:43.824][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f8550eb8400
[HCTR][04:14:43.824][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f8628eb8400
[HCTR][04:14:43.824][ERROR][RK0][tid #140340069660416]: 1 allocated 3276800 at 0x7f8644238400
[HCTR][04:14:43.824][ERROR][RK0][tid #140340069660416]: 1 allocated 6553600 at 0x7f8644558400
[HCTR][04:14:43.824][ERROR][RK0][tid #140340069660416]: 1 allocated 3276800 at 0x7f8644b98400
[HCTR][04:14:43.824][ERROR][RK0][tid #140340069660416]: 1 allocated 6553600 at 0x7f8644eb8400
[HCTR][04:14:43.824][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f8570238400
[HCTR][04:14:43.824][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f8570558400
[HCTR][04:14:43.824][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f8570b98400
[HCTR][04:14:43.824][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f8570eb8400
[HCTR][04:14:43.827][ERROR][RK0][tid #140340128376576]: 0 allocated 3276800 at 0x7f863c320000
[HCTR][04:14:43.827][ERROR][RK0][tid #140340128376576]: 0 allocated 6553600 at 0x7f863c640000
[HCTR][04:14:43.827][ERROR][RK0][tid #140340128376576]: 0 allocated 3276800 at 0x7f863cc80000
[HCTR][04:14:43.827][ERROR][RK0][tid #140340128376576]: 0 allocated 6553600 at 0x7f863cfa0000
