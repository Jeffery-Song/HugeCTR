2022-12-12 00:04:13.440697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.445643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.454508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.459909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.467123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.478052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.487171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.500092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.554473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.561993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.565811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.567502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.569769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.571513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.573671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.575555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.575663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.577782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.577899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.580055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.580346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.582290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.582853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.584274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.585070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.586369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.587302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.588585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.589654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.590822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.591981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.593227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.597134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.598937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.600116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.601138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.602222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.603213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.604129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.606322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.612071: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:13.613054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.614226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.615406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.616440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.617609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.618607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.619656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.620804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.622156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.623139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.624151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.627448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.629132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.631312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.633634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.633965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.635959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.636596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.639046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.639291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.640150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.640672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.642876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.642937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.644092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.644928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.645652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.646298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.646558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.647867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.648511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.650327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.650452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.650621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.653223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.653883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.655177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.655783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.655803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.657719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.658276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.659167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.660388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.660662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.665498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.695476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.695845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.696009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.696562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.698176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.698229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.698649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.702640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.703051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.703151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.704293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.704618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.705025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.707488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.707912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.708223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.709952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.710611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.711085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.712431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.712839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.714498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.715338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.715961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.717240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.717569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.718866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.720056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.721434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.721782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.722716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.724410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.724914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.725744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.727284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.727514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.728382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.730015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.730144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.731023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.732783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.732908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.734000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.735526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.736418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.736436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.738277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.739117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.739291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.740767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.741888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.742146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.743614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.744469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.744837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.746239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.746259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.747222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.747950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.749422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.749530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.751298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.751544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.752384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.753253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.754317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.754658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.755502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.756395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.757132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.757461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.758122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.759017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.760263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.761056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.761638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.762350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.763623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.764242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.765098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.765130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.765696: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:13.765850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.767323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.767622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.768176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.769576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.770074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.771071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.772991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.773424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.774141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.774943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.774992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.775579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.776008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.778150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.779049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.779089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.780163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.780335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.780828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.781078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.783195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.784184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.784574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.785738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.786025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.786342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.788409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.789419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.789650: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:13.790654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.790976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.792600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.793154: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:13.793240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.794815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.795246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.796542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.797578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.798688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.799232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.799292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.800965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.802006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.802545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.803218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.803857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.803909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.805633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.806861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.807460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.808249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.808986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.809139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.810760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.811852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.812375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.813534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.815739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.816157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.817479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.819405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.822504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.822762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.823786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.825448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.856287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.856619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.857575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.858833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.861462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.861803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.864121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.864336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.866420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.866816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.870773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.871038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.872860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.873050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.877308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.877840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.880017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.880823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.883981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.884397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.913174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.913866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.917398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.917749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.920150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.922307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.922553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.923471: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:13.925105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.932232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.933076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.933135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.937492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.940442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.941324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.941557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.944323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.945631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.946614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.950125: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:13.959891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:13.978196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.008088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.009644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.010959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.016601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.017813: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:14.020744: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 00:04:14.027875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.030625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.034046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.037380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.039993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.043422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.928663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.929967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.930973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.932189: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:14.932250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:04:14.950006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.951560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.952583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.953689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.954897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:14.956206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 00:04:15.000635: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.000839: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.031817: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 00:04:15.179331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.179964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.180486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.180954: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.181009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:04:15.198910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.199722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.200414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.201204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.201751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.202389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 00:04:15.216805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.217426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.217952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.218562: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.218625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:04:15.225058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.225877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.226398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.226860: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.226913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:04:15.235842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.236467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.236999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.237810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.238324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.238801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 00:04:15.244586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.245440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.246072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.246652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.247178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.247664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 00:04:15.286235: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.286446: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.287229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.287367: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 00:04:15.288050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.288586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.289176: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.289259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:04:15.290701: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.290847: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.291845: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 00:04:15.301179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.301778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.302292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.302763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.302813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:04:15.306862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.307481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.308018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.308590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.309114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.309582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 00:04:15.310138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.311008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.311567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.312039: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.312086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:04:15.315359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.315956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.316486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.316955: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 00:04:15.317000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:04:15.319873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.320531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.321079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.321658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.322166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.322636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 00:04:15.329920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.330920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.331085: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.331245: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.331527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.332126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.332151: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 00:04:15.332644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.333118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 00:04:15.333947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.334533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.335051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.335637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.336153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 00:04:15.336627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 00:04:15.354071: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.354278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.355455: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 00:04:15.366939: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.367098: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.368091: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 00:04:15.376782: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.376904: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.377775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 00:04:15.379297: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.379490: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 00:04:15.380519: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][00:04:16.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 94it [00:01, 78.67it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 189it [00:01, 171.94it/s]warmup run: 97it [00:01, 83.25it/s]warmup run: 101it [00:01, 86.06it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 103it [00:01, 89.42it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 286it [00:01, 277.65it/s]warmup run: 193it [00:01, 179.05it/s]warmup run: 202it [00:01, 186.43it/s]warmup run: 98it [00:01, 85.49it/s]warmup run: 92it [00:01, 79.90it/s]warmup run: 99it [00:01, 86.39it/s]warmup run: 206it [00:01, 193.48it/s]warmup run: 100it [00:01, 87.48it/s]warmup run: 384it [00:01, 388.99it/s]warmup run: 289it [00:01, 284.28it/s]warmup run: 303it [00:01, 296.75it/s]warmup run: 196it [00:01, 184.81it/s]warmup run: 184it [00:01, 172.82it/s]warmup run: 198it [00:01, 186.81it/s]warmup run: 310it [00:01, 308.63it/s]warmup run: 201it [00:01, 190.11it/s]warmup run: 483it [00:02, 500.25it/s]warmup run: 387it [00:01, 396.25it/s]warmup run: 404it [00:01, 411.50it/s]warmup run: 295it [00:01, 294.94it/s]warmup run: 281it [00:01, 281.44it/s]warmup run: 299it [00:01, 299.21it/s]warmup run: 414it [00:01, 427.59it/s]warmup run: 303it [00:01, 303.86it/s]warmup run: 585it [00:02, 608.47it/s]warmup run: 484it [00:02, 503.30it/s]warmup run: 506it [00:02, 524.43it/s]warmup run: 394it [00:01, 408.20it/s]warmup run: 381it [00:01, 398.37it/s]warmup run: 399it [00:01, 413.57it/s]warmup run: 518it [00:01, 542.89it/s]warmup run: 404it [00:01, 419.18it/s]warmup run: 686it [00:02, 700.76it/s]warmup run: 607it [00:02, 626.51it/s]warmup run: 582it [00:02, 602.98it/s]warmup run: 492it [00:01, 516.28it/s]warmup run: 481it [00:01, 511.14it/s]warmup run: 499it [00:01, 524.35it/s]warmup run: 624it [00:02, 651.48it/s]warmup run: 503it [00:01, 526.67it/s]warmup run: 789it [00:02, 780.24it/s]warmup run: 706it [00:02, 710.21it/s]warmup run: 680it [00:02, 688.98it/s]warmup run: 591it [00:02, 616.83it/s]warmup run: 581it [00:02, 614.75it/s]warmup run: 601it [00:02, 629.27it/s]warmup run: 730it [00:02, 745.26it/s]warmup run: 605it [00:02, 630.46it/s]warmup run: 891it [00:02, 841.82it/s]warmup run: 804it [00:02, 776.59it/s]warmup run: 777it [00:02, 758.09it/s]warmup run: 692it [00:02, 708.29it/s]warmup run: 683it [00:02, 707.94it/s]warmup run: 701it [00:02, 714.47it/s]warmup run: 835it [00:02, 820.03it/s]warmup run: 707it [00:02, 719.92it/s]warmup run: 992it [00:02, 885.18it/s]warmup run: 874it [00:02, 812.61it/s]warmup run: 902it [00:02, 826.99it/s]warmup run: 794it [00:02, 784.48it/s]warmup run: 783it [00:02, 780.32it/s]warmup run: 800it [00:02, 781.82it/s]warmup run: 939it [00:02, 876.57it/s]warmup run: 808it [00:02, 792.17it/s]warmup run: 1093it [00:02, 919.89it/s]warmup run: 973it [00:02, 858.48it/s]warmup run: 1000it [00:02, 864.62it/s]warmup run: 886it [00:02, 845.40it/s]warmup run: 894it [00:02, 839.03it/s]warmup run: 899it [00:02, 836.11it/s]warmup run: 1042it [00:02, 915.14it/s]warmup run: 910it [00:02, 850.70it/s]warmup run: 1194it [00:02, 931.37it/s]warmup run: 1098it [00:02, 894.84it/s]warmup run: 1070it [00:02, 881.97it/s]warmup run: 987it [00:02, 889.43it/s]warmup run: 993it [00:02, 873.70it/s]warmup run: 998it [00:02, 877.47it/s]warmup run: 1145it [00:02, 928.62it/s]warmup run: 1012it [00:02, 895.31it/s]warmup run: 1293it [00:02, 942.86it/s]warmup run: 1196it [00:02, 911.45it/s]warmup run: 1166it [00:02, 888.99it/s]warmup run: 1091it [00:02, 929.76it/s]warmup run: 1092it [00:02, 905.38it/s]warmup run: 1097it [00:02, 905.04it/s]warmup run: 1246it [00:02, 946.37it/s]warmup run: 1116it [00:02, 935.08it/s]warmup run: 1392it [00:02, 956.39it/s]warmup run: 1293it [00:02, 927.59it/s]warmup run: 1261it [00:02, 897.53it/s]warmup run: 1193it [00:02, 953.88it/s]warmup run: 1191it [00:02, 926.37it/s]warmup run: 1196it [00:02, 922.30it/s]warmup run: 1347it [00:02, 945.47it/s]warmup run: 1218it [00:02, 953.22it/s]warmup run: 1491it [00:03, 963.86it/s]warmup run: 1390it [00:02, 936.56it/s]warmup run: 1355it [00:02, 909.09it/s]warmup run: 1295it [00:02, 970.47it/s]warmup run: 1292it [00:02, 947.88it/s]warmup run: 1294it [00:02, 937.76it/s]warmup run: 1319it [00:02, 966.71it/s]warmup run: 1446it [00:02, 946.27it/s]warmup run: 1590it [00:03, 964.45it/s]warmup run: 1487it [00:03, 945.62it/s]warmup run: 1449it [00:03, 916.01it/s]warmup run: 1398it [00:02, 985.07it/s]warmup run: 1394it [00:02, 966.46it/s]warmup run: 1392it [00:02, 943.18it/s]warmup run: 1420it [00:02, 975.58it/s]warmup run: 1544it [00:03, 943.28it/s]warmup run: 1688it [00:03, 967.77it/s]warmup run: 1584it [00:03, 950.36it/s]warmup run: 1545it [00:03, 928.61it/s]warmup run: 1496it [00:03, 979.70it/s]warmup run: 1500it [00:03, 983.85it/s]warmup run: 1492it [00:03, 957.41it/s]warmup run: 1522it [00:02, 986.42it/s]warmup run: 1641it [00:03, 945.47it/s]warmup run: 1790it [00:03, 981.06it/s]warmup run: 1681it [00:03, 955.91it/s]warmup run: 1645it [00:03, 948.36it/s]warmup run: 1597it [00:03, 988.17it/s]warmup run: 1601it [00:03, 980.73it/s]warmup run: 1591it [00:03, 964.30it/s]warmup run: 1624it [00:03, 996.29it/s]warmup run: 1741it [00:03, 958.54it/s]warmup run: 1891it [00:03, 989.44it/s]warmup run: 1778it [00:03, 959.77it/s]warmup run: 1743it [00:03, 955.72it/s]warmup run: 1698it [00:03, 993.43it/s]warmup run: 1693it [00:03, 978.73it/s]warmup run: 1701it [00:03, 973.72it/s]warmup run: 1726it [00:03, 1000.08it/s]warmup run: 1839it [00:03, 962.26it/s]warmup run: 1993it [00:03, 995.56it/s]warmup run: 1875it [00:03, 961.79it/s]warmup run: 1840it [00:03, 957.16it/s]warmup run: 1799it [00:03, 996.91it/s]warmup run: 1795it [00:03, 990.92it/s]warmup run: 1800it [00:03, 969.75it/s]warmup run: 1828it [00:03, 1003.82it/s]warmup run: 1940it [00:03, 975.24it/s]warmup run: 2115it [00:03, 1060.83it/s]warmup run: 1973it [00:03, 964.63it/s]warmup run: 1937it [00:03, 948.92it/s]warmup run: 1900it [00:03, 1000.41it/s]warmup run: 1898it [00:03, 1000.51it/s]warmup run: 1899it [00:03, 973.13it/s]warmup run: 1930it [00:03, 1007.36it/s]warmup run: 2046it [00:03, 998.22it/s]warmup run: 2239it [00:03, 1112.89it/s]warmup run: 2086it [00:03, 1011.68it/s]warmup run: 2037it [00:03, 962.25it/s]warmup run: 2001it [00:03, 1000.24it/s]warmup run: 2000it [00:03, 1005.54it/s]warmup run: 1999it [00:03, 980.09it/s]warmup run: 2032it [00:03, 1009.12it/s]warmup run: 2165it [00:03, 1054.49it/s]warmup run: 2363it [00:03, 1150.59it/s]warmup run: 2207it [00:03, 1070.64it/s]warmup run: 2153it [00:03, 1019.86it/s]warmup run: 2120it [00:03, 1056.36it/s]warmup run: 2121it [00:03, 1065.04it/s]warmup run: 2121it [00:03, 1049.20it/s]warmup run: 2147it [00:03, 1049.51it/s]warmup run: 2282it [00:03, 1086.24it/s]warmup run: 2487it [00:03, 1176.83it/s]warmup run: 2329it [00:03, 1112.72it/s]warmup run: 2268it [00:03, 1057.29it/s]warmup run: 2240it [00:03, 1097.34it/s]warmup run: 2245it [00:03, 1116.42it/s]warmup run: 2244it [00:03, 1100.84it/s]warmup run: 2270it [00:03, 1100.86it/s]warmup run: 2401it [00:03, 1116.64it/s]warmup run: 2611it [00:04, 1194.79it/s]warmup run: 2450it [00:03, 1139.98it/s]warmup run: 2383it [00:03, 1084.43it/s]warmup run: 2360it [00:03, 1126.26it/s]warmup run: 2370it [00:03, 1154.18it/s]warmup run: 2367it [00:03, 1137.24it/s]warmup run: 2393it [00:03, 1138.35it/s]warmup run: 2520it [00:03, 1138.22it/s]warmup run: 2734it [00:04, 1203.17it/s]warmup run: 2573it [00:04, 1164.28it/s]warmup run: 2500it [00:04, 1108.16it/s]warmup run: 2480it [00:03, 1145.91it/s]warmup run: 2495it [00:03, 1180.46it/s]warmup run: 2490it [00:03, 1163.02it/s]warmup run: 2517it [00:03, 1166.87it/s]warmup run: 2638it [00:04, 1150.53it/s]warmup run: 2858it [00:04, 1214.02it/s]warmup run: 2695it [00:04, 1179.33it/s]warmup run: 2616it [00:04, 1122.06it/s]warmup run: 2600it [00:04, 1159.83it/s]warmup run: 2619it [00:04, 1197.25it/s]warmup run: 2613it [00:04, 1181.39it/s]warmup run: 2639it [00:04, 1180.18it/s]warmup run: 2758it [00:04, 1163.96it/s]warmup run: 2983it [00:04, 1221.98it/s]warmup run: 2817it [00:04, 1190.34it/s]warmup run: 2731it [00:04, 1128.91it/s]warmup run: 3000it [00:04, 681.97it/s] warmup run: 2718it [00:04, 1165.48it/s]warmup run: 2741it [00:04, 1202.40it/s]warmup run: 2732it [00:04, 1179.19it/s]warmup run: 2759it [00:04, 1184.22it/s]warmup run: 2877it [00:04, 1171.45it/s]warmup run: 2937it [00:04, 1193.05it/s]warmup run: 2848it [00:04, 1140.13it/s]warmup run: 2840it [00:04, 1181.39it/s]warmup run: 2863it [00:04, 1207.05it/s]warmup run: 2850it [00:04, 1177.26it/s]warmup run: 2880it [00:04, 1189.19it/s]warmup run: 2997it [00:04, 1179.65it/s]warmup run: 3000it [00:04, 680.96it/s] warmup run: 3000it [00:04, 690.38it/s] warmup run: 2963it [00:04, 1126.74it/s]warmup run: 2961it [00:04, 1189.90it/s]warmup run: 2984it [00:04, 1205.04it/s]warmup run: 2968it [00:04, 1177.01it/s]warmup run: 3000it [00:04, 693.24it/s] warmup run: 3000it [00:04, 668.30it/s] warmup run: 3000it [00:04, 696.23it/s] warmup run: 3000it [00:04, 690.63it/s] warmup run: 3000it [00:04, 687.14it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1679.18it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.72it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.20it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.81it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1621.89it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1603.61it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.84it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.49it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1674.07it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.53it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.46it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1687.00it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1608.08it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1636.51it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1616.22it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1635.77it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1619.58it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1638.29it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1679.19it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1619.63it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1664.14it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1656.54it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1632.68it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1633.66it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1622.03it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1619.24it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1681.56it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1643.29it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1635.01it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1654.83it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1629.31it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1659.24it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1624.82it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1620.29it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1643.97it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1635.60it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1679.26it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1652.59it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1625.25it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1654.17it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1620.26it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1641.28it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1677.18it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1631.88it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1614.30it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1646.17it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1618.87it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1646.62it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1620.59it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1643.95it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1676.86it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1631.66it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1613.63it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1644.92it/s]warmup should be done:  38%|      | 1145/3000 [00:00<00:01, 1618.64it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1637.65it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1620.73it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1645.84it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1674.03it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1631.71it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1612.40it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1644.13it/s]warmup should be done:  44%|     | 1308/3000 [00:00<00:01, 1619.41it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1634.45it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1645.88it/s]warmup should be done:  49%|     | 1465/3000 [00:00<00:00, 1620.78it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1631.65it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1612.30it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1644.16it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1618.05it/s]warmup should be done:  51%|     | 1517/3000 [00:00<00:00, 1664.67it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1632.02it/s]warmup should be done:  55%|    | 1651/3000 [00:01<00:00, 1645.03it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1619.64it/s]warmup should be done:  55%|    | 1642/3000 [00:01<00:00, 1631.91it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1612.28it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1644.58it/s]warmup should be done:  54%|    | 1632/3000 [00:01<00:00, 1617.63it/s]warmup should be done:  56%|    | 1684/3000 [00:01<00:00, 1662.08it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1631.83it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1644.92it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1620.60it/s]warmup should be done:  60%|    | 1806/3000 [00:01<00:00, 1632.51it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1617.20it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1647.47it/s]warmup should be done:  60%|    | 1787/3000 [00:01<00:00, 1610.35it/s]warmup should be done:  62%|   | 1851/3000 [00:01<00:00, 1658.05it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1631.35it/s]warmup should be done:  66%|   | 1981/3000 [00:01<00:00, 1645.36it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1621.37it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1632.75it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1616.84it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1652.42it/s]warmup should be done:  65%|   | 1949/3000 [00:01<00:00, 1607.46it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1656.29it/s]warmup should be done:  66%|   | 1987/3000 [00:01<00:00, 1630.82it/s]warmup should be done:  72%|  | 2146/3000 [00:01<00:00, 1644.54it/s]warmup should be done:  71%|   | 2117/3000 [00:01<00:00, 1620.96it/s]warmup should be done:  71%|   | 2134/3000 [00:01<00:00, 1632.45it/s]warmup should be done:  71%|   | 2118/3000 [00:01<00:00, 1616.76it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1653.76it/s]warmup should be done:  70%|   | 2110/3000 [00:01<00:00, 1605.80it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1653.55it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1629.91it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1639.73it/s]warmup should be done:  77%|  | 2324/3000 [00:01<00:00, 1654.16it/s]warmup should be done:  77%|  | 2298/3000 [00:01<00:00, 1630.40it/s]warmup should be done:  76%|  | 2280/3000 [00:01<00:00, 1617.96it/s]warmup should be done:  76%|  | 2280/3000 [00:01<00:00, 1612.77it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1604.58it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1650.64it/s]warmup should be done:  77%|  | 2314/3000 [00:01<00:00, 1628.31it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1640.79it/s]warmup should be done:  82%| | 2463/3000 [00:01<00:00, 1636.23it/s]warmup should be done:  83%| | 2490/3000 [00:01<00:00, 1654.22it/s]warmup should be done:  81%| | 2442/3000 [00:01<00:00, 1617.24it/s]warmup should be done:  81%| | 2442/3000 [00:01<00:00, 1613.82it/s]warmup should be done:  81%|  | 2433/3000 [00:01<00:00, 1606.93it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1651.02it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1621.36it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1646.08it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1641.12it/s]warmup should be done:  89%| | 2657/3000 [00:01<00:00, 1657.25it/s]warmup should be done:  87%| | 2604/3000 [00:01<00:00, 1615.87it/s]warmup should be done:  87%| | 2604/3000 [00:01<00:00, 1610.75it/s]warmup should be done:  86%| | 2595/3000 [00:01<00:00, 1608.11it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1647.89it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1621.85it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1643.36it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1653.72it/s]warmup should be done:  94%|| 2826/3000 [00:01<00:00, 1664.40it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1619.25it/s]warmup should be done:  92%|| 2766/3000 [00:01<00:00, 1611.35it/s]warmup should be done:  92%|| 2758/3000 [00:01<00:00, 1612.43it/s]warmup should be done:  95%|| 2846/3000 [00:01<00:00, 1644.31it/s]warmup should be done:  93%|| 2803/3000 [00:01<00:00, 1622.99it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.00it/s]warmup should be done:  99%|| 2972/3000 [00:01<00:00, 1646.67it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1662.28it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1671.02it/s]warmup should be done:  98%|| 2932/3000 [00:01<00:00, 1625.93it/s]warmup should be done:  98%|| 2929/3000 [00:01<00:00, 1616.25it/s]warmup should be done:  97%|| 2923/3000 [00:01<00:00, 1623.48it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.42it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1627.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1643.50it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.21it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.67it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1614.18it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1708.67it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.22it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.21it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1713.64it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.18it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1670.51it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.35it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.62it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1707.25it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1672.12it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1671.65it/s]warmup should be done:  12%|        | 345/3000 [00:00<00:01, 1720.46it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.36it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1672.80it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1666.48it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1666.56it/s]warmup should be done:  17%|        | 518/3000 [00:00<00:01, 1723.96it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1686.13it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1675.89it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1679.03it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1648.51it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1671.10it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1699.78it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1670.80it/s]warmup should be done:  23%|       | 691/3000 [00:00<00:01, 1724.09it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1687.00it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1676.89it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1693.63it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1657.82it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1702.16it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1672.17it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1673.70it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1695.86it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1676.11it/s]warmup should be done:  29%|       | 864/3000 [00:00<00:01, 1723.63it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1691.64it/s]warmup should be done:  28%|       | 831/3000 [00:00<00:01, 1662.56it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1705.50it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1671.27it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1668.60it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1676.65it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1697.33it/s]warmup should be done:  35%|      | 1037/3000 [00:00<00:01, 1724.45it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1662.82it/s]warmup should be done:  34%|      | 1028/3000 [00:00<00:01, 1707.71it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1672.26it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1684.39it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1664.24it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1677.18it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1697.89it/s]warmup should be done:  40%|      | 1210/3000 [00:00<00:01, 1724.16it/s]warmup should be done:  40%|      | 1200/3000 [00:00<00:01, 1710.20it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1661.95it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1672.61it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1662.44it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1671.69it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1676.73it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:00, 1697.68it/s]warmup should be done:  46%|     | 1384/3000 [00:00<00:00, 1727.51it/s]warmup should be done:  46%|     | 1373/3000 [00:00<00:00, 1715.08it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1673.86it/s]warmup should be done:  44%|     | 1332/3000 [00:00<00:01, 1661.70it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1661.61it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1669.52it/s]warmup should be done:  51%|     | 1528/3000 [00:00<00:00, 1697.37it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1675.77it/s]warmup should be done:  52%|    | 1557/3000 [00:00<00:00, 1727.02it/s]warmup should be done:  52%|    | 1546/3000 [00:00<00:00, 1717.09it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1673.31it/s]warmup should be done:  50%|     | 1499/3000 [00:00<00:00, 1662.23it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1659.98it/s]warmup should be done:  51%|     | 1520/3000 [00:00<00:00, 1666.15it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1676.67it/s]warmup should be done:  57%|    | 1699/3000 [00:01<00:00, 1698.42it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1726.26it/s]warmup should be done:  57%|    | 1718/3000 [00:01<00:00, 1717.93it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1663.81it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1675.52it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1661.99it/s]warmup should be done:  56%|    | 1687/3000 [00:01<00:00, 1665.38it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1679.35it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1701.52it/s]warmup should be done:  63%|   | 1903/3000 [00:01<00:00, 1726.32it/s]warmup should be done:  63%|   | 1891/3000 [00:01<00:00, 1718.89it/s]warmup should be done:  61%|    | 1834/3000 [00:01<00:00, 1667.04it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1676.79it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1669.37it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1666.28it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1678.43it/s]warmup should be done:  68%|   | 2041/3000 [00:01<00:00, 1701.39it/s]warmup should be done:  69%|   | 2076/3000 [00:01<00:00, 1726.03it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1666.48it/s]warmup should be done:  69%|   | 2063/3000 [00:01<00:00, 1713.62it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1674.39it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1672.73it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1664.68it/s]warmup should be done:  73%|  | 2185/3000 [00:01<00:00, 1677.35it/s]warmup should be done:  75%|  | 2249/3000 [00:01<00:00, 1724.32it/s]warmup should be done:  72%|  | 2168/3000 [00:01<00:00, 1665.99it/s]warmup should be done:  74%|  | 2212/3000 [00:01<00:00, 1694.31it/s]warmup should be done:  73%|  | 2185/3000 [00:01<00:00, 1672.33it/s]warmup should be done:  74%|  | 2235/3000 [00:01<00:00, 1705.71it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1676.19it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1662.59it/s]warmup should be done:  78%|  | 2354/3000 [00:01<00:00, 1678.19it/s]warmup should be done:  81%|  | 2422/3000 [00:01<00:00, 1724.27it/s]warmup should be done:  78%|  | 2336/3000 [00:01<00:00, 1667.45it/s]warmup should be done:  79%|  | 2382/3000 [00:01<00:00, 1689.95it/s]warmup should be done:  78%|  | 2353/3000 [00:01<00:00, 1674.00it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1679.29it/s]warmup should be done:  80%|  | 2406/3000 [00:01<00:00, 1699.23it/s]warmup should be done:  78%|  | 2355/3000 [00:01<00:00, 1663.56it/s]warmup should be done:  84%| | 2522/3000 [00:01<00:00, 1678.69it/s]warmup should be done:  86%| | 2595/3000 [00:01<00:00, 1721.85it/s]warmup should be done:  83%| | 2503/3000 [00:01<00:00, 1667.26it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1675.56it/s]warmup should be done:  85%| | 2552/3000 [00:01<00:00, 1688.36it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1680.84it/s]warmup should be done:  86%| | 2576/3000 [00:01<00:00, 1692.97it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1669.57it/s]warmup should be done:  90%| | 2690/3000 [00:01<00:00, 1678.35it/s]warmup should be done:  92%|| 2768/3000 [00:01<00:00, 1722.80it/s]warmup should be done:  89%| | 2670/3000 [00:01<00:00, 1666.73it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1675.25it/s]warmup should be done:  91%| | 2721/3000 [00:01<00:00, 1687.09it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1682.12it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1684.20it/s]warmup should be done:  92%|| 2747/3000 [00:01<00:00, 1695.95it/s]warmup should be done:  95%|| 2858/3000 [00:01<00:00, 1676.62it/s]warmup should be done:  95%|| 2837/3000 [00:01<00:00, 1664.39it/s]warmup should be done:  98%|| 2941/3000 [00:01<00:00, 1712.38it/s]warmup should be done:  95%|| 2857/3000 [00:01<00:00, 1671.40it/s]warmup should be done:  96%|| 2890/3000 [00:01<00:00, 1684.20it/s]warmup should be done:  95%|| 2858/3000 [00:01<00:00, 1681.22it/s]warmup should be done:  96%|| 2868/3000 [00:01<00:00, 1693.61it/s]warmup should be done:  97%|| 2918/3000 [00:01<00:00, 1699.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1721.34it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1705.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1691.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.63it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1673.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1672.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.69it/s]2022-12-12 00:05:51.510559: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f53c002cd00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:51.510623: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:51.545146: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f53b002c770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:51.545207: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:51.586555: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6f9bf910d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:51.586627: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:52.170060: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6f9f82f720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:52.170132: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:52.197952: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6fa782b8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:52.198021: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:52.199908: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6f9b82bfd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:52.199955: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:52.247526: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f53a402fa10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:52.247600: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:52.257292: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6f9f832fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:05:52.257360: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:05:53.781064: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:53.797244: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:53.899369: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:54.495218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:54.498298: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:54.511655: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:54.513789: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:54.514694: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:05:56.643933: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:56.667189: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:56.776261: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:57.343559: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:57.366965: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:57.366964: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:57.368514: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:05:57.415451: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:06:33.248][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:06:33.249][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.254][ERROR][RK0][main]: coll ps creation done
[HCTR][00:06:33.254][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][00:06:33.301][ERROR][RK0][tid #140117721208576]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:06:33.301][ERROR][RK0][tid #140117721208576]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.306][ERROR][RK0][tid #140117721208576]: coll ps creation done
[HCTR][00:06:33.306][ERROR][RK0][tid #140117721208576]: replica 4 waits for coll ps creation barrier
[HCTR][00:06:33.440][ERROR][RK0][tid #140117838640896]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:06:33.440][ERROR][RK0][tid #140117838640896]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.448][ERROR][RK0][tid #140117838640896]: coll ps creation done
[HCTR][00:06:33.448][ERROR][RK0][tid #140117838640896]: replica 3 waits for coll ps creation barrier
[HCTR][00:06:33.457][ERROR][RK0][tid #140117905749760]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:06:33.457][ERROR][RK0][tid #140117905749760]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.464][ERROR][RK0][tid #140117905749760]: coll ps creation done
[HCTR][00:06:33.464][ERROR][RK0][tid #140117905749760]: replica 1 waits for coll ps creation barrier
[HCTR][00:06:33.470][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:06:33.471][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.475][ERROR][RK0][main]: coll ps creation done
[HCTR][00:06:33.475][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][00:06:33.482][ERROR][RK0][tid #140118174185216]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:06:33.482][ERROR][RK0][tid #140118174185216]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.487][ERROR][RK0][tid #140118174185216]: coll ps creation done
[HCTR][00:06:33.487][ERROR][RK0][tid #140118174185216]: replica 2 waits for coll ps creation barrier
[HCTR][00:06:33.514][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:06:33.514][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.521][ERROR][RK0][main]: coll ps creation done
[HCTR][00:06:33.521][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][00:06:33.543][ERROR][RK0][tid #140118316795648]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:06:33.543][ERROR][RK0][tid #140118316795648]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:06:33.548][ERROR][RK0][tid #140118316795648]: coll ps creation done
[HCTR][00:06:33.548][ERROR][RK0][tid #140118316795648]: replica 7 waits for coll ps creation barrier
[HCTR][00:06:33.548][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][00:06:34.413][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][00:06:34.447][ERROR][RK0][tid #140117905749760]: replica 1 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][tid #140117721208576]: replica 4 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][main]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][tid #140118316795648]: replica 7 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][tid #140117838640896]: replica 3 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][tid #140118174185216]: replica 2 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][00:06:34.447][ERROR][RK0][tid #140117905749760]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][tid #140117721208576]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][main]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][tid #140118316795648]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][tid #140117838640896]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][tid #140118174185216]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][main]: Calling build_v2
[HCTR][00:06:34.447][ERROR][RK0][tid #140117905749760]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][tid #140117721208576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][tid #140118316795648]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][tid #140117838640896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][tid #140118174185216]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:06:34.447][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 00:06:34.447867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136] using concurrent impl MPS
[[[[[[[2022-12-12 00:06:342022-12-12 00:06:342022-12-12 00:06:342022-12-12 00:06:342022-12-12 00:06:34...2022-12-12 00:06:34.2022-12-12 00:06:34.447928447931447936.447931.447935: : : 447945: 447937: EEE: E: E   E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136136136:136:136] ] ] 136] 136] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS] using concurrent impl MPS


using concurrent impl MPS
using concurrent impl MPS


[2022-12-12 00:06:34.449553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:06:34.449590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 00:06:34.449645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-12 00:06:34.449685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-12 00:06:34.449713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 00:06:34.451956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:06:34.451996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu[
2022-12-12 00:06:34.452009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 00:06:34.452053: [E2022-12-12 00:06:34 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:06:34452055:.: 196452066E[] :  2022-12-12 00:06:34assigning 8 to cpuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
 :452103/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178: [:] E2022-12-12 00:06:34212v100x8, slow pcie .] 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc452146build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:06:34:[[: 
.1782022-12-12 00:06:342022-12-12 00:06:34E452184] .. : [v100x8, slow pcie452194452200/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 00:06:34
: : : .E[E178[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc452236  ] 2022-12-12 00:06:342022-12-12 00:06:34:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie..212E::
452261452275]  178196[: : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] 2022-12-12 00:06:34EE
:v100x8, slow pcieassigning 8 to cpu.  213

452369[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 2022-12-12 00:06:342022-12-12 00:06:34::[remote time is 8.68421E..178196] 2022-12-12 00:06:34
 452439452460] v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : [assigning 8 to cpu
452476:EE2022-12-12 00:06:34
: 196  [.E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:06:34452537 assigning 8 to cpu::.: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
213196452576E2022-12-12 00:06:34:] ] :  .212remote time is 8.68421assigning 8 to cpuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc452620] 
[
 :: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:06:34[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E
.2022-12-12 00:06:34:] [ 452685.196[cpu time is 97.05882022-12-12 00:06:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 452709] 2022-12-12 00:06:34
.:E: assigning 8 to cpu.452750212 E
452767: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 
212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[] :212
2022-12-12 00:06:342022-12-12 00:06:34cpu time is 97.0588213] ..
[] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 84528754528812022-12-12 00:06:34remote time is 8.68421
: : .
[EE4529182022-12-12 00:06:34[  : .2022-12-12 00:06:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE452968.:: : 452975212213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: ] ] : Eremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 

] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[remote time is 8.68421213:2022-12-12 00:06:34
] 214.[remote time is 8.68421] 453091[2022-12-12 00:06:34
cpu time is 97.0588: 2022-12-12 00:06:34.
453106E[.:  2022-12-12 00:06:34453115E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:  :453145E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:  :] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213cpu time is 97.0588 :] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214remote time is 8.68421:] 
214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 00:06:34.453289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 00:07:51.489362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:07:51.529514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:07:51.529585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 3999999
[2022-12-12 00:07:51.640748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:07:51.640835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:07:51.640868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:07:51.640899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:07:51.641366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.642373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.643060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.656043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 00:07:51.656102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 00:07:51[.2022-12-12 00:07:51656413.: 656425E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 202] 4 solved1 solved

[2022-12-12 00:07:51.[[6564932022-12-12 00:07:512022-12-12 00:07:51: ..E656509656511 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE:  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] ::eager alloc mem 381.47 MB205205
] ] worker 0 thread 4 initing device 4worker 0 thread 1 initing device 1

[[2022-12-12 00:07:512022-12-12 00:07:51..656962656962: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 00:07:51.658535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.658591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.658644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.660313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.660370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.660413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.660555: E[ 2022-12-12 00:07:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:660560202: ] E2 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [7 solved2022-12-12 00:07:51
.660648: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 00:07:51:.205660669] : worker 0 thread 2 initing device 2E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 00:07:51.660764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[[2022-12-12 00:07:512022-12-12 00:07:51..660797660817: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 6 solvedworker 0 thread 5 initing device 5

[2022-12-12 00:07:51.660924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 00:07:51.661062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.661129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.661269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.661336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.664888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.664995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.665055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.665111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.667469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.667518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.667583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.667637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:07:51.721894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 00:07:51.722299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:07:51.727877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:07:51.727980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:07:51.728028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:07:51.729138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.729887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.730880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:51.730972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:07:51.731654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:07:51.731699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[2022-12-12 00:07:512022-12-12 00:07:512022-12-12 00:07:512022-12-12 00:07:512022-12-12 00:07:51.....742942742942742942742942742942: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[2022-12-12 00:07:51[[.2022-12-12 00:07:51[2022-12-12 00:07:51743404.2022-12-12 00:07:51.: [743407.743407E2022-12-12 00:07:51: 743414:  .E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu743423 E :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:]  1980:1980eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980] 
:eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes1980
eager alloc mem 1024.00 Bytes
] 
eager alloc mem 1024.00 Bytes
[[2022-12-12 00:07:512022-12-12 00:07:51..753098753098: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 00:07:51.753447[: 2022-12-12 00:07:51E. 753454/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes:
1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:07:51.760394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:07:51.760469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:07:51eager release cuda mem 2.
760471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:07:51.760527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 00:07:51
.760547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:07:51.760594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:07:51.760607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:07:51.760682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2[
2022-12-12 00:07:51.760691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024[
2022-12-12 00:07:51.760732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:07:51.760758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:07:51.760773: E[ 2022-12-12 00:07:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:760802638: ] Eeager release cuda mem 1024 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:07:51.760851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:07:51.760895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:07:51.761077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:07:51.761145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:07:51[.2022-12-12 00:07:51761186.: 761175E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[2022-12-12 00:07:51.761263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:07:51.761307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:07:51.761638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.762338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.763286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.763856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.764681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.765364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.765876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:07:51.766451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.766608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.766954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.766987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.767252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.767282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:51.767393: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:07:51:.1980767414] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:51.767535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:07:51.767560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:51.767642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:07:51.767906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:51.767932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:51.767988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:07:51.768014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:07:51.[7682062022-12-12 00:07:51: .E768211 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE2022-12-12 00:07:51: .638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc768227] :: eager release cuda mem 25855638E
]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:[6382022-12-12 00:07:51] .[eager release cuda mem 6256637683082022-12-12 00:07:51
: .E768320 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[] :2022-12-12 00:07:51eager alloc mem 1.91 GB638.
] 768363eager release cuda mem 25855[: 
2022-12-12 00:07:51E[.[ 2022-12-12 00:07:517683842022-12-12 00:07:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: .:768399E7684151980:  : ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager alloc mem 25.25 KB
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1980eager release cuda mem 6256631980] 
] eager alloc mem 25.25 KBeager alloc mem 1.91 GB

[2022-12-12 00:07:51.768604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:07:51.768657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:07:51.768682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:07:51638.] 768698eager release cuda mem 25855: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:07:51.768732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:07:51.769148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:07:51.769187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB[
2022-12-12 00:07:51.769204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:07:51.769252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:07:51.769279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:07:51.769320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[[[2022-12-12 00:07:522022-12-12 00:07:522022-12-12 00:07:522022-12-12 00:07:522022-12-12 00:07:522022-12-12 00:07:522022-12-12 00:07:522022-12-12 00:07:52........152919152913152913152913152913152913152918152927: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[2022-12-12 00:07:522022-12-12 00:07:52.[[[.[1540302022-12-12 00:07:52[2022-12-12 00:07:52[2022-12-12 00:07:521540302022-12-12 00:07:52: .2022-12-12 00:07:52.2022-12-12 00:07:52.: .E154040.154040.154040E154044 : 154048: 154050:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: E E : 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] :eager release cuda mem 625663638:638:638eager release cuda mem 625663638
] 638] 638] 
] eager release cuda mem 625663] eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663
eager release cuda mem 625663


[
2022-12-12 00:07:52[.2022-12-12 00:07:52154325.: 154345E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:07:521980[:.] [2022-12-12 00:07:52[1980154372[eager alloc mem 611.00 KB2022-12-12 00:07:52.[2022-12-12 00:07:52] : 2022-12-12 00:07:52
.1543802022-12-12 00:07:52.eager alloc mem 611.00 KBE.154386: .154387
 154392: E154398: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E : E:E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB:1980] :1980
1980] eager alloc mem 611.00 KB1980] ] eager alloc mem 611.00 KB
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[2022-12-12 00:07:52.155165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.155208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.155237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.155279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.155334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.155358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52[.2022-12-12 00:07:52155381[.[: 2022-12-12 00:07:52[1553872022-12-12 00:07:52E.2022-12-12 00:07:52: [. 155390.E2022-12-12 00:07:52155395/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 155404 .: :E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc155430E638 E::  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  :
638:eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638] 1980
:] eager release cuda mem 625663] 1980eager release cuda mem 625663
eager alloc mem 611.00 KB] 

[eager alloc mem 611.00 KB2022-12-12 00:07:52
.155628: E[ 2022-12-12 00:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1556511980: ] E[eager alloc mem 611.00 KB 2022-12-12 00:07:52
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-12 00:07:52:155672.1980: 155683] E: eager alloc mem 611.00 KB E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 00:07:52.155984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.156025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.156054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.156092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52[.2022-12-12 00:07:52156367.: 156372E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 00:07:52.156418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.[1564542022-12-12 00:07:52: .E156459 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[] :2022-12-12 00:07:52eager alloc mem 611.00 KB1980[.
] 2022-12-12 00:07:52156489eager alloc mem 611.00 KB.: 
156498[E[: 2022-12-12 00:07:52 2022-12-12 00:07:52E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. 156517:156523/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 1980: :E] E638 eager alloc mem 611.00 KB ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663::
638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 00:07:52.156707: E[ [2022-12-12 00:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:07:52.:.1567201980156724: ] : Eeager alloc mem 611.00 KBE 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::[198019802022-12-12 00:07:52] ] .eager alloc mem 611.00 KBeager alloc mem 611.00 KB156799

[: 2022-12-12 00:07:52E. 156840/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 00:07:52.156922[: 2022-12-12 00:07:52E. 156930/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.157232: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:07:52:.638157244] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.157308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:07:521980.] 157321eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.157369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.157434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.157521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.[1575692022-12-12 00:07:52: .E157576[ : 2022-12-12 00:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE.: 157589638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ] :Eeager release cuda mem 625663638 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980[] 2022-12-12 00:07:52[eager alloc mem 611.00 KB.2022-12-12 00:07:52
[157684.2022-12-12 00:07:52: 157695.E: [157703 E2022-12-12 00:07:52: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc .E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc157733 638:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 638E:eager release cuda mem 625663]  1980
eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
:eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.157862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:07:521980.] 157877eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.158063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:07:52] .eager release cuda mem 625663158079
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.158135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:07:52eager alloc mem 611.00 KB.
158152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.158182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.158245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.158438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.158512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.158567: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:07:52:.638158580] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.158621: [E2022-12-12 00:07:52 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc158632:: 638[E] 2022-12-12 00:07:52 eager release cuda mem 625663[./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-12 00:07:52158648:.: 638158666E] :  eager release cuda mem 625663[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
2022-12-12 00:07:52 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980158725:] : 1980eager alloc mem 611.00 KBE] [
 eager alloc mem 611.00 KB2022-12-12 00:07:52/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
.:1587811980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.158884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 00:07:52
.158903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.158953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 00:07:52
.158972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 00:07:521980.] 158990eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.159063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.159261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.159329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:07:52.159522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 00:07:52eager release cuda mem 625663.
[1595472022-12-12 00:07:52[: .2022-12-12 00:07:52E159558. : 159573/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E: :2022-12-12 00:07:52 E638./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] 159607:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663: 638:
E] 638 eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager release cuda mem 625663:
1980[[] 2022-12-12 00:07:522022-12-12 00:07:52eager alloc mem 611.00 KB[[..[
2022-12-12 00:07:522022-12-12 00:07:521597091597122022-12-12 00:07:52..: : .159721159720EE159733: :   : EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE  :2022-12-12 00:07:52: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638.638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::] 159807] :6381980eager release cuda mem 625663: eager release cuda mem 16399996638] ] 
E
] eager release cuda mem 16399996eager alloc mem 611.00 KB eager release cuda mem 625663
[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-12 00:07:52:.638159913] : eager release cuda mem 625663[E
2022-12-12 00:07:52 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc159944:: [638E2022-12-12 00:07:52]  .eager release cuda mem 16399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc159964
:: 638E]  eager release cuda mem 16399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 16399996
[2022-12-12 00:07:52.160074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.160110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:07:52.160484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.160522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:07:52.160662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:07:52.160702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:07:52.164926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.508439 secs 
[2022-12-12 00:07:52.168882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.507618 secs 
[2022-12-12 00:07:52.169296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.51234 secs 
[2022-12-12 00:07:52.169714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.508659 secs 
[2022-12-12 00:07:52.169806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.512851 secs 
[2022-12-12 00:07:52.170220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.528859 secs 
[2022-12-12 00:07:52.170322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.509206 secs 
[2022-12-12 00:07:52.170411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.509083 secs 
[2022-12-12 00:07:52.174626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.55 GB
[2022-12-12 00:07:53.706469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.81 GB
[2022-12-12 00:07:53.706699: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.81 GB
[2022-12-12 00:07:53.707030: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.81 GB
[2022-12-12 00:07:55. 55054: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.07 GB
[2022-12-12 00:07:55. 77949: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.07 GB
[2022-12-12 00:07:55. 78708: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.07 GB
[2022-12-12 00:07:56.265245: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.29 GB
[2022-12-12 00:07:56.265634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.29 GB
[2022-12-12 00:07:56.266148: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.29 GB
[2022-12-12 00:07:57.250277: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.50 GB
[2022-12-12 00:07:57.251335: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.50 GB
[2022-12-12 00:07:57.252654: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.50 GB
[2022-12-12 00:07:58.628692: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.96 GB
[2022-12-12 00:07:58.629360: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.96 GB
[2022-12-12 00:07:58.631916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.96 GB
[2022-12-12 00:08:00.222223: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.16 GB
[2022-12-12 00:08:00.248125: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.16 GB
[HCTR][00:08:01.310][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][tid #140117721208576]: replica 4 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][tid #140117905749760]: replica 1 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][tid #140118174185216]: replica 2 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][tid #140118316795648]: replica 7 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][tid #140117838640896]: replica 3 calling init per replica done, doing barrier
[HCTR][00:08:01.310][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][tid #140118316795648]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][tid #140117721208576]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][tid #140117905749760]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][tid #140117838640896]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][tid #140118174185216]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:08:01.310][ERROR][RK0][main]: init per replica done
[HCTR][00:08:01.310][ERROR][RK0][tid #140118316795648]: init per replica done
[HCTR][00:08:01.310][ERROR][RK0][tid #140117905749760]: init per replica done
[HCTR][00:08:01.310][ERROR][RK0][tid #140117721208576]: init per replica done
[HCTR][00:08:01.310][ERROR][RK0][tid #140117838640896]: init per replica done
[HCTR][00:08:01.310][ERROR][RK0][main]: init per replica done
[HCTR][00:08:01.310][ERROR][RK0][tid #140118174185216]: init per replica done
[HCTR][00:08:01.312][ERROR][RK0][main]: init per replica done
[HCTR][00:08:01.316][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f5cbf520000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117838640896]: 3 allocated 3276800 at 0x7f5cbb520000
[HCTR][00:08:01.316][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f718ec00000
[HCTR][00:08:01.316][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f5cbb520000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117838640896]: 3 allocated 6553600 at 0x7f718ec00000
[HCTR][00:08:01.316][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f5cbb520000
[HCTR][00:08:01.316][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f718f240000
[HCTR][00:08:01.316][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f718ec00000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117838640896]: 3 allocated 3276800 at 0x7f718f240000
[HCTR][00:08:01.316][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f718ec00000
[HCTR][00:08:01.316][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f718f240000
[HCTR][00:08:01.316][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f718f560000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117838640896]: 3 allocated 6553600 at 0x7f718f560000
[HCTR][00:08:01.316][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f718f240000
[HCTR][00:08:01.316][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f718f560000
[HCTR][00:08:01.316][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f5cbf520000
[HCTR][00:08:01.316][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f718f560000
[HCTR][00:08:01.316][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f718ec00000
[HCTR][00:08:01.316][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f718f240000
[HCTR][00:08:01.316][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f718f560000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117914142464]: 6 allocated 3276800 at 0x7f5cbb520000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117914142464]: 6 allocated 6553600 at 0x7f718ec00000
[HCTR][00:08:01.316][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f5cbf520000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117914142464]: 6 allocated 3276800 at 0x7f718f240000
[HCTR][00:08:01.316][ERROR][RK0][tid #140117914142464]: 6 allocated 6553600 at 0x7f718f560000
[HCTR][00:08:01.316][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7190c00000
[HCTR][00:08:01.316][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7191240000
[HCTR][00:08:01.316][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7191560000
[HCTR][00:08:01.319][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f7190d20000
[HCTR][00:08:01.319][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f7191200000
[HCTR][00:08:01.319][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f7191f0e800
[HCTR][00:08:01.319][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f719222e800








