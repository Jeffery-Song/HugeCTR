2022-12-11 23:04:10.740225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.747789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.753599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.759291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.764489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.770981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.783564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.791794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.841023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.846033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.851524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.863041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.874344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.880061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.888816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.891309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.893522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.895239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.895647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.904285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.905151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.908493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.908768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.910198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.910334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.911825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.911876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.913328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.913565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.915098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.915328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.916962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.918773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.919759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.920795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.921810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.922833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.923851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.924813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.925731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.931073: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:10.931455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.932538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.933532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.934610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.935715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.936830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.937795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.939057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.940807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.941266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.942046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.942940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.944005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.944899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.946246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.947846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.949776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.952016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.952625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.953914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.954716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.957022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.959355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.960031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.961144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.962015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.962296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.963090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.964682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.964787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.965603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.967124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.967280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.968226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.975795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.976207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.977125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.978507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.979067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.980063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.980698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.981074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:10.985165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.000352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.015529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.016421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.017503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.017512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.018021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.018410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.019301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.020548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.021733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.021753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.022232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.022416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.022858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.025930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.026441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.026507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.026960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.027967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.028104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.030651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.030846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.030894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.031659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.031979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.032201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.035500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.035682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.035723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.036203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.036501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.036999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.040020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.040277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.040706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.040888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.041116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.042034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.043962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.044429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.045070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.045423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.046478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.047752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.047990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.048413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.048694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.050878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.051059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.051379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.051764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.054151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.054304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.055011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.055774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.057788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.058413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.058715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.058838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.060649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.061520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.061914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.062194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.063683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.064382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.064817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.065205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.066324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.067214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.067626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.067925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.068152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.069398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.070325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.070750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.071054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.071470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.072650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.074794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.075350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.075445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.075602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.077046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.078745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.078839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.079006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.079095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.081736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.081789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.081867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.082210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.082260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.085188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.085349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.085520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.085580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.087990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.088117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.088269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.088318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.089811: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.089993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.092400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.092584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.092629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.092648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.094031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.094073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.096224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.096343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.096515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.097120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.097702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.097870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.099838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.099990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.100153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.100365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.101181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.102001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.102182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.104365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.104621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.104710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.106181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.106340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.108544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.108706: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.109086: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.109156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.109298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.110852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.110965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.112951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.114209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.114486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.116228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.116623: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.117140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.117426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.118211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.118638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.119359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.120656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.120971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.122334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.122712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.123364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.125500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.125770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.126347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.128334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.129107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.130251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.131219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.131274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.131489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.134906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.135788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.135929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.136164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.139538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.140698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.141000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.172123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.172910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.173139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.177423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.178349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.178652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.184021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.185050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.187521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.192757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.193989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.194384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.197341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.200632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.201390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.232757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.234569: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.234761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.237994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.239232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.244491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.244577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.250670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.253361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.254608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.255926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.260415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.260733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.261810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.327207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.329683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.360238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.362697: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.371240: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:04:11.372501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.379616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.380005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.385941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.386525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:11.392487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.250653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.251298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.251819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.252299: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.252351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:04:12.269537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.270587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.271303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.271891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.272417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.272901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:04:12.317694: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.317906: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.347726: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:04:12.498569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.499209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.500238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.500712: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.500763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:04:12.517665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.518524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.519033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.519627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.520175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.520646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:04:12.522360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.523141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.523757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.524257: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.524309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:04:12.540319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.540923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.541437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.542243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.542752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.543234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:04:12.549797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.551111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.552916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.554690: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.554741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:04:12.571078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.572422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.573412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.574501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.575570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.575703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.578604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:04:12.578826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.580018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.580966: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.581014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:04:12.592336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.593944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.594942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.596493: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.596553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:04:12.597626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.598658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.599820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.601006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.601324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.601355: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.601557: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.602458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.602920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.603277: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:04:12.603845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:04:12.604313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.605265: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.605321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:04:12.613246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.613246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.615783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.615813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.616969: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.617150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.617531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.617541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.618594: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:04:12.618649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:04:12.618754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.618785: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:04:12.619560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.622515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:04:12.623005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.623101: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.623265: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.624482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.624984: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:04:12.625256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.626637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.627475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.628092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:04:12.636503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.637174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.637688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.638268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.638781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:04:12.639264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:04:12.648985: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.649179: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.650826: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:04:12.667674: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.667879: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.669622: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:04:12.673382: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.673530: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.675386: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:04:12.683205: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.683396: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:04:12.685587: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][23:04:13.958][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.958][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.959][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.959][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.960][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.961][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.987][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:04:13.987][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 71it [00:01, 59.74it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 102it [00:01, 85.83it/s]warmup run: 94it [00:01, 78.90it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 95it [00:01, 81.08it/s]warmup run: 167it [00:01, 156.58it/s]warmup run: 100it [00:01, 85.83it/s]warmup run: 204it [00:01, 186.29it/s]warmup run: 187it [00:01, 170.14it/s]warmup run: 100it [00:01, 87.19it/s]warmup run: 93it [00:01, 80.70it/s]warmup run: 97it [00:01, 85.63it/s]warmup run: 191it [00:01, 176.74it/s]warmup run: 261it [00:01, 260.93it/s]warmup run: 200it [00:01, 185.94it/s]warmup run: 307it [00:01, 298.27it/s]warmup run: 278it [00:01, 268.19it/s]warmup run: 196it [00:01, 184.11it/s]warmup run: 181it [00:01, 169.03it/s]warmup run: 193it [00:01, 183.74it/s]warmup run: 289it [00:01, 284.65it/s]warmup run: 359it [00:01, 375.56it/s]warmup run: 300it [00:01, 295.68it/s]warmup run: 409it [00:01, 412.97it/s]warmup run: 369it [00:01, 369.94it/s]warmup run: 297it [00:01, 296.85it/s]warmup run: 276it [00:01, 275.28it/s]warmup run: 288it [00:01, 289.52it/s]warmup run: 386it [00:01, 395.08it/s]warmup run: 454it [00:02, 481.78it/s]warmup run: 401it [00:01, 410.60it/s]warmup run: 514it [00:02, 530.44it/s]warmup run: 460it [00:02, 469.88it/s]warmup run: 397it [00:01, 411.73it/s]warmup run: 377it [00:01, 394.35it/s]warmup run: 385it [00:01, 401.25it/s]warmup run: 483it [00:02, 501.83it/s]warmup run: 554it [00:02, 589.27it/s]warmup run: 501it [00:02, 521.07it/s]warmup run: 618it [00:02, 636.23it/s]warmup run: 556it [00:02, 571.45it/s]warmup run: 497it [00:01, 522.88it/s]warmup run: 476it [00:02, 506.22it/s]warmup run: 483it [00:01, 510.45it/s]warmup run: 581it [00:02, 601.99it/s]warmup run: 654it [00:02, 683.06it/s]warmup run: 603it [00:02, 625.39it/s]warmup run: 721it [00:02, 725.69it/s]warmup run: 655it [00:02, 666.75it/s]warmup run: 599it [00:02, 627.66it/s]warmup run: 576it [00:02, 610.40it/s]warmup run: 579it [00:02, 605.57it/s]warmup run: 680it [00:02, 691.56it/s]warmup run: 750it [00:02, 749.60it/s]warmup run: 706it [00:02, 717.89it/s]warmup run: 822it [00:02, 795.83it/s]warmup run: 756it [00:02, 749.72it/s]warmup run: 701it [00:02, 717.90it/s]warmup run: 676it [00:02, 700.68it/s]warmup run: 678it [00:02, 694.25it/s]warmup run: 780it [00:02, 767.73it/s]warmup run: 846it [00:02, 802.85it/s]warmup run: 809it [00:02, 794.98it/s]warmup run: 923it [00:02, 849.62it/s]warmup run: 856it [00:02, 814.12it/s]warmup run: 803it [00:02, 793.18it/s]warmup run: 777it [00:02, 776.02it/s]warmup run: 779it [00:02, 772.56it/s]warmup run: 879it [00:02, 825.58it/s]warmup run: 942it [00:02, 844.53it/s]warmup run: 912it [00:02, 854.55it/s]warmup run: 1024it [00:02, 889.98it/s]warmup run: 954it [00:02, 858.52it/s]warmup run: 905it [00:02, 851.97it/s]warmup run: 878it [00:02, 835.46it/s]warmup run: 880it [00:02, 833.94it/s]warmup run: 977it [00:02, 867.19it/s]warmup run: 1038it [00:02, 876.43it/s]warmup run: 1014it [00:02, 898.10it/s]warmup run: 1125it [00:02, 923.15it/s]warmup run: 1055it [00:02, 899.53it/s]warmup run: 1007it [00:02, 897.43it/s]warmup run: 978it [00:02, 877.95it/s]warmup run: 980it [00:02, 878.59it/s]warmup run: 1076it [00:02, 899.78it/s]warmup run: 1136it [00:02, 903.71it/s]warmup run: 1117it [00:02, 932.96it/s]warmup run: 1227it [00:02, 949.91it/s]warmup run: 1156it [00:02, 930.87it/s]warmup run: 1109it [00:02, 927.59it/s]warmup run: 1078it [00:02, 909.80it/s]warmup run: 1081it [00:02, 915.09it/s]warmup run: 1175it [00:02, 925.15it/s]warmup run: 1238it [00:02, 935.01it/s]warmup run: 1219it [00:02, 956.14it/s]warmup run: 1329it [00:02, 969.06it/s]warmup run: 1257it [00:02, 953.64it/s]warmup run: 1210it [00:02, 947.45it/s]warmup run: 1179it [00:02, 936.40it/s]warmup run: 1183it [00:02, 942.98it/s]warmup run: 1274it [00:02, 942.08it/s]warmup run: 1339it [00:02, 955.46it/s]warmup run: 1321it [00:02, 969.49it/s]warmup run: 1432it [00:02, 985.17it/s]warmup run: 1357it [00:02, 966.16it/s]warmup run: 1312it [00:02, 966.09it/s]warmup run: 1280it [00:02, 955.71it/s]warmup run: 1285it [00:02, 963.41it/s]warmup run: 1373it [00:02, 953.99it/s]warmup run: 1439it [00:03, 967.49it/s]warmup run: 1424it [00:02, 984.55it/s]warmup run: 1535it [00:03, 995.95it/s]warmup run: 1457it [00:03, 951.80it/s]warmup run: 1414it [00:02, 981.67it/s]warmup run: 1382it [00:02, 972.58it/s]warmup run: 1387it [00:02, 977.19it/s]warmup run: 1472it [00:03, 959.22it/s]warmup run: 1539it [00:03, 976.55it/s]warmup run: 1526it [00:03, 994.15it/s]warmup run: 1638it [00:03, 1003.92it/s]warmup run: 1559it [00:03, 969.62it/s]warmup run: 1516it [00:02, 990.68it/s]warmup run: 1483it [00:03, 972.06it/s]warmup run: 1488it [00:02, 986.61it/s]warmup run: 1573it [00:03, 972.35it/s]warmup run: 1640it [00:03, 984.23it/s]warmup run: 1628it [00:03, 993.12it/s]warmup run: 1740it [00:03, 1004.88it/s]warmup run: 1661it [00:03, 982.81it/s]warmup run: 1618it [00:03, 994.09it/s]warmup run: 1583it [00:03, 979.83it/s]warmup run: 1590it [00:03, 994.13it/s]warmup run: 1676it [00:03, 987.99it/s]warmup run: 1740it [00:03, 985.06it/s]warmup run: 1842it [00:03, 1004.38it/s]warmup run: 1729it [00:03, 974.37it/s]warmup run: 1761it [00:03, 975.10it/s]warmup run: 1719it [00:03, 998.14it/s]warmup run: 1683it [00:03, 983.23it/s]warmup run: 1691it [00:03, 997.33it/s]warmup run: 1778it [00:03, 995.94it/s]warmup run: 1841it [00:03, 991.71it/s]warmup run: 1944it [00:03, 1002.73it/s]warmup run: 1828it [00:03, 960.03it/s]warmup run: 1863it [00:03, 986.17it/s]warmup run: 1820it [00:03, 989.87it/s]warmup run: 1783it [00:03, 987.04it/s]warmup run: 1792it [00:03, 997.64it/s]warmup run: 1880it [00:03, 1000.42it/s]warmup run: 1942it [00:03, 994.41it/s]warmup run: 2049it [00:03, 1016.53it/s]warmup run: 1925it [00:03, 955.15it/s]warmup run: 1965it [00:03, 995.60it/s]warmup run: 1920it [00:03, 989.40it/s]warmup run: 1883it [00:03, 989.95it/s]warmup run: 1893it [00:03, 993.97it/s]warmup run: 1981it [00:03, 1002.63it/s]warmup run: 2051it [00:03, 1022.61it/s]warmup run: 2168it [00:03, 1065.71it/s]warmup run: 2024it [00:03, 965.19it/s]warmup run: 2078it [00:03, 1033.13it/s]warmup run: 2022it [00:03, 998.30it/s]warmup run: 1983it [00:03, 990.96it/s]warmup run: 1994it [00:03, 998.02it/s]warmup run: 2098it [00:03, 1051.94it/s]warmup run: 2172it [00:03, 1076.01it/s]warmup run: 2288it [00:03, 1103.36it/s]warmup run: 2145it [00:03, 1035.15it/s]warmup run: 2198it [00:03, 1080.94it/s]warmup run: 2139it [00:03, 1048.17it/s]warmup run: 2096it [00:03, 1030.25it/s]warmup run: 2112it [00:03, 1050.22it/s]warmup run: 2220it [00:03, 1101.04it/s]warmup run: 2292it [00:03, 1110.86it/s]warmup run: 2407it [00:03, 1129.13it/s]warmup run: 2266it [00:03, 1084.68it/s]warmup run: 2318it [00:03, 1115.17it/s]warmup run: 2256it [00:03, 1084.04it/s]warmup run: 2209it [00:03, 1058.65it/s]warmup run: 2223it [00:03, 1065.82it/s]warmup run: 2342it [00:03, 1135.13it/s]warmup run: 2407it [00:03, 1121.11it/s]warmup run: 2525it [00:03, 1143.00it/s]warmup run: 2387it [00:03, 1120.01it/s]warmup run: 2438it [00:03, 1138.58it/s]warmup run: 2374it [00:03, 1109.81it/s]warmup run: 2323it [00:03, 1081.80it/s]warmup run: 2336it [00:03, 1083.06it/s]warmup run: 2464it [00:03, 1159.16it/s]warmup run: 2524it [00:04, 1135.06it/s]warmup run: 2642it [00:04, 1148.59it/s]warmup run: 2508it [00:03, 1144.64it/s]warmup run: 2558it [00:04, 1155.23it/s]warmup run: 2491it [00:03, 1126.11it/s]warmup run: 2438it [00:03, 1099.71it/s]warmup run: 2455it [00:03, 1113.13it/s]warmup run: 2586it [00:04, 1175.89it/s]warmup run: 2643it [00:04, 1149.72it/s]warmup run: 2760it [00:04, 1157.35it/s]warmup run: 2629it [00:04, 1162.54it/s]warmup run: 2677it [00:04, 1162.92it/s]warmup run: 2606it [00:04, 1132.74it/s]warmup run: 2554it [00:04, 1115.89it/s]warmup run: 2574it [00:03, 1133.89it/s]warmup run: 2707it [00:04, 1183.99it/s]warmup run: 2764it [00:04, 1166.51it/s]warmup run: 2878it [00:04, 1162.23it/s]warmup run: 2750it [00:04, 1174.79it/s]warmup run: 2797it [00:04, 1172.28it/s]warmup run: 2722it [00:04, 1140.35it/s]warmup run: 2670it [00:04, 1128.71it/s]warmup run: 2692it [00:04, 1145.28it/s]warmup run: 2829it [00:04, 1193.15it/s]warmup run: 2885it [00:04, 1179.43it/s]warmup run: 2996it [00:04, 1164.80it/s]warmup run: 2869it [00:04, 1177.76it/s]warmup run: 2916it [00:04, 1177.49it/s]warmup run: 3000it [00:04, 685.16it/s] warmup run: 2839it [00:04, 1148.09it/s]warmup run: 2788it [00:04, 1141.64it/s]warmup run: 2811it [00:04, 1156.38it/s]warmup run: 2951it [00:04, 1199.59it/s]warmup run: 3000it [00:04, 671.55it/s] warmup run: 3000it [00:04, 673.10it/s] warmup run: 2987it [00:04, 1164.61it/s]warmup run: 3000it [00:04, 683.68it/s] warmup run: 3000it [00:04, 685.84it/s] warmup run: 2955it [00:04, 1150.89it/s]warmup run: 2906it [00:04, 1151.56it/s]warmup run: 2928it [00:04, 1159.81it/s]warmup run: 3000it [00:04, 688.82it/s] warmup run: 3000it [00:04, 688.84it/s] warmup run: 3000it [00:04, 681.43it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1617.34it/s]warmup should be done:   5%|         | 152/3000 [00:00<00:01, 1518.40it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1624.76it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1652.35it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.03it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.11it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1570.29it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1580.52it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1616.51it/s]warmup should be done:  10%|         | 306/3000 [00:00<00:01, 1527.16it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1632.09it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.15it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1657.90it/s]warmup should be done:  11%|         | 317/3000 [00:00<00:01, 1579.96it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1617.98it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1594.79it/s]warmup should be done:  15%|        | 459/3000 [00:00<00:01, 1525.97it/s]warmup should be done:  16%|        | 475/3000 [00:00<00:01, 1579.43it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1617.77it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1629.23it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1631.89it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1653.26it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1586.89it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1602.11it/s]warmup should be done:  21%|        | 633/3000 [00:00<00:01, 1576.23it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1614.91it/s]warmup should be done:  20%|        | 612/3000 [00:00<00:01, 1522.29it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1627.24it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1630.39it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1651.42it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1603.66it/s]warmup should be done:  21%|       | 639/3000 [00:00<00:01, 1584.91it/s]warmup should be done:  26%|       | 791/3000 [00:00<00:01, 1574.47it/s]warmup should be done:  26%|       | 765/3000 [00:00<00:01, 1522.07it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1611.23it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1622.04it/s]warmup should be done:  27%|       | 798/3000 [00:00<00:01, 1586.09it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1607.56it/s]warmup should be done:  28%|       | 831/3000 [00:00<00:01, 1647.71it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1621.51it/s]warmup should be done:  31%|       | 920/3000 [00:00<00:01, 1529.47it/s]warmup should be done:  32%|      | 949/3000 [00:00<00:01, 1568.37it/s]warmup should be done:  32%|      | 957/3000 [00:00<00:01, 1584.17it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1608.04it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1641.92it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1613.80it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1616.17it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1590.56it/s]warmup should be done:  36%|      | 1077/3000 [00:00<00:01, 1540.64it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1609.69it/s]warmup should be done:  37%|      | 1106/3000 [00:00<00:01, 1564.67it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1642.80it/s]warmup should be done:  37%|      | 1116/3000 [00:00<00:01, 1579.43it/s]warmup should be done:  38%|      | 1143/3000 [00:00<00:01, 1615.83it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1618.71it/s]warmup should be done:  38%|      | 1131/3000 [00:00<00:01, 1595.90it/s]warmup should be done:  41%|      | 1235/3000 [00:00<00:01, 1550.66it/s]warmup should be done:  42%|     | 1265/3000 [00:00<00:01, 1572.63it/s]warmup should be done:  43%|     | 1297/3000 [00:00<00:01, 1610.57it/s]warmup should be done:  42%|     | 1275/3000 [00:00<00:01, 1579.57it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1616.24it/s]warmup should be done:  43%|     | 1292/3000 [00:00<00:01, 1599.20it/s]warmup should be done:  44%|     | 1308/3000 [00:00<00:01, 1614.16it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1631.88it/s]warmup should be done:  46%|     | 1393/3000 [00:00<00:01, 1557.98it/s]warmup should be done:  48%|     | 1426/3000 [00:00<00:00, 1582.37it/s]warmup should be done:  49%|     | 1459/3000 [00:00<00:00, 1610.84it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1623.55it/s]warmup should be done:  48%|     | 1433/3000 [00:00<00:00, 1578.71it/s]warmup should be done:  48%|     | 1453/3000 [00:00<00:00, 1601.27it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1612.55it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1622.53it/s]warmup should be done:  52%|    | 1551/3000 [00:01<00:00, 1561.79it/s]warmup should be done:  53%|    | 1587/3000 [00:01<00:00, 1588.45it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1630.24it/s]warmup should be done:  54%|    | 1621/3000 [00:01<00:00, 1609.75it/s]warmup should be done:  53%|    | 1591/3000 [00:01<00:00, 1577.34it/s]warmup should be done:  54%|    | 1614/3000 [00:01<00:00, 1601.67it/s]warmup should be done:  54%|    | 1632/3000 [00:01<00:00, 1608.07it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1617.39it/s]warmup should be done:  58%|    | 1747/3000 [00:01<00:00, 1591.44it/s]warmup should be done:  59%|    | 1782/3000 [00:01<00:00, 1609.24it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1636.41it/s]warmup should be done:  57%|    | 1708/3000 [00:01<00:00, 1554.42it/s]warmup should be done:  58%|    | 1749/3000 [00:01<00:00, 1575.54it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1601.64it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1604.78it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1615.80it/s]warmup should be done:  64%|   | 1908/3000 [00:01<00:00, 1594.62it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1639.67it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1560.33it/s]warmup should be done:  64%|   | 1907/3000 [00:01<00:00, 1574.87it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1601.48it/s]warmup should be done:  65%|   | 1936/3000 [00:01<00:00, 1601.48it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1603.34it/s]warmup should be done:  66%|   | 1977/3000 [00:01<00:00, 1616.81it/s]warmup should be done:  69%|   | 2068/3000 [00:01<00:00, 1592.87it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1641.74it/s]warmup should be done:  67%|   | 2024/3000 [00:01<00:00, 1564.02it/s]warmup should be done:  69%|   | 2065/3000 [00:01<00:00, 1571.33it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1600.69it/s]warmup should be done:  70%|   | 2097/3000 [00:01<00:00, 1601.82it/s]warmup should be done:  70%|   | 2115/3000 [00:01<00:00, 1601.67it/s]warmup should be done:  71%|  | 2139/3000 [00:01<00:00, 1617.32it/s]warmup should be done:  74%|  | 2228/3000 [00:01<00:00, 1594.31it/s]warmup should be done:  76%|  | 2295/3000 [00:01<00:00, 1640.25it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1570.49it/s]warmup should be done:  74%|  | 2223/3000 [00:01<00:00, 1572.05it/s]warmup should be done:  76%|  | 2265/3000 [00:01<00:00, 1600.09it/s]warmup should be done:  75%|  | 2258/3000 [00:01<00:00, 1597.37it/s]warmup should be done:  76%|  | 2276/3000 [00:01<00:00, 1601.21it/s]warmup should be done:  77%|  | 2301/3000 [00:01<00:00, 1608.11it/s]warmup should be done:  80%|  | 2388/3000 [00:01<00:00, 1593.72it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1642.06it/s]warmup should be done:  78%|  | 2342/3000 [00:01<00:00, 1574.42it/s]warmup should be done:  79%|  | 2381/3000 [00:01<00:00, 1570.60it/s]warmup should be done:  81%|  | 2426/3000 [00:01<00:00, 1600.73it/s]warmup should be done:  81%|  | 2418/3000 [00:01<00:00, 1597.84it/s]warmup should be done:  81%| | 2438/3000 [00:01<00:00, 1604.81it/s]warmup should be done:  82%| | 2462/3000 [00:01<00:00, 1596.17it/s]warmup should be done:  85%| | 2549/3000 [00:01<00:00, 1596.14it/s]warmup should be done:  88%| | 2625/3000 [00:01<00:00, 1642.28it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1579.61it/s]warmup should be done:  85%| | 2539/3000 [00:01<00:00, 1572.03it/s]warmup should be done:  86%| | 2587/3000 [00:01<00:00, 1601.70it/s]warmup should be done:  86%| | 2579/3000 [00:01<00:00, 1598.71it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1607.26it/s]warmup should be done:  87%| | 2623/3000 [00:01<00:00, 1597.22it/s]warmup should be done:  90%| | 2710/3000 [00:01<00:00, 1598.13it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1643.16it/s]warmup should be done:  89%| | 2662/3000 [00:01<00:00, 1583.47it/s]warmup should be done:  90%| | 2697/3000 [00:01<00:00, 1573.18it/s]warmup should be done:  92%|| 2748/3000 [00:01<00:00, 1603.11it/s]warmup should be done:  91%|| 2743/3000 [00:01<00:00, 1608.52it/s]warmup should be done:  92%|| 2762/3000 [00:01<00:00, 1610.14it/s]warmup should be done:  93%|| 2783/3000 [00:01<00:00, 1592.63it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1603.15it/s]warmup should be done:  99%|| 2957/3000 [00:01<00:00, 1649.00it/s]warmup should be done:  94%|| 2822/3000 [00:01<00:00, 1587.49it/s]warmup should be done:  95%|| 2857/3000 [00:01<00:00, 1580.05it/s]warmup should be done:  97%|| 2909/3000 [00:01<00:00, 1603.23it/s]warmup should be done:  97%|| 2906/3000 [00:01<00:00, 1613.39it/s]warmup should be done:  98%|| 2925/3000 [00:01<00:00, 1615.72it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1597.95it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1634.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1618.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1613.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1606.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1603.88it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1588.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1577.71it/s]warmup should be done:  99%|| 2983/3000 [00:01<00:00, 1591.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1561.81it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1618.45it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.49it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.88it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1637.46it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1645.20it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.64it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.37it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.60it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1628.55it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1626.72it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1640.49it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.01it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1652.02it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1660.80it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1694.61it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1605.97it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1651.45it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1635.48it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1625.65it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1664.11it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1642.22it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1661.74it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1697.82it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1600.35it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1629.65it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1656.40it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1641.74it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1698.26it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1666.48it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1640.94it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1661.00it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1608.87it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1656.52it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1643.00it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1633.28it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1668.97it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1642.62it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1694.39it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1657.90it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1609.24it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1655.99it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1643.67it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1670.19it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1631.48it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1645.27it/s]warmup should be done:  34%|      | 1021/3000 [00:00<00:01, 1693.79it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1659.23it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1618.01it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1657.78it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1645.82it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1646.16it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1670.51it/s]warmup should be done:  40%|      | 1191/3000 [00:00<00:01, 1693.28it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1630.50it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1663.40it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1625.29it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1655.35it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1694.65it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1673.99it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1641.54it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1673.02it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1627.13it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1621.93it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1618.01it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1656.88it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1678.85it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1694.56it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1643.91it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1679.93it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1629.47it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1630.36it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1626.97it/s]warmup should be done:  55%|    | 1663/3000 [00:01<00:00, 1662.01it/s]warmup should be done:  57%|    | 1701/3000 [00:01<00:00, 1694.91it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1682.53it/s]warmup should be done:  55%|    | 1651/3000 [00:01<00:00, 1646.89it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1631.83it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1686.16it/s]warmup should be done:  55%|    | 1636/3000 [00:01<00:00, 1636.37it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1631.14it/s]warmup should be done:  61%|    | 1831/3000 [00:01<00:00, 1667.13it/s]warmup should be done:  62%|   | 1871/3000 [00:01<00:00, 1695.12it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1687.06it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1648.92it/s]warmup should be done:  60%|    | 1802/3000 [00:01<00:00, 1632.13it/s]warmup should be done:  62%|   | 1851/3000 [00:01<00:00, 1690.93it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1639.79it/s]warmup should be done:  60%|    | 1814/3000 [00:01<00:00, 1637.12it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1667.48it/s]warmup should be done:  68%|   | 2041/3000 [00:01<00:00, 1694.80it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1688.05it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1647.77it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1629.24it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1679.54it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1640.61it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1639.22it/s]warmup should be done:  72%|  | 2165/3000 [00:01<00:00, 1666.80it/s]warmup should be done:  74%|  | 2211/3000 [00:01<00:00, 1694.11it/s]warmup should be done:  73%|  | 2188/3000 [00:01<00:00, 1687.35it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1647.75it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1629.14it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1683.36it/s]warmup should be done:  71%|   | 2131/3000 [00:01<00:00, 1635.57it/s]warmup should be done:  71%|  | 2144/3000 [00:01<00:00, 1641.22it/s]warmup should be done:  78%|  | 2332/3000 [00:01<00:00, 1665.00it/s]warmup should be done:  77%|  | 2312/3000 [00:01<00:00, 1648.32it/s]warmup should be done:  79%|  | 2358/3000 [00:01<00:00, 1688.46it/s]warmup should be done:  79%|  | 2381/3000 [00:01<00:00, 1691.09it/s]warmup should be done:  76%|  | 2293/3000 [00:01<00:00, 1629.64it/s]warmup should be done:  79%|  | 2362/3000 [00:01<00:00, 1688.75it/s]warmup should be done:  77%|  | 2309/3000 [00:01<00:00, 1643.35it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1637.65it/s]warmup should be done:  83%| | 2499/3000 [00:01<00:00, 1664.31it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1644.46it/s]warmup should be done:  84%| | 2528/3000 [00:01<00:00, 1689.24it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1687.23it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1628.09it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1691.14it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1643.37it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1632.82it/s]warmup should be done:  89%| | 2666/3000 [00:01<00:00, 1662.03it/s]warmup should be done:  90%| | 2698/3000 [00:01<00:00, 1690.83it/s]warmup should be done:  88%| | 2642/3000 [00:01<00:00, 1638.82it/s]warmup should be done:  87%| | 2619/3000 [00:01<00:00, 1627.43it/s]warmup should be done:  91%| | 2720/3000 [00:01<00:00, 1684.41it/s]warmup should be done:  90%| | 2702/3000 [00:01<00:00, 1693.53it/s]warmup should be done:  88%| | 2639/3000 [00:01<00:00, 1642.98it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1633.07it/s]warmup should be done:  94%|| 2833/3000 [00:01<00:00, 1659.64it/s]warmup should be done:  93%|| 2783/3000 [00:01<00:00, 1629.73it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1642.17it/s]warmup should be done:  96%|| 2889/3000 [00:01<00:00, 1680.54it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1694.24it/s]warmup should be done:  96%|| 2868/3000 [00:01<00:00, 1676.18it/s]warmup should be done:  93%|| 2804/3000 [00:01<00:00, 1644.27it/s]warmup should be done:  93%|| 2789/3000 [00:01<00:00, 1637.30it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1659.83it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1631.69it/s]warmup should be done:  99%|| 2974/3000 [00:01<00:00, 1646.08it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1646.42it/s]warmup should be done:  98%|| 2953/3000 [00:01<00:00, 1634.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.07it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.97it/s]2022-12-11 23:05:49.934170: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5dd002cd00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:49.934240: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:49.945904: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5e0802fa10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:49.945965: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:49.953990: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5ec802fb20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:49.954045: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:49.958515: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a4ff92390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:49.958554: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:50.200019: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a5b82f170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:50.200098: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:50.203835: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5e9c02cb70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:50.203891: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:50.298257: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a5b82c010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:50.298334: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:50.299401: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7a5782bd90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:05:50.299460: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:05:52.204701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.212333: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.233762: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.295241: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.476162: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.498371: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.634565: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:52.636634: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:05:55.089355: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.102749: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.140051: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.220874: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.331449: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.407466: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.540776: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:05:55.564859: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:06:32.801][ERROR][RK0][tid #140163866928896]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:06:32.802][ERROR][RK0][tid #140163866928896]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:32.807][ERROR][RK0][tid #140163866928896]: coll ps creation done
[HCTR][23:06:32.807][ERROR][RK0][tid #140163866928896]: replica 4 waits for coll ps creation barrier
[HCTR][23:06:32.885][ERROR][RK0][tid #140163992753920]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:06:32.885][ERROR][RK0][tid #140163992753920]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:32.895][ERROR][RK0][tid #140163992753920]: coll ps creation done
[HCTR][23:06:32.895][ERROR][RK0][tid #140163992753920]: replica 5 waits for coll ps creation barrier
[HCTR][23:06:32.991][ERROR][RK0][tid #140163866928896]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:06:32.991][ERROR][RK0][tid #140163866928896]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:32.995][ERROR][RK0][tid #140163866928896]: coll ps creation done
[HCTR][23:06:32.995][ERROR][RK0][tid #140163866928896]: replica 6 waits for coll ps creation barrier
[HCTR][23:06:33.096][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:06:33.096][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:33.104][ERROR][RK0][main]: coll ps creation done
[HCTR][23:06:33.104][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][23:06:33.132][ERROR][RK0][tid #140163858536192]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:06:33.132][ERROR][RK0][tid #140163858536192]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:33.142][ERROR][RK0][tid #140163858536192]: coll ps creation done
[HCTR][23:06:33.142][ERROR][RK0][tid #140163858536192]: replica 2 waits for coll ps creation barrier
[HCTR][23:06:33.147][ERROR][RK0][tid #140163858536192]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:06:33.147][ERROR][RK0][tid #140163858536192]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:33.148][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:06:33.148][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:33.152][ERROR][RK0][main]: coll ps creation done
[HCTR][23:06:33.152][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:06:33.155][ERROR][RK0][tid #140163858536192]: coll ps creation done
[HCTR][23:06:33.155][ERROR][RK0][tid #140163858536192]: replica 3 waits for coll ps creation barrier
[HCTR][23:06:33.189][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:06:33.189][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:06:33.194][ERROR][RK0][main]: coll ps creation done
[HCTR][23:06:33.194][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][23:06:33.194][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][23:06:34.125][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][23:06:34.163][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][tid #140163992753920]: replica 5 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][tid #140163858536192]: replica 2 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][tid #140163866928896]: replica 4 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][tid #140163866928896]: replica 6 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][tid #140163858536192]: replica 3 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][23:06:34.163][ERROR][RK0][main]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][main]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][tid #140163992753920]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][tid #140163858536192]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][tid #140163866928896]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][tid #140163866928896]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][tid #140163858536192]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][tid #140163858536192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][tid #140163858536192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][main]: Calling build_v2
[HCTR][23:06:34.163][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][tid #140163992753920]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][tid #140163866928896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][tid #140163866928896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:06:34.163][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 23:06:342022-12-11 23:06:34.2022-12-11 23:06:34.1637122022-12-11 23:06:342022-12-11 23:06:34[.2022-12-11 23:06:34163717: ..2022-12-11 23:06:34163720.: E1637121637352022-12-11 23:06:34.: 163738E : : .E163714:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccEE163747 : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE :136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] :: 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] using concurrent impl MPSPhase136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] :136using concurrent impl MPSPhase
] ] :using concurrent impl MPSPhase136] 
using concurrent impl MPSPhaseusing concurrent impl MPSPhase136
] using concurrent impl MPSPhase

] using concurrent impl MPSPhase
using concurrent impl MPSPhase

[2022-12-11 23:06:34.167813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:06:34.167852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-11 23:06:34.167908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-11 23:06:34.167948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-11 23:06:34[.2022-12-11 23:06:34167967.: 167977E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 214v100x8, slow pcie] 
[cpu time is 97.05882022-12-11 23:06:34
[.2022-12-11 23:06:34168011.: 168024E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 196v100x8, slow pcie] [
assigning 8 to cpu2022-12-11 23:06:34
.[1680572022-12-11 23:06:34: .E168077 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] :2022-12-11 23:06:34v100x8, slow pcie196.
] 168099assigning 8 to cpu: 
[E[2022-12-11 23:06:34 2022-12-11 23:06:34./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.168129:168131[: 178: [2022-12-11 23:06:34E] E2022-12-11 23:06:34. v100x8, slow pcie .168145/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc168156: ::: E[196212E 2022-12-11 23:06:34] ]  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 8 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:06:34:168195

:.178: 212168205] [E] [: v100x8, slow pcie[2022-12-11 23:06:34 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 23:06:34E
2022-12-11 23:06:34./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
. .[168249:168262/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc168272[2022-12-11 23:06:34: 196: :: 2022-12-11 23:06:34.E] EE178.168308 assigning 8 to cpu  ] 168324: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie: E:::
E 178213212 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[v100x8, slow pcieremote time is 8.684212022-12-11 23:06:34build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:1962022-12-11 23:06:34

.
213] .168453[[] assigning 8 to cpu168466: [2022-12-11 23:06:342022-12-11 23:06:34remote time is 8.68421
: E2022-12-11 23:06:34..
E .[168514168516 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1685272022-12-11 23:06:34: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: .EE[:196E168580  2022-12-11 23:06:34212]  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE::168609build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
: 196214: 
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] E] :[assigning 8 to cpucpu time is 97.0588 remote time is 8.684212142022-12-11 23:06:34

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] .:[cpu time is 97.05882022-12-11 23:06:34168718212[
.: ] 2022-12-11 23:06:34168750Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[168763:  
: 2022-12-11 23:06:34E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE. [: 168792/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:06:34:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .212] :E168828] remote time is 8.68421214 : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
cpu time is 97.0588[: 
2022-12-11 23:06:34212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[] :1689202022-12-11 23:06:34build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213: .
] E168941remote time is 8.68421 : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[:[ 2022-12-11 23:06:342142022-12-11 23:06:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] .:169010cpu time is 97.0588169019213: 
: ] EEremote time is 8.68421  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::213[214] 2022-12-11 23:06:34] remote time is 8.68421.cpu time is 97.0588
169125
: [E2022-12-11 23:06:34 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc169178:: 214E]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-11 23:07:53.817139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:07:53.857587: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:07:53.857684: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:07:53.858736: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 23:07:53.939520: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 23:07:54.330442: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 23:07:54.330537: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 23:08:01. 16516: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-11 23:08:01. 16638: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 23:08:02.770743: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 23:08:02.770830: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-11 23:08:02.773619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 23:08:02.773674: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-11 23:08:03.598912: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 23:08:03.627524: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 23:08:03.628984: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 23:08:03.649116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 23:08:04.257105: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 23:08:04.259414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 23:08:04.262410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 23:08:04.265319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 23:08:04.268206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 23:08:04.271098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 23:08:04.274019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 23:08:04.276942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 23:08:04.280307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 23:08:20.839385: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 23:08:20.847277: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 23:08:20.850586: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 23:08:20.898218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:08:20.898316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:08:20.898356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:08:20.898385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:08:20.898880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:08:20.898930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.899855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.900541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.913757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 23:08:20.913833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 23:08:20.913867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 23:08:20.913941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 23:08:20.914249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:08:20.914301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.914334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-11 23:08:207 solved.
914376: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:08:20:.1815914399] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:08:20.914443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.914797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:08:20.914841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 23:08:202022-12-11 23:08:20.[.9165052022-12-11 23:08:20916514: .: E916528E [:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 23:08:20E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:. :202916577/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980] [: :] 1 solved2022-12-11 23:08:20E202eager alloc mem 381.47 MB
. ] 
916630/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[5 solved: :2022-12-11 23:08:20
E1980. ] [916687/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB2022-12-11 23:08:20: :
.E1980916726 ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MBE:
 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 1 initing device 1205
] worker 0 thread 5 initing device 5
[2022-12-11 23:08:20.917199[: 2022-12-11 23:08:20E. 917206/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:08:20.917284: [E2022-12-11 23:08:20 .917292: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.918614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:08:20.[9186692022-12-11 23:08:20: .E918662 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 3 initing device 3202
] 2 solved
[2022-12-11 23:08:20.918759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 23:08:20.919108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-11 23:08:202022-12-11 23:08:20..919163919164: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151980] ] Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB

[2022-12-11 23:08:20.919275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.919601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.920771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.920823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.921463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.921526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.923726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.923800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.925288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.925393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.926005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.926514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:08:20.978636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 23:08:20.983980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:08:20.984098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:08:20.984920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:20.985535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:20.986536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:20.986585: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.41 MB
[2022-12-11 23:08:20.989403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:20.990157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:20.990201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:08:21.   466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [[[eager alloc mem 1023.00 Bytes2022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:21
...   524   524   517: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes


[2022-12-11 23:08:21.   705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 23:08:21.   860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 23:08:21.  4545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 23:08:21.  8777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:08:21.  8864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:08:21.  8964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:08:21.  9053: [E2022-12-11 23:08:21 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  9047:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1023
[2022-12-11 23:08:21[.2022-12-11 23:08:21  9129.:   9148E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1023] 
eager release cuda mem 400000000
[2022-12-11 23:08:21.  9230[: 2022-12-11 23:08:21E.   9225/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1023
[2022-12-11 23:08:21.[  93272022-12-11 23:08:21: .E  9321 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1023
[2022-12-11 23:08:21.  9452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[2022-12-11 23:08:212022-12-11 23:08:21..  9732  9741: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 1023eager alloc mem 11.83 MB

[2022-12-11 23:08:21.  9831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:08:21. 10652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:21. 11829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:21. 12337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:21. 12853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:21. 13368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:21. 14013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:08:21. 14481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21. 14910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21. 15267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21. 15386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.[ 154542022-12-11 23:08:21: .E 15455 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-11 23:08:21:eager release cuda mem 625663.1980
 15499] : eager alloc mem 611.00 KBE
[ [2022-12-11 23:08:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:08:21.:. 155521980 15565: ] : Eeager alloc mem 611.00 KBW 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::198043] ] eager alloc mem 611.00 KBWORKER[0] alloc host memory 11.44 MB

[2022-12-11 23:08:21. 15886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21. 15933: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:08:21. 16234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21. 16289: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.42 MB
[2022-12-11 23:08:21. 16357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21. 16401: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:08:21. 16506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:08:212022-12-11 23:08:21..[ 16545 165512022-12-11 23:08:21: : .EW 16564  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccE:: 63843/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] :eager release cuda mem 625663WORKER[0] alloc host memory 11.41 MB638

] eager release cuda mem 625663
[2022-12-11 23:08:21. 16701: [W2022-12-11 23:08:21 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc 16710:: 43W]  WORKER[0] alloc host memory 11.43 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 11.41 MB
[2022-12-11 23:08:21. 24324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:21. 24664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:21. 24746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:21. 24941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:21. 24983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:08:21. 25268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:21. 25310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:08:21.[ 253342022-12-11 23:08:21: .E 25360 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 25.25 KB638
] eager release cuda mem 25855
[2022-12-11 23:08:21. 25451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:08:21. 25462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:21. 25827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:21. 25992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:21. 26036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:08:21. 26067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:21. 26109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:08:21. 26372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:08:21. 26443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:21. 26488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:08:21. 26985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:08:21. 27029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[[[[2022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:21........474936474936474936474937474936474936474937474937: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 5 init p2p of link 6Device 2 init p2p of link 1Device 0 init p2p of link 3Device 6 init p2p of link 0Device 3 init p2p of link 2Device 1 init p2p of link 7Device 4 init p2p of link 5







[[[2022-12-11 23:08:212022-12-11 23:08:212022-12-11 23:08:21...475489[475492475492: [2022-12-11 23:08:21: : [E2022-12-11 23:08:21.[EE2022-12-11 23:08:21 .[4755082022-12-11 23:08:21  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu4755132022-12-11 23:08:21: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu475521:: .E475533::: 1980E475545 : 19801980E]  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] ]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: eager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :1980] :eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-11 23:08:21.476522: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 23:08:21:2022-12-11 23:08:21.638.476536] 476539: eager release cuda mem 625663: E
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-11 23:08:212022-12-11 23:08:21..476595476598: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::[6386382022-12-11 23:08:21] [] .eager release cuda mem 6256632022-12-11 23:08:21eager release cuda mem 625663476631
[.
: 2022-12-11 23:08:21476638E.:  476660E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663:] 
638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 23:08:21.490368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:08:21.490432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 23:08:21.490512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.490578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.490787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:08:21.490939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.491041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:08:21.491125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:08:21.491213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.491304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 23:08:21] .eager alloc mem 611.00 KB491320
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.491397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.491412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:08:21.491471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 23:08:21.491566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.491623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.491660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:08:21.491736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.491864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.492016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.492121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.492365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.492411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.492697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.504066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:08:21.504187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.504859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 23:08:21[.2022-12-11 23:08:21504977.: 504979E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 23:08:21.505050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:08:21.505165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.505262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:08:21.505389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.505469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 23:08:21.505589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:08:212022-12-11 23:08:21..505759505761: : EE[  2022-12-11 23:08:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.::50580819261926: ] ] EDevice 4 init p2p of link 2Device 0 init p2p of link 1 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 23:08:212022-12-11 23:08:21[..2022-12-11 23:08:21505963505963.: : 505974EE:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6381980:] ] 1980eager release cuda mem 625663eager alloc mem 611.00 KB] 

eager alloc mem 611.00 KB
[2022-12-11 23:08:21.506172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.506222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:08:21.506382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 23:08:21
.506398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.506859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:08:21eager release cuda mem 625663.
506879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.507252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.520726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:08:21.520846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:08:212022-12-11 23:08:21..521645521637: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381926] ] eager release cuda mem 625663Device 5 init p2p of link 3

[2022-12-11 23:08:21.521711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:08:21.521780: [E2022-12-11 23:08:21 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu521782:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 2 init p2p of link 4[
2022-12-11 23:08:21.521831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.521918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.522000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:08:21.522118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.522409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:08:21.522528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.522583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.522632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.522703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.522923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.523186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[[2022-12-11 23:08:212022-12-11 23:08:21..523322523328: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 23:08:21.523582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 23:08:21.523727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:08:21.524188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.524542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:08:21.535339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.536788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.537620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.537839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.538359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2992529 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8987420 / 100000000 nodes ( 8.99 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.623525 secs 
[2022-12-11 23:08:21.538684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.538783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.538846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2997890 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8982059 / 100000000 nodes ( 8.98 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.621569 secs 
[2022-12-11 23:08:21.538936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2997595 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8982354 / 100000000 nodes ( 8.98 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.619777 secs 
[2022-12-11 23:08:21.539027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2999720 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8980229 / 100000000 nodes ( 8.98 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.44 GB | 0.619761 secs 
[2022-12-11 23:08:21[.2022-12-11 23:08:21539419.: 539431E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1955eager release cuda mem 12399996] 
Asymm Coll cache (policy: coll_cache_asymm_link) | local 2998955 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8980994 / 100000000 nodes ( 8.98 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.625142 secs 
[2022-12-11 23:08:21.539534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2990309 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8989640 / 100000000 nodes ( 8.99 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.622249 secs 
[2022-12-11 23:08:21.539681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:08:21.540706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2992325 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8987624 / 100000000 nodes ( 8.99 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.641791 secs 
[2022-12-11 23:08:21.542692: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.07 GB
[2022-12-11 23:08:21.544198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2990575 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8989374 / 100000000 nodes ( 8.99 %) | cpu 88020051 / 100000000 nodes ( 88.02 %) | 1.43 GB | 0.629767 secs 
[2022-12-11 23:08:22.913447: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.33 GB
[2022-12-11 23:08:22.913586: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.33 GB
[2022-12-11 23:08:22.913820: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.33 GB
[2022-12-11 23:08:24.303030: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.59 GB
[2022-12-11 23:08:24.303181: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.59 GB
[2022-12-11 23:08:24.303612: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.59 GB
[2022-12-11 23:08:25.618014: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.81 GB
[2022-12-11 23:08:25.618850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.81 GB
[2022-12-11 23:08:25.620491: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.81 GB
[2022-12-11 23:08:27. 40088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.02 GB
[2022-12-11 23:08:27. 40230: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.02 GB
[2022-12-11 23:08:27. 40832: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 9.02 GB
[2022-12-11 23:08:27. 41015: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 9.02 GB
[2022-12-11 23:08:27. 41296: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.02 GB
[2022-12-11 23:08:28.665759: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.22 GB
[2022-12-11 23:08:28.666769: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.22 GB
[HCTR][23:08:29.216][ERROR][RK0][tid #140163858536192]: replica 2 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][tid #140163866928896]: replica 6 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][tid #140163992753920]: replica 5 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][tid #140163866928896]: replica 4 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][tid #140163858536192]: replica 3 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:08:29.216][ERROR][RK0][tid #140163866928896]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163858536192]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163866928896]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163858536192]: init per replica done
[HCTR][23:08:29.216][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163992753920]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][main]: init per replica done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163858536192]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163866928896]: init per replica done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163866928896]: init per replica done
[HCTR][23:08:29.216][ERROR][RK0][main]: init per replica done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163992753920]: init per replica done
[HCTR][23:08:29.216][ERROR][RK0][tid #140163858536192]: init per replica done
[HCTR][23:08:29.219][ERROR][RK0][main]: init per replica done
[HCTR][23:08:29.222][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7c42f20000
[HCTR][23:08:29.222][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7c43400000
[HCTR][23:08:29.222][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7c43a40000
[HCTR][23:08:29.222][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7c43d60000
[HCTR][23:08:29.222][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f7c42f20000
[HCTR][23:08:29.222][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f7c43400000
[HCTR][23:08:29.222][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f7c43a40000
[HCTR][23:08:29.223][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f7c43d60000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163992753920]: 5 allocated 3276800 at 0x7f7c40f20000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163992753920]: 5 allocated 6553600 at 0x7f7c41400000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163992753920]: 5 allocated 3276800 at 0x7f7c41a40000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163992753920]: 5 allocated 6553600 at 0x7f7c41d60000
[HCTR][23:08:29.223][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7c40f20000
[HCTR][23:08:29.223][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7c41400000
[HCTR][23:08:29.223][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7c41a40000
[HCTR][23:08:29.223][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7c41d60000
[HCTR][23:08:29.223][ERROR][RK0][tid #140164462515968]: 1 allocated 3276800 at 0x7f7c3ef20000
[HCTR][23:08:29.223][ERROR][RK0][tid #140164462515968]: 1 allocated 6553600 at 0x7f7c3f400000
[HCTR][23:08:29.223][ERROR][RK0][tid #140164462515968]: 1 allocated 3276800 at 0x7f7c3fa40000
[HCTR][23:08:29.223][ERROR][RK0][tid #140164462515968]: 1 allocated 6553600 at 0x7f7c3fd60000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163858536192]: 2 allocated 3276800 at 0x7f7c42f20000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163858536192]: 2 allocated 6553600 at 0x7f7c43400000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163858536192]: 2 allocated 3276800 at 0x7f7c43a40000
[HCTR][23:08:29.223][ERROR][RK0][tid #140163858536192]: 2 allocated 6553600 at 0x7f7c43d60000
[HCTR][23:08:29.224][ERROR][RK0][tid #140163866928896]: 4 allocated 3276800 at 0x7f7c42f20000
[HCTR][23:08:29.224][ERROR][RK0][tid #140163866928896]: 4 allocated 6553600 at 0x7f7c43400000
[HCTR][23:08:29.224][ERROR][RK0][tid #140163866928896]: 4 allocated 3276800 at 0x7f7c43a40000
[HCTR][23:08:29.224][ERROR][RK0][tid #140163866928896]: 4 allocated 6553600 at 0x7f7c43d60000
[HCTR][23:08:29.225][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f7c45920000
[HCTR][23:08:29.225][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f7c45e00000
[HCTR][23:08:29.225][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f7c46b0e800
[HCTR][23:08:29.225][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f7c46e2e800








