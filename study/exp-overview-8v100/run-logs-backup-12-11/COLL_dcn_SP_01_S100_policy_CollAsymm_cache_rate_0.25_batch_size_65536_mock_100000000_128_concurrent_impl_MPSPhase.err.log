2022-12-12 07:15:11.721934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.730361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.736718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.741073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.754275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.760456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.766751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.779399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.831414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.835117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.836171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.837445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.839237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.840061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.840401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.841847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.841882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.843366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.843431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.844988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.845285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.846742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.846979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.848633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.849115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.850281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.851023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.851860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.852739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.853395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.854796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.855912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.857744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.858928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.859927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.860877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.861945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.863038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.864076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.865099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.870372: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:11.872862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.874504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.875908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.876072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.877247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.877650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.878765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.879158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.880809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.880906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.881144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.883788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.883843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.884221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.885259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.886806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.887082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.887721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.888961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.889008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.891058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.892480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.892566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.894329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.895718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.895807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.896627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.898858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.898901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.898932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.899979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.900552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.902082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.902158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.902198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.903267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.904061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.905131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.905401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.905443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.906693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.907524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.908101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.908780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.908819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.910070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.910933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.911140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.912200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.913272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.913874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.913995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.915830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.915997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.916345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.925015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.925060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.926553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.927932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.928495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.929109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.935078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.941836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.957194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.958799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.964546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.966726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.966775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.966862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.966902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.967338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.970543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.970586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.970683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.970722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.971082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.972147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.974136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.974854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.974898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.975047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.975088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.976619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.978251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.979965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.980623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.981185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.981290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.981328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.981865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.983015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.985266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.985786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.986111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.986169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.986260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.986662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.987950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.990792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.991370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.991658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.991808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.991940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.993095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.993492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.995120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.995995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.996270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.996502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.996609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.998230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:11.998371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.000319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.000901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.001237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.001290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.001526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.002900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.002943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.004862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.005391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.005693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.005789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.006014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.007559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.007658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.009946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.010443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.011279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.011417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.011662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.013964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.014112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.015804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.016400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.016761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.016949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.017449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.018674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.018844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.020796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.021837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.022196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.022280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.022855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.024073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.024136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.026185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.026810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.027552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.027584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.028213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.029468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.029470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.031368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.031835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.032149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.032249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.032777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.034023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.035522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.036047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.036323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.036388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.036913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.038251: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.038297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.039580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.041772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.041978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.042078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.042448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.043547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.044765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.046558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.046698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.046771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.046976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.047280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.048630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.049044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.050576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.050698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.050839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.051344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.052891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.053314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.053543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.055461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.055566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.055728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.055939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.057730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.058273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.058516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.060425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.060608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.060648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.060773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.063448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.064618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.066697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.066847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.066896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.066968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.068713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.069371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.071531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.071641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.071675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.072302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.073174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.074059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.075861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.075991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.076068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.076745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.077968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.078608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.081929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.082260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.084353: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.084550: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.084644: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.084871: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.085628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.085751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.091618: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.091740: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:15:12.095605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.095717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.095764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.095803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.102234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.102412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.115355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.115526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.115554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.115620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.115665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.115687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.120354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.120493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.120523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.120565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.120613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:12.120910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.294257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.295762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.297002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.298029: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.298089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:15:13.317095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.318168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.319180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.320626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.321637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.322886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:15:13.366748: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.366937: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.427747: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:15:13.549182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.550278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.551304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.552228: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.552286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:15:13.570003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.572824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.573727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.574813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.575728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.576558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:15:13.652234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.653064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.653248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.653582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.654718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.654997: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:15:13.656669: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.656767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:15:13.672459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.673066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.673268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.673268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.674129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.674284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.675274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.675567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.675597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.676656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.676854: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.676916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:15:13.677750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.678072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.678109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.681237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.682222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.682813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.682995: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.683049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:15:13.683199: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.683249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:15:13.684384: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.684431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:15:13.685357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.685454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.686713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.686836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.688274: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:15:13.688333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:15:13.688343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:15:13.695206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.695802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.696317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.696885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.697402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.697865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:15:13.701097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.701729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.701739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.702914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.702984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.703219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.704314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.704386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.704572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.705756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.705859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.705966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.705997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.707469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:15:13.707758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.707978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.708030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.709108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:15:13.709506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.709565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.710744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:15:13.710823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.711374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:15:13.711846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:15:13.742336: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.742561: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.744552: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 07:15:13.752495: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.752673: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.754328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.754435: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:15:13.754457: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.756271: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:15:13.756846: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.757017: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.757836: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.757973: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.758039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:15:13.759873: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 07:15:13.779987: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.780172: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:15:13.781048: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][07:15:15.017][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.017][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.032][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.050][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.050][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.050][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.050][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:15:15.055][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 95it [00:01, 80.54it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 99it [00:01, 84.02it/s]warmup run: 99it [00:01, 83.35it/s]warmup run: 191it [00:01, 175.75it/s]warmup run: 197it [00:01, 181.14it/s]warmup run: 97it [00:01, 81.16it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 199it [00:01, 181.88it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 286it [00:01, 279.30it/s]warmup run: 292it [00:01, 284.15it/s]warmup run: 193it [00:01, 175.08it/s]warmup run: 99it [00:01, 85.33it/s]warmup run: 99it [00:01, 85.64it/s]warmup run: 297it [00:01, 288.07it/s]warmup run: 97it [00:01, 83.19it/s]warmup run: 98it [00:01, 85.40it/s]warmup run: 384it [00:01, 391.75it/s]warmup run: 389it [00:01, 393.72it/s]warmup run: 195it [00:01, 181.45it/s]warmup run: 290it [00:01, 280.11it/s]warmup run: 188it [00:01, 174.32it/s]warmup run: 395it [00:01, 398.51it/s]warmup run: 190it [00:01, 175.80it/s]warmup run: 198it [00:01, 186.73it/s]warmup run: 482it [00:02, 501.18it/s]warmup run: 288it [00:01, 282.94it/s]warmup run: 485it [00:02, 498.42it/s]warmup run: 382it [00:01, 381.34it/s]warmup run: 277it [00:01, 271.40it/s]warmup run: 496it [00:02, 512.03it/s]warmup run: 285it [00:01, 280.06it/s]warmup run: 296it [00:01, 295.00it/s]warmup run: 580it [00:02, 601.79it/s]warmup run: 581it [00:02, 595.36it/s]warmup run: 381it [00:01, 387.47it/s]warmup run: 375it [00:01, 386.36it/s]warmup run: 481it [00:02, 493.11it/s]warmup run: 599it [00:02, 619.63it/s]warmup run: 380it [00:01, 388.05it/s]warmup run: 392it [00:01, 403.19it/s]warmup run: 678it [00:02, 687.91it/s]warmup run: 679it [00:02, 683.47it/s]warmup run: 474it [00:02, 499.80it/s]warmup run: 475it [00:02, 490.37it/s]warmup run: 583it [00:02, 602.02it/s]warmup run: 702it [00:02, 712.84it/s]warmup run: 477it [00:02, 496.34it/s]warmup run: 488it [00:01, 508.18it/s]warmup run: 776it [00:02, 759.96it/s]warmup run: 776it [00:02, 753.31it/s]warmup run: 569it [00:02, 585.22it/s]warmup run: 576it [00:02, 609.17it/s]warmup run: 685it [00:02, 696.52it/s]warmup run: 801it [00:02, 780.15it/s]warmup run: 575it [00:02, 597.69it/s]warmup run: 588it [00:02, 611.56it/s]warmup run: 875it [00:02, 818.03it/s]warmup run: 874it [00:02, 810.84it/s]warmup run: 663it [00:02, 667.27it/s]warmup run: 679it [00:02, 705.70it/s]warmup run: 786it [00:02, 773.75it/s]warmup run: 900it [00:02, 827.62it/s]warmup run: 673it [00:02, 685.80it/s]warmup run: 690it [00:02, 706.28it/s]warmup run: 973it [00:02, 861.03it/s]warmup run: 971it [00:02, 853.33it/s]warmup run: 758it [00:02, 735.98it/s]warmup run: 887it [00:02, 834.45it/s]warmup run: 782it [00:02, 784.61it/s]warmup run: 770it [00:02, 755.79it/s]warmup run: 998it [00:02, 861.40it/s]warmup run: 791it [00:02, 781.62it/s]warmup run: 1072it [00:02, 896.32it/s]warmup run: 1069it [00:02, 888.46it/s]warmup run: 852it [00:02, 789.26it/s]warmup run: 884it [00:02, 845.41it/s]warmup run: 989it [00:02, 882.55it/s]warmup run: 867it [00:02, 811.58it/s]warmup run: 1095it [00:02, 888.82it/s]warmup run: 890it [00:02, 834.94it/s]warmup run: 1172it [00:02, 923.97it/s]warmup run: 1166it [00:02, 911.05it/s]warmup run: 946it [00:02, 828.61it/s]warmup run: 987it [00:02, 893.67it/s]warmup run: 1091it [00:02, 919.02it/s]warmup run: 965it [00:02, 855.84it/s]warmup run: 989it [00:02, 877.03it/s]warmup run: 1192it [00:02, 908.43it/s]warmup run: 1270it [00:02, 938.23it/s]warmup run: 1263it [00:02, 926.24it/s]warmup run: 1040it [00:02, 858.84it/s]warmup run: 1091it [00:02, 933.05it/s]warmup run: 1193it [00:02, 945.37it/s]warmup run: 1062it [00:02, 886.71it/s]warmup run: 1088it [00:02, 906.46it/s]warmup run: 1289it [00:02, 920.03it/s]warmup run: 1370it [00:02, 954.14it/s]warmup run: 1360it [00:02, 938.42it/s]warmup run: 1135it [00:02, 882.92it/s]warmup run: 1294it [00:02, 963.29it/s]warmup run: 1193it [00:02, 953.91it/s]warmup run: 1188it [00:02, 932.35it/s]warmup run: 1159it [00:02, 903.25it/s]warmup run: 1385it [00:02, 929.37it/s]warmup run: 1469it [00:03, 963.81it/s]warmup run: 1457it [00:03, 943.94it/s]warmup run: 1229it [00:02, 899.19it/s]warmup run: 1396it [00:02, 979.39it/s]warmup run: 1294it [00:02, 969.87it/s]warmup run: 1289it [00:02, 953.45it/s]warmup run: 1255it [00:02, 913.89it/s]warmup run: 1482it [00:03, 939.86it/s]warmup run: 1568it [00:03, 968.54it/s]warmup run: 1323it [00:02, 909.79it/s]warmup run: 1555it [00:03, 951.90it/s]warmup run: 1497it [00:03, 986.73it/s]warmup run: 1396it [00:02, 982.56it/s]warmup run: 1390it [00:02, 967.91it/s]warmup run: 1578it [00:03, 945.11it/s]warmup run: 1351it [00:02, 911.21it/s]warmup run: 1668it [00:03, 977.25it/s]warmup run: 1418it [00:03, 920.20it/s]warmup run: 1653it [00:03, 958.27it/s]warmup run: 1598it [00:03, 992.20it/s]warmup run: 1499it [00:03, 995.66it/s]warmup run: 1490it [00:02, 977.33it/s]warmup run: 1683it [00:03, 975.00it/s]warmup run: 1445it [00:03, 908.69it/s]warmup run: 1770it [00:03, 989.24it/s]warmup run: 1752it [00:03, 967.65it/s]warmup run: 1513it [00:03, 927.60it/s]warmup run: 1700it [00:03, 998.57it/s]warmup run: 1602it [00:03, 1004.45it/s]warmup run: 1590it [00:03, 983.99it/s]warmup run: 1789it [00:03, 997.63it/s]warmup run: 1538it [00:03, 907.13it/s]warmup run: 1871it [00:03, 995.11it/s]warmup run: 1851it [00:03, 973.53it/s]warmup run: 1610it [00:03, 937.70it/s]warmup run: 1801it [00:03, 1001.54it/s]warmup run: 1704it [00:03, 1005.64it/s]warmup run: 1690it [00:03, 988.53it/s]warmup run: 1893it [00:03, 1008.63it/s]warmup run: 1630it [00:03, 910.31it/s]warmup run: 1973it [00:03, 1001.21it/s]warmup run: 1950it [00:03, 977.28it/s]warmup run: 1708it [00:03, 947.71it/s]warmup run: 1903it [00:03, 1004.27it/s]warmup run: 1808it [00:03, 1013.57it/s]warmup run: 1791it [00:03, 994.22it/s]warmup run: 1998it [00:03, 1018.25it/s]warmup run: 1722it [00:03, 911.23it/s]warmup run: 2088it [00:03, 1043.27it/s]warmup run: 2058it [00:03, 1006.72it/s]warmup run: 1806it [00:03, 957.12it/s]warmup run: 2004it [00:03, 1005.31it/s]warmup run: 1912it [00:03, 1018.91it/s]warmup run: 1892it [00:03, 998.48it/s]warmup run: 2120it [00:03, 1078.18it/s]warmup run: 1814it [00:03, 913.72it/s]warmup run: 2208it [00:03, 1089.30it/s]warmup run: 2176it [00:03, 1056.81it/s]warmup run: 1904it [00:03, 962.95it/s]warmup run: 2124it [00:03, 1061.43it/s]warmup run: 2018it [00:03, 1029.17it/s]warmup run: 1995it [00:03, 1005.67it/s]warmup run: 2244it [00:03, 1124.13it/s]warmup run: 1907it [00:03, 916.69it/s]warmup run: 2328it [00:03, 1121.79it/s]warmup run: 2295it [00:03, 1094.36it/s]warmup run: 2003it [00:03, 969.30it/s]warmup run: 2244it [00:03, 1101.11it/s]warmup run: 2141it [00:03, 1087.10it/s]warmup run: 2113it [00:03, 1057.19it/s]warmup run: 2368it [00:03, 1155.96it/s]warmup run: 2000it [00:03, 919.58it/s]warmup run: 2448it [00:03, 1144.77it/s]warmup run: 2414it [00:03, 1120.76it/s]warmup run: 2122it [00:03, 1033.07it/s]warmup run: 2364it [00:03, 1128.41it/s]warmup run: 2264it [00:03, 1128.80it/s]warmup run: 2233it [00:03, 1098.96it/s]warmup run: 2492it [00:03, 1178.23it/s]warmup run: 2112it [00:03, 978.46it/s]warmup run: 2569it [00:04, 1162.12it/s]warmup run: 2532it [00:04, 1138.28it/s]warmup run: 2241it [00:03, 1077.88it/s]warmup run: 2484it [00:03, 1148.41it/s]warmup run: 2387it [00:03, 1157.89it/s]warmup run: 2353it [00:03, 1128.46it/s]warmup run: 2616it [00:04, 1194.07it/s]warmup run: 2225it [00:03, 1022.87it/s]warmup run: 2690it [00:04, 1174.58it/s]warmup run: 2650it [00:04, 1150.70it/s]warmup run: 2360it [00:03, 1109.78it/s]warmup run: 2604it [00:04, 1162.19it/s]warmup run: 2510it [00:03, 1178.23it/s]warmup run: 2473it [00:03, 1148.44it/s]warmup run: 2740it [00:04, 1205.41it/s]warmup run: 2338it [00:03, 1054.32it/s]warmup run: 2810it [00:04, 1179.71it/s]warmup run: 2767it [00:04, 1154.76it/s]warmup run: 2479it [00:04, 1131.59it/s]warmup run: 2724it [00:04, 1171.88it/s]warmup run: 2630it [00:04, 1182.76it/s]warmup run: 2592it [00:04, 1160.14it/s]warmup run: 2862it [00:04, 1209.01it/s]warmup run: 2451it [00:04, 1075.36it/s]warmup run: 2931it [00:04, 1186.01it/s]warmup run: 2887it [00:04, 1167.70it/s]warmup run: 2598it [00:04, 1146.42it/s]warmup run: 2843it [00:04, 1174.98it/s]warmup run: 2753it [00:04, 1192.55it/s]warmup run: 3000it [00:04, 679.71it/s] warmup run: 2712it [00:04, 1170.03it/s]warmup run: 2985it [00:04, 1213.41it/s]warmup run: 2564it [00:04, 1089.78it/s]warmup run: 3000it [00:04, 683.02it/s] warmup run: 3000it [00:04, 674.18it/s] warmup run: 2715it [00:04, 1152.79it/s]warmup run: 2961it [00:04, 1174.28it/s]warmup run: 2875it [00:04, 1198.15it/s]warmup run: 2831it [00:04, 1173.25it/s]warmup run: 3000it [00:04, 679.45it/s] warmup run: 2676it [00:04, 1097.82it/s]warmup run: 2832it [00:04, 1156.27it/s]warmup run: 2995it [00:04, 1185.29it/s]warmup run: 3000it [00:04, 692.13it/s] warmup run: 2951it [00:04, 1180.60it/s]warmup run: 2788it [00:04, 1102.44it/s]warmup run: 3000it [00:04, 689.92it/s] warmup run: 2951it [00:04, 1165.33it/s]warmup run: 2900it [00:04, 1107.22it/s]warmup run: 3000it [00:04, 670.79it/s] warmup run: 3000it [00:04, 660.84it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1627.75it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.16it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1589.71it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.02it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1672.91it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.05it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1610.33it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.64it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1678.14it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.91it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.53it/s]warmup should be done:  11%|█         | 318/3000 [00:00<00:01, 1585.41it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1621.36it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.81it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1617.30it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1585.21it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1674.77it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1679.49it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1620.38it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1629.46it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1615.02it/s]warmup should be done:  16%|█▌        | 477/3000 [00:00<00:01, 1577.81it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1635.31it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1610.43it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1674.75it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1679.61it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1633.52it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1625.33it/s]warmup should be done:  21%|██▏       | 638/3000 [00:00<00:01, 1590.42it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1637.39it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1597.72it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1607.26it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1677.15it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1640.81it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1597.34it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1627.95it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1669.29it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1640.92it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1598.10it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1616.64it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1676.28it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1627.52it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1644.25it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1645.83it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1660.62it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1612.22it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1622.71it/s]warmup should be done:  32%|███▏      | 959/3000 [00:00<00:01, 1570.64it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1644.52it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1673.27it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1625.35it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1640.70it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1620.88it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1656.35it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1626.85it/s]warmup should be done:  37%|███▋      | 1118/3000 [00:00<00:01, 1574.60it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1673.70it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1625.62it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1646.74it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1646.76it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1620.15it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1659.98it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1636.57it/s]warmup should be done:  43%|████▎     | 1279/3000 [00:00<00:01, 1584.74it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1647.55it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1625.38it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1671.42it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1650.39it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1661.52it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1620.25it/s]warmup should be done:  48%|████▊     | 1440/3000 [00:00<00:00, 1592.00it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1644.71it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1625.37it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1648.35it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1670.83it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1645.41it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1663.46it/s]warmup should be done:  54%|█████▍    | 1626/3000 [00:01<00:00, 1620.11it/s]warmup should be done:  53%|█████▎    | 1602/3000 [00:01<00:00, 1597.87it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1646.38it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1625.80it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1648.16it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1664.81it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1665.26it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1642.46it/s]warmup should be done:  60%|█████▉    | 1789/3000 [00:01<00:00, 1620.39it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1646.96it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1593.86it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1648.40it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1664.74it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1658.73it/s]warmup should be done:  65%|██████▌   | 1952/3000 [00:01<00:00, 1620.06it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1646.53it/s]warmup should be done:  64%|██████▍   | 1922/3000 [00:01<00:00, 1576.09it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1602.44it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1647.46it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1623.10it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1664.10it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1620.51it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1654.42it/s]warmup should be done:  71%|███████▏  | 2139/3000 [00:01<00:00, 1646.18it/s]warmup should be done:  69%|██████▉   | 2080/3000 [00:01<00:00, 1576.04it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1587.84it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1647.26it/s]warmup should be done:  76%|███████▌  | 2283/3000 [00:01<00:00, 1622.77it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1661.86it/s]warmup should be done:  76%|███████▌  | 2278/3000 [00:01<00:00, 1619.96it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1645.27it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1648.59it/s]warmup should be done:  75%|███████▍  | 2238/3000 [00:01<00:00, 1570.02it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1596.43it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1643.54it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1620.66it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1617.33it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1653.60it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1651.14it/s]warmup should be done:  82%|████████▏ | 2469/3000 [00:01<00:00, 1637.51it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1578.64it/s]warmup should be done:  82%|████████▏ | 2469/3000 [00:01<00:00, 1601.09it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1644.57it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1620.78it/s]warmup should be done:  87%|████████▋ | 2602/3000 [00:01<00:00, 1617.16it/s]warmup should be done:  89%|████████▉ | 2676/3000 [00:01<00:00, 1653.49it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1631.03it/s]warmup should be done:  89%|████████▉ | 2681/3000 [00:01<00:00, 1639.66it/s]warmup should be done:  85%|████████▌ | 2556/3000 [00:01<00:00, 1565.85it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1602.24it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1645.47it/s]warmup should be done:  92%|█████████▏| 2772/3000 [00:01<00:00, 1622.79it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1618.48it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1643.83it/s]warmup should be done:  93%|█████████▎| 2797/3000 [00:01<00:00, 1625.56it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1623.71it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1591.50it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1608.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.44it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1651.10it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1629.74it/s]warmup should be done:  98%|█████████▊| 2929/3000 [00:01<00:00, 1624.62it/s]warmup should be done:  99%|█████████▊| 2960/3000 [00:01<00:00, 1626.15it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1613.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.70it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1588.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1617.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1590.61it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.88it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.75it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.26it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.93it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.27it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1692.38it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.50it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.16it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.81it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.83it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.47it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1652.78it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.38it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.89it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1660.44it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1678.16it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1659.62it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1657.55it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1692.61it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1696.80it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1681.70it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1665.58it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1678.42it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1649.07it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1666.29it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1664.65it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1701.36it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1687.91it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1696.20it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1679.25it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1687.33it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1653.14it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1669.53it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1704.40it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1672.13it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1692.89it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1697.84it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1698.78it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1691.55it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1654.91it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1705.27it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1677.43it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1669.76it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1695.12it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1705.64it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1697.07it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1695.49it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1640.67it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1679.65it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1667.49it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1705.50it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1701.38it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1693.95it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1697.29it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1690.90it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1649.71it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1685.17it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1702.82it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1669.25it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1700.86it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1711.54it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:00, 1699.29it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1694.58it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1655.47it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1684.44it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1714.06it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1703.42it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1704.52it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1670.51it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1706.29it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1692.98it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1660.37it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1705.41it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1687.59it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1705.05it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1673.41it/s]warmup should be done:  57%|█████▋    | 1711/3000 [00:01<00:00, 1716.33it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1704.72it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1694.79it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1662.36it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1690.85it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1705.22it/s]warmup should be done:  62%|██████▏   | 1871/3000 [00:01<00:00, 1707.05it/s]warmup should be done:  63%|██████▎   | 1884/3000 [00:01<00:00, 1719.31it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1676.27it/s]warmup should be done:  62%|██████▎   | 1875/3000 [00:01<00:00, 1703.57it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1694.75it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1659.50it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1710.64it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1691.14it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1705.70it/s]warmup should be done:  69%|██████▊   | 2057/3000 [00:01<00:00, 1721.13it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1675.03it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1702.83it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1694.89it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1654.48it/s]warmup should be done:  73%|███████▎  | 2193/3000 [00:01<00:00, 1690.92it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1710.07it/s]warmup should be done:  74%|███████▍  | 2230/3000 [00:01<00:00, 1719.49it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1671.96it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1697.46it/s]warmup should be done:  74%|███████▍  | 2213/3000 [00:01<00:00, 1693.20it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1699.69it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1652.08it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1692.17it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1709.51it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1673.94it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1715.17it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1693.20it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1690.17it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1693.73it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1653.51it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1694.66it/s]warmup should be done:  85%|████████▌ | 2559/3000 [00:01<00:00, 1710.96it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1674.58it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1711.21it/s]warmup should be done:  85%|████████▌ | 2561/3000 [00:01<00:00, 1692.64it/s]warmup should be done:  85%|████████▌ | 2553/3000 [00:01<00:00, 1688.48it/s]warmup should be done:  85%|████████▌ | 2557/3000 [00:01<00:00, 1691.75it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1654.49it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1694.08it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1711.61it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1673.95it/s]warmup should be done:  91%|█████████ | 2731/3000 [00:01<00:00, 1691.19it/s]warmup should be done:  92%|█████████▏| 2746/3000 [00:01<00:00, 1706.17it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1690.49it/s]warmup should be done:  91%|█████████ | 2727/3000 [00:01<00:00, 1692.02it/s]warmup should be done:  89%|████████▉ | 2665/3000 [00:01<00:00, 1652.56it/s]warmup should be done:  96%|█████████▌| 2874/3000 [00:01<00:00, 1693.93it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1712.04it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1675.81it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1689.34it/s]warmup should be done:  96%|█████████▋| 2893/3000 [00:01<00:00, 1692.08it/s]warmup should be done:  97%|█████████▋| 2917/3000 [00:01<00:00, 1702.77it/s]warmup should be done:  97%|█████████▋| 2897/3000 [00:01<00:00, 1690.85it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1651.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1705.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1701.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1694.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1672.30it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1652.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.96it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1caa6220>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1cdaae80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1ca980d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1ca97190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1caa51c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1ca99070>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1ca9b2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3f1cdab730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 07:16:45.065431: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a5a830360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:45.065492: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:45.074226: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:45.130016: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a5702db30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:45.130087: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:45.140005: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:45.180548: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a5b02d6e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:45.180609: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:45.189627: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:45.282614: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a4e834eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:45.282677: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:45.291786: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:45.936806: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a5282d070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:45.936869: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:45.938689: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a4a831220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:45.938754: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:45.945683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:45.948199: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:46.007957: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a4a8354b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:46.008022: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:46.010950: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3a56f92f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:16:46.011008: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:16:46.018139: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:46.020652: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:16:52.355567: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.368572: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.469009: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.652462: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.672829: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.933841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.935929: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:16:52.941931: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:17:42.439][ERROR][RK0][tid #139888812873472]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:17:42.440][ERROR][RK0][tid #139888812873472]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.445][ERROR][RK0][tid #139888812873472]: coll ps creation done
[HCTR][07:17:42.445][ERROR][RK0][tid #139888812873472]: replica 7 waits for coll ps creation barrier
[HCTR][07:17:42.512][ERROR][RK0][tid #139888871589632]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:17:42.512][ERROR][RK0][tid #139888871589632]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.520][ERROR][RK0][tid #139888871589632]: coll ps creation done
[HCTR][07:17:42.520][ERROR][RK0][tid #139888871589632]: replica 5 waits for coll ps creation barrier
[HCTR][07:17:42.709][ERROR][RK0][tid #139888812873472]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:17:42.710][ERROR][RK0][tid #139888812873472]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.718][ERROR][RK0][tid #139888812873472]: coll ps creation done
[HCTR][07:17:42.718][ERROR][RK0][tid #139888812873472]: replica 4 waits for coll ps creation barrier
[HCTR][07:17:42.724][ERROR][RK0][tid #139888947091200]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:17:42.724][ERROR][RK0][tid #139888947091200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.727][ERROR][RK0][tid #139889744004864]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:17:42.727][ERROR][RK0][tid #139889744004864]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.729][ERROR][RK0][tid #139888947091200]: coll ps creation done
[HCTR][07:17:42.729][ERROR][RK0][tid #139888947091200]: replica 6 waits for coll ps creation barrier
[HCTR][07:17:42.731][ERROR][RK0][tid #139889744004864]: coll ps creation done
[HCTR][07:17:42.731][ERROR][RK0][tid #139889744004864]: replica 1 waits for coll ps creation barrier
[HCTR][07:17:42.760][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:17:42.760][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.765][ERROR][RK0][main]: coll ps creation done
[HCTR][07:17:42.765][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][07:17:42.804][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:17:42.804][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.812][ERROR][RK0][main]: coll ps creation done
[HCTR][07:17:42.812][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][07:17:42.948][ERROR][RK0][tid #139888871589632]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:17:42.948][ERROR][RK0][tid #139888871589632]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:17:42.953][ERROR][RK0][tid #139888871589632]: coll ps creation done
[HCTR][07:17:42.953][ERROR][RK0][tid #139888871589632]: replica 2 waits for coll ps creation barrier
[HCTR][07:17:42.953][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][07:17:43.810][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][07:17:43.889][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][tid #139888812873472]: replica 4 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][tid #139889744004864]: replica 1 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][tid #139888871589632]: replica 2 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][tid #139888812873472]: replica 7 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][tid #139888947091200]: replica 6 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][tid #139888871589632]: replica 5 calling init per replica
[HCTR][07:17:43.889][ERROR][RK0][main]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][tid #139888812873472]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][main]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][tid #139889744004864]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][tid #139888871589632]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][tid #139888812873472]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][tid #139888947091200]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][tid #139888871589632]: Calling build_v2
[HCTR][07:17:43.889][ERROR][RK0][tid #139888812873472]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][tid #139889744004864]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][tid #139888871589632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][tid #139888812873472]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][tid #139888947091200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:17:43.889][ERROR][RK0][tid #139888871589632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 07:17:43[2022-12-12 07:17:432022-12-12 07:17:432022-12-12 07:17:432022-12-12 07:17:43.2022-12-12 07:17:432022-12-12 07:17:43....889218..2022-12-12 07:17:43889220889218889217889220: 889220889221.: : : : E: : 889260EEEE EE:     /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ::::136::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136136] 136136:] ] ] ] using concurrent impl MPSPhase] ] 136using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhaseusing concurrent impl MPSPhase] 





using concurrent impl MPSPhase
[2022-12-12 07:17:43.893963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:17:43.894001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [assigning 8 to cpu2022-12-12 07:17:43
.894012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 07:17:432022-12-12 07:17:43[..2022-12-12 07:17:43894063894059.: : 894071EE: [  E2022-12-12 07:17:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc894110196178[:: ] ] [2022-12-12 07:17:43212Eassigning 8 to cpu
v100x8, slow pcie2022-12-12 07:17:43.]  
.894155build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc894200[: [
:: 2022-12-12 07:17:43E2022-12-12 07:17:43[[178E. .2022-12-12 07:17:432022-12-12 07:17:43]  894249/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[894256..v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :2022-12-12 07:17:43: 894276894294
:E178.E: : 178 ] [894307 EE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 07:17:43: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  v100x8, slow pcie:
.E[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
178894384 2022-12-12 07:17:43196::] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 212213v100x8, slow pcieE2022-12-12 07:17:43:894467assigning 8 to cpu] ] 
 .178: 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc894493[] E

:: 2022-12-12 07:17:43v100x8, slow pcie 196E.[
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  [8945692022-12-12 07:17:432022-12-12 07:17:43:[assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:17:43: ..1962022-12-12 07:17:43
:.E894612894616] .196894620 : : assigning 8 to cpu894640] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE
: assigning 8 to cpuE2022-12-12 07:17:43:  E
 .196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc894712] ::2022-12-12 07:17:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: assigning 8 to cpu214212[.:213E
] ] 2022-12-12 07:17:43894779196]  cpu time is 97.0588build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: ] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[
894802Eassigning 8 to cpu
:2022-12-12 07:17:43:  
212.[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 8948552022-12-12 07:17:43 :2022-12-12 07:17:43build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212.
E894894[:] 894899 : 2022-12-12 07:17:43212[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.] 2022-12-12 07:17:43
E: 894961build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: 
894969/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :2022-12-12 07:17:43E: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213[. E214
] 2022-12-12 07:17:43895019/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] remote time is 8.68421.: [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588
895054E2022-12-12 07:17:43212:
:  .[] 213E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc8950972022-12-12 07:17:43build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]  :: .
remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E895147
:]  [: 213remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 07:17:43E] 
:2022-12-12 07:17:43. remote time is 8.68421213.[895215/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] 8952332022-12-12 07:17:43: :remote time is 8.68421: .[E214
E8952832022-12-12 07:17:43 ]  : .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE8953182022-12-12 07:17:43:
: : .213214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE895354] ] : : remote time is 8.68421cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE

] : cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] :[cpu time is 97.05882142022-12-12 07:17:43
] .cpu time is 97.0588895452
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 07:19:00.839337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:19:00.879555: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:19:00.879644: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:19:00.880866: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 07:19:00.959725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 07:19:01.352279: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 07:19:01.352371: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 07:19:07.674219: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 07:19:07.674312: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 07:19:09.425991: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 07:19:09.426093: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 07:19:09.428789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 07:19:09.428855: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 07:19:09.729246: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 07:19:09.757956: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 07:19:09.759410: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 07:19:09.780304: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 07:19:10.305415: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 07:19:10.307698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 07:19:10.310714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 07:19:10.313632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 07:19:10.316560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 07:19:10.319500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 07:19:10.322412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 07:19:10.325340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 07:19:10.328263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 07:19:45.538184: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 07:19:45.548247: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 07:19:45.548764: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 07:19:45.600927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:19:45.601087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:19:45.601149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:19:45.601219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:19:45.601879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:19:45.601977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.603151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.604010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.616507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 07:19:45.616576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 07:19:45.616759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[[2022-12-12 07:19:452022-12-12 07:19:45..616802616815: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[::2022-12-12 07:19:45202205.] ] 6168383 solvedworker 0 thread 5 initing device 5: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 07:19:45202.] 6168974 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-12 07:19:45] .worker 0 thread 3 initing device 3616927
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 07:19:45.617022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:19:45.617076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.617286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:19:45.617336: [E2022-12-12 07:19:45 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6173422022-12-12 07:19:45:: .1980E617351]  : eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE
: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:19:45.[6174332022-12-12 07:19:45: .E617439 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[[2022-12-12 07:19:452022-12-12 07:19:45..618729618722: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved7 solved

[[[2022-12-12 07:19:452022-12-12 07:19:452022-12-12 07:19:45...618838618857618859: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:::202] 2052056 solved] ] 
worker 0 thread 1 initing device 1worker 0 thread 7 initing device 7
[
2022-12-12 07:19:45.619001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 07:19:45.619424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 07:19:452022-12-12 07:19:45..619450619454: : E[E 2022-12-12 07:19:45 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:619475:1815: 1815] E] Building Coll Cache with ... num gpu device is 8 Building Coll Cache with ... num gpu device is 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[[2022-12-12 07:19:452022-12-12 07:19:45..619583619587: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 07:19:45.619747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.619803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.619862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.620403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.624010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.624141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.624201: [E2022-12-12 07:19:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu624225:: 1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
[:2022-12-12 07:19:451980.] 624280eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.624336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.624383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.628336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.628390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.628444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:19:45.685329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 07:19:45.690630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:19:45.690727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:19:45.691565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.692272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.693395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:45.693452: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 94.81 MB
[[2022-12-12 07:19:452022-12-12 07:19:45..703450703450: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980[1980[[[] 2022-12-12 07:19:45] 2022-12-12 07:19:452022-12-12 07:19:452022-12-12 07:19:45eager alloc mem 1024.00 Bytes.eager alloc mem 1024.00 Bytes...
703494
703494703494703494: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 07:19:45.709044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 07:19:45.710000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:19:45.710083: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 07:19:45:.638710083] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:19:45.710167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:19:45.710179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 07:19:452022-12-12 07:19:45..710247710267: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[[2022-12-12 07:19:452022-12-12 07:19:45..710347710334: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-12 07:19:45.[7104142022-12-12 07:19:45: .E710437 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1024638
] eager release cuda mem 400000000
[2022-12-12 07:19:45.710510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:19:45.718794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.719483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.725314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.725888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.726401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.726922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.728102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.728312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.728599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.728643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.728687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.728727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.728758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:19:45.728832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:19:45.729175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:45.729223: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.36 MB
[2022-12-12 07:19:45.729380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:45.729426: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.33 MB
[2022-12-12 07:19:45.729606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 07:19:45.729675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:45.729722: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 07:19:45:.43729729] : WORKER[0] alloc host memory 95.22 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:45.[7297712022-12-12 07:19:45: .[E7297772022-12-12 07:19:45 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE729789: : 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW] : eager release cuda mem 625663638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
] :eager release cuda mem 62566343
] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:19:45.729854: [W2022-12-12 07:19:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc729863:: 43W]  WORKER[0] alloc host memory 95.35 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 95.17 MB
[2022-12-12 07:19:45.730199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:45.731289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:45.731338: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.34 MB
[2022-12-12 07:19:45.756130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.756748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.756790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.86 GB
[2022-12-12 07:19:45.791362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.791997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.792039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:19:45.792090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.792701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.792744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:19:451980.] 792753eager alloc mem 11.90 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.793360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.793401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.91 GB
[2022-12-12 07:19:45.793576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.794024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.794110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.794176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.794220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 07:19:45.794636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.794680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 07:19:45.794715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.794758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 07:19:45.795740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:19:45.796354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:19:45.796398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[[[[[[[[2022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:49........597820597821597822597821597821597821597824597821: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 4 init p2p of link 5Device 1 init p2p of link 7Device 5 init p2p of link 6Device 7 init p2p of link 4Device 6 init p2p of link 0Device 0 init p2p of link 3Device 3 init p2p of link 2







[[[[2022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:492022-12-12 07:19:49..[[..598380[598380[2022-12-12 07:19:492022-12-12 07:19:49598381598380: 2022-12-12 07:19:49: 2022-12-12 07:19:49..: : E.E.598397598397EE 598407 598408: :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E:E  ::1980 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::] ] eager alloc mem 611.00 KB:eager alloc mem 611.00 KB:19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KB
1980
1980] ] 

] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-12 07:19:49.599536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 07:19:49.599562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[638[[2022-12-12 07:19:49] 2022-12-12 07:19:492022-12-12 07:19:49.eager release cuda mem 625663..599581
599582599587: : : [[E[EE2022-12-12 07:19:492022-12-12 07:19:49 2022-12-12 07:19:49  ../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc599640599640:599642::: : 638: 638638EE] E] ]   eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
 eager release cuda mem 625663eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc

::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 07:19:49.617951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:19:49.618209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.619083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:19:49.619187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.619241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.620131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 07:19:49.620188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.620284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.620805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 07:19:49.620849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 07:19:49.620964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.621005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.621229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 07:19:492022-12-12 07:19:49..621525621525: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 2 init p2p of link 3Device 7 init p2p of link 1

[2022-12-12 07:19:49.621742: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 07:19:49:.1980621756] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.621841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.621928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.622017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:19:49.622187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.622678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.622705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.623115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.636243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 07:19:49.636391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.637329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.637768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 07:19:49.637887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.638160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:19:49.638289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.638809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.639044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 07:19:49.639172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.639232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.639392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 07:19:49.639513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.640076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.640116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:19:49.640238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.640402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.640686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 07:19:49.640810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.641049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 07:19:49.641136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.641178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.641732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.642015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.658709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:19:49.658836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.659772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.660858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:19:49.660979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.661888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.664252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:19:49.664372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.664630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:19:49.664747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.665279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.665672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.665783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:19:49.[6658772022-12-12 07:19:49: .E665897 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 0 init p2p of link 21980
] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.666019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.666197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 07:19:49.666316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.666830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.666962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.667215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.668348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:19:49.668464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:19:49.669278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:19:49.676407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.678109[: 2022-12-12 07:19:49E. 678126/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 100400000:
1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24993459 / 100000000 nodes ( 24.99 %~25.00 %) | remote 73310463 / 100000000 nodes ( 73.31 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.92 GB | 4.05866 secs 
[2022-12-12 07:19:49.678776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24960558 / 100000000 nodes ( 24.96 %~25.00 %) | remote 73343364 / 100000000 nodes ( 73.34 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.91 GB | 4.06135 secs 
[2022-12-12 07:19:49.683321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.684042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24996269 / 100000000 nodes ( 25.00 %~25.00 %) | remote 73307653 / 100000000 nodes ( 73.31 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.92 GB | 4.06698 secs 
[2022-12-12 07:19:49.687384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.687881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.689295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.689562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.690612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 07:19:49.735449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24948968 / 100000000 nodes ( 24.95 %~25.00 %) | remote 73354954 / 100000000 nodes ( 73.35 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.90 GB | 4.11802 secs 
[2022-12-12 07:19:49.735636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24990801 / 100000000 nodes ( 24.99 %~25.00 %) | remote 73313121 / 100000000 nodes ( 73.31 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.92 GB | 4.11606 secs 
[2022-12-12 07:19:49.735840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24999903 / 100000000 nodes ( 25.00 %~25.00 %) | remote 73304019 / 100000000 nodes ( 73.30 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.93 GB | 4.11852 secs 
[2022-12-12 07:19:49.736029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24996962 / 100000000 nodes ( 25.00 %~25.00 %) | remote 73306960 / 100000000 nodes ( 73.31 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.92 GB | 4.11646 secs 
[2022-12-12 07:19:49.736263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 24854936 / 100000000 nodes ( 24.85 %~25.00 %) | remote 73448986 / 100000000 nodes ( 73.45 %) | cpu 1696078 / 100000000 nodes ( 1.70 %) | 11.86 GB | 4.13431 secs 
[2022-12-12 07:19:49.736699: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 20.18 GB
[2022-12-12 07:19:51. 29782: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 20.44 GB
[2022-12-12 07:19:51. 30663: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 20.44 GB
[2022-12-12 07:19:51. 30946: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 20.44 GB
[2022-12-12 07:19:52.399254: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 20.70 GB
[2022-12-12 07:19:52.401919: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 20.70 GB
[2022-12-12 07:19:52.402296: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 20.70 GB
[2022-12-12 07:19:53.630753: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 20.92 GB
[2022-12-12 07:19:53.630884: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 20.92 GB
[2022-12-12 07:19:53.631189: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 20.92 GB
[2022-12-12 07:19:54.602101: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 21.13 GB
[2022-12-12 07:19:54.602628: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 21.13 GB
[2022-12-12 07:19:54.604296: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 21.13 GB
[2022-12-12 07:19:54.604888: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 21.13 GB
[2022-12-12 07:19:54.606137: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 21.13 GB
[2022-12-12 07:19:56. 41728: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 21.33 GB
[2022-12-12 07:19:56. 42392: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 21.33 GB
[HCTR][07:19:57.124][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][tid #139888812873472]: replica 7 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][tid #139889744004864]: replica 1 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][tid #139888947091200]: replica 6 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][tid #139888871589632]: replica 5 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][tid #139888871589632]: replica 2 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][tid #139888812873472]: replica 4 calling init per replica done, doing barrier
[HCTR][07:19:57.124][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][tid #139889744004864]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888812873472]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888871589632]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888871589632]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888947091200]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888812873472]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:19:57.124][ERROR][RK0][main]: init per replica done
[HCTR][07:19:57.124][ERROR][RK0][tid #139889744004864]: init per replica done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888871589632]: init per replica done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888812873472]: init per replica done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888871589632]: init per replica done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888947091200]: init per replica done
[HCTR][07:19:57.124][ERROR][RK0][tid #139888812873472]: init per replica done
[HCTR][07:19:57.127][ERROR][RK0][main]: init per replica done
[HCTR][07:19:57.162][ERROR][RK0][tid #139888812873472]: 7 allocated 3276800 at 0x7f1be0238400
[HCTR][07:19:57.162][ERROR][RK0][tid #139888812873472]: 7 allocated 6553600 at 0x7f1be0558400
[HCTR][07:19:57.162][ERROR][RK0][tid #139888812873472]: 7 allocated 3276800 at 0x7f1be0b98400
[HCTR][07:19:57.163][ERROR][RK0][tid #139888812873472]: 7 allocated 6553600 at 0x7f1be0eb8400
[HCTR][07:19:57.163][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f1b50238400
[HCTR][07:19:57.163][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f1b8c238400
[HCTR][07:19:57.163][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f1b50558400
[HCTR][07:19:57.163][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f1b8c558400
[HCTR][07:19:57.163][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f1b50b98400
[HCTR][07:19:57.163][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f1b8cb98400
[HCTR][07:19:57.163][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f1b50eb8400
[HCTR][07:19:57.163][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f1b8ceb8400
[HCTR][07:19:57.163][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f1b58238400
[HCTR][07:19:57.163][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f1b58558400
[HCTR][07:19:57.163][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f1b58b98400
[HCTR][07:19:57.163][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f1b58eb8400
[HCTR][07:19:57.163][ERROR][RK0][tid #139888955483904]: 3 allocated 3276800 at 0x7f1adc238400
[HCTR][07:19:57.163][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f1c08238400
[HCTR][07:19:57.163][ERROR][RK0][tid #139888955483904]: 3 allocated 6553600 at 0x7f1adc558400
[HCTR][07:19:57.163][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f1c08558400
[HCTR][07:19:57.163][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f1b18238400
[HCTR][07:19:57.163][ERROR][RK0][tid #139888955483904]: 3 allocated 3276800 at 0x7f1adcb98400
[HCTR][07:19:57.163][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f1c08b98400
[HCTR][07:19:57.163][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f1b18558400
[HCTR][07:19:57.163][ERROR][RK0][tid #139888955483904]: 3 allocated 6553600 at 0x7f1adceb8400
[HCTR][07:19:57.163][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f1c08eb8400
[HCTR][07:19:57.163][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f1b18b98400
[HCTR][07:19:57.163][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f1b18eb8400
[HCTR][07:19:57.165][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f1bb8320000
[HCTR][07:19:57.165][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f1bb8640000
[HCTR][07:19:57.165][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f1bb8c80000
[HCTR][07:19:57.165][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f1bb8fa0000
