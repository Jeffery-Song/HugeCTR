2022-12-11 22:10:53.393377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.398494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.405644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.410614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.422825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.430827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.436522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.447241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.498381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.501481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.502517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.503704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.504838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.505918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.507190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.508933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.509671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.510137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.511396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.511577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.513101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.513128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.514706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.514764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.516537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.516537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.518222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.518276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.520006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.520307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.521549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.522063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.523884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.524927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.525922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.526910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.527951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.529048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.530186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.531341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.536726: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.538008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.539160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.540170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.541277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.542931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.544461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.544845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.546158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.546562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.546686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.547044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.548517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.549147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.549282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.549528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.550848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.551804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.551916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.554084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.554189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.556714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.556910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.558307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.559731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.559884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.561409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.561983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.562781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.562913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.564528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.565267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.565698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.565974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.567605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.568284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.568343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.568888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.570489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.570984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.571155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.571823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.573201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.573588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.573859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.574864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.575875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.576021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.576395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.577504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.578462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.578508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.584982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.587270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.587723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.588651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.589612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.589622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.596632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.612892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.612893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.625546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.625837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.627661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.627691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.627771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.628185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.631066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.631348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.631386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.631692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.631948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.634791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.635042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.635844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.636038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.636276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.636599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.639028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.639175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.639623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.639971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.640147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.640475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.643310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.643447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.643757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.644149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.644263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.644583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.647576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.647755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.648081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.648514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.648528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.648765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.651387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.651572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.651890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.652371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.652573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.652698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.655135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.655269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.655588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.656167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.656395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.656409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.658813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.658940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.659391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.660031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.660196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.660727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.662615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.662658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.663235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.664013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.664217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.664791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.666365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.666458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.667049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.667785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.667979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.668710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.670381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.670477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.671000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.671665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.671920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.672308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.673982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.674176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.674732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.675660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.675966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.676314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.677400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.678242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.684618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.688403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.689581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.689886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.690236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.691486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.692090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.692150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.692493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.693765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.693907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.694627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.695693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.696500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.696982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.697439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.698163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.698900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.699596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.699786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.700449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.701014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.701476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.702119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.703950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.703959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.704472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.704918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.705369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.706759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.707271: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.707451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.707619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.708469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.708617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.709119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.710422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.711029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.711176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.712423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.712475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.712818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.714484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.715059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.715235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.716377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.716395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.716518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.716842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.718382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.719032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.719271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.720535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.720586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.720633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.721608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.722511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.723048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.723507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.724442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.724614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.724657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.725736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.726569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.727350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.727682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.728798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.730574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.731024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.732249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.732711: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.732901: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.733897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.734165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.734586: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.735071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.737554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.739930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.740534: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.741269: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.741770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.741965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.741970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.743378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.745324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.745328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.746567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.746632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.748205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.748284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.749546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.750033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.750148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.750168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.754103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.754443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.754675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.801860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.802219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.802674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.810498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.815627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.860574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.867380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.876514: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:10:53.885079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.889671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:53.899223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.925681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.926300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.927551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.928038: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:54.928093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:10:54.946286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.946929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.947455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.948481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.949016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:54.949690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:10:54.995174: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:54.995394: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.031820: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:10:55.138481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.139290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.139850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.140331: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.140386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:10:55.157948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.158602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.159115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.159944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.160978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.161464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:10:55.216923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.217526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.218052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.218521: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.218580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:10:55.224964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.225773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.226365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.226453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.227263: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.227322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:10:55.227417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.227551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.228685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.228699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.229880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.229895: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.229944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:10:55.230573: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.230618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:10:55.231218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.231840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.232359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.232832: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.232873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:10:55.236395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.237029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.237536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.238120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.238649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.239120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:10:55.245731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.246398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.246410: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.246592: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.246914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.247788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.248312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.248356: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:10:55.248807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:10:55.249364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.249678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.250463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.250716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.250896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.251646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.252182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.252497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.253289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.253805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.253922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.254770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.255372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.255602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.256388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:10:55.256829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:10:55.257006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.257476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:10:55.275181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.275821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.276341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.276826: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:10:55.276877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:10:55.293064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.293283: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.294985: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:10:55.295468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.296129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.296651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.297230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.297772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:10:55.298242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:10:55.300467: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.300663: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.301727: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:10:55.302636: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.302784: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.302813: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.302949: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.303862: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:10:55.304466: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:10:55.343816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.344019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.345056: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:10:55.345649: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.345801: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:10:55.346762: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.618][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:10:56.658][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 83.45it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 101it [00:01, 85.66it/s]warmup run: 97it [00:01, 82.75it/s]warmup run: 98it [00:01, 84.47it/s]warmup run: 197it [00:01, 179.99it/s]warmup run: 97it [00:01, 83.89it/s]warmup run: 201it [00:01, 184.71it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 195it [00:01, 180.40it/s]warmup run: 197it [00:01, 183.88it/s]warmup run: 296it [00:01, 288.01it/s]warmup run: 193it [00:01, 180.44it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 302it [00:01, 295.09it/s]warmup run: 102it [00:01, 87.46it/s]warmup run: 100it [00:01, 87.75it/s]warmup run: 292it [00:01, 286.51it/s]warmup run: 296it [00:01, 292.82it/s]warmup run: 396it [00:01, 401.42it/s]warmup run: 290it [00:01, 287.85it/s]warmup run: 97it [00:01, 85.77it/s]warmup run: 401it [00:01, 406.55it/s]warmup run: 202it [00:01, 187.25it/s]warmup run: 198it [00:01, 187.27it/s]warmup run: 390it [00:01, 397.73it/s]warmup run: 395it [00:01, 405.28it/s]warmup run: 495it [00:02, 510.36it/s]warmup run: 387it [00:01, 398.46it/s]warmup run: 194it [00:01, 185.04it/s]warmup run: 501it [00:02, 517.23it/s]warmup run: 302it [00:01, 296.82it/s]warmup run: 295it [00:01, 294.75it/s]warmup run: 487it [00:02, 503.87it/s]warmup run: 495it [00:02, 516.70it/s]warmup run: 595it [00:02, 612.94it/s]warmup run: 485it [00:02, 507.46it/s]warmup run: 292it [00:01, 294.74it/s]warmup run: 603it [00:02, 622.01it/s]warmup run: 403it [00:01, 411.71it/s]warmup run: 394it [00:01, 408.31it/s]warmup run: 587it [00:02, 607.43it/s]warmup run: 597it [00:02, 622.07it/s]warmup run: 696it [00:02, 703.19it/s]warmup run: 584it [00:02, 608.49it/s]warmup run: 390it [00:01, 407.22it/s]warmup run: 704it [00:02, 711.53it/s]warmup run: 503it [00:02, 521.94it/s]warmup run: 491it [00:01, 514.80it/s]warmup run: 687it [00:02, 697.40it/s]warmup run: 698it [00:02, 710.51it/s]warmup run: 797it [00:02, 778.52it/s]warmup run: 683it [00:02, 696.41it/s]warmup run: 488it [00:01, 515.41it/s]warmup run: 805it [00:02, 785.12it/s]warmup run: 605it [00:02, 626.35it/s]warmup run: 591it [00:02, 616.97it/s]warmup run: 787it [00:02, 772.01it/s]warmup run: 798it [00:02, 782.21it/s]warmup run: 898it [00:02, 837.87it/s]warmup run: 783it [00:02, 770.08it/s]warmup run: 586it [00:02, 613.96it/s]warmup run: 905it [00:02, 841.09it/s]warmup run: 707it [00:02, 716.25it/s]warmup run: 689it [00:02, 701.55it/s]warmup run: 887it [00:02, 829.58it/s]warmup run: 898it [00:02, 839.10it/s]warmup run: 998it [00:02, 880.34it/s]warmup run: 882it [00:02, 825.58it/s]warmup run: 685it [00:02, 701.70it/s]warmup run: 1005it [00:02, 869.42it/s]warmup run: 808it [00:02, 787.66it/s]warmup run: 788it [00:02, 772.15it/s]warmup run: 986it [00:02, 872.27it/s]warmup run: 999it [00:02, 883.29it/s]warmup run: 1098it [00:02, 913.26it/s]warmup run: 982it [00:02, 871.72it/s]warmup run: 785it [00:02, 775.57it/s]warmup run: 1105it [00:02, 904.01it/s]warmup run: 909it [00:02, 843.68it/s]warmup run: 888it [00:02, 831.65it/s]warmup run: 1085it [00:02, 904.31it/s]warmup run: 1100it [00:02, 917.45it/s]warmup run: 1198it [00:02, 936.04it/s]warmup run: 1083it [00:02, 908.86it/s]warmup run: 885it [00:02, 832.91it/s]warmup run: 1205it [00:02, 930.89it/s]warmup run: 1009it [00:02, 885.85it/s]warmup run: 986it [00:02, 871.17it/s]warmup run: 1184it [00:02, 926.86it/s]warmup run: 1203it [00:02, 947.01it/s]warmup run: 1298it [00:02, 951.85it/s]warmup run: 1182it [00:02, 931.31it/s]warmup run: 984it [00:02, 875.35it/s]warmup run: 1304it [00:02, 947.77it/s]warmup run: 1110it [00:02, 920.49it/s]warmup run: 1084it [00:02, 898.38it/s]warmup run: 1283it [00:02, 941.54it/s]warmup run: 1304it [00:02, 963.78it/s]warmup run: 1398it [00:02, 965.48it/s]warmup run: 1282it [00:02, 949.16it/s]warmup run: 1084it [00:02, 907.90it/s]warmup run: 1404it [00:02, 961.37it/s]warmup run: 1212it [00:02, 947.71it/s]warmup run: 1182it [00:02, 919.11it/s]warmup run: 1385it [00:02, 962.05it/s]warmup run: 1408it [00:02, 983.33it/s]warmup run: 1498it [00:03, 975.21it/s]warmup run: 1381it [00:02, 910.40it/s]warmup run: 1185it [00:02, 935.02it/s]warmup run: 1504it [00:03, 970.42it/s]warmup run: 1313it [00:02, 961.97it/s]warmup run: 1280it [00:02, 923.16it/s]warmup run: 1485it [00:03, 971.28it/s]warmup run: 1511it [00:03, 994.79it/s]warmup run: 1599it [00:03, 982.85it/s]warmup run: 1476it [00:03, 917.27it/s]warmup run: 1284it [00:02, 950.35it/s]warmup run: 1605it [00:03, 981.54it/s]warmup run: 1414it [00:02, 973.80it/s]warmup run: 1376it [00:02, 928.18it/s]warmup run: 1585it [00:03, 964.88it/s]warmup run: 1614it [00:03, 1003.86it/s]warmup run: 1699it [00:03, 987.90it/s]warmup run: 1577it [00:03, 942.36it/s]warmup run: 1386it [00:02, 970.02it/s]warmup run: 1707it [00:03, 990.36it/s]warmup run: 1516it [00:03, 984.30it/s]warmup run: 1472it [00:02, 935.44it/s]warmup run: 1683it [00:03, 965.92it/s]warmup run: 1716it [00:03, 1008.02it/s]warmup run: 1799it [00:03, 986.06it/s]warmup run: 1679it [00:03, 963.45it/s]warmup run: 1486it [00:02, 978.40it/s]warmup run: 1810it [00:03, 1000.14it/s]warmup run: 1618it [00:03, 992.25it/s]warmup run: 1569it [00:03, 942.90it/s]warmup run: 1782it [00:03, 971.17it/s]warmup run: 1819it [00:03, 1012.49it/s]warmup run: 1899it [00:03, 983.99it/s]warmup run: 1780it [00:03, 974.42it/s]warmup run: 1586it [00:03, 983.33it/s]warmup run: 1913it [00:03, 1007.23it/s]warmup run: 1719it [00:03, 997.46it/s]warmup run: 1665it [00:03, 945.27it/s]warmup run: 1882it [00:03, 978.29it/s]warmup run: 1921it [00:03, 1012.70it/s]warmup run: 1999it [00:03, 988.33it/s]warmup run: 1881it [00:03, 983.90it/s]warmup run: 1686it [00:03, 985.86it/s]warmup run: 2015it [00:03, 1002.64it/s]warmup run: 1823it [00:03, 1009.01it/s]warmup run: 1765it [00:03, 959.72it/s]warmup run: 1981it [00:03, 977.24it/s]warmup run: 2026it [00:03, 1022.39it/s]warmup run: 2118it [00:03, 1046.09it/s]warmup run: 1981it [00:03, 988.32it/s]warmup run: 1788it [00:03, 993.92it/s]warmup run: 2136it [00:03, 1063.27it/s]warmup run: 1929it [00:03, 1023.68it/s]warmup run: 1867it [00:03, 975.37it/s]warmup run: 2096it [00:03, 1028.01it/s]warmup run: 2149it [00:03, 1082.29it/s]warmup run: 2237it [00:03, 1088.37it/s]warmup run: 2098it [00:03, 1041.06it/s]warmup run: 1891it [00:03, 1001.88it/s]warmup run: 2039it [00:03, 1045.27it/s]warmup run: 2256it [00:03, 1101.66it/s]warmup run: 1968it [00:03, 982.79it/s]warmup run: 2214it [00:03, 1070.96it/s]warmup run: 2272it [00:03, 1125.03it/s]warmup run: 2358it [00:03, 1122.92it/s]warmup run: 2220it [00:03, 1092.92it/s]warmup run: 1992it [00:03, 992.05it/s] warmup run: 2377it [00:03, 1133.05it/s]warmup run: 2162it [00:03, 1098.09it/s]warmup run: 2082it [00:03, 1029.20it/s]warmup run: 2336it [00:03, 1113.13it/s]warmup run: 2395it [00:03, 1155.67it/s]warmup run: 2479it [00:03, 1148.35it/s]warmup run: 2342it [00:03, 1129.20it/s]warmup run: 2111it [00:03, 1048.79it/s]warmup run: 2498it [00:03, 1155.11it/s]warmup run: 2285it [00:03, 1136.37it/s]warmup run: 2204it [00:03, 1083.96it/s]warmup run: 2458it [00:03, 1143.53it/s]warmup run: 2518it [00:03, 1176.76it/s]warmup run: 2600it [00:04, 1166.00it/s]warmup run: 2464it [00:03, 1154.42it/s]warmup run: 2233it [00:03, 1097.10it/s]warmup run: 2619it [00:04, 1170.67it/s]warmup run: 2409it [00:03, 1164.54it/s]warmup run: 2326it [00:03, 1122.27it/s]warmup run: 2580it [00:04, 1164.94it/s]warmup run: 2642it [00:04, 1192.96it/s]warmup run: 2722it [00:04, 1179.42it/s]warmup run: 2586it [00:04, 1172.21it/s]warmup run: 2355it [00:03, 1131.73it/s]warmup run: 2740it [00:04, 1182.09it/s]warmup run: 2532it [00:03, 1182.22it/s]warmup run: 2448it [00:03, 1151.37it/s]warmup run: 2702it [00:04, 1179.41it/s]warmup run: 2764it [00:04, 1200.53it/s]warmup run: 2842it [00:04, 1185.22it/s]warmup run: 2708it [00:04, 1184.65it/s]warmup run: 2476it [00:03, 1154.08it/s]warmup run: 2860it [00:04, 1185.60it/s]warmup run: 2655it [00:04, 1195.94it/s]warmup run: 2571it [00:04, 1173.04it/s]warmup run: 2822it [00:04, 1185.39it/s]warmup run: 2887it [00:04, 1208.55it/s]warmup run: 2964it [00:04, 1193.31it/s]warmup run: 2830it [00:04, 1192.93it/s]warmup run: 3000it [00:04, 682.14it/s] warmup run: 2597it [00:03, 1168.68it/s]warmup run: 2980it [00:04, 1189.18it/s]warmup run: 2776it [00:04, 1198.39it/s]warmup run: 2694it [00:04, 1187.57it/s]warmup run: 3000it [00:04, 685.16it/s] warmup run: 2943it [00:04, 1192.49it/s]warmup run: 3000it [00:04, 693.98it/s] warmup run: 3000it [00:04, 681.45it/s] warmup run: 2950it [00:04, 1193.33it/s]warmup run: 2718it [00:04, 1179.08it/s]warmup run: 2898it [00:04, 1202.61it/s]warmup run: 2815it [00:04, 1193.71it/s]warmup run: 3000it [00:04, 684.00it/s] warmup run: 3000it [00:04, 694.14it/s] warmup run: 2836it [00:04, 1172.07it/s]warmup run: 2938it [00:04, 1201.85it/s]warmup run: 3000it [00:04, 688.27it/s] warmup run: 2954it [00:04, 1152.15it/s]warmup run: 3000it [00:04, 690.79it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.64it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.37it/s]warmup should be done:   5%|▌         | 153/3000 [00:00<00:01, 1522.25it/s]warmup should be done:   5%|▌         | 157/3000 [00:00<00:01, 1563.69it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.88it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.62it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.06it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.30it/s]warmup should be done:  10%|█         | 313/3000 [00:00<00:01, 1567.17it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1627.39it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.40it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.61it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1654.90it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.78it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.72it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1614.76it/s]warmup should be done:  16%|█▌        | 472/3000 [00:00<00:01, 1577.07it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1646.65it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1624.05it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1665.46it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1627.95it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1660.58it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1630.35it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1645.60it/s]warmup should be done:  21%|██        | 631/3000 [00:00<00:01, 1580.79it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1645.33it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1635.53it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.16it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1663.82it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1620.83it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1637.78it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1608.48it/s]warmup should be done:  26%|██▋       | 790/3000 [00:00<00:01, 1583.59it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1639.93it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1659.82it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1630.04it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1660.33it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1618.38it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1630.70it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1598.81it/s]warmup should be done:  32%|███▏      | 953/3000 [00:00<00:01, 1598.11it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1657.83it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1629.87it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1615.65it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1657.73it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1623.02it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1624.35it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1590.86it/s]warmup should be done:  37%|███▋      | 1114/3000 [00:00<00:01, 1601.37it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1653.51it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1630.21it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1654.64it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1611.48it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1618.30it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1586.16it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1591.36it/s]warmup should be done:  43%|████▎     | 1277/3000 [00:00<00:01, 1609.25it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1632.28it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1655.41it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1650.01it/s]warmup should be done:  43%|████▎     | 1301/3000 [00:00<00:01, 1610.21it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1617.45it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1589.41it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1568.81it/s]warmup should be done:  48%|████▊     | 1440/3000 [00:00<00:00, 1613.24it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1633.47it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1654.35it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1609.14it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1637.60it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1617.35it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1592.91it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1560.07it/s]warmup should be done:  53%|█████▎    | 1603/3000 [00:01<00:00, 1617.37it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1634.59it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1655.76it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1608.79it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1641.62it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1616.68it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1595.81it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1567.80it/s]warmup should be done:  59%|█████▉    | 1766/3000 [00:01<00:00, 1620.85it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1633.80it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1655.92it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1608.56it/s]warmup should be done:  60%|██████    | 1802/3000 [00:01<00:00, 1617.37it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1634.48it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1597.57it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1582.77it/s]warmup should be done:  64%|██████▍   | 1929/3000 [00:01<00:00, 1622.54it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1656.34it/s]warmup should be done:  65%|██████▍   | 1946/3000 [00:01<00:00, 1608.41it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1625.53it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1617.58it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1623.02it/s]warmup should be done:  65%|██████▍   | 1940/3000 [00:01<00:00, 1580.83it/s]warmup should be done:  65%|██████▌   | 1952/3000 [00:01<00:00, 1593.28it/s]warmup should be done:  70%|██████▉   | 2092/3000 [00:01<00:00, 1624.08it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1655.86it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1608.47it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1617.06it/s]warmup should be done:  71%|███████   | 2128/3000 [00:01<00:00, 1608.57it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1614.58it/s]warmup should be done:  70%|███████   | 2101/3000 [00:01<00:00, 1587.95it/s]warmup should be done:  70%|███████   | 2114/3000 [00:01<00:00, 1600.57it/s]warmup should be done:  75%|███████▌  | 2255/3000 [00:01<00:00, 1619.51it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1656.99it/s]warmup should be done:  76%|███████▌  | 2268/3000 [00:01<00:00, 1607.69it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1623.23it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1609.17it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1608.22it/s]warmup should be done:  75%|███████▌  | 2262/3000 [00:01<00:00, 1592.07it/s]warmup should be done:  76%|███████▌  | 2277/3000 [00:01<00:00, 1607.91it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1612.92it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1653.67it/s]warmup should be done:  81%|████████  | 2429/3000 [00:01<00:00, 1601.68it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1625.24it/s]warmup should be done:  82%|████████▏ | 2454/3000 [00:01<00:00, 1615.96it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1613.72it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1592.77it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1609.34it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1609.36it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1654.56it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1611.85it/s]warmup should be done:  87%|████████▋ | 2619/3000 [00:01<00:00, 1633.89it/s]warmup should be done:  87%|████████▋ | 2619/3000 [00:01<00:00, 1624.05it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1620.51it/s]warmup should be done:  86%|████████▌ | 2582/3000 [00:01<00:00, 1594.65it/s]warmup should be done:  87%|████████▋ | 2600/3000 [00:01<00:00, 1608.30it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1607.29it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1656.74it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1619.81it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1641.17it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1629.39it/s]warmup should be done:  91%|█████████▏| 2742/3000 [00:01<00:00, 1596.17it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1623.04it/s]warmup should be done:  92%|█████████▏| 2761/3000 [00:01<00:00, 1605.55it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1659.72it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1629.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.53it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1651.21it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1590.04it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1636.04it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1595.32it/s]warmup should be done:  97%|█████████▋| 2922/3000 [00:01<00:00, 1604.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.97it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1497.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.44it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.71it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.36it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1705.61it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1705.66it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.31it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.55it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.16it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.98it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.24it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.43it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.24it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.55it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1702.90it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1670.43it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1624.43it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1701.52it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1679.93it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1649.93it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1640.70it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1691.94it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1629.93it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1675.93it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1702.70it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1701.82it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1680.28it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1647.06it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1692.74it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1630.27it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1639.20it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1705.74it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1672.53it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1704.52it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1684.19it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1645.07it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1631.92it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1635.73it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1707.50it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1688.37it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1682.98it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1697.39it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1665.72it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1645.38it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1632.57it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1706.98it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1688.22it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1637.22it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1689.95it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1663.35it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1686.68it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1634.79it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1647.13it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1640.88it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1702.96it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1684.20it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1688.58it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1661.40it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1676.57it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1644.24it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1704.52it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1685.97it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1629.41it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1635.60it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1692.74it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1665.10it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1676.54it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1645.21it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1685.20it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1633.46it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1702.58it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1638.18it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1693.91it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1661.91it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1681.30it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1646.87it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1684.87it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1637.33it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1701.67it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1640.87it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1695.30it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1661.82it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1679.49it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1647.37it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1684.75it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1638.24it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1703.69it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1641.91it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1697.69it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1664.38it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1677.90it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1646.36it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1705.29it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1683.50it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1636.47it/s]warmup should be done:  66%|██████▌   | 1977/3000 [00:01<00:00, 1640.09it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1698.66it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1666.44it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1677.11it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1647.11it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1704.38it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1636.55it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1679.41it/s]warmup should be done:  74%|███████▍  | 2213/3000 [00:01<00:00, 1698.19it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1639.69it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1666.25it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1674.63it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1648.61it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1703.96it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1638.70it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1697.61it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1641.96it/s]warmup should be done:  79%|███████▉  | 2367/3000 [00:01<00:00, 1671.40it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1669.33it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1674.37it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1648.61it/s]warmup should be done:  82%|████████▏ | 2462/3000 [00:01<00:00, 1638.33it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1699.40it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1641.92it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1695.68it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1671.34it/s]warmup should be done:  84%|████████▍ | 2535/3000 [00:01<00:00, 1667.93it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1674.04it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1646.53it/s]warmup should be done:  88%|████████▊ | 2626/3000 [00:01<00:00, 1637.74it/s]warmup should be done:  91%|█████████ | 2725/3000 [00:01<00:00, 1701.62it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1699.05it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1640.75it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1671.97it/s]warmup should be done:  90%|█████████ | 2702/3000 [00:01<00:00, 1664.80it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1672.93it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1646.38it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1638.80it/s]warmup should be done:  97%|█████████▋| 2896/3000 [00:01<00:00, 1700.99it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1645.74it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1699.95it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1666.46it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1660.77it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1668.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1702.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1695.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.22it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1647.24it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1639.80it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1650.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.14it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e679c00a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e679b11f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e679bf1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e679b42e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e684e4e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e684e5730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e679bf2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f0e679b1130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 22:12:25.654646: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f099a82ba60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:25.654709: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:25.656666: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f099e830340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:25.656725: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:25.663756: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:25.665247: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:25.846274: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0993030f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:25.846336: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:25.855820: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:26.351197: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f098f031880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:26.351261: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:26.361494: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:26.420281: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0992830190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:26.420350: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:26.420521: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f099682fae0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:26.420567: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:26.428422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:26.429558: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:26.495300: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0992833a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:26.495364: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:26.504840: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:26.507206: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f098ef92ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:12:26.507262: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:12:26.515039: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:12:32.565056: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:32.747699: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:32.901529: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:33.154706: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:33.160469: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:33.170345: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:33.373821: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:12:33.454417: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:13:34.987][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:13:34.987][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:34.996][ERROR][RK0][main]: coll ps creation done
[HCTR][22:13:34.996][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][22:13:35.393][ERROR][RK0][tid #139679617758976]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:13:35.393][ERROR][RK0][tid #139679617758976]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.401][ERROR][RK0][tid #139679617758976]: coll ps creation done
[HCTR][22:13:35.402][ERROR][RK0][tid #139679617758976]: replica 0 waits for coll ps creation barrier
[HCTR][22:13:35.405][ERROR][RK0][tid #139679970088704]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:13:35.405][ERROR][RK0][tid #139679970088704]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.413][ERROR][RK0][tid #139679970088704]: coll ps creation done
[HCTR][22:13:35.413][ERROR][RK0][tid #139679970088704]: replica 1 waits for coll ps creation barrier
[HCTR][22:13:35.435][ERROR][RK0][tid #139679508719360]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:13:35.436][ERROR][RK0][tid #139679508719360]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.441][ERROR][RK0][tid #139679508719360]: coll ps creation done
[HCTR][22:13:35.441][ERROR][RK0][tid #139679508719360]: replica 6 waits for coll ps creation barrier
[HCTR][22:13:35.448][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:13:35.448][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.456][ERROR][RK0][main]: coll ps creation done
[HCTR][22:13:35.456][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][22:13:35.522][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:13:35.522][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.530][ERROR][RK0][main]: coll ps creation done
[HCTR][22:13:35.530][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][22:13:35.632][ERROR][RK0][tid #139679961696000]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:13:35.632][ERROR][RK0][tid #139679961696000]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.640][ERROR][RK0][tid #139679961696000]: coll ps creation done
[HCTR][22:13:35.640][ERROR][RK0][tid #139679961696000]: replica 4 waits for coll ps creation barrier
[HCTR][22:13:35.646][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:13:35.646][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:13:35.654][ERROR][RK0][main]: coll ps creation done
[HCTR][22:13:35.654][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][22:13:35.654][ERROR][RK0][tid #139679617758976]: replica 0 preparing frequency
[HCTR][22:13:36.511][ERROR][RK0][tid #139679617758976]: replica 0 preparing frequency done
[HCTR][22:13:36.543][ERROR][RK0][tid #139679617758976]: replica 0 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][tid #139679961696000]: replica 4 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][tid #139679970088704]: replica 1 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][tid #139679508719360]: replica 6 calling init per replica
[HCTR][22:13:36.543][ERROR][RK0][tid #139679617758976]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][main]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][main]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][main]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][tid #139679961696000]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][tid #139679970088704]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][main]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][tid #139679617758976]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][tid #139679508719360]: Calling build_v2
[HCTR][22:13:36.543][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][tid #139679961696000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][tid #139679970088704]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:13:36.543][ERROR][RK0][tid #139679508719360]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 22:13:36.548370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[178] v100x8, slow pcie
2022-12-11 22:13:36.548411[: 2022-12-11 22:13:36E. 548451/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:[
196] assigning 0 to cpu
[2022-12-11 22:13:362022-12-11 22:13:36..548502[548466: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::196[178] 2022-12-11 22:13:36] assigning 0 to cpu.v100x8, slow pcie2022-12-11 22:13:36
548541
.: 548502E:  E[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 22:13:36:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212:2022-12-11 22:13:36[548576] 178.2022-12-11 22:13:36: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[] 548557.E
v100x8, slow pcie: 5486042022-12-11 22:13:36 
E: [.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E[5486032022-12-11 22:13:36:2022-12-11 22:13:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 22:13:36: .196[.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E548675] 548655178:2022-12-11 22:13:36548687 : assigning 0 to cpu: ] 212.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
Ev100x8, slow pcie] 548712E:  
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ::2022-12-11 22:13:36. 2022-12-11 22:13:36[:v100x8, slow pcie213178548886/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 22:13:36196
] ] : :548889.] remote time is 8.68421[v100x8, slow pcieE: 178548924assigning 0 to cpu
2022-12-11 22:13:36
 E] : [
.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc v100x8, slow pcieE2022-12-11 22:13:365490122022-12-11 22:13:36:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 [.: .212:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:13:36549075[E549067] 196:.: 2022-12-11 22:13:36 : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 213549116E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 
assigning 0 to cpu] :  549146:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196:
 2022-12-11 22:13:36:E] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[196 assigning 0 to cpu] [:5492622022-12-11 22:13:36] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.05882022-12-11 22:13:36212: .assigning 0 to cpu:
.] E549310
[196549321build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 : 2022-12-11 22:13:36] : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.assigning 0 to cpuE: 549410[
[ 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 22:13:362022-12-11 22:13:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :E..:remote time is 8.68421214 [549451549448212
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:13:36: : ] cpu time is 97.0588:[.EEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
2122022-12-11 22:13:36549514  
] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8549565:E[:
: 213 2022-12-11 22:13:36212E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[]  remote time is 8.68421:5496502022-12-11 22:13:36build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212: .
:[] E5496882142022-12-11 22:13:36build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [: ] .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:13:36Ecpu time is 97.0588549745:. 
: [213549769/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 22:13:36] : : .remote time is 8.68421E213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc549820
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: remote time is 8.68421[:214E
2022-12-11 22:13:36213]  .[] cpu time is 97.0588remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc5499052022-12-11 22:13:36

:: .213E549943[]  : 2022-12-11 22:13:36remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.
: 549994214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :[Ecpu time is 97.05882142022-12-11 22:13:36 
] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588550034:
: 214E]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-11 22:14:53.403742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:14:53.443873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 22:14:53.443935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 1999999
[2022-12-11 22:14:53.549611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:14:53.549696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:14:53.701497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:14:53.701533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:14:53.701997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.702941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.703760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.716709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 22:14:53.[7167662022-12-11 22:14:53: .E716747 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 2 initing device 2202
] 6 solved
[2022-12-11 22:14:53.716869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:14:53.716963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 22:14:53.[7170232022-12-11 22:14:53: .E717014 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[] :2022-12-11 22:14:53worker 0 thread 4 initing device 4202.
] 7170491 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[[] 2022-12-11 22:14:532022-12-11 22:14:535 solved..
717102717091: : EE[  2022-12-11 22:14:53[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.2022-12-11 22:14:53::717135.205202: 717129] ] E: worker 0 thread 1 initing device 17 solved E

/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205[:] 2022-12-11 22:14:53[202worker 0 thread 5 initing device 5.2022-12-11 22:14:53] 
717206.3 solved: 717208
E: [ [2022-12-11 22:14:53E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 22:14:53. :.717263/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu205717270: :] : E1980worker 0 thread 7 initing device 7E ] 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:
:1980205] ] eager alloc mem 381.47 MBworker 0 thread 3 initing device 3

[2022-12-11 22:14:53.717450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.717549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.717583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.717706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-11 22:14:53.717726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.721968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.722029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.722139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.722195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.722238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.722305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.722356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.726710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:14:53.779934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:14:53.780241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:14:53.785139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:14:53.785206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:14:53.785248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:14:53.786020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.786474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.787427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:53.787515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:14:53.788184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:14:53.788224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:14:53.808097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:14:53.808395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:14:53.813197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:14:53.813257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:14:53.813304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:14:53.814060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.814510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.815462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:53.815548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:14:53.816236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:14:53.816281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[[[[[[2022-12-11 22:14:532022-12-11 22:14:532022-12-11 22:14:532022-12-11 22:14:532022-12-11 22:14:532022-12-11 22:14:53......820594820594820594820594820594820594: : : : : : EEEEEE      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::198019801980198019801980] ] ] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes





[[2022-12-11 22:14:53[[2022-12-11 22:14:53[[.2022-12-11 22:14:532022-12-11 22:14:53.2022-12-11 22:14:532022-12-11 22:14:53821048..821051..: 821053821052: 821052821053E: : E: :  EE EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::1980::] 19801980] 19801980eager alloc mem 1024.00 Bytes] ] eager alloc mem 1024.00 Bytes] ] 
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-11 22:14:53.827639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:14:53.827714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 22022-12-11 22:14:53
.827717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:14:53.827767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 22:14:53.827789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:14:53.827806: [E2022-12-11 22:14:53 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc827834:: 638E]  eager release cuda mem 1024/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[[2022-12-11 22:14:532022-12-11 22:14:53..827886827874: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 1024

[2022-12-11 22:14:53.[8279562022-12-11 22:14:53: .E827960 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E:2022-12-11 22:14:53 638./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 827965:eager release cuda mem 400000000: 638
E]  eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-11 22:14:53.828044: E[[ 2022-12-11 22:14:532022-12-11 22:14:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..:828041828054638: : ] EEeager release cuda mem 400000000  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 2

[2022-12-11 22:14:53[.2022-12-11 22:14:53828142.: 828143E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 2
[2022-12-11 22:14:53.828221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:14:53.835414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.835944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.836452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.836956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.837465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.837988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:14:53.839353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.839398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.839466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.839500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.839544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.839650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:53.840283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:53.840336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:53.840365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:14:53.840408[: 2022-12-11 22:14:53E. 840417/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: [:E2022-12-11 22:14:53638 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu840433eager release cuda mem 625663:: 
1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 22:14:53.840483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:53.840518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB[
2022-12-11 22:14:53.840539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:14:53.840565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:14:531980.] 840577eager alloc mem 25.25 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:53.840669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:14:53.841038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:14:53.841077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:14:53.841121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:14:53.841161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:14:53.841191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 22:14:53eager release cuda mem 25855.
841209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
[2022-12-11 22:14:532022-12-11 22:14:53..841237841246: : E[E 2022-12-11 22:14:53 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:841270:1980: 638] E] eager alloc mem 976.56 MB eager release cuda mem 25855
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 976.56 MB
[2022-12-11 22:14:53.841333: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:14:53:.1980841343] : eager alloc mem 976.56 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:14:53.841397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[[[[[[2022-12-11 22:14:542022-12-11 22:14:542022-12-11 22:14:54[2022-12-11 22:14:542022-12-11 22:14:542022-12-11 22:14:54..[.2022-12-11 22:14:54... 28706 287022022-12-11 22:14:54 28702. 28702 28702 28706: : .:  28744: : : EE 28744E: EEE  :  E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:198019801980] ] :] 1980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB

] 
eager alloc mem 611.00 KB


eager alloc mem 611.00 KB

[[2022-12-11 22:14:542022-12-11 22:14:54.. 29801 29803[[: : 2022-12-11 22:14:542022-12-11 22:14:54[EE.[.2022-12-11 22:14:54[   298192022-12-11 22:14:54 29817.2022-12-11 22:14:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[: .:  29829.::2022-12-11 22:14:54E 29829E:  29835638638. :  E: ] ]  29859/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc Eeager release cuda mem 625663eager release cuda mem 625663: : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 

E638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] :] 638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663[638eager release cuda mem 625663] 638:
[2022-12-11 22:14:54] 
eager release cuda mem 625663] 6382022-12-11 22:14:54.eager release cuda mem 625663
eager release cuda mem 625663] . 30045

eager release cuda mem 625663 30047: [
[: E2022-12-11 22:14:542022-12-11 22:14:54E ..[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 30121 301302022-12-11 22:14:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: : .[:1980[EE 301542022-12-11 22:14:541980] [2022-12-11 22:14:54  : .] eager alloc mem 611.00 KB2022-12-11 22:14:54./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 30172eager alloc mem 611.00 KB
. 30177:: : 
 30193: 19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: E] ] : E eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB1980:1980
] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-11 22:14:54.[ 310092022-12-11 22:14:54: .E 31017 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 22:14:54. 31086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:14:54] .[eager release cuda mem 625663 311042022-12-11 22:14:54[
: .2022-12-11 22:14:54E 31112[. [: 2022-12-11 22:14:54 31119/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:14:54[E[.[: :.2022-12-11 22:14:54 2022-12-11 22:14:54 311412022-12-11 22:14:54E638 31146./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: . ] :  31164: 31182E 31170/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663E: 1980:  : :
 E] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager alloc mem 611.00 KB : ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB638::[] :
] 63819802022-12-11 22:14:54eager release cuda mem 625663638eager release cuda mem 625663] ] .
] 
eager release cuda mem 625663eager alloc mem 611.00 KB 31334eager release cuda mem 625663

: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54[.2022-12-11 22:14:54 31435.[:  314402022-12-11 22:14:54[E: .2022-12-11 22:14:54 E 31454./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :  31463:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980: E] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager alloc mem 611.00 KB] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager alloc mem 611.00 KB1980:
] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 32070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:14:54] .eager release cuda mem 625663 32088
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:14:542022-12-11 22:14:54.. 32147 32148: [: E2022-12-11 22:14:54E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 32163/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:: :2022-12-11 22:14:54638E1980.]  ]  32183eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB: 
:
E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-11 22:14:54. 32266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 32293[: 2022-12-11 22:14:54E.  32299/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980[ ] 2022-12-11 22:14:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[eager alloc mem 611.00 KB.[:2022-12-11 22:14:54
 323372022-12-11 22:14:54638.: .]  32352E 32358eager release cuda mem 625663:  : 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] :638eager release cuda mem 625663638] [
] eager release cuda mem 6256632022-12-11 22:14:54eager release cuda mem 625663
.
 32467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 32529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:14:54] [.eager alloc mem 611.00 KB2022-12-11 22:14:54 32547
.:  32553E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 22:14:54.[ 329572022-12-11 22:14:54: .E 32965 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 22:14:54. 33022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54.[ 330502022-12-11 22:14:54: .E 33055 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB[1980
2022-12-11 22:14:54] .eager alloc mem 611.00 KB 33086
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 33135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 33199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 33254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:14:542022-12-11 22:14:54.. 33329 33329: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980[638[] 2022-12-11 22:14:54] 2022-12-11 22:14:54eager alloc mem 611.00 KB.eager release cuda mem 625663.
 33390
 33395: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] [] eager release cuda mem 6256632022-12-11 22:14:54eager release cuda mem 625663
.
 33483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 33551[: 2022-12-11 22:14:54E.  33556/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[[2022-12-11 22:14:542022-12-11 22:14:54.. 33830 33833: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:14:54::.638638 33856] ] : eager release cuda mem 625663eager release cuda mem 625663E

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54[.2022-12-11 22:14:54 33931.: [ 33934E2022-12-11 22:14:54:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 33945 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980E:[]  19802022-12-11 22:14:54eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] .
:eager alloc mem 611.00 KB 339741980
: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 34068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 34161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 34228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 34286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 34354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 34376: [E2022-12-11 22:14:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 34382:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 34482: [E2022-12-11 22:14:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 34491:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 34724: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:14:54:.638[ 34739] 2022-12-11 22:14:54: eager release cuda mem 625663.E
 34750 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 22:14:54. 34810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54[.[2022-12-11 22:14:54 348382022-12-11 22:14:54.: . 34842E 34843:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] :1980eager alloc mem 611.00 KB638] 
] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 22:14:54. 34953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 34981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 35049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 35139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 35206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 35311: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:14:54:.638 35323] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 35408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:14:541980.]  35422eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:14:54. 35558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 35596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[[2022-12-11 22:14:542022-12-11 22:14:54.. 35642 35643: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-11 22:14:542022-12-11 22:14:54.. 35702 35704: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-11 22:14:54:638.638]  35728] eager release cuda mem 8399996: eager release cuda mem 8399996
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 35786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:14:54] .eager release cuda mem 8399996 35799
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 35857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:14:54. 35980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:14:54. 36017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:14:54. 36212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:14:54] .eager release cuda mem 625663
 36227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:14:54] .eager release cuda mem 625663 36269
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 83999962022-12-11 22:14:54
. 36310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:14:54. 41798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.324254 secs 
[2022-12-11 22:14:54. 42576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.325134 secs 
[2022-12-11 22:14:54. 42979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.325279 secs 
[2022-12-11 22:14:54. 47358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.330107 secs 
[2022-12-11 22:14:54. 47776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.330572 secs 
[2022-12-11 22:14:54. 48171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.33045 secs 
[2022-12-11 22:14:54. 48577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.330999 secs 
[2022-12-11 22:14:54. 48987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.346999 secs 
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][tid #139679961696000]: replica 4 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][tid #139679617758976]: replica 0 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][tid #139679508719360]: replica 6 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][tid #139679970088704]: replica 1 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679617758976]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679970088704]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679961696000]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679508719360]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:14:54.049][ERROR][RK0][main]: init per replica done
[HCTR][22:14:54.049][ERROR][RK0][main]: init per replica done
[HCTR][22:14:54.049][ERROR][RK0][main]: init per replica done
[HCTR][22:14:54.049][ERROR][RK0][main]: init per replica done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679970088704]: init per replica done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679961696000]: init per replica done
[HCTR][22:14:54.049][ERROR][RK0][tid #139679508719360]: init per replica done
[HCTR][22:14:54.051][ERROR][RK0][tid #139679617758976]: init per replica done
