2022-12-11 19:05:20.294146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.302685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.308784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.313376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.325811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.331945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.337836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.349727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.402455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.406340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.407927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.408936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.409891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.411206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.413052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.413869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.414408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.415712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.415836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.417430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.417483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.419025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.419077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.420609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.420706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.422338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.422545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.423940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.424354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.425451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.426094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.426985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.429071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.430190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.431220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.432286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.433342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.434414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.435484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.436513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.441135: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.444466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.445950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.447509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.447568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.449001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.449119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.450507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.450646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.450722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.452501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.452656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.452924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.454995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.455086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.455491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.458004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.458534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.458854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.461982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.462536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.462875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.465195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.466011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.466450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.466726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.468934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.469553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.469826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.471888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.472313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.472386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.472816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.473880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.475014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.475220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.475687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.475948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.477087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.478292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.478332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.478821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.479124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.480451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.481271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.481731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.482061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.482477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.483537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.484114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.484978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.485193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.498985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.500232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.501513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.502296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.502418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.504214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.504370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.505603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.512383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.528561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.544306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.544349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.545196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.545398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.545470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.546316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.547954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.548120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.549443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.549799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.549937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.550512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.552735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.552926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.553673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.554086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.555119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.555759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.557365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.557414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.557998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.558544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.559048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.559601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.561238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.561321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.562596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.563079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.563423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.563962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.565807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.565888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.566895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.567424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.567760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.568174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.570001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.570144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.570608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.571613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.571956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.572642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.574179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.574261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.575097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.575513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.576068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.576638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.578353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.578762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.578935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.579336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.579892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.580489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.582356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.582880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.583016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.583865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.584813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.585771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.587641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.588057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.588061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.588485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.589224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.589876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.591588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.591993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.592058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.592150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.592614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.593923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.594637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.596447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.597060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.597070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.597155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.597561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.598803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.599618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.601367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.601730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.601806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.602079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.602647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.603184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.604045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.605594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.606334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.606457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.606800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.607155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.607594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.608498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.610282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.610649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.611406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.611797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.612370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.613081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.613483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.615965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.616519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.617601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.617760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.618198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.619268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.620412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.620424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.621485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.621663: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.621741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.622081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.623339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.624229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.624427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.625370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.625562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.625862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.627210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.628129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.628286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.629323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.629465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.629982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.631146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.631376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.632537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.632745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.635067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.635297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.635868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.637096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.637258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.637954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.638145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.639895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.640116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.640423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.642271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.642321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.643399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.643513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.645317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.645554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.645726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.646886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.647838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.648063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.650912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.651346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.651777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.653823: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.653872: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.653924: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.655158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.657402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.658566: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.659499: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.660255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.662270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.663842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.663888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.664004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.666176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.667925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.668006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.668149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.668333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.668943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.669676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.672286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.672483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.672644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.672914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.673455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.674149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.677370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.684808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.685180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.727665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.733319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.770577: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:05:20.780409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.786634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:20.792293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.796598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.797226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.797762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.798234: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:21.798289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:05:21.816394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.817262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.818723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.819364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.820026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:21.820642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:05:21.866243: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:21.866469: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:21.920462: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:05:22.040996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.041622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.042215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.042718: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.042773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:05:22.060720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.061372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.061888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.062463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.063344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.063837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:05:22.103041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.103045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.104149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.104188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.105154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.105194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.106095: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.106151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:05:22.106196: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.106243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:05:22.111629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.112266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.112808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.113281: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.113334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:05:22.122089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.122715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.123253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.123743: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.123794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:05:22.124173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.124824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.124956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.125892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.125999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.127025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.127315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.128146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.128458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.129015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:05:22.129309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.129791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:05:22.131266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.131894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.132315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.132540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.133361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.133812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.134617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.135007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.135474: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.135541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:05:22.135812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:05:22.140907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.141536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.142163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.142905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.143489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.143966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:05:22.152663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.153313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.153828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.154416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.154942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.155461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:05:22.156868: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.157060: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.157596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.158198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.158729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.158869: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:05:22.159218: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:05:22.159272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:05:22.174749: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.174963: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.175736: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.175933: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.176563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.176796: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 19:05:22.177246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.177728: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 19:05:22.177769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.178357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.178885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:05:22.179373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:05:22.189542: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.189705: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.191532: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:05:22.200998: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.201194: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.202956: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:05:22.224951: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.225127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.226916: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:05:22.230508: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.230692: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:05:22.232334: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][19:05:23.495][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.495][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.495][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.495][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.495][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.496][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.543][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:05:23.566][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.69it/s]warmup run: 1it [00:00,  2.62it/s]warmup run: 1it [00:00,  2.56it/s]warmup run: 1it [00:00,  2.55it/s]warmup run: 68it [00:00, 187.15it/s]warmup run: 66it [00:00, 178.65it/s]warmup run: 63it [00:00, 166.86it/s]warmup run: 61it [00:00, 161.88it/s]warmup run: 125it [00:00, 294.33it/s]warmup run: 121it [00:00, 281.94it/s]warmup run: 111it [00:00, 253.02it/s]warmup run: 109it [00:00, 248.89it/s]warmup run: 182it [00:00, 372.01it/s]warmup run: 177it [00:00, 360.07it/s]warmup run: 159it [00:00, 316.36it/s]warmup run: 159it [00:00, 318.48it/s]warmup run: 238it [00:00, 426.15it/s]warmup run: 234it [00:00, 418.87it/s]warmup run: 208it [00:00, 365.43it/s]warmup run: 208it [00:00, 365.84it/s]warmup run: 295it [00:00, 466.89it/s]warmup run: 288it [00:00, 453.65it/s]warmup run: 257it [00:00, 400.47it/s]warmup run: 257it [00:00, 400.83it/s]warmup run: 352it [00:00, 497.12it/s]warmup run: 345it [00:00, 485.91it/s]warmup run: 305it [00:00, 423.09it/s]warmup run: 305it [00:00, 423.50it/s]warmup run: 410it [00:01, 519.73it/s]warmup run: 402it [00:01, 508.21it/s]warmup run: 354it [00:01, 440.32it/s]warmup run: 353it [00:01, 440.02it/s]warmup run: 468it [00:01, 535.31it/s]warmup run: 459it [00:01, 524.79it/s]warmup run: 403it [00:01, 452.96it/s]warmup run: 402it [00:01, 452.56it/s]warmup run: 525it [00:01, 542.90it/s]warmup run: 515it [00:01, 529.55it/s]warmup run: 454it [00:01, 468.73it/s]warmup run: 453it [00:01, 468.71it/s]warmup run: 582it [00:01, 550.83it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 570it [00:01, 532.20it/s]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 1it [00:01,  1.39s/it]warmup run: 505it [00:01, 481.64it/s]warmup run: 505it [00:01, 477.85it/s]warmup run: 639it [00:01, 555.56it/s]warmup run: 98it [00:01, 91.12it/s]warmup run: 94it [00:01, 87.41it/s]warmup run: 626it [00:01, 538.14it/s]warmup run: 96it [00:01, 89.11it/s]warmup run: 95it [00:01, 88.06it/s]warmup run: 556it [00:01, 488.40it/s]warmup run: 554it [00:01, 472.12it/s]warmup run: 697it [00:01, 560.89it/s]warmup run: 195it [00:01, 194.37it/s]warmup run: 191it [00:01, 190.98it/s]warmup run: 683it [00:01, 544.88it/s]warmup run: 192it [00:01, 191.17it/s]warmup run: 194it [00:01, 193.66it/s]warmup run: 606it [00:01, 483.28it/s]warmup run: 603it [00:01, 463.84it/s]warmup run: 754it [00:01, 562.53it/s]warmup run: 294it [00:01, 308.33it/s]warmup run: 288it [00:01, 302.89it/s]warmup run: 286it [00:01, 298.23it/s]warmup run: 293it [00:01, 307.60it/s]warmup run: 655it [00:01, 484.96it/s]warmup run: 739it [00:01, 527.35it/s]warmup run: 651it [00:01, 465.95it/s]warmup run: 811it [00:01, 563.56it/s]warmup run: 391it [00:01, 420.89it/s]warmup run: 384it [00:01, 414.69it/s]warmup run: 379it [00:01, 405.07it/s]warmup run: 391it [00:01, 421.43it/s]warmup run: 706it [00:01, 489.89it/s]warmup run: 794it [00:01, 532.69it/s]warmup run: 700it [00:01, 470.78it/s]warmup run: 868it [00:01, 562.64it/s]warmup run: 487it [00:01, 526.20it/s]warmup run: 479it [00:01, 519.22it/s]warmup run: 473it [00:01, 508.75it/s]warmup run: 489it [00:01, 529.69it/s]warmup run: 756it [00:01, 492.87it/s]warmup run: 849it [00:01, 537.45it/s]warmup run: 756it [00:01, 495.82it/s]warmup run: 925it [00:01, 561.53it/s]warmup run: 585it [00:01, 624.37it/s]warmup run: 574it [00:01, 613.21it/s]warmup run: 568it [00:01, 603.62it/s]warmup run: 588it [00:01, 629.08it/s]warmup run: 806it [00:02, 491.46it/s]warmup run: 904it [00:02, 531.87it/s]warmup run: 813it [00:02, 516.09it/s]warmup run: 982it [00:02, 563.33it/s]warmup run: 683it [00:02, 708.45it/s]warmup run: 669it [00:02, 692.91it/s]warmup run: 664it [00:02, 686.81it/s]warmup run: 685it [00:02, 708.65it/s]warmup run: 857it [00:02, 494.86it/s]warmup run: 960it [00:02, 539.24it/s]warmup run: 870it [00:02, 531.10it/s]warmup run: 782it [00:02, 778.44it/s]warmup run: 1039it [00:02, 556.92it/s]warmup run: 764it [00:02, 757.44it/s]warmup run: 760it [00:02, 753.69it/s]warmup run: 780it [00:02, 769.51it/s]warmup run: 907it [00:02, 485.87it/s]warmup run: 1017it [00:02, 541.74it/s]warmup run: 927it [00:02, 540.98it/s]warmup run: 881it [00:02, 834.35it/s]warmup run: 1095it [00:02, 551.22it/s]warmup run: 860it [00:02, 810.33it/s]warmup run: 856it [00:02, 806.94it/s]warmup run: 875it [00:02, 816.31it/s]warmup run: 958it [00:02, 491.52it/s]warmup run: 1074it [00:02, 547.84it/s]warmup run: 982it [00:02, 541.24it/s]warmup run: 978it [00:02, 866.87it/s]warmup run: 959it [00:02, 857.64it/s]warmup run: 950it [00:02, 842.88it/s]warmup run: 973it [00:02, 860.88it/s]warmup run: 1151it [00:02, 529.31it/s]warmup run: 1008it [00:02, 493.44it/s]warmup run: 1129it [00:02, 540.02it/s]warmup run: 1037it [00:02, 524.06it/s]warmup run: 1076it [00:02, 897.64it/s]warmup run: 1056it [00:02, 889.07it/s]warmup run: 1050it [00:02, 885.14it/s]warmup run: 1075it [00:02, 903.81it/s]warmup run: 1205it [00:02, 521.60it/s]warmup run: 1058it [00:02, 491.48it/s]warmup run: 1184it [00:02, 532.40it/s]warmup run: 1090it [00:02, 511.75it/s]warmup run: 1176it [00:02, 924.30it/s]warmup run: 1176it [00:02, 932.13it/s]warmup run: 1146it [00:02, 902.91it/s]warmup run: 1262it [00:02, 534.98it/s]warmup run: 1111it [00:02, 501.99it/s]warmup run: 1152it [00:02, 821.55it/s]warmup run: 1238it [00:02, 524.24it/s]warmup run: 1142it [00:02, 507.27it/s]warmup run: 1276it [00:02, 944.86it/s]warmup run: 1242it [00:02, 914.94it/s]warmup run: 1319it [00:02, 544.77it/s]warmup run: 1275it [00:02, 904.63it/s]warmup run: 1166it [00:02, 515.29it/s]warmup run: 1291it [00:02, 519.86it/s]warmup run: 1198it [00:02, 519.88it/s]warmup run: 1375it [00:02, 952.37it/s]warmup run: 1240it [00:02, 710.72it/s]warmup run: 1374it [00:02, 543.28it/s]warmup run: 1218it [00:02, 509.03it/s]warmup run: 1337it [00:02, 836.56it/s]warmup run: 1344it [00:02, 498.93it/s]warmup run: 1251it [00:02, 504.84it/s]warmup run: 1370it [00:02, 758.48it/s]warmup run: 1429it [00:02, 538.12it/s]warmup run: 1269it [00:02, 504.09it/s]warmup run: 1318it [00:02, 658.64it/s]warmup run: 1473it [00:02, 788.54it/s]warmup run: 1395it [00:02, 491.83it/s]warmup run: 1302it [00:02, 497.16it/s]warmup run: 1425it [00:03, 722.33it/s]warmup run: 1484it [00:03, 540.86it/s]warmup run: 1320it [00:03, 501.31it/s]warmup run: 1453it [00:03, 670.53it/s]warmup run: 1389it [00:03, 631.86it/s]warmup run: 1447it [00:03, 498.56it/s]warmup run: 1352it [00:03, 491.84it/s]warmup run: 1559it [00:03, 710.72it/s]warmup run: 1540it [00:03, 544.41it/s]warmup run: 1376it [00:03, 517.49it/s]warmup run: 1503it [00:03, 658.57it/s]warmup run: 1497it [00:03, 496.32it/s]warmup run: 1402it [00:03, 486.40it/s]warmup run: 1456it [00:03, 614.73it/s]warmup run: 1526it [00:03, 636.52it/s]warmup run: 1595it [00:03, 544.85it/s]warmup run: 1431it [00:03, 524.65it/s]warmup run: 1636it [00:03, 668.25it/s]warmup run: 1548it [00:03, 498.47it/s]warmup run: 1452it [00:03, 489.10it/s]warmup run: 1573it [00:03, 624.23it/s]warmup run: 1520it [00:03, 602.08it/s]warmup run: 1594it [00:03, 614.23it/s]warmup run: 1650it [00:03, 541.13it/s]warmup run: 1484it [00:03, 521.21it/s]warmup run: 1598it [00:03, 495.62it/s]warmup run: 1707it [00:03, 639.91it/s]warmup run: 1503it [00:03, 492.60it/s]warmup run: 1639it [00:03, 607.50it/s]warmup run: 1582it [00:03, 591.12it/s]warmup run: 1658it [00:03, 598.39it/s]warmup run: 1705it [00:03, 543.38it/s]warmup run: 1537it [00:03, 518.87it/s]warmup run: 1648it [00:03, 493.11it/s]warmup run: 1556it [00:03, 503.08it/s]warmup run: 1774it [00:03, 618.51it/s]warmup run: 1642it [00:03, 584.46it/s]warmup run: 1702it [00:03, 588.35it/s]warmup run: 1762it [00:03, 549.12it/s]warmup run: 1720it [00:03, 595.21it/s]warmup run: 1592it [00:03, 525.77it/s]warmup run: 1698it [00:03, 491.64it/s]warmup run: 1608it [00:03, 507.58it/s]warmup run: 1701it [00:03, 579.92it/s]warmup run: 1838it [00:03, 590.34it/s]warmup run: 1762it [00:03, 576.37it/s]warmup run: 1819it [00:03, 554.52it/s]warmup run: 1781it [00:03, 586.02it/s]warmup run: 1648it [00:03, 533.29it/s]warmup run: 1748it [00:03, 493.40it/s]warmup run: 1659it [00:03, 503.81it/s]warmup run: 1760it [00:03, 576.46it/s]warmup run: 1875it [00:03, 554.64it/s]warmup run: 1821it [00:03, 566.40it/s]warmup run: 1899it [00:03, 563.26it/s]warmup run: 1705it [00:03, 543.02it/s]warmup run: 1841it [00:03, 578.05it/s]warmup run: 1799it [00:03, 496.58it/s]warmup run: 1710it [00:03, 500.95it/s]warmup run: 1818it [00:03, 573.33it/s]warmup run: 1931it [00:03, 552.33it/s]warmup run: 1879it [00:03, 558.42it/s]warmup run: 1762it [00:03, 549.77it/s]warmup run: 1957it [00:03, 558.00it/s]warmup run: 1901it [00:03, 581.62it/s]warmup run: 1850it [00:03, 499.27it/s]warmup run: 1761it [00:03, 495.58it/s]warmup run: 1876it [00:03, 571.33it/s]warmup run: 1987it [00:03, 553.66it/s]warmup run: 1819it [00:03, 554.21it/s]warmup run: 1936it [00:03, 550.84it/s]warmup run: 1961it [00:03, 584.13it/s]warmup run: 2014it [00:03, 552.80it/s]warmup run: 1901it [00:03, 500.29it/s]warmup run: 1811it [00:04, 491.82it/s]warmup run: 1934it [00:04, 569.46it/s]warmup run: 2043it [00:04, 552.58it/s]warmup run: 1875it [00:04, 554.58it/s]warmup run: 1993it [00:04, 553.70it/s]warmup run: 2020it [00:04, 584.81it/s]warmup run: 2070it [00:04, 547.48it/s]warmup run: 1952it [00:04, 499.40it/s]warmup run: 1861it [00:04, 478.87it/s]warmup run: 1992it [00:04, 569.98it/s]warmup run: 2100it [00:04, 556.91it/s]warmup run: 1932it [00:04, 556.76it/s]warmup run: 2050it [00:04, 557.62it/s]warmup run: 2079it [00:04, 584.65it/s]warmup run: 2125it [00:04, 544.27it/s]warmup run: 2002it [00:04, 498.78it/s]warmup run: 1916it [00:04, 498.04it/s]warmup run: 2156it [00:04, 555.06it/s]warmup run: 2050it [00:04, 552.92it/s]warmup run: 1989it [00:04, 558.95it/s]warmup run: 2138it [00:04, 576.94it/s]warmup run: 2106it [00:04, 534.42it/s]warmup run: 2180it [00:04, 535.42it/s]warmup run: 2054it [00:04, 503.78it/s]warmup run: 1972it [00:04, 515.55it/s]warmup run: 2213it [00:04, 558.22it/s]warmup run: 2106it [00:04, 542.36it/s]warmup run: 2046it [00:04, 561.35it/s]warmup run: 2196it [00:04, 572.12it/s]warmup run: 2162it [00:04, 541.09it/s]warmup run: 2234it [00:04, 535.28it/s]warmup run: 2108it [00:04, 513.72it/s]warmup run: 2029it [00:04, 529.61it/s]warmup run: 2270it [00:04, 561.63it/s]warmup run: 2161it [00:04, 543.34it/s]warmup run: 2104it [00:04, 563.90it/s]warmup run: 2254it [00:04, 572.22it/s]warmup run: 2217it [00:04, 542.87it/s]warmup run: 2291it [00:04, 544.90it/s]warmup run: 2163it [00:04, 522.58it/s]warmup run: 2086it [00:04, 540.19it/s]warmup run: 2328it [00:04, 565.39it/s]warmup run: 2217it [00:04, 546.79it/s]warmup run: 2162it [00:04, 566.90it/s]warmup run: 2312it [00:04, 573.16it/s]warmup run: 2346it [00:04, 545.85it/s]warmup run: 2272it [00:04, 526.77it/s]warmup run: 2216it [00:04, 524.53it/s]warmup run: 2144it [00:04, 549.06it/s]warmup run: 2385it [00:04, 563.94it/s]warmup run: 2272it [00:04, 540.63it/s]warmup run: 2219it [00:04, 564.37it/s]warmup run: 2370it [00:04, 573.15it/s]warmup run: 2326it [00:04, 527.83it/s]warmup run: 2401it [00:04, 526.01it/s]warmup run: 2269it [00:04, 507.66it/s]warmup run: 2199it [00:04, 539.27it/s]warmup run: 2442it [00:04, 559.35it/s]warmup run: 2279it [00:04, 573.69it/s]warmup run: 2327it [00:04, 532.85it/s]warmup run: 2428it [00:04, 570.14it/s]warmup run: 2379it [00:04, 520.36it/s]warmup run: 2454it [00:04, 505.20it/s]warmup run: 2320it [00:04, 497.84it/s]warmup run: 2256it [00:04, 546.94it/s]warmup run: 2498it [00:04, 552.20it/s]warmup run: 2337it [00:04, 573.50it/s]warmup run: 2381it [00:04, 530.50it/s]warmup run: 2486it [00:04, 559.27it/s]warmup run: 2506it [00:04, 507.25it/s]warmup run: 2432it [00:04, 501.97it/s]warmup run: 2370it [00:04, 498.23it/s]warmup run: 2312it [00:04, 548.09it/s]warmup run: 2554it [00:04, 539.80it/s]warmup run: 2435it [00:04, 532.56it/s]warmup run: 2395it [00:04, 556.13it/s]warmup run: 2543it [00:04, 561.69it/s]warmup run: 2558it [00:05, 510.79it/s]warmup run: 2483it [00:05, 492.80it/s]warmup run: 2420it [00:05, 492.79it/s]warmup run: 2367it [00:05, 526.64it/s]warmup run: 2609it [00:05, 540.98it/s]warmup run: 2489it [00:05, 526.88it/s]warmup run: 2603it [00:05, 570.81it/s]warmup run: 2451it [00:05, 545.19it/s]warmup run: 2615it [00:05, 527.50it/s]warmup run: 2538it [00:05, 506.76it/s]warmup run: 2470it [00:05, 493.79it/s]warmup run: 2420it [00:05, 512.41it/s]warmup run: 2542it [00:05, 526.07it/s]warmup run: 2506it [00:05, 536.95it/s]warmup run: 2661it [00:05, 561.56it/s]warmup run: 2590it [00:05, 510.36it/s]warmup run: 2522it [00:05, 499.38it/s]warmup run: 2474it [00:05, 518.10it/s]warmup run: 2595it [00:05, 518.07it/s]warmup run: 2561it [00:05, 538.87it/s]warmup run: 2643it [00:05, 514.41it/s]warmup run: 2574it [00:05, 504.51it/s]warmup run: 2531it [00:05, 530.76it/s]warmup run: 2664it [00:05, 343.34it/s]warmup run: 2647it [00:05, 510.61it/s]warmup run: 2618it [00:05, 545.80it/s]warmup run: 2672it [00:05, 346.33it/s]warmup run: 2625it [00:05, 504.51it/s]warmup run: 2587it [00:05, 537.77it/s]warmup run: 2716it [00:05, 379.12it/s]warmup run: 2718it [00:05, 355.25it/s]warmup run: 2730it [00:05, 394.82it/s]warmup run: 2771it [00:05, 416.58it/s]warmup run: 2770it [00:05, 388.68it/s]warmup run: 2788it [00:05, 436.29it/s]warmup run: 2695it [00:05, 329.17it/s]warmup run: 2699it [00:05, 339.43it/s]warmup run: 2824it [00:05, 443.81it/s]warmup run: 2822it [00:05, 417.20it/s]warmup run: 2839it [00:05, 452.78it/s]warmup run: 2752it [00:05, 378.78it/s]warmup run: 2676it [00:05, 319.28it/s]warmup run: 2673it [00:05, 321.99it/s]warmup run: 2641it [00:05, 353.98it/s]warmup run: 2753it [00:05, 381.81it/s]warmup run: 2876it [00:05, 462.10it/s]warmup run: 2873it [00:05, 437.98it/s]warmup run: 2890it [00:05, 463.79it/s]warmup run: 2811it [00:05, 425.99it/s]warmup run: 2726it [00:05, 355.94it/s]warmup run: 2729it [00:05, 368.25it/s]warmup run: 2692it [00:05, 386.14it/s]warmup run: 2806it [00:05, 415.83it/s]warmup run: 2932it [00:05, 488.15it/s]warmup run: 2925it [00:05, 457.55it/s]warmup run: 2946it [00:05, 488.92it/s]warmup run: 2866it [00:05, 456.16it/s]warmup run: 2776it [00:05, 388.98it/s]warmup run: 2786it [00:05, 411.19it/s]warmup run: 2740it [00:05, 408.07it/s]warmup run: 2861it [00:05, 447.78it/s]warmup run: 2987it [00:05, 504.09it/s]warmup run: 2975it [00:05, 468.28it/s]warmup run: 3000it [00:06, 499.48it/s]warmup run: 3000it [00:06, 499.33it/s]warmup run: 2920it [00:06, 477.70it/s]warmup run: 2824it [00:06, 411.49it/s]warmup run: 2842it [00:06, 446.29it/s]warmup run: 2789it [00:06, 427.74it/s]warmup run: 3000it [00:06, 496.54it/s]warmup run: 2915it [00:06, 469.89it/s]warmup run: 2976it [00:06, 498.29it/s]warmup run: 2871it [00:06, 426.40it/s]warmup run: 2897it [00:06, 472.51it/s]warmup run: 2847it [00:06, 466.07it/s]warmup run: 3000it [00:06, 487.19it/s]warmup run: 2971it [00:06, 493.23it/s]warmup run: 2921it [00:06, 444.43it/s]warmup run: 3000it [00:06, 481.94it/s]warmup run: 2950it [00:06, 487.75it/s]warmup run: 2900it [00:06, 482.63it/s]warmup run: 2970it [00:06, 454.33it/s]warmup run: 3000it [00:06, 473.76it/s]warmup run: 2951it [00:06, 486.67it/s]warmup run: 3000it [00:06, 469.24it/s]warmup run: 3000it [00:06, 465.49it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   4%|         | 122/3000 [00:00<00:02, 1217.92it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1689.89it/s]warmup should be done:   4%|         | 123/3000 [00:00<00:02, 1227.62it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1676.02it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.91it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1644.68it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1652.45it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1609.01it/s]warmup should be done:  10%|         | 294/3000 [00:00<00:01, 1512.30it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1680.32it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1702.73it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1686.86it/s]warmup should be done:  10%|         | 294/3000 [00:00<00:01, 1507.19it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1668.88it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1646.12it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1508.55it/s]warmup should be done:  16%|        | 467/3000 [00:00<00:01, 1610.94it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1704.83it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1681.45it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1690.38it/s]warmup should be done:  16%|        | 465/3000 [00:00<00:01, 1596.96it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1674.30it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1636.27it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1602.64it/s]warmup should be done:  21%|       | 639/3000 [00:00<00:01, 1652.70it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1690.79it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1680.42it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1700.70it/s]warmup should be done:  21%|        | 635/3000 [00:00<00:01, 1633.49it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1672.96it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1609.54it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1649.74it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1675.71it/s]warmup should be done:  27%|       | 805/3000 [00:00<00:01, 1656.26it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1674.54it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1678.30it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1697.96it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1615.49it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1651.46it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1675.76it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1685.39it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1674.49it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1671.82it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1677.14it/s]warmup should be done:  34%|      | 1024/3000 [00:00<00:01, 1692.43it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1616.55it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1628.91it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1691.28it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1686.09it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1682.71it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1680.25it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1672.92it/s]warmup should be done:  40%|      | 1194/3000 [00:00<00:01, 1688.84it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1618.42it/s]warmup should be done:  40%|      | 1187/3000 [00:00<00:01, 1698.65it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1616.38it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:00, 1683.77it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1684.62it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:00, 1685.44it/s]warmup should be done:  46%|     | 1366/3000 [00:00<00:00, 1696.26it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1665.99it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1619.85it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1704.65it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:01, 1602.06it/s]warmup should be done:  50%|     | 1489/3000 [00:00<00:00, 1683.37it/s]warmup should be done:  51%|     | 1517/3000 [00:00<00:00, 1687.44it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1690.13it/s]warmup should be done:  51%|     | 1536/3000 [00:00<00:00, 1697.11it/s]warmup should be done:  50%|     | 1514/3000 [00:00<00:00, 1662.41it/s]warmup should be done:  49%|     | 1475/3000 [00:00<00:00, 1620.72it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1707.83it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1598.29it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1681.22it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1692.25it/s]warmup should be done:  56%|    | 1686/3000 [00:01<00:00, 1680.99it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1689.68it/s]warmup should be done:  56%|    | 1681/3000 [00:01<00:00, 1659.70it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1619.72it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1707.04it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1605.24it/s]warmup should be done:  61%|    | 1827/3000 [00:01<00:00, 1678.64it/s]warmup should be done:  61%|    | 1828/3000 [00:01<00:00, 1690.64it/s]warmup should be done:  62%|   | 1856/3000 [00:01<00:00, 1684.34it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1686.72it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1657.34it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1620.75it/s]warmup should be done:  62%|   | 1874/3000 [00:01<00:00, 1707.89it/s]warmup should be done:  61%|    | 1831/3000 [00:01<00:00, 1623.09it/s]warmup should be done:  67%|   | 1996/3000 [00:01<00:00, 1679.70it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1693.39it/s]warmup should be done:  68%|   | 2026/3000 [00:01<00:00, 1687.73it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1656.88it/s]warmup should be done:  68%|   | 2044/3000 [00:01<00:00, 1683.85it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1609.54it/s]warmup should be done:  68%|   | 2046/3000 [00:01<00:00, 1709.57it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1635.89it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1679.64it/s]warmup should be done:  72%|  | 2169/3000 [00:01<00:00, 1696.97it/s]warmup should be done:  73%|  | 2196/3000 [00:01<00:00, 1690.03it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1656.44it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1683.42it/s]warmup should be done:  71%|   | 2127/3000 [00:01<00:00, 1615.37it/s]warmup should be done:  74%|  | 2217/3000 [00:01<00:00, 1709.50it/s]warmup should be done:  72%|  | 2165/3000 [00:01<00:00, 1645.32it/s]warmup should be done:  78%|  | 2339/3000 [00:01<00:00, 1697.77it/s]warmup should be done:  78%|  | 2332/3000 [00:01<00:00, 1676.58it/s]warmup should be done:  79%|  | 2366/3000 [00:01<00:00, 1691.15it/s]warmup should be done:  78%|  | 2345/3000 [00:01<00:00, 1657.44it/s]warmup should be done:  79%|  | 2382/3000 [00:01<00:00, 1680.21it/s]warmup should be done:  76%|  | 2290/3000 [00:01<00:00, 1617.55it/s]warmup should be done:  80%|  | 2388/3000 [00:01<00:00, 1706.26it/s]warmup should be done:  78%|  | 2330/3000 [00:01<00:00, 1646.36it/s]warmup should be done:  84%| | 2509/3000 [00:01<00:00, 1698.21it/s]warmup should be done:  83%| | 2500/3000 [00:01<00:00, 1677.53it/s]warmup should be done:  85%| | 2536/3000 [00:01<00:00, 1691.25it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1681.92it/s]warmup should be done:  84%| | 2511/3000 [00:01<00:00, 1647.30it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1619.60it/s]warmup should be done:  85%| | 2559/3000 [00:01<00:00, 1698.72it/s]warmup should be done:  83%| | 2497/3000 [00:01<00:00, 1652.68it/s]warmup should be done:  89%| | 2668/3000 [00:01<00:00, 1676.94it/s]warmup should be done:  90%| | 2706/3000 [00:01<00:00, 1691.56it/s]warmup should be done:  89%| | 2679/3000 [00:01<00:00, 1686.29it/s]warmup should be done:  89%| | 2678/3000 [00:01<00:00, 1653.24it/s]warmup should be done:  91%| | 2720/3000 [00:01<00:00, 1678.43it/s]warmup should be done:  87%| | 2616/3000 [00:01<00:00, 1619.71it/s]warmup should be done:  91%| | 2729/3000 [00:01<00:00, 1692.51it/s]warmup should be done:  89%| | 2664/3000 [00:01<00:00, 1657.73it/s]warmup should be done:  95%|| 2836/3000 [00:01<00:00, 1674.33it/s]warmup should be done:  96%|| 2877/3000 [00:01<00:00, 1694.79it/s]warmup should be done:  95%|| 2850/3000 [00:01<00:00, 1691.65it/s]warmup should be done:  95%|| 2846/3000 [00:01<00:00, 1660.18it/s]warmup should be done:  96%|| 2889/3000 [00:01<00:00, 1681.31it/s]warmup should be done:  93%|| 2779/3000 [00:01<00:00, 1621.33it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1690.16it/s]warmup should be done:  94%|| 2831/3000 [00:01<00:00, 1660.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1684.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1684.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1670.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.73it/s]warmup should be done:  98%|| 2943/3000 [00:01<00:00, 1624.28it/s]warmup should be done: 100%|| 2999/3000 [00:01<00:00, 1663.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.76it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.82it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1729.76it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1729.50it/s]warmup should be done:   6%|         | 175/3000 [00:00<00:01, 1748.83it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.03it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1726.40it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.47it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1715.52it/s]warmup should be done:   6%|         | 175/3000 [00:00<00:01, 1741.61it/s]warmup should be done:  12%|        | 347/3000 [00:00<00:01, 1735.66it/s]warmup should be done:  12%|        | 346/3000 [00:00<00:01, 1728.21it/s]warmup should be done:  12%|        | 345/3000 [00:00<00:01, 1723.44it/s]warmup should be done:  12%|        | 347/3000 [00:00<00:01, 1731.88it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.17it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1711.66it/s]warmup should be done:  12%|        | 351/3000 [00:00<00:01, 1749.96it/s]warmup should be done:  12%|        | 351/3000 [00:00<00:01, 1748.78it/s]warmup should be done:  17%|        | 521/3000 [00:00<00:01, 1736.60it/s]warmup should be done:  17%|        | 522/3000 [00:00<00:01, 1738.63it/s]warmup should be done:  17%|        | 515/3000 [00:00<00:01, 1714.94it/s]warmup should be done:  17%|        | 521/3000 [00:00<00:01, 1732.90it/s]warmup should be done:  17%|        | 520/3000 [00:00<00:01, 1731.04it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1681.58it/s]warmup should be done:  18%|        | 527/3000 [00:00<00:01, 1749.82it/s]warmup should be done:  18%|        | 526/3000 [00:00<00:01, 1741.66it/s]warmup should be done:  23%|       | 697/3000 [00:00<00:01, 1742.39it/s]warmup should be done:  23%|       | 696/3000 [00:00<00:01, 1735.61it/s]warmup should be done:  23%|       | 687/3000 [00:00<00:01, 1714.04it/s]warmup should be done:  23%|       | 694/3000 [00:00<00:01, 1732.81it/s]warmup should be done:  23%|       | 696/3000 [00:00<00:01, 1735.96it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1679.35it/s]warmup should be done:  23%|       | 703/3000 [00:00<00:01, 1750.23it/s]warmup should be done:  23%|       | 701/3000 [00:00<00:01, 1730.02it/s]warmup should be done:  29%|       | 873/3000 [00:00<00:01, 1746.62it/s]warmup should be done:  29%|       | 870/3000 [00:00<00:01, 1737.00it/s]warmup should be done:  29%|       | 870/3000 [00:00<00:01, 1734.21it/s]warmup should be done:  29%|       | 868/3000 [00:00<00:01, 1732.66it/s]warmup should be done:  29%|       | 859/3000 [00:00<00:01, 1711.43it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1677.04it/s]warmup should be done:  29%|       | 879/3000 [00:00<00:01, 1749.27it/s]warmup should be done:  29%|       | 875/3000 [00:00<00:01, 1721.71it/s]warmup should be done:  35%|      | 1044/3000 [00:00<00:01, 1736.64it/s]warmup should be done:  35%|      | 1042/3000 [00:00<00:01, 1734.12it/s]warmup should be done:  35%|      | 1045/3000 [00:00<00:01, 1736.36it/s]warmup should be done:  34%|      | 1031/3000 [00:00<00:01, 1712.52it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1679.00it/s]warmup should be done:  35%|      | 1054/3000 [00:00<00:01, 1748.60it/s]warmup should be done:  35%|      | 1048/3000 [00:00<00:01, 1719.75it/s]warmup should be done:  35%|      | 1048/3000 [00:00<00:01, 1717.04it/s]warmup should be done:  41%|      | 1216/3000 [00:00<00:01, 1735.07it/s]warmup should be done:  41%|      | 1218/3000 [00:00<00:01, 1734.17it/s]warmup should be done:  40%|      | 1203/3000 [00:00<00:01, 1712.80it/s]warmup should be done:  41%|      | 1219/3000 [00:00<00:01, 1734.65it/s]warmup should be done:  39%|      | 1180/3000 [00:00<00:01, 1677.93it/s]warmup should be done:  41%|      | 1229/3000 [00:00<00:01, 1747.76it/s]warmup should be done:  41%|      | 1223/3000 [00:00<00:01, 1729.18it/s]warmup should be done:  41%|      | 1220/3000 [00:00<00:01, 1715.22it/s]warmup should be done:  46%|     | 1392/3000 [00:00<00:00, 1741.00it/s]warmup should be done:  46%|     | 1392/3000 [00:00<00:00, 1734.13it/s]warmup should be done:  46%|     | 1376/3000 [00:00<00:00, 1715.25it/s]warmup should be done:  46%|     | 1394/3000 [00:00<00:00, 1735.88it/s]warmup should be done:  47%|     | 1405/3000 [00:00<00:00, 1749.45it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1678.69it/s]warmup should be done:  47%|     | 1400/3000 [00:00<00:00, 1741.99it/s]warmup should be done:  46%|     | 1393/3000 [00:00<00:00, 1717.16it/s]warmup should be done:  52%|    | 1569/3000 [00:00<00:00, 1747.27it/s]warmup should be done:  52%|    | 1566/3000 [00:00<00:00, 1732.34it/s]warmup should be done:  53%|    | 1580/3000 [00:00<00:00, 1749.22it/s]warmup should be done:  52%|    | 1549/3000 [00:00<00:00, 1717.35it/s]warmup should be done:  51%|     | 1517/3000 [00:00<00:00, 1678.23it/s]warmup should be done:  52%|    | 1569/3000 [00:00<00:00, 1737.37it/s]warmup should be done:  53%|    | 1577/3000 [00:00<00:00, 1750.50it/s]warmup should be done:  52%|    | 1565/3000 [00:00<00:00, 1715.41it/s]warmup should be done:  58%|    | 1745/3000 [00:01<00:00, 1748.80it/s]warmup should be done:  57%|    | 1722/3000 [00:01<00:00, 1720.76it/s]warmup should be done:  58%|    | 1740/3000 [00:01<00:00, 1732.10it/s]warmup should be done:  58%|    | 1755/3000 [00:01<00:00, 1747.93it/s]warmup should be done:  56%|    | 1685/3000 [00:01<00:00, 1678.17it/s]warmup should be done:  58%|    | 1747/3000 [00:01<00:00, 1748.45it/s]warmup should be done:  58%|    | 1754/3000 [00:01<00:00, 1755.98it/s]warmup should be done:  58%|    | 1737/3000 [00:01<00:00, 1714.37it/s]warmup should be done:  64%|   | 1921/3000 [00:01<00:00, 1749.82it/s]warmup should be done:  64%|   | 1914/3000 [00:01<00:00, 1733.18it/s]warmup should be done:  63%|   | 1896/3000 [00:01<00:00, 1724.71it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1680.34it/s]warmup should be done:  64%|   | 1931/3000 [00:01<00:00, 1748.82it/s]warmup should be done:  64%|   | 1926/3000 [00:01<00:00, 1759.17it/s]warmup should be done:  64%|   | 1931/3000 [00:01<00:00, 1759.54it/s]warmup should be done:  64%|   | 1909/3000 [00:01<00:00, 1715.17it/s]warmup should be done:  70%|   | 2097/3000 [00:01<00:00, 1751.72it/s]warmup should be done:  70%|   | 2088/3000 [00:01<00:00, 1735.06it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1681.23it/s]warmup should be done:  70%|   | 2107/3000 [00:01<00:00, 1750.65it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1726.42it/s]warmup should be done:  70%|   | 2105/3000 [00:01<00:00, 1767.25it/s]warmup should be done:  70%|   | 2109/3000 [00:01<00:00, 1763.39it/s]warmup should be done:  69%|   | 2081/3000 [00:01<00:00, 1716.48it/s]warmup should be done:  75%|  | 2262/3000 [00:01<00:00, 1734.84it/s]warmup should be done:  76%|  | 2273/3000 [00:01<00:00, 1752.31it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1772.64it/s]warmup should be done:  73%|  | 2192/3000 [00:01<00:00, 1680.66it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1750.01it/s]warmup should be done:  75%|  | 2243/3000 [00:01<00:00, 1716.75it/s]warmup should be done:  76%|  | 2286/3000 [00:01<00:00, 1764.97it/s]warmup should be done:  75%|  | 2253/3000 [00:01<00:00, 1715.58it/s]warmup should be done:  81%|  | 2437/3000 [00:01<00:00, 1739.32it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1752.86it/s]warmup should be done:  82%| | 2462/3000 [00:01<00:00, 1774.47it/s]warmup should be done:  82%| | 2459/3000 [00:01<00:00, 1748.93it/s]warmup should be done:  81%|  | 2417/3000 [00:01<00:00, 1722.43it/s]warmup should be done:  82%| | 2463/3000 [00:01<00:00, 1760.96it/s]warmup should be done:  81%|  | 2425/3000 [00:01<00:00, 1714.66it/s]warmup should be done:  79%|  | 2361/3000 [00:01<00:00, 1596.28it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1747.37it/s]warmup should be done:  88%| | 2625/3000 [00:01<00:00, 1753.97it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1775.46it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1747.38it/s]warmup should be done:  86%| | 2592/3000 [00:01<00:00, 1729.32it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1760.01it/s]warmup should be done:  87%| | 2597/3000 [00:01<00:00, 1715.23it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1610.10it/s]warmup should be done:  93%|| 2792/3000 [00:01<00:00, 1754.27it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1755.87it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1777.26it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1747.42it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1735.47it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1754.32it/s]warmup should be done:  92%|| 2770/3000 [00:01<00:00, 1718.26it/s]warmup should be done:  90%| | 2700/3000 [00:01<00:00, 1646.99it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1759.61it/s]warmup should be done: 100%|| 2997/3000 [00:01<00:00, 1777.88it/s]warmup should be done:  99%|| 2979/3000 [00:01<00:00, 1757.37it/s]warmup should be done:  99%|| 2984/3000 [00:01<00:00, 1747.59it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1739.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1756.72it/s]warmup should be done:  98%|| 2943/3000 [00:01<00:00, 1720.02it/s]warmup should be done: 100%|| 2993/3000 [00:01<00:00, 1750.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1748.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1748.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1746.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1741.29it/s]warmup should be done:  96%|| 2873/3000 [00:01<00:00, 1670.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1723.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1719.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1669.20it/s]2022-12-11 19:06:29.230654: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb906c27690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.230711: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.231253: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb8fefa3460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.231311: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.236202: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb906ee55e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.236249: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.301586: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.302899: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.306942: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.431916: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb906c27670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.431983: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.499780: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.852032: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb906c47670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.852090: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.875388: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb906c23250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.875388: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb906c22ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.875453: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.875461: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.875608: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb90ac47800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:06:29.875661: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:06:29.933717: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.949009: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.959438: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:29.961808: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:06:31.165748: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.180503: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.201076: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.201858: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.494883: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.524947: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.547735: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:06:31.562120: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:06:51.977][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:06:51.978][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:51.991][ERROR][RK0][main]: coll ps creation done
[HCTR][19:06:51.991][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][19:06:52.093][ERROR][RK0][tid #140432814110464]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:06:52.093][ERROR][RK0][tid #140432814110464]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.103][ERROR][RK0][tid #140432814110464]: coll ps creation done
[HCTR][19:06:52.103][ERROR][RK0][tid #140432814110464]: replica 7 waits for coll ps creation barrier
[HCTR][19:06:52.192][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:06:52.192][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.209][ERROR][RK0][main]: coll ps creation done
[HCTR][19:06:52.209][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][19:06:52.214][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:06:52.214][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.223][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:06:52.224][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.228][ERROR][RK0][main]: coll ps creation done
[HCTR][19:06:52.228][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][19:06:52.231][ERROR][RK0][main]: coll ps creation done
[HCTR][19:06:52.231][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][19:06:52.250][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:06:52.251][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.259][ERROR][RK0][main]: coll ps creation done
[HCTR][19:06:52.259][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][19:06:52.266][ERROR][RK0][tid #140433082545920]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:06:52.266][ERROR][RK0][tid #140433082545920]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.274][ERROR][RK0][tid #140433082545920]: coll ps creation done
[HCTR][19:06:52.274][ERROR][RK0][tid #140433082545920]: replica 0 waits for coll ps creation barrier
[HCTR][19:06:52.293][ERROR][RK0][tid #140432939935488]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:06:52.294][ERROR][RK0][tid #140432939935488]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][19:06:52.299][ERROR][RK0][tid #140432939935488]: coll ps creation done
[HCTR][19:06:52.299][ERROR][RK0][tid #140432939935488]: replica 4 waits for coll ps creation barrier
[HCTR][19:06:52.299][ERROR][RK0][tid #140433082545920]: replica 0 preparing frequency
[HCTR][19:06:59.348][ERROR][RK0][tid #140433082545920]: replica 0 preparing frequency done
[HCTR][19:06:59.385][ERROR][RK0][tid #140433082545920]: replica 0 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][tid #140432814110464]: replica 7 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][tid #140432939935488]: replica 4 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][19:06:59.385][ERROR][RK0][tid #140433082545920]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][tid #140432814110464]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][tid #140432939935488]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][tid #140433082545920]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][main]: Calling build_v2
[HCTR][19:06:59.385][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][tid #140432814110464]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][tid #140432939935488]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:06:59.385][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 19:06:592022-12-11 19:06:592022-12-11 19:06:592022-12-11 19:06:592022-12-11 19:06:59.[2022-12-11 19:06:59.2022-12-11 19:06:59...385473.385473.385489385495385495: 2022-12-11 19:06:59385495: 385497: : : E.: E: EEE 385521E E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136 :136:136136136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] 136] ] ] using concurrent impl MPS:] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
136using concurrent impl MPS
using concurrent impl MPS


] 

using concurrent impl MPS
[2022-12-11 19:06:59.389798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:06:59.389836: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59:.196389841] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:06:59.389891[: 2022-12-11 19:06:59E. [389893/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59: :.E[196389913 2022-12-11 19:06:59] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 8 to cpuE:389936
 [178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59] E:.v100x8, slow pcie 212389982
[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 2022-12-11 19:06:59[2022-12-11 19:06:59:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E.2022-12-11 19:06:59.178
[ 390026.390032] 2022-12-11 19:06:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 390049: [v100x8, slow pcie.:[E: E2022-12-11 19:06:59
3900751782022-12-11 19:06:59 E .: [] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390116E2022-12-11 19:06:59v100x8, slow pcie390122:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::  .
: 178:[212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390175E] 1962022-12-11 19:06:59]  ::  v100x8, slow pcie] .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
assigning 8 to cpu390244
:]  :
: [213v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178[E2022-12-11 19:06:59] 
:] 2022-12-11 19:06:59[ .remote time is 8.68421196[v100x8, slow pcie.2022-12-11 19:06:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390312
] 2022-12-11 19:06:59
390328.:: assigning 8 to cpu.[: 390381[196E
3903762022-12-11 19:06:59E: 2022-12-11 19:06:59]  : . E.assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE390434/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 390446
: [: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59E213:E] :. [] 212 assigning 8 to cpu196390514/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] : :.
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:assigning 8 to cpuE214390565
196[
 ] [: ] 2022-12-11 19:06:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.05882022-12-11 19:06:59Eassigning 8 to cpu.:[2022-12-11 19:06:59
. 
3906592122022-12-11 19:06:59.390670/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] .390689: :Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8390708: [E212 
: E2022-12-11 19:06:59 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390788:2022-12-11 19:06:59
214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 212.] :213E[] 390836cpu time is 97.0588212]  2022-12-11 19:06:59build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:390887 
[212: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59] E2022-12-11 19:06:59:[.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 .2132022-12-11 19:06:59390956
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc390962] .: :: remote time is 8.68421390977[E213E
: 2022-12-11 19:06:59 ]  E[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 19:06:59391026:
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 213214[:391058E] ] 2022-12-11 19:06:59213:  remote time is 8.68421cpu time is 97.0588.] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

391100remote time is 8.68421 :: [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213E2022-12-11 19:06:59:]  [.214remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:06:59391172] 
:.: cpu time is 97.0588214391194[E
] : 2022-12-11 19:06:59 cpu time is 97.0588E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 391234:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 214:E] 214 cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-11 19:08:42.878982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:08:43.231051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 19:08:44.637272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:08:44.637342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:08:44.637382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:08:44.637421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:08:44.637948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.638001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.642373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.646437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.770581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 19:08:44.770657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:08:44.771060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.771109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.772272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:08:44.772332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:08:44.772699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.772739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.772874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 19:08:44.772932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-11 19:08:44.2022-12-11 19:08:44773096.: 773099E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 2022 solved] 
4 solved
[2022-12-11 19:08:44.773178: [E2022-12-11 19:08:44 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc773186:: 205E]  worker 0 thread 2 initing device 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 4 initing device 4
[2022-12-11 19:08:44.773291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.773330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.773566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.773606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.773663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.773714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.779949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 19:08:44.780001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 19:08:44.780376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.780415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.780948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:08:44.781019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:08:44.781405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:08:44.781452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.798143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.798219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.798490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.798614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.798714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.806025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.806146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.825055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.825129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.825411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.825487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.825565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.832888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:44.833011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:08:45.312714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 19:08:45.312926: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 19:08:45.330490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:08:45.330635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:08:45.334972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.335752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.343066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.343584: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.441896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.446504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.446562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[2022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:45.......575321575321575321575321575321575321575321: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes






[[[[[[2022-12-11 19:08:45[2022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:452022-12-11 19:08:45.2022-12-11 19:08:45.....575665.575666575665575673575667575669: 575670: : : : : W: WWWWW W     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::1855:18551855185518551855] 1855] ] ] ] ] using empty feat=27] using empty feat=27using empty feat=27using empty feat=27using empty feat=27using empty feat=27
using empty feat=27





[2022-12-11 19:08:45.594464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:08:45.594576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:08:45.594626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 19:08:45.[5947152022-12-11 19:08:45: .E594707 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 3531098340638
] eager release cuda mem 5
[2022-12-11 19:08:45[.2022-12-11 19:08:45594792.: 594812E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5] 
eager release cuda mem 3531098340
[2022-12-11 19:08:45.594863: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:08:45:.638594894] : eager release cuda mem 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[[2022-12-11 19:08:452022-12-11 19:08:45..594955594946: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 3531098340eager release cuda mem 5

[[2022-12-11 19:08:452022-12-11 19:08:45..595023595043: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 3531098340

[2022-12-11 19:08:45.595136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:08:45.598888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.603443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.607744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.612054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.615962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.620046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.623960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:08:45.625134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.625847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.626138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.626239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.626275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.626326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.626387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:45.633610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.633880: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.634248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.634513: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.634565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 19:08:45] .eager release cuda mem 5518079634586
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.634653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.634692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.634751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:45.635198: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.635342: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.635494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.635569: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.635649: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:08:45.723318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.725940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.726262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.727393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.727867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.727912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:08:45.728118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.728539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.730467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.730512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:08:45.730774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.730821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:08:45.730857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:08:45.731896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.731941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:08:45.732651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.732712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:08:45.733030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.733073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:08:45.735386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:08:45.735434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[[2022-12-11 19:08:482022-12-11 19:08:482022-12-11 19:08:482022-12-11 19:08:482022-12-11 19:08:482022-12-11 19:08:482022-12-11 19:08:482022-12-11 19:08:48........967969967969967969967969967969967969967969967974: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] Device 0 init p2p of link 3] ] ] Device 7 init p2p of link 4Device 3 init p2p of link 2Device 4 init p2p of link 5Device 6 init p2p of link 0
Device 5 init p2p of link 6Device 2 init p2p of link 1Device 1 init p2p of link 7






[2022-12-11 19:08:48.968503[: [2022-12-11 19:08:48[[E[2022-12-11 19:08:48.2022-12-11 19:08:482022-12-11 19:08:48[ 2022-12-11 19:08:48.968511..2022-12-11 19:08:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.968511: 968515968516.:968518[: E: : 9685261980: 2022-12-11 19:08:48E EE: ] E. /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  Eeager alloc mem 5.26 MB 968555/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E1980] 19801980:1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 5.26 MB] ] 1980] :eager alloc mem 5.26 MB
eager alloc mem 5.26 MBeager alloc mem 5.26 MB] eager alloc mem 5.26 MB1980


eager alloc mem 5.26 MB
] 
eager alloc mem 5.26 MB
[2022-12-11 19:08:48.977738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.977961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.978019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 19:08:48] .eager release cuda mem 5518079978044
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.978105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.978173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.978212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.978265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:48.995218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 19:08:48.995367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:48.998502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 19:08:48.998642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49.  6359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49.  6743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 14352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 19:08:49. 14510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 15260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 19:08:49. 15420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 15514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 19:08:49. 15641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 7 init p2p of link 12022-12-11 19:08:49
. 15677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 15808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 16647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 19:08:49. 16782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-11 19:08:49Device 2 init p2p of link 3.
 16815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 16951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 20933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 22583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 19:08:49eager release cuda mem 5518079.
 22603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 22804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 24069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 24116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 27508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 19:08:49. 27626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 30847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 19:08:49. 30971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 31232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 19:08:49. 31357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 36545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 19:08:49. 36671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 40745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 41157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 42105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 45124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 53212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 19:08:49. 53346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 53470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 19:08:49. 53599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 54708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 19:08:49. 54838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 54883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 19:08:49. 55018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 60147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 60249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 61832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 61881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 70016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 19:08:49. 70143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 70608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 19:08:49. 70734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 78407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 79241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 79944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 19:08:49. 80064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 82500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 19:08:49. 82626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 84059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 19:08:49. 84318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 85084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 19:08:49. 85209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49. 88198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 90806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 91663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49. 91985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49.105428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 19:08:49.105559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49.106993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 19:08:49.107122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:08:49.112100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49.113631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:08:49.116440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.116483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.117581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.117762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.34444 secs 
[2022-12-11 19:08:49.118045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.34531 secs 
[2022-12-11 19:08:49.118776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.34507 secs 
[2022-12-11 19:08:49.119285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.119735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.34613 secs 
[2022-12-11 19:08:49.131267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.131725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.35132 secs 
[2022-12-11 19:08:49.132723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.133190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.35175 secs 
[2022-12-11 19:08:49.133864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.134322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.49633 secs 
[2022-12-11 19:08:49.134742: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 27.79 GB
[2022-12-11 19:08:49.136085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:08:49.136539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 35310982 / 882774585 nodes ( 4.00 %~4.00 %) | remote 105932946 / 882774585 nodes ( 12.00 %) | cpu 741530657 / 882774585 nodes ( 84.00 %) | 16.88 GB | 4.36545 secs 
[2022-12-11 19:08:50.851858: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.06 GB
[2022-12-11 19:08:50.875924: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.06 GB
[2022-12-11 19:08:50.901459: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.06 GB
[2022-12-11 19:08:52.425025: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.32 GB
[2022-12-11 19:08:52.425234: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.32 GB
[2022-12-11 19:08:52.425584: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.32 GB
[2022-12-11 19:08:54. 14070: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.53 GB
[2022-12-11 19:08:54. 14377: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.53 GB
[2022-12-11 19:08:54. 16090: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.53 GB
[2022-12-11 19:08:55.715962: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 28.75 GB
[2022-12-11 19:08:55.716716: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 28.75 GB
[2022-12-11 19:08:55.717204: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 28.75 GB
[2022-12-11 19:08:57.649726: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.21 GB
[2022-12-11 19:08:57.650856: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.21 GB
[2022-12-11 19:08:57.651426: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 29.21 GB
[2022-12-11 19:08:59.689049: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 29.41 GB
[2022-12-11 19:08:59.689380: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 29.41 GB
[HCTR][19:08:59.777][ERROR][RK0][tid #140433082545920]: replica 0 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][tid #140432814110464]: replica 7 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][tid #140432939935488]: replica 4 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][19:08:59.777][ERROR][RK0][tid #140433082545920]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][tid #140432814110464]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][tid #140432939935488]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:08:59.777][ERROR][RK0][tid #140432814110464]: init per replica done
[HCTR][19:08:59.777][ERROR][RK0][main]: init per replica done
[HCTR][19:08:59.777][ERROR][RK0][tid #140432939935488]: init per replica done
[HCTR][19:08:59.777][ERROR][RK0][main]: init per replica done
[HCTR][19:08:59.777][ERROR][RK0][main]: init per replica done
[HCTR][19:08:59.777][ERROR][RK0][main]: init per replica done
[HCTR][19:08:59.777][ERROR][RK0][main]: init per replica done
[HCTR][19:08:59.796][ERROR][RK0][tid #140433082545920]: init per replica done








