2022-12-12 06:41:22.334116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.343416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.349588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.354768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.367334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.372922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.380087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.391528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.443666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.447602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.449222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.450120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.451122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.452082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.453150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.454218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.455293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.456321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.457374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.458394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.459369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.460297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.461229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.462278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.464106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.465836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.466832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.466995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.468868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.468949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.470559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.470601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.472578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.472706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.474958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.475282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.475866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.476945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.477358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.478070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.479568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.480070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.480983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.481684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.483052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.483363: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.484149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.485230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.486316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.489555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.490707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.491733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.492786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.493814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.494170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.495268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.495704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.497235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.497395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.499562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.500385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.502400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.504080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.505754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.507504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.509649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.511187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.511703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.513735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.514247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.516276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.516670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.531380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.531804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.533558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.534500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.534885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.535450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.535695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.536725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.537952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.538635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.538899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.538953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.545268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.559902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.575001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.576098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.576834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.576965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.576998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.579029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.579178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.579929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.581382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.582095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.582340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.582603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.585191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.585373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.587374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.588579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.588688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.588869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.589465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.591278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.591479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.593837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.594015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.594129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.595197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.595531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.595727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.598194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.598516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.598837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.599685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.599773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.599816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.602478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.603090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.604084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.604126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.604281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.605984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.606568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.607478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.607662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.609394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.609688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.610439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.610528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.612274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.612511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.613214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.613346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.615140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.615442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.616402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.616540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.618145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.618486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.619438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.619494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.621176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.621686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.622371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.622414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.624084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.624745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.625229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.625407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.627596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.627801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.627905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.628277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.630293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.630495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.630602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.631370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.633289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.634257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.634495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.634495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.635779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.636921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.637002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.637453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.637696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.638805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.640158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.640331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.640876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.640927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.642379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.643565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.643703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.644065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.644254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.645768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.646975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.647215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.647225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.647843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.648117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.649708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.651011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.651187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.651286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.651809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.652001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.652499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.655142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.655377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.655886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.656519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.656871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.658324: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.658349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.658774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.659189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.659428: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.659755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.660211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.661903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.662121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.663246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.665069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.665252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.665794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.666385: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.667354: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.667516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.667783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.668299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.669278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.670410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.670432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.670507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.671361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.672571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.673966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.674029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.674040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.674967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.675975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.676898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.677595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.677877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.677934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.678072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.679331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.681917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.682589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.682719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.682759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.684336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.686492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.687104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.687203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.687387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.688905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.692397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.692489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.723325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.726382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.727924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.728368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.731518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.732786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.733639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.737021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.738153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.738679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.746778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.746909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.747172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.753113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.754033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.755187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.757149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.758843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.760266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.761797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.773790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.776220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.789882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.790711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.792246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.795335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.797711: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.808186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.848523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.852417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.881945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.882939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.885839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.886834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.889208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.952097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.953473: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.962214: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:41:22.964148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.970058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.972809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.975269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.978862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:22.985135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.865428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.866047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.866563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.867451: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:23.867520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:41:23.886035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.886681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.887212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.887805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.888320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:23.889040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:41:23.935574: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:23.935789: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:23.971683: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:41:24.122585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.122950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.123415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.123985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.124618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.124984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.125477: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.125542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:41:24.126093: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.126160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:41:24.129191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.129889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.130417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.131108: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.131180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:41:24.135341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.135982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.136736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.137204: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.137263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:41:24.143991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.144069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.145178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.145178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.146155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.146203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.147387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.147500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.148667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.148780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.149158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.149769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:41:24.150124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:41:24.150389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.150933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.151516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.152043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.152737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:41:24.155111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.156491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.157337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.157955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.158474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.158948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:41:24.191361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.192219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.192755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.193217: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.193276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:41:24.198302: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.198473: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.200335: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:41:24.201296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.201928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.202459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.202942: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.203006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:41:24.205434: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.205590: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.205957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.206528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.207051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.207494: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:41:24.207533: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:41:24.207585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:41:24.210950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.211595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.212104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.212679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.213188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.213658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:41:24.221890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.222528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.223047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.223650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.224166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.224647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:41:24.225353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.226011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.226509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.227083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.227638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:41:24.228108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:41:24.235090: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.235263: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.236875: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:41:24.237194: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.237362: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.238910: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:41:24.259656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.259833: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.261453: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:41:24.272425: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.272618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.274397: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:41:24.274401: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.274550: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:41:24.276259: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][06:41:25.532][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.532][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.532][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.532][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.532][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.532][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.560][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:41:25.560][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 100it [00:01, 84.70it/s]warmup run: 99it [00:01, 84.41it/s]warmup run: 101it [00:01, 86.89it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 199it [00:01, 182.61it/s]warmup run: 189it [00:01, 173.09it/s]warmup run: 201it [00:01, 186.99it/s]warmup run: 100it [00:01, 85.64it/s]warmup run: 97it [00:01, 83.61it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 298it [00:01, 290.48it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 289it [00:01, 284.10it/s]warmup run: 302it [00:01, 298.15it/s]warmup run: 202it [00:01, 187.58it/s]warmup run: 193it [00:01, 179.86it/s]warmup run: 101it [00:01, 88.57it/s]warmup run: 397it [00:01, 402.26it/s]warmup run: 101it [00:01, 85.97it/s]warmup run: 89it [00:01, 78.17it/s]warmup run: 386it [00:01, 394.39it/s]warmup run: 400it [00:01, 408.54it/s]warmup run: 303it [00:01, 298.30it/s]warmup run: 290it [00:01, 287.02it/s]warmup run: 198it [00:01, 186.80it/s]warmup run: 497it [00:02, 513.01it/s]warmup run: 202it [00:01, 186.38it/s]warmup run: 185it [00:01, 176.68it/s]warmup run: 484it [00:02, 503.65it/s]warmup run: 498it [00:02, 514.95it/s]warmup run: 406it [00:01, 415.93it/s]warmup run: 387it [00:01, 397.62it/s]warmup run: 295it [00:01, 294.14it/s]warmup run: 598it [00:02, 616.64it/s]warmup run: 302it [00:01, 295.65it/s]warmup run: 286it [00:01, 291.33it/s]warmup run: 583it [00:02, 605.91it/s]warmup run: 597it [00:02, 615.27it/s]warmup run: 508it [00:02, 528.47it/s]warmup run: 484it [00:02, 505.05it/s]warmup run: 394it [00:01, 408.04it/s]warmup run: 698it [00:02, 704.61it/s]warmup run: 404it [00:01, 412.15it/s]warmup run: 387it [00:01, 409.20it/s]warmup run: 684it [00:02, 698.08it/s]warmup run: 611it [00:02, 633.41it/s]warmup run: 692it [00:02, 672.52it/s]warmup run: 582it [00:02, 604.26it/s]warmup run: 494it [00:01, 520.25it/s]warmup run: 798it [00:02, 776.50it/s]warmup run: 505it [00:02, 523.45it/s]warmup run: 488it [00:01, 522.75it/s]warmup run: 783it [00:02, 770.25it/s]warmup run: 715it [00:02, 725.79it/s]warmup run: 792it [00:02, 750.76it/s]warmup run: 681it [00:02, 692.97it/s]warmup run: 598it [00:02, 630.80it/s]warmup run: 898it [00:02, 834.71it/s]warmup run: 607it [00:02, 626.95it/s]warmup run: 592it [00:02, 631.90it/s]warmup run: 883it [00:02, 828.31it/s]warmup run: 818it [00:02, 799.88it/s]warmup run: 893it [00:02, 815.95it/s]warmup run: 780it [00:02, 766.02it/s]warmup run: 701it [00:02, 723.73it/s]warmup run: 998it [00:02, 878.81it/s]warmup run: 709it [00:02, 717.18it/s]warmup run: 695it [00:02, 723.41it/s]warmup run: 982it [00:02, 870.27it/s]warmup run: 920it [00:02, 856.64it/s]warmup run: 994it [00:02, 867.21it/s]warmup run: 879it [00:02, 822.64it/s]warmup run: 802it [00:02, 795.11it/s]warmup run: 1097it [00:02, 897.68it/s]warmup run: 808it [00:02, 784.32it/s]warmup run: 797it [00:02, 796.52it/s]warmup run: 1080it [00:02, 900.11it/s]warmup run: 1023it [00:02, 902.25it/s]warmup run: 1094it [00:02, 902.17it/s]warmup run: 977it [00:02, 865.29it/s]warmup run: 902it [00:02, 845.95it/s]warmup run: 1195it [00:02, 906.90it/s]warmup run: 907it [00:02, 836.28it/s]warmup run: 899it [00:02, 854.54it/s]warmup run: 1126it [00:02, 936.15it/s]warmup run: 1194it [00:02, 928.80it/s]warmup run: 1178it [00:02, 843.17it/s]warmup run: 1075it [00:02, 895.10it/s]warmup run: 1002it [00:02, 883.83it/s]warmup run: 1292it [00:02, 910.71it/s]warmup run: 1006it [00:02, 873.69it/s]warmup run: 1001it [00:02, 897.64it/s]warmup run: 1228it [00:02, 959.60it/s]warmup run: 1293it [00:02, 939.92it/s]warmup run: 1271it [00:02, 865.35it/s]warmup run: 1173it [00:02, 919.11it/s]warmup run: 1103it [00:02, 918.75it/s]warmup run: 1387it [00:02, 915.76it/s]warmup run: 1105it [00:02, 905.13it/s]warmup run: 1102it [00:02, 927.95it/s]warmup run: 1330it [00:02, 973.90it/s]warmup run: 1391it [00:02, 950.42it/s]warmup run: 1369it [00:02, 894.85it/s]warmup run: 1271it [00:02, 933.83it/s]warmup run: 1204it [00:02, 942.00it/s]warmup run: 1482it [00:03, 922.92it/s]warmup run: 1205it [00:02, 931.63it/s]warmup run: 1203it [00:02, 948.80it/s]warmup run: 1433it [00:02, 988.26it/s]warmup run: 1489it [00:03, 958.97it/s]warmup run: 1467it [00:03, 918.65it/s]warmup run: 1369it [00:02, 936.70it/s]warmup run: 1304it [00:02, 958.03it/s]warmup run: 1579it [00:03, 935.23it/s]warmup run: 1304it [00:02, 946.62it/s]warmup run: 1304it [00:02, 962.55it/s]warmup run: 1536it [00:03, 997.94it/s]warmup run: 1588it [00:03, 967.01it/s]warmup run: 1565it [00:03, 934.09it/s]warmup run: 1467it [00:03, 946.70it/s]warmup run: 1405it [00:02, 971.84it/s]warmup run: 1678it [00:03, 949.70it/s]warmup run: 1403it [00:02, 950.95it/s]warmup run: 1406it [00:02, 977.19it/s]warmup run: 1638it [00:03, 1003.77it/s]warmup run: 1688it [00:03, 974.84it/s]warmup run: 1663it [00:03, 944.65it/s]warmup run: 1566it [00:03, 957.85it/s]warmup run: 1505it [00:02, 973.29it/s]warmup run: 1777it [00:03, 960.31it/s]warmup run: 1501it [00:03, 959.11it/s]warmup run: 1509it [00:02, 991.29it/s]warmup run: 1742it [00:03, 1011.79it/s]warmup run: 1788it [00:03, 979.67it/s]warmup run: 1760it [00:03, 951.21it/s]warmup run: 1666it [00:03, 967.93it/s]warmup run: 1605it [00:03, 976.49it/s]warmup run: 1880it [00:03, 979.00it/s]warmup run: 1599it [00:03, 960.94it/s]warmup run: 1613it [00:03, 1004.67it/s]warmup run: 1888it [00:03, 984.82it/s]warmup run: 1845it [00:03, 983.33it/s] warmup run: 1857it [00:03, 953.07it/s]warmup run: 1766it [00:03, 974.49it/s]warmup run: 1704it [00:03, 974.73it/s]warmup run: 1981it [00:03, 985.92it/s]warmup run: 1699it [00:03, 971.23it/s]warmup run: 1716it [00:03, 1011.60it/s]warmup run: 1989it [00:03, 991.89it/s]warmup run: 1945it [00:03, 976.72it/s]warmup run: 1953it [00:03, 954.49it/s]warmup run: 1866it [00:03, 979.38it/s]warmup run: 1803it [00:03, 976.91it/s]warmup run: 2095it [00:03, 1030.61it/s]warmup run: 1802it [00:03, 987.55it/s]warmup run: 1819it [00:03, 1012.96it/s]warmup run: 2106it [00:03, 1043.05it/s]warmup run: 2048it [00:03, 990.51it/s]warmup run: 2060it [00:03, 987.37it/s]warmup run: 1965it [00:03, 979.29it/s]warmup run: 1905it [00:03, 989.35it/s]warmup run: 2211it [00:03, 1067.83it/s]warmup run: 1903it [00:03, 992.39it/s]warmup run: 1921it [00:03, 999.55it/s] warmup run: 2226it [00:03, 1087.43it/s]warmup run: 2160it [00:03, 1026.31it/s]warmup run: 2179it [00:03, 1046.78it/s]warmup run: 2078it [00:03, 1022.21it/s]warmup run: 2007it [00:03, 998.07it/s]warmup run: 2327it [00:03, 1093.85it/s]warmup run: 2003it [00:03, 992.34it/s]warmup run: 2022it [00:03, 1002.08it/s]warmup run: 2347it [00:03, 1122.66it/s]warmup run: 2272it [00:03, 1053.08it/s]warmup run: 2300it [00:03, 1094.19it/s]warmup run: 2199it [00:03, 1077.20it/s]warmup run: 2130it [00:03, 1065.30it/s]warmup run: 2443it [00:03, 1111.23it/s]warmup run: 2125it [00:03, 1058.31it/s]warmup run: 2142it [00:03, 1058.65it/s]warmup run: 2469it [00:03, 1150.02it/s]warmup run: 2384it [00:03, 1070.62it/s]warmup run: 2422it [00:03, 1129.41it/s]warmup run: 2320it [00:03, 1115.62it/s]warmup run: 2253it [00:03, 1113.21it/s]warmup run: 2559it [00:04, 1124.49it/s]warmup run: 2248it [00:03, 1107.24it/s]warmup run: 2262it [00:03, 1098.40it/s]warmup run: 2591it [00:04, 1169.81it/s]warmup run: 2497it [00:03, 1087.06it/s]warmup run: 2544it [00:04, 1154.02it/s]warmup run: 2441it [00:03, 1142.57it/s]warmup run: 2376it [00:03, 1146.03it/s]warmup run: 2677it [00:04, 1140.12it/s]warmup run: 2371it [00:03, 1141.44it/s]warmup run: 2382it [00:03, 1126.34it/s]warmup run: 2714it [00:04, 1185.22it/s]warmup run: 2613it [00:04, 1106.94it/s]warmup run: 2666it [00:04, 1171.26it/s]warmup run: 2562it [00:04, 1161.67it/s]warmup run: 2499it [00:03, 1168.27it/s]warmup run: 2792it [00:04, 1141.11it/s]warmup run: 2491it [00:03, 1157.38it/s]warmup run: 2502it [00:03, 1145.98it/s]warmup run: 2833it [00:04, 1186.33it/s]warmup run: 2727it [00:04, 1116.32it/s]warmup run: 2786it [00:04, 1178.56it/s]warmup run: 2683it [00:04, 1175.97it/s]warmup run: 2622it [00:03, 1184.81it/s]warmup run: 2912it [00:04, 1155.89it/s]warmup run: 2611it [00:04, 1169.81it/s]warmup run: 2622it [00:03, 1160.46it/s]warmup run: 2954it [00:04, 1191.86it/s]warmup run: 2843it [00:04, 1128.63it/s]warmup run: 2908it [00:04, 1188.92it/s]warmup run: 3000it [00:04, 684.23it/s] warmup run: 2803it [00:04, 1180.23it/s]warmup run: 3000it [00:04, 674.34it/s] warmup run: 2741it [00:04, 1181.58it/s]warmup run: 2733it [00:04, 1183.19it/s]warmup run: 2741it [00:04, 1169.06it/s]warmup run: 3000it [00:04, 673.03it/s] warmup run: 2959it [00:04, 1137.97it/s]warmup run: 2923it [00:04, 1185.42it/s]warmup run: 3000it [00:04, 683.03it/s] warmup run: 2860it [00:04, 1183.67it/s]warmup run: 2852it [00:04, 1181.99it/s]warmup run: 2859it [00:04, 1170.66it/s]warmup run: 3000it [00:04, 682.21it/s] warmup run: 2981it [00:04, 1190.58it/s]warmup run: 3000it [00:04, 693.94it/s] warmup run: 2971it [00:04, 1183.91it/s]warmup run: 2979it [00:04, 1177.92it/s]warmup run: 3000it [00:04, 693.96it/s] warmup run: 3000it [00:04, 685.09it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.49it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.60it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.24it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.91it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.21it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.75it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.47it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.57it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1653.38it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.25it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.05it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.95it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.51it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.68it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1657.74it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.45it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1640.06it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1650.94it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.71it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1642.97it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1652.43it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1650.56it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1650.58it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1652.62it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1639.10it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1649.34it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1642.45it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1651.46it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1648.81it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1649.28it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1650.71it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1643.27it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1637.15it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1641.41it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1650.89it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1647.22it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1646.79it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1643.63it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1646.37it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1635.83it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1634.38it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1647.78it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1650.90it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1645.45it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1639.04it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1635.83it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1642.11it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1629.31it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1646.72it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1635.44it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1646.76it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1619.37it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1629.06it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1636.31it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1619.40it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1611.46it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1651.12it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1636.52it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1645.80it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1628.42it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1638.10it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1589.03it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1590.26it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1583.55it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1656.80it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1637.22it/s]warmup should be done:  50%|████▉     | 1492/3000 [00:00<00:00, 1645.76it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1626.65it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1636.82it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1584.68it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1568.95it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1565.59it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1659.86it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1635.97it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1647.34it/s]warmup should be done:  55%|█████▌    | 1654/3000 [00:01<00:00, 1636.23it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1623.80it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1587.37it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1556.72it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1554.49it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1661.00it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1635.23it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1647.62it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1639.24it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1621.57it/s]warmup should be done:  60%|██████    | 1803/3000 [00:01<00:00, 1593.43it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1549.79it/s]warmup should be done:  60%|█████▉    | 1790/3000 [00:01<00:00, 1548.26it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1662.08it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1633.74it/s]warmup should be done:  66%|██████▋   | 1988/3000 [00:01<00:00, 1647.63it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1642.62it/s]warmup should be done:  66%|██████▌   | 1972/3000 [00:01<00:00, 1620.97it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1597.92it/s]warmup should be done:  65%|██████▍   | 1942/3000 [00:01<00:00, 1543.20it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1542.24it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1660.99it/s]warmup should be done:  71%|███████▏  | 2139/3000 [00:01<00:00, 1638.50it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1646.00it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1640.36it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1603.07it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1603.07it/s]warmup should be done:  70%|██████▉   | 2097/3000 [00:01<00:00, 1536.89it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1536.09it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1661.51it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1645.96it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1643.11it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1598.69it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1611.69it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1561.35it/s]warmup should be done:  75%|███████▌  | 2251/3000 [00:01<00:00, 1530.14it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1529.61it/s]warmup should be done:  83%|████████▎ | 2483/3000 [00:01<00:00, 1643.09it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1654.50it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1648.03it/s]warmup should be done:  82%|████████▏ | 2456/3000 [00:01<00:00, 1588.82it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1615.16it/s]warmup should be done:  80%|████████  | 2405/3000 [00:01<00:00, 1523.50it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1522.98it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1513.11it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1644.13it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1649.99it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1654.30it/s]warmup should be done:  87%|████████▋ | 2620/3000 [00:01<00:00, 1601.96it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1616.12it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1523.28it/s]warmup should be done:  85%|████████▌ | 2560/3000 [00:01<00:00, 1522.62it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1548.36it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1645.74it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1649.99it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1657.08it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1610.03it/s]warmup should be done:  93%|█████████▎| 2777/3000 [00:01<00:00, 1613.17it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1511.22it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1573.41it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1483.78it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1650.72it/s]warmup should be done:  99%|█████████▉| 2982/3000 [00:01<00:00, 1659.92it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1651.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.32it/s]warmup should be done:  98%|█████████▊| 2949/3000 [00:01<00:00, 1623.79it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1622.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.35it/s]warmup should be done:  95%|█████████▌| 2863/3000 [00:01<00:00, 1506.83it/s]warmup should be done:  98%|█████████▊| 2954/3000 [00:01<00:00, 1597.23it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1483.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1558.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1552.67it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.81it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.66it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.74it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.21it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1693.46it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1550.60it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1552.65it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.71it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.45it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.66it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.15it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1685.53it/s]warmup should be done:  10%|█         | 312/3000 [00:00<00:01, 1553.08it/s]warmup should be done:  10%|█         | 313/3000 [00:00<00:01, 1558.11it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.27it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1650.30it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1692.48it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1675.11it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1690.56it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1674.02it/s]warmup should be done:  16%|█▌        | 470/3000 [00:00<00:01, 1563.44it/s]warmup should be done:  16%|█▌        | 471/3000 [00:00<00:01, 1566.04it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1664.98it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.26it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1693.45it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1681.51it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1695.20it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1676.32it/s]warmup should be done:  21%|██        | 629/3000 [00:00<00:01, 1570.85it/s]warmup should be done:  21%|██        | 630/3000 [00:00<00:01, 1572.08it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1672.21it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1694.45it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1677.03it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1685.61it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1689.71it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1697.54it/s]warmup should be done:  26%|██▋       | 788/3000 [00:00<00:01, 1573.31it/s]warmup should be done:  26%|██▌       | 787/3000 [00:00<00:01, 1572.14it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1674.27it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1697.92it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1686.42it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1683.29it/s]warmup should be done:  32%|███▏      | 946/3000 [00:00<00:01, 1572.84it/s]warmup should be done:  32%|███▏      | 945/3000 [00:00<00:01, 1571.97it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1672.51it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1672.01it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1691.57it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1698.85it/s]warmup should be done:  37%|███▋      | 1104/3000 [00:00<00:01, 1574.92it/s]warmup should be done:  37%|███▋      | 1103/3000 [00:00<00:01, 1574.46it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1686.40it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1696.51it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1667.94it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1668.54it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1687.67it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1653.50it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1688.16it/s]warmup should be done:  42%|████▏     | 1262/3000 [00:00<00:01, 1570.61it/s]warmup should be done:  42%|████▏     | 1261/3000 [00:00<00:01, 1570.30it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1698.78it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1668.41it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1669.55it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1686.78it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:01, 1642.09it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1687.06it/s]warmup should be done:  47%|████▋     | 1420/3000 [00:00<00:01, 1574.96it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1697.89it/s]warmup should be done:  47%|████▋     | 1422/3000 [00:00<00:01, 1576.93it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1665.22it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1668.47it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1680.85it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1626.51it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1685.59it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1668.27it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1666.57it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1691.61it/s]warmup should be done:  53%|█████▎    | 1580/3000 [00:01<00:00, 1572.02it/s]warmup should be done:  53%|█████▎    | 1578/3000 [00:01<00:00, 1566.69it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1685.26it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1626.39it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1687.59it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1673.84it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1670.56it/s]warmup should be done:  58%|█████▊    | 1738/3000 [00:01<00:00, 1572.16it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1686.00it/s]warmup should be done:  58%|█████▊    | 1736/3000 [00:01<00:00, 1568.14it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1683.12it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1613.38it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1688.85it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1677.20it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1672.15it/s]warmup should be done:  68%|██████▊   | 2036/3000 [00:01<00:00, 1686.09it/s]warmup should be done:  63%|██████▎   | 1897/3000 [00:01<00:00, 1575.20it/s]warmup should be done:  63%|██████▎   | 1895/3000 [00:01<00:00, 1572.51it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1676.32it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1607.09it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1685.63it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1678.76it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1686.04it/s]warmup should be done:  69%|██████▊   | 2056/3000 [00:01<00:00, 1577.94it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1576.06it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1665.44it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1669.82it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1609.30it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1682.87it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1676.96it/s]warmup should be done:  74%|███████▍  | 2214/3000 [00:01<00:00, 1575.65it/s]warmup should be done:  78%|███████▊  | 2348/3000 [00:01<00:00, 1665.05it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1679.11it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1571.01it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1670.95it/s]warmup should be done:  77%|███████▋  | 2323/3000 [00:01<00:00, 1593.66it/s]warmup should be done:  84%|████████▍ | 2522/3000 [00:01<00:00, 1677.22it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1679.90it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1669.28it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1572.09it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1679.75it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1563.49it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1675.35it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1604.94it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1675.30it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1677.69it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1669.31it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1573.10it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1673.26it/s]warmup should be done:  84%|████████▍ | 2528/3000 [00:01<00:00, 1566.54it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1682.15it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1597.19it/s]warmup should be done:  95%|█████████▌| 2858/3000 [00:01<00:00, 1672.25it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1678.46it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1670.00it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1573.23it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1667.78it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1568.79it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1685.89it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1604.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.88it/s]warmup should be done:  95%|█████████▍| 2846/3000 [00:01<00:00, 1571.99it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1568.95it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1608.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1572.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1569.83it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d5bd7730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d5bd6e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d48810d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d5bd9d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d4871130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d48721f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d48801c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fd9d48742b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 06:42:54.849219: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd4f3029af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:54.849281: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:54.858802: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.391526: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd502834430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.391596: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.399526: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.412468: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd4fb02da40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.412528: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.421774: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.511754: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd4fa795220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.511818: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.519490: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.669558: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd4fa8350e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.669626: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.678861: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd502834c20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.678930: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.679482: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd4f70291e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.679530: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.679878: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.687376: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.687693: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:42:55.689804: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd4f6834c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:42:55.689846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:42:55.698982: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:43:02.108157: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.151937: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.191851: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.415293: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.423734: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.441861: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.732435: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:43:02.741769: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:43:52.429][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:43:52.429][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.440][ERROR][RK0][main]: coll ps creation done
[HCTR][06:43:52.440][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:43:52.440][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:43:52.440][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.445][ERROR][RK0][main]: coll ps creation done
[HCTR][06:43:52.445][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][06:43:52.594][ERROR][RK0][tid #140553626818304]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:43:52.594][ERROR][RK0][tid #140553626818304]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.599][ERROR][RK0][tid #140553626818304]: coll ps creation done
[HCTR][06:43:52.599][ERROR][RK0][tid #140553626818304]: replica 3 waits for coll ps creation barrier
[HCTR][06:43:52.612][ERROR][RK0][tid #140554172081920]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:43:52.612][ERROR][RK0][tid #140554172081920]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.619][ERROR][RK0][tid #140554172081920]: coll ps creation done
[HCTR][06:43:52.619][ERROR][RK0][tid #140554172081920]: replica 4 waits for coll ps creation barrier
[HCTR][06:43:52.633][ERROR][RK0][tid #140553232557824]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:43:52.633][ERROR][RK0][tid #140553232557824]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.638][ERROR][RK0][tid #140553232557824]: coll ps creation done
[HCTR][06:43:52.638][ERROR][RK0][tid #140553232557824]: replica 1 waits for coll ps creation barrier
[HCTR][06:43:52.689][ERROR][RK0][tid #140553836537600]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:43:52.689][ERROR][RK0][tid #140553836537600]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.698][ERROR][RK0][tid #140553836537600]: coll ps creation done
[HCTR][06:43:52.698][ERROR][RK0][tid #140553836537600]: replica 6 waits for coll ps creation barrier
[HCTR][06:43:52.768][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:43:52.768][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.775][ERROR][RK0][main]: coll ps creation done
[HCTR][06:43:52.775][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][06:43:52.823][ERROR][RK0][tid #140553366775552]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:43:52.823][ERROR][RK0][tid #140553366775552]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:43:52.828][ERROR][RK0][tid #140553366775552]: coll ps creation done
[HCTR][06:43:52.828][ERROR][RK0][tid #140553366775552]: replica 0 waits for coll ps creation barrier
[HCTR][06:43:52.828][ERROR][RK0][tid #140553366775552]: replica 0 preparing frequency
[HCTR][06:43:53.668][ERROR][RK0][tid #140553366775552]: replica 0 preparing frequency done
[HCTR][06:43:53.713][ERROR][RK0][tid #140553366775552]: replica 0 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][tid #140553626818304]: replica 3 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][tid #140553836537600]: replica 6 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][tid #140554172081920]: replica 4 calling init per replica
[HCTR][06:43:53.713][ERROR][RK0][tid #140553232557824]: replica 1 calling init per replica
[HCTR][06:43:53.714][ERROR][RK0][tid #140553366775552]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][tid #140553626818304]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][main]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][main]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][tid #140553836537600]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][tid #140553626818304]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][main]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][tid #140554172081920]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][tid #140553366775552]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][tid #140553232557824]: Calling build_v2
[HCTR][06:43:53.714][ERROR][RK0][tid #140554172081920]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][tid #140553836537600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:43:53.714][ERROR][RK0][tid #140553232557824]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 06:43:53.718042[: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:43:53.7181302022-12-12 06:43:53: .E718084 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 1962022-12-12 06:43:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:assigning 0 to cpu718127178
[: ] Ev100x8, slow pcie 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:43:53:.178[718169] 2022-12-12 06:43:53: v100x8, slow pcie.E
718208 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[E:2022-12-12 06:43:532022-12-12 06:43:53 2022-12-12 06:43:53178../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 718236718237:718218v100x8, slow pcie: : 196: 
EE] E  [assigning 0 to cpu [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:43:53::2022-12-12 06:43:53:.212196[.178718295] ] 718270] : v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:43:53assigning 0 to cpu: [E[

.
E2022-12-12 06:43:53 7183152022-12-12 06:43:53[ ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: .2022-12-12 06:43:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc718367:2022-12-12 06:43:53E[718360.:: 196. 2022-12-12 06:43:53: 718414178E] 718429/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E: ]  assigning 0 to cpu: :718457 Ev100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
: ] [E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 06:43:53 178:] [:
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:43:53213718641:[v100x8, slow pcie] 
.] : 2122022-12-12 06:43:53
assigning 0 to cpu718661remote time is 8.68421E] [.
: 
[ build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:43:53718698E2022-12-12 06:43:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
.: [ .:2022-12-12 06:43:53718746E2022-12-12 06:43:53[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc718759196.:  .2022-12-12 06:43:53:: ] 718790E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:718802.212Eassigning 0 to cpu:  196: 718827]  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :assigning 0 to cpu E
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[213
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 196:2022-12-12 06:43:53[] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214.2022-12-12 06:43:53remote time is 8.68421212:assigning 0 to cpu] 718972.
] [213
cpu time is 97.0588: 718998build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:43:53[] 
E: 
.2022-12-12 06:43:53remote time is 8.68421 E719055.[
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : 7190802022-12-12 06:43:532022-12-12 06:43:53:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: ..2122022-12-12 06:43:53: E719144719148] .213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8719177] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE
: remote time is 8.68421212:  E
] 214[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] [2022-12-12 06:43:53::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.05882022-12-12 06:43:53.212213:
.719328] [] 214719346: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 06:43:53remote time is 8.68421] : E
.
cpu time is 97.0588E 719401
[ [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 06:43:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:43:53:E.:.213 719461214719463] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] : remote time is 8.68421:Ecpu time is 97.0588E
213 
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:2022-12-12 06:43:53:
213.214] [719569] remote time is 8.684212022-12-12 06:43:53: cpu time is 97.0588
.E
719609 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:43:53E:. 214719647/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : :cpu time is 97.0588E214
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 06:45:13.147605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:45:13.187809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 06:45:13.333583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:45:13.333642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:45:13.333675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:45:13.333705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:45:13.334158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:45:13.334203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.335039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.335724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.348891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 06:45:13.348950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-12 06:45:132022-12-12 06:45:13..349178349206: : E[E [2022-12-12 06:45:13 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 06:45:13./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[.349242:2022022-12-12 06:45:13349257: 202] .: E] 1 solved[349298E 5 solved
2022-12-12 06:45:13: [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
.E2022-12-12 06:45:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:349372 [.:202: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 06:45:13349423202] E:.: ] 7 solved 202349446E4 solved
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :  
:[6 solvedE/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc18152022-12-12 06:45:13[
 :] .2022-12-12 06:45:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205[Building Coll Cache with ... num gpu device is 8349569.:] 2022-12-12 06:45:13
: 349589205worker 0 thread 1 initing device 1.E: ] 
349623[ Eworker 0 thread 5 initing device 5: 2022-12-12 06:45:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 
E.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 349689205:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: ] 205:Eworker 0 thread 7 initing device 7] 205]  
worker 0 thread 4 initing device 4worker 0 thread 6 initing device 6/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.350128: E[ 2022-12-12 06:45:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:3501411815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:45:13.[3502142022-12-12 06:45:13: .E[350221 [2022-12-12 06:45:13: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:45:13.E2022-12-12 06:45:13:.350236 .1980350243: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu350250] : E:: eager alloc mem 381.47 MBE 1980E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815
:1815] 1815] Building Coll Cache with ... num gpu device is 8] Building Coll Cache with ... num gpu device is 8
Building Coll Cache with ... num gpu device is 8

[2022-12-12 06:45:13.350451[[: 2022-12-12 06:45:132022-12-12 06:45:13E.. 350457350457/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : :EE1980  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB::
19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 06:45:13.353160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.353521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 06:45:13.353602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 06:45:13.354112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:45:13.354179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.354290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.354332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.354387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.354551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.354606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.357843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.358937: E[ 2022-12-12 06:45:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:3589681980: ] Eeager alloc mem 381.47 MB 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:45:13:.1980359021] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.359113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.359172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.359284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.363214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:45:13.410344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:45:13.424137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:45:13.424284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.427706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.428504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:13.429621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.429670: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[[2022-12-12 06:45:132022-12-12 06:45:13..440197440197: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 06:45:13.443587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:45:13.444945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:45:13.445775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:45:13.445840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 06:45:13638.] 445872eager release cuda mem 5: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.445930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.446784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.447339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.447405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-12 06:45:132022-12-12 06:45:13..447485447485: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 06:45:13.448755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:13.448792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:13.449870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.449916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[:2022-12-12 06:45:1343.] 449914WORKER[0] alloc host memory 95.37 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.449973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:45:13.450014: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB[
2022-12-12 06:45:13.450051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.450344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:45:13.450420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.451860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.452869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.453019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:45:13.453106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.453571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:45:13.2022-12-12 06:45:13453780.: 453776E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 5[
2022-12-12 06:45:13.453859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:45:13.453924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 06:45:13638.] 453949eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:45:13.454164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.454637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.454685: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:45:13.454910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.454957: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:45:13.455181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.455694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:45:13.456165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:13.456547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:13.456598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:13.457254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.457300: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:45:13.457615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.457660: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 06:45:13:.43457669] : WORKER[0] alloc host memory 95.37 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:13.457727: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:45:13.494471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.495105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.495159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.514130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.514751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.514793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.515624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.516260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.516307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.517565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.518089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.518182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.518225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.518337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.518701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.518745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.518961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.519006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.519549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.520167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.520209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:45:13.521310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:45:13.521918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:45:13.521960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 06:45:172022-12-12 06:45:172022-12-12 06:45:172022-12-12 06:45:172022-12-12 06:45:172022-12-12 06:45:172022-12-12 06:45:172022-12-12 06:45:17........890806890807890807890806890809890807890807890807: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] Device 0 init p2p of link 3] ] ] ] ] ] Device 2 init p2p of link 1
Device 3 init p2p of link 2Device 6 init p2p of link 0Device 5 init p2p of link 6Device 7 init p2p of link 4Device 1 init p2p of link 7Device 4 init p2p of link 5






[[2022-12-12 06:45:172022-12-12 06:45:17[..2022-12-12 06:45:17[[891415891414.[2022-12-12 06:45:17[2022-12-12 06:45:17[: : 8914232022-12-12 06:45:17.2022-12-12 06:45:17.2022-12-12 06:45:17EE: .891427.891428.  E891435: 891440: 891443/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : E: E: ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E E19801980: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] ] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBeager alloc mem 611.00 KB] :1980:1980:

eager alloc mem 611.00 KB1980] 1980] 1980
] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[2022-12-12 06:45:17.892505[: 2022-12-12 06:45:17E. 892512/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 06:45:17.892607[: 2022-12-12 06:45:17E. 892616/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E[638 2022-12-12 06:45:17] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager release cuda mem 625663:892636
638: ] Eeager release cuda mem 625663[ [
2022-12-12 06:45:17[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:45:17.2022-12-12 06:45:17:.892666.638892668: 892675] : E: eager release cuda mem 625663E E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:638] 638] eager release cuda mem 625663] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-12 06:45:17.906334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 06:45:17.906479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.906630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:45:17.906781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.907424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.907762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.915787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 06:45:17.915937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.916009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 06:45:17.916154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.916166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 06:45:17.916250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 06:45:17.916321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.916414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.916615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 06:45:17.916708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.916765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.917088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.917166[: 2022-12-12 06:45:17E. 917188/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccDevice 5 init p2p of link 4:
638] eager release cuda mem 625663
[2022-12-12 06:45:17.917291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.917358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.917696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.918319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.920662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 06:45:17.920781: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.920798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 06:45:17.920919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.921712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.921859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.929523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:45:17.929650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.930010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 06:45:17.930137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.930622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.931090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.938446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:45:17.938569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.938945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 06:45:17.939064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.939180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 06:45:17.939302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.939358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.939991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.940062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 06:45:17Device 5 init p2p of link 7.
940094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.940202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.941129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.946917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:45:17.947030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.947710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:45:17.947822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.947982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.948771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.950868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:45:17.950978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.951166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:45:17.951298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.951920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.952260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.953470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 06:45:17.953598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.953851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:45:17.953969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.954541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.954910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.965333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:45:17.965452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.966170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:45:17.966227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.966285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:45:17.967063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:45:17.968925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.969847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.970028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61586 secs 
[2022-12-12 06:45:17.970396: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.970850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62117 secs 
[2022-12-12 06:45:17.971197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62075 secs 
[2022-12-12 06:45:17.971531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.971958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62174 secs 
[2022-12-12 06:45:17.976303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.976732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62628 secs 
[2022-12-12 06:45:17.976925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.977366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62717 secs 
[2022-12-12 06:45:17.978701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.979076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:45:17.979122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64493 secs 
[2022-12-12 06:45:17.979515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62906 secs 
[HCTR][06:45:17.979][ERROR][RK0][tid #140554172081920]: replica 4 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][tid #140553626818304]: replica 3 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][tid #140553232557824]: replica 1 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][tid #140553366775552]: replica 0 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][tid #140553836537600]: replica 6 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][06:45:17.979][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553626818304]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553366775552]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][main]: init per replica done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553836537600]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553232557824]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][tid #140554172081920]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553626818304]: init per replica done
[HCTR][06:45:17.979][ERROR][RK0][main]: init per replica done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553836537600]: init per replica done
[HCTR][06:45:17.979][ERROR][RK0][main]: init per replica done
[HCTR][06:45:17.979][ERROR][RK0][tid #140553232557824]: init per replica done
[HCTR][06:45:17.979][ERROR][RK0][tid #140554172081920]: init per replica done
[HCTR][06:45:17.982][ERROR][RK0][tid #140553366775552]: init per replica done
