2022-12-11 22:30:55.513082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.518767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.523493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.533796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.543730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.549330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.562185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.568855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.616238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.617881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.618803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.619180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.620477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.620748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.622191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.622270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.623859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.623911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.625249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.625403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.626924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.627123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.628475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.628934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.630018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.630580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.631605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.632685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.633654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.634590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.635616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.636656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.638537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.639583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.640513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.641431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.642452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.643482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.644514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.645544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.650555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.650913: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.651664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.652683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.653690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.654906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.656331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.658132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.658620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.659868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.660715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.660875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.663173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.663467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.663596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.666475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.666530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.666678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.666759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.669959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.670171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.670419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.670823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.673019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.673242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.673534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.674272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.676137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.676473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.676613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.676987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.678003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.679797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.680359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.680450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.680931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.682217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.683672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.684143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.684446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.685041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.686141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.686951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.687755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.687990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.688548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.689745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.690241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.691296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.691790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.692851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.692969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.694053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.695543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.695676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.696547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.697573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.698343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.706116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.706853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.706868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.707730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.708692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.708748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.709762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.728480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.735152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.737942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.742246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.745207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.746914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.748136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.748385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.748465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.748521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.749089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.751101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.752627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.752807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.752846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.753115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.753333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.755791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.757051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.757966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.758140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.758446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.758477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.760334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.761354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.761894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.762120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.762502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.762581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.764196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.765806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.766156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.766374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.766730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.766985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.769606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.770137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.770548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.770643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.771164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.771387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.773585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.774084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.774733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.774787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.775300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.775486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.777857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.778051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.778569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.778698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.779295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.779490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.781886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.782082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.782601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.782733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.783247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.783499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.785879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.786166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.786787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.786917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.787547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.787952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.788967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.790095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.790356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.791282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.791322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.791813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.792532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.793815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.794983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.794994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.795831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.796007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.796484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.797082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.798342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.799371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.799427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.800426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.800507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.801080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.801726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.802685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.803888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.804541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.804624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.805667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.806498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.806557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.807359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.808126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.808168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.808216: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.809608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.810465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.810471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.812004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.812837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.812882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.814332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.814349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.814858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.815655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.816614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.816684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.818068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.818209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.818339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.819517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.819756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.821146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.821220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.822570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.822629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.822795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.824121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.824224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.825653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.825697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.827021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.827161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.827299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.828993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.829032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.830149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.830186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.831860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.832269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.833495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.833938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.834566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.834790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.836394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.836515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.837914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.838497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.839291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.839722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.840695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.842240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.842992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.843410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.843810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.844522: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.844864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.846865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.848335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.850105: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.850543: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.850945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.851084: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.853700: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.853892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.854390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.856161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.856930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.858695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.859233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.859916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.860371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.861068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.863160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.863216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.873152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.873364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.873809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.874965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.875074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.877716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.877952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.878561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.879484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.879696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.941189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.947470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.957836: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:30:55.967522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.973760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:55.979114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.004436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.005074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.005596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.006063: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.006119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:30:57.025463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.026268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.027007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.027973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.028958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.029436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:30:57.076401: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.076604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.123841: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:30:57.239924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.240537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.241072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.241531: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.241586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:30:57.258820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.259692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.260306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.260897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.261422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.261899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:30:57.280416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.281396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.281964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.282807: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.282867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:30:57.296786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.296885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.297844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.297916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.298791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.298870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.299852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.300290: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.300368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:30:57.300383: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.300450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:30:57.301765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.302296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.302877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.303600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.304210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:30:57.309325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.309900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.310407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.310869: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.310916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:30:57.318208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.318343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.318608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.319508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.319806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.320051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.320929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.321185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.321447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.322459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.322703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.322754: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.322803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:30:57.323740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.323866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.324638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:30:57.324753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:30:57.326787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.327413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.328216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.328818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.329334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.329804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:30:57.341948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.342599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.343183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.343350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.344179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.344352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.345039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.345268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.345924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:30:57.346070: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:30:57.346117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:30:57.349966: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.350141: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.352067: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:30:57.363704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.364368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.364887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.365466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.365995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:30:57.366460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:30:57.370994: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.371179: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.372767: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.372963: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.373024: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:30:57.374898: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:30:57.375656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.375797: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.377642: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:30:57.388036: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.388184: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.390021: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:30:57.392209: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.392376: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.394093: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:30:57.413070: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.413259: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:30:57.415123: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][22:30:58.678][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.678][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.678][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.678][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.679][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.731][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:30:58.731][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 98it [00:01, 82.68it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 83.63it/s]warmup run: 98it [00:01, 82.98it/s]warmup run: 97it [00:01, 81.77it/s]warmup run: 96it [00:01, 81.14it/s]warmup run: 197it [00:01, 180.48it/s]warmup run: 99it [00:01, 83.18it/s]warmup run: 98it [00:01, 84.17it/s]warmup run: 199it [00:01, 182.48it/s]warmup run: 191it [00:01, 174.62it/s]warmup run: 196it [00:01, 179.52it/s]warmup run: 191it [00:01, 174.99it/s]warmup run: 299it [00:01, 292.15it/s]warmup run: 195it [00:01, 177.39it/s]warmup run: 199it [00:01, 185.56it/s]warmup run: 298it [00:01, 290.28it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 286it [00:01, 278.13it/s]warmup run: 300it [00:01, 294.15it/s]warmup run: 291it [00:01, 285.21it/s]warmup run: 401it [00:01, 408.15it/s]warmup run: 291it [00:01, 281.38it/s]warmup run: 301it [00:01, 298.10it/s]warmup run: 398it [00:01, 403.49it/s]warmup run: 96it [00:01, 84.06it/s]warmup run: 381it [00:01, 385.32it/s]warmup run: 405it [00:01, 414.66it/s]warmup run: 386it [00:01, 391.45it/s]warmup run: 503it [00:02, 521.11it/s]warmup run: 387it [00:01, 389.46it/s]warmup run: 403it [00:01, 414.42it/s]warmup run: 496it [00:02, 510.36it/s]warmup run: 193it [00:01, 182.73it/s]warmup run: 475it [00:02, 488.05it/s]warmup run: 509it [00:02, 530.85it/s]warmup run: 477it [00:02, 487.29it/s]warmup run: 605it [00:02, 625.09it/s]warmup run: 482it [00:02, 493.35it/s]warmup run: 505it [00:02, 527.46it/s]warmup run: 594it [00:02, 608.17it/s]warmup run: 288it [00:01, 287.77it/s]warmup run: 570it [00:02, 584.54it/s]warmup run: 611it [00:02, 633.23it/s]warmup run: 571it [00:02, 581.29it/s]warmup run: 705it [00:02, 710.80it/s]warmup run: 579it [00:02, 592.31it/s]warmup run: 609it [00:02, 635.22it/s]warmup run: 694it [00:02, 697.18it/s]warmup run: 384it [00:01, 397.46it/s]warmup run: 665it [00:02, 668.75it/s]warmup run: 711it [00:02, 716.57it/s]warmup run: 665it [00:02, 662.80it/s]warmup run: 804it [00:02, 777.66it/s]warmup run: 676it [00:02, 677.90it/s]warmup run: 714it [00:02, 729.79it/s]warmup run: 795it [00:02, 773.17it/s]warmup run: 479it [00:01, 501.74it/s]warmup run: 759it [00:02, 735.80it/s]warmup run: 812it [00:02, 787.51it/s]warmup run: 758it [00:02, 727.22it/s]warmup run: 902it [00:02, 830.09it/s]warmup run: 773it [00:02, 748.88it/s]warmup run: 819it [00:02, 808.50it/s]warmup run: 897it [00:02, 836.47it/s]warmup run: 576it [00:02, 600.17it/s]warmup run: 853it [00:02, 787.03it/s]warmup run: 916it [00:02, 853.20it/s]warmup run: 1000it [00:02, 869.41it/s]warmup run: 870it [00:02, 805.52it/s]warmup run: 850it [00:02, 744.63it/s]warmup run: 923it [00:02, 868.77it/s]warmup run: 999it [00:02, 885.40it/s]warmup run: 673it [00:02, 686.27it/s]warmup run: 946it [00:02, 824.34it/s]warmup run: 1019it [00:02, 900.70it/s]warmup run: 1099it [00:02, 901.56it/s]warmup run: 970it [00:02, 856.02it/s]warmup run: 952it [00:02, 815.86it/s]warmup run: 1026it [00:02, 908.44it/s]warmup run: 1102it [00:02, 923.59it/s]warmup run: 770it [00:02, 755.68it/s]warmup run: 1039it [00:02, 851.91it/s]warmup run: 1121it [00:02, 927.26it/s]warmup run: 1199it [00:02, 928.34it/s]warmup run: 1070it [00:02, 895.57it/s]warmup run: 1054it [00:02, 869.57it/s]warmup run: 1130it [00:02, 943.24it/s]warmup run: 1205it [00:02, 953.69it/s]warmup run: 866it [00:02, 808.16it/s]warmup run: 1133it [00:02, 875.18it/s]warmup run: 1222it [00:02, 941.08it/s]warmup run: 1300it [00:02, 949.06it/s]warmup run: 1169it [00:02, 919.66it/s]warmup run: 1158it [00:02, 915.48it/s]warmup run: 1233it [00:02, 967.54it/s]warmup run: 1307it [00:02, 971.48it/s]warmup run: 966it [00:02, 859.16it/s]warmup run: 1229it [00:02, 897.39it/s]warmup run: 1322it [00:02, 947.56it/s]warmup run: 1403it [00:02, 970.38it/s]warmup run: 1267it [00:02, 936.25it/s]warmup run: 1261it [00:02, 946.44it/s]warmup run: 1336it [00:02, 982.66it/s]warmup run: 1409it [00:02, 980.18it/s]warmup run: 1066it [00:02, 897.85it/s]warmup run: 1327it [00:02, 920.84it/s]warmup run: 1426it [00:02, 973.75it/s]warmup run: 1508it [00:03, 991.83it/s]warmup run: 1366it [00:02, 951.34it/s]warmup run: 1363it [00:02, 966.94it/s]warmup run: 1439it [00:02, 992.81it/s]warmup run: 1510it [00:03, 982.24it/s]warmup run: 1164it [00:02, 920.43it/s]warmup run: 1427it [00:03, 941.42it/s]warmup run: 1532it [00:03, 996.73it/s]warmup run: 1613it [00:03, 1008.04it/s]warmup run: 1467it [00:03, 965.82it/s]warmup run: 1465it [00:03, 982.38it/s]warmup run: 1542it [00:03, 1000.25it/s]warmup run: 1611it [00:03, 984.66it/s]warmup run: 1262it [00:02, 933.92it/s]warmup run: 1526it [00:03, 955.06it/s]warmup run: 1637it [00:03, 1009.49it/s]warmup run: 1716it [00:03, 1011.61it/s]warmup run: 1566it [00:03, 969.72it/s]warmup run: 1567it [00:03, 993.30it/s]warmup run: 1646it [00:03, 1009.59it/s]warmup run: 1711it [00:03, 985.09it/s]warmup run: 1360it [00:02, 946.20it/s]warmup run: 1625it [00:03, 962.81it/s]warmup run: 1742it [00:03, 1021.06it/s]warmup run: 1665it [00:03, 972.42it/s]warmup run: 1819it [00:03, 997.16it/s] warmup run: 1669it [00:03, 999.64it/s]warmup run: 1749it [00:03, 1014.68it/s]warmup run: 1811it [00:03, 983.96it/s]warmup run: 1458it [00:02, 951.56it/s]warmup run: 1726it [00:03, 976.71it/s]warmup run: 1847it [00:03, 1029.24it/s]warmup run: 1764it [00:03, 974.95it/s]warmup run: 1772it [00:03, 1006.75it/s]warmup run: 1920it [00:03, 988.92it/s]warmup run: 1852it [00:03, 1018.33it/s]warmup run: 1911it [00:03, 983.65it/s]warmup run: 1556it [00:03, 958.58it/s]warmup run: 1829it [00:03, 990.88it/s]warmup run: 1953it [00:03, 1037.15it/s]warmup run: 1863it [00:03, 978.44it/s]warmup run: 1875it [00:03, 1011.26it/s]warmup run: 2021it [00:03, 994.24it/s]warmup run: 2015it [00:03, 998.02it/s]warmup run: 1955it [00:03, 956.51it/s] warmup run: 1930it [00:03, 994.00it/s]warmup run: 2066it [00:03, 1064.27it/s]warmup run: 1654it [00:03, 895.68it/s]warmup run: 1962it [00:03, 980.21it/s]warmup run: 1978it [00:03, 1015.16it/s]warmup run: 2140it [00:03, 1051.10it/s]warmup run: 2136it [00:03, 1060.21it/s]warmup run: 2052it [00:03, 957.85it/s]warmup run: 2035it [00:03, 1008.83it/s]warmup run: 2189it [00:03, 1111.73it/s]warmup run: 1746it [00:03, 898.71it/s]warmup run: 2074it [00:03, 1019.61it/s]warmup run: 2095it [00:03, 1059.16it/s]warmup run: 2259it [00:03, 1091.34it/s]warmup run: 2258it [00:03, 1106.26it/s]warmup run: 2174it [00:03, 1032.53it/s]warmup run: 2156it [00:03, 1068.32it/s]warmup run: 2313it [00:03, 1147.86it/s]warmup run: 1840it [00:03, 908.03it/s]warmup run: 2194it [00:03, 1073.31it/s]warmup run: 2217it [00:03, 1104.39it/s]warmup run: 2379it [00:03, 1121.84it/s]warmup run: 2380it [00:03, 1138.96it/s]warmup run: 2296it [00:03, 1086.48it/s]warmup run: 2277it [00:03, 1110.46it/s]warmup run: 2437it [00:03, 1173.23it/s]warmup run: 1934it [00:03, 915.63it/s]warmup run: 2315it [00:03, 1111.87it/s]warmup run: 2338it [00:03, 1135.20it/s]warmup run: 2499it [00:03, 1143.55it/s]warmup run: 2500it [00:03, 1156.73it/s]warmup run: 2418it [00:03, 1124.21it/s]warmup run: 2398it [00:03, 1139.76it/s]warmup run: 2560it [00:03, 1187.24it/s]warmup run: 2033it [00:03, 936.00it/s]warmup run: 2436it [00:03, 1140.60it/s]warmup run: 2459it [00:03, 1156.02it/s]warmup run: 2620it [00:04, 1162.01it/s]warmup run: 2618it [00:04, 1161.97it/s]warmup run: 2540it [00:03, 1151.29it/s]warmup run: 2519it [00:04, 1160.20it/s]warmup run: 2680it [00:04, 1190.59it/s]warmup run: 2151it [00:03, 1005.59it/s]warmup run: 2557it [00:04, 1160.44it/s]warmup run: 2580it [00:04, 1171.49it/s]warmup run: 2741it [00:04, 1176.03it/s]warmup run: 2737it [00:04, 1169.26it/s]warmup run: 2662it [00:04, 1169.57it/s]warmup run: 2640it [00:04, 1174.25it/s]warmup run: 2800it [00:04, 1191.62it/s]warmup run: 2269it [00:03, 1055.12it/s]warmup run: 2680it [00:04, 1178.74it/s]warmup run: 2702it [00:04, 1183.01it/s]warmup run: 2862it [00:04, 1183.78it/s]warmup run: 2855it [00:04, 1170.14it/s]warmup run: 2783it [00:04, 1180.03it/s]warmup run: 2759it [00:04, 1178.03it/s]warmup run: 2921it [00:04, 1196.33it/s]warmup run: 2386it [00:03, 1088.63it/s]warmup run: 2801it [00:04, 1185.88it/s]warmup run: 2821it [00:04, 1182.63it/s]warmup run: 2981it [00:04, 1170.18it/s]warmup run: 3000it [00:04, 682.28it/s] warmup run: 3000it [00:04, 691.47it/s] warmup run: 2974it [00:04, 1175.34it/s]warmup run: 2905it [00:04, 1189.35it/s]warmup run: 2879it [00:04, 1183.87it/s]warmup run: 3000it [00:04, 682.87it/s] warmup run: 2503it [00:04, 1110.62it/s]warmup run: 2920it [00:04, 1186.91it/s]warmup run: 2940it [00:04, 1180.78it/s]warmup run: 3000it [00:04, 691.23it/s] warmup run: 3000it [00:04, 676.97it/s] warmup run: 3000it [00:04, 675.69it/s] warmup run: 2999it [00:04, 1187.78it/s]warmup run: 3000it [00:04, 672.71it/s] warmup run: 2622it [00:04, 1132.02it/s]warmup run: 2740it [00:04, 1144.54it/s]warmup run: 2857it [00:04, 1150.21it/s]warmup run: 2976it [00:04, 1160.56it/s]warmup run: 3000it [00:04, 673.46it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.69it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.56it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.97it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.98it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.30it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.76it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.72it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1533.85it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1679.64it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1659.60it/s]warmup should be done:  10%|█         | 309/3000 [00:00<00:01, 1542.44it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.21it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.54it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1605.36it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1608.98it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1604.77it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1678.69it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1656.62it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1656.30it/s]warmup should be done:  15%|█▌        | 464/3000 [00:00<00:01, 1541.46it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1601.08it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1659.77it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1606.06it/s]warmup should be done:  16%|█▌        | 484/3000 [00:00<00:01, 1591.76it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1654.76it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1658.79it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1668.43it/s]warmup should be done:  21%|██        | 619/3000 [00:00<00:01, 1538.15it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1608.92it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1594.44it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1648.55it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1596.10it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1653.51it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1658.43it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1668.36it/s]warmup should be done:  26%|██▌       | 777/3000 [00:00<00:01, 1551.81it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1611.44it/s]warmup should be done:  27%|██▋       | 804/3000 [00:00<00:01, 1586.94it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1634.96it/s]warmup should be done:  27%|██▋       | 805/3000 [00:00<00:01, 1580.34it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1658.01it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1671.48it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1652.16it/s]warmup should be done:  31%|███▏      | 938/3000 [00:00<00:01, 1569.14it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1612.09it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1585.12it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1590.65it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1627.78it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1654.49it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1670.02it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1648.84it/s]warmup should be done:  37%|███▋      | 1098/3000 [00:00<00:01, 1576.03it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1609.29it/s]warmup should be done:  37%|███▋      | 1122/3000 [00:00<00:01, 1577.44it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1624.95it/s]warmup should be done:  38%|███▊      | 1127/3000 [00:00<00:01, 1583.82it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1655.31it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1648.35it/s]warmup should be done:  42%|████▏     | 1257/3000 [00:00<00:01, 1577.55it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1609.16it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:01, 1656.08it/s]warmup should be done:  43%|████▎     | 1280/3000 [00:00<00:01, 1575.12it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1621.16it/s]warmup should be done:  43%|████▎     | 1286/3000 [00:00<00:01, 1559.73it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1652.20it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1650.19it/s]warmup should be done:  47%|████▋     | 1420/3000 [00:00<00:00, 1591.36it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1608.44it/s]warmup should be done:  48%|████▊     | 1438/3000 [00:00<00:00, 1569.90it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1628.64it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1623.15it/s]warmup should be done:  48%|████▊     | 1444/3000 [00:00<00:00, 1563.89it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1651.79it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1649.58it/s]warmup should be done:  53%|█████▎    | 1582/3000 [00:01<00:00, 1597.30it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1609.92it/s]warmup should be done:  53%|█████▎    | 1597/3000 [00:01<00:00, 1573.81it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1635.55it/s]warmup should be done:  53%|█████▎    | 1603/3000 [00:01<00:00, 1571.65it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1604.58it/s]warmup should be done:  61%|██████    | 1825/3000 [00:01<00:00, 1652.79it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1651.25it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1608.31it/s]warmup should be done:  58%|█████▊    | 1745/3000 [00:01<00:00, 1604.91it/s]warmup should be done:  59%|█████▊    | 1756/3000 [00:01<00:00, 1577.58it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1639.73it/s]warmup should be done:  59%|█████▊    | 1761/3000 [00:01<00:00, 1567.03it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1601.81it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1654.18it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1652.95it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1608.75it/s]warmup should be done:  64%|██████▎   | 1908/3000 [00:01<00:00, 1610.77it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1642.11it/s]warmup should be done:  64%|██████▍   | 1915/3000 [00:01<00:00, 1579.64it/s]warmup should be done:  64%|██████▍   | 1921/3000 [00:01<00:00, 1576.14it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1601.11it/s]warmup should be done:  72%|███████▏  | 2157/3000 [00:01<00:00, 1654.43it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1651.42it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1607.84it/s]warmup should be done:  69%|██████▉   | 2072/3000 [00:01<00:00, 1618.97it/s]warmup should be done:  72%|███████▏  | 2148/3000 [00:01<00:00, 1643.77it/s]warmup should be done:  69%|██████▉   | 2074/3000 [00:01<00:00, 1581.16it/s]warmup should be done:  69%|██████▉   | 2080/3000 [00:01<00:00, 1578.66it/s]warmup should be done:  72%|███████▏  | 2155/3000 [00:01<00:00, 1601.43it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1656.14it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1608.05it/s]warmup should be done:  75%|███████▍  | 2237/3000 [00:01<00:00, 1625.40it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1648.54it/s]warmup should be done:  74%|███████▍  | 2233/3000 [00:01<00:00, 1583.13it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1645.85it/s]warmup should be done:  75%|███████▍  | 2242/3000 [00:01<00:00, 1588.28it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1601.77it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1654.80it/s]warmup should be done:  81%|████████  | 2424/3000 [00:01<00:00, 1606.24it/s]warmup should be done:  80%|████████  | 2401/3000 [00:01<00:00, 1626.84it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1641.86it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1582.48it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1650.19it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1591.36it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1594.59it/s]warmup should be done:  89%|████████▊ | 2657/3000 [00:01<00:00, 1656.72it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1607.41it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1641.24it/s]warmup should be done:  88%|████████▊ | 2649/3000 [00:01<00:00, 1657.93it/s]warmup should be done:  85%|████████▌ | 2552/3000 [00:01<00:00, 1585.20it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1596.42it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1590.91it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1607.76it/s]warmup should be done:  94%|█████████▍| 2824/3000 [00:01<00:00, 1658.43it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1608.15it/s]warmup should be done:  94%|█████████▍| 2821/3000 [00:01<00:00, 1644.91it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1663.72it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1584.83it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1582.34it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1618.22it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1566.01it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1663.25it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1612.12it/s]warmup should be done: 100%|█████████▉| 2988/3000 [00:01<00:00, 1649.87it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1670.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.26it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1588.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.66it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1595.65it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.51it/s]warmup should be done:  99%|█████████▉| 2972/3000 [00:01<00:00, 1630.16it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1569.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1590.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1584.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1580.16it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1698.64it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1599.52it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1726.90it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.71it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.42it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.44it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.74it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1690.64it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1727.97it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1706.89it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1630.99it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1685.16it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.28it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1634.33it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1678.54it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1689.72it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1727.48it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1712.72it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1685.73it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1660.46it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1676.69it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1689.45it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1636.93it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1622.70it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1714.71it/s]warmup should be done:  23%|██▎       | 693/3000 [00:00<00:01, 1729.00it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1677.86it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1687.87it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.80it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1640.13it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1690.33it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1610.16it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1712.94it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1688.38it/s]warmup should be done:  29%|██▉       | 867/3000 [00:00<00:01, 1729.47it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1683.56it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1662.78it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1691.98it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1640.45it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1636.11it/s]warmup should be done:  35%|███▍      | 1040/3000 [00:00<00:01, 1729.58it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1687.32it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1688.89it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1711.73it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1661.14it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1689.37it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1636.07it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1651.33it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1689.02it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1684.94it/s]warmup should be done:  40%|████      | 1202/3000 [00:00<00:01, 1711.57it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1687.23it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1659.41it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1637.59it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1663.76it/s]warmup should be done:  40%|████      | 1213/3000 [00:00<00:01, 1703.27it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1694.02it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1688.02it/s]warmup should be done:  46%|████▌     | 1375/3000 [00:00<00:00, 1714.34it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1660.92it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1691.72it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1634.96it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1670.83it/s]warmup should be done:  46%|████▌     | 1385/3000 [00:00<00:00, 1706.15it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1696.47it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1693.70it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1714.11it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1692.90it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1660.21it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1636.96it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1677.21it/s]warmup should be done:  52%|█████▏    | 1556/3000 [00:00<00:00, 1706.22it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1694.37it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1693.22it/s]warmup should be done:  57%|█████▋    | 1719/3000 [00:01<00:00, 1713.73it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1662.88it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1695.07it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1639.05it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1678.38it/s]warmup should be done:  58%|█████▊    | 1727/3000 [00:01<00:00, 1700.87it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1696.93it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1690.89it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1712.76it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1663.43it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1693.65it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1638.91it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1683.59it/s]warmup should be done:  63%|██████▎   | 1898/3000 [00:01<00:00, 1697.73it/s]warmup should be done:  68%|██████▊   | 2037/3000 [00:01<00:00, 1700.80it/s]warmup should be done:  69%|██████▉   | 2063/3000 [00:01<00:00, 1714.10it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1687.30it/s]warmup should be done:  67%|██████▋   | 2003/3000 [00:01<00:00, 1662.97it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1637.08it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1688.67it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1686.31it/s]warmup should be done:  69%|██████▉   | 2069/3000 [00:01<00:00, 1699.05it/s]warmup should be done:  74%|███████▎  | 2209/3000 [00:01<00:00, 1703.99it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1709.14it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1662.93it/s]warmup should be done:  73%|███████▎  | 2203/3000 [00:01<00:00, 1683.94it/s]warmup should be done:  71%|███████▏  | 2140/3000 [00:01<00:00, 1640.35it/s]warmup should be done:  74%|███████▎  | 2209/3000 [00:01<00:00, 1682.21it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1677.08it/s]warmup should be done:  75%|███████▍  | 2240/3000 [00:01<00:00, 1701.94it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1711.32it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1663.12it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1645.82it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1704.89it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1682.04it/s]warmup should be done:  78%|███████▊  | 2345/3000 [00:01<00:00, 1677.56it/s]warmup should be done:  80%|████████  | 2411/3000 [00:01<00:00, 1703.34it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1672.98it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1716.73it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1664.61it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1651.92it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1683.80it/s]warmup should be done:  86%|████████▌ | 2577/3000 [00:01<00:00, 1703.94it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1679.27it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1705.81it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1668.95it/s]warmup should be done:  91%|█████████ | 2727/3000 [00:01<00:00, 1715.91it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1670.01it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1684.61it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1679.43it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1644.64it/s]warmup should be done:  92%|█████████▏| 2748/3000 [00:01<00:00, 1686.31it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1707.45it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1662.80it/s]warmup should be done:  97%|█████████▋| 2899/3000 [00:01<00:00, 1709.01it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1683.96it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1681.79it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1679.85it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1650.49it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1708.12it/s]warmup should be done:  97%|█████████▋| 2917/3000 [00:01<00:00, 1671.26it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1655.09it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1709.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1698.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.35it/s]warmup should be done: 100%|█████████▉| 2990/3000 [00:01<00:00, 1694.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.81it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ce5040>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ffa730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ffcd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ce51c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ce8190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ce71f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3cf52b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feee3ff9b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 22:32:29.016447: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea1e82c380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.016513: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.025620: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.595144: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea0e830780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.595206: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.604683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.677474: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea1702ca30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.677543: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.685124: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.786038: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea1e795ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.786102: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.794219: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.926784: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea1f02d600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.926849: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.932275: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea1b028bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.932325: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.937542: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.939641: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.955295: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea168380a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.955360: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.965250: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:29.972054: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fea1e8379f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:32:29.972112: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:32:29.982254: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:32:36.099090: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.577143: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.676354: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.678827: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.702750: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.738168: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.800789: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:32:36.874873: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:33:38.102][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:33:38.103][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.109][ERROR][RK0][main]: coll ps creation done
[HCTR][22:33:38.109][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][22:33:38.227][ERROR][RK0][tid #140644265748224]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:33:38.227][ERROR][RK0][tid #140644265748224]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.232][ERROR][RK0][tid #140644265748224]: coll ps creation done
[HCTR][22:33:38.232][ERROR][RK0][tid #140644265748224]: replica 5 waits for coll ps creation barrier
[HCTR][22:33:38.471][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:33:38.471][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.472][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:33:38.472][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.479][ERROR][RK0][main]: coll ps creation done
[HCTR][22:33:38.479][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][22:33:38.480][ERROR][RK0][main]: coll ps creation done
[HCTR][22:33:38.480][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][22:33:38.500][ERROR][RK0][tid #140644072814336]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:33:38.501][ERROR][RK0][tid #140644072814336]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.508][ERROR][RK0][tid #140644072814336]: coll ps creation done
[HCTR][22:33:38.508][ERROR][RK0][tid #140644072814336]: replica 3 waits for coll ps creation barrier
[HCTR][22:33:38.513][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:33:38.513][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.518][ERROR][RK0][main]: coll ps creation done
[HCTR][22:33:38.518][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][22:33:38.633][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:33:38.633][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.640][ERROR][RK0][main]: coll ps creation done
[HCTR][22:33:38.640][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][22:33:38.701][ERROR][RK0][tid #140643661768448]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:33:38.702][ERROR][RK0][tid #140643661768448]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:33:38.706][ERROR][RK0][tid #140643661768448]: coll ps creation done
[HCTR][22:33:38.706][ERROR][RK0][tid #140643661768448]: replica 1 waits for coll ps creation barrier
[HCTR][22:33:38.706][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][22:33:39.556][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][22:33:39.586][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][tid #140643661768448]: replica 1 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][tid #140644265748224]: replica 5 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][tid #140644072814336]: replica 3 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][22:33:39.586][ERROR][RK0][main]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][main]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][tid #140643661768448]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][tid #140644265748224]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][main]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][tid #140644072814336]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][main]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][main]: Calling build_v2
[HCTR][22:33:39.586][ERROR][RK0][tid #140643661768448]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][tid #140644265748224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][tid #140644072814336]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:33:39.586][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-11 22:33:39[2022-12-11 22:33:39[2022-12-11 22:33:39.2022-12-11 22:33:392022-12-11 22:33:392022-12-11 22:33:39[.2022-12-11 22:33:39.586333...586331.586331: 586352586353586362: 2022-12-11 22:33:39586352: E: : : E.: E EEE 586384E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:::136 :136] 136136136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] using concurrent impl MPSPhase] ] ] using concurrent impl MPSPhase:] using concurrent impl MPSPhase
using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase

136using concurrent impl MPSPhase


] 
using concurrent impl MPSPhase
[2022-12-11 22:33:39.590837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:33:39.590875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-11 22:33:39] .assigning 8 to cpu590883
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:33:39.590930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu[
[2022-12-11 22:33:392022-12-11 22:33:39..590935590956: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::178212] ] v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[

[2022-12-11 22:33:39[2022-12-11 22:33:39.2022-12-11 22:33:39[.591002590996.[2022-12-11 22:33:39: : 5910182022-12-11 22:33:39.EE[: .591033  2022-12-11 22:33:39E591037: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. [: E::591086/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:33:39E 212[178: :. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 22:33:39] Ev100x8, slow pcie196591147/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. 
] : :213
591189/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuE178] [: E:
 [] remote time is 8.684212022-12-11 22:33:39 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:33:39v100x8, slow pcie
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [:.
591295:[v100x8, slow pcie2022-12-11 22:33:39178591319: 178[2022-12-11 22:33:39
.] : E] 2022-12-11 22:33:39.591394v100x8, slow pcie[E v100x8, slow pcie.591392: 
2022-12-11 22:33:39 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
591416: E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:: E[ 591457:2022-12-11 22:33:39196E 2022-12-11 22:33:39/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 213.]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:E] 591517assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:591529212 remote time is 8.68421: 
:214: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E196] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] [cpu time is 97.0588 
196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 8 to cpu2022-12-11 22:33:39
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :[
2022-12-11 22:33:39.:assigning 8 to cpu1962022-12-11 22:33:39.591678196
] .591714: [] assigning 8 to cpu591724: E2022-12-11 22:33:39assigning 8 to cpu
: [E .
E2022-12-11 22:33:39 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc591788[ ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 2022-12-11 22:33:39/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc591827:214E.[:: 212]  5918702022-12-11 22:33:39213E] cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
:E591918remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212 : 
:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[:[ ] 
2022-12-11 22:33:392122022-12-11 22:33:39/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] .:[
592047build asymm link desc with 8X Tesla V100-SXM2-32GB out of 85920502122022-12-11 22:33:39: 
: [] .EE2022-12-11 22:33:39[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8592100  .2022-12-11 22:33:39
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc592135.E214[:: 592159 ] 2022-12-11 22:33:39213E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588.]  E:
592218remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 213: 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:213remote time is 8.68421 [213] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:33:39] remote time is 8.68421[:.remote time is 8.68421
2022-12-11 22:33:39213592337
.] [: 592377remote time is 8.68421[2022-12-11 22:33:39E: 
2022-12-11 22:33:39. E.592420[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 592432: 2022-12-11 22:33:39:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E.214:E 592467] 214 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E
cpu time is 97.0588:214 
214] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] cpu time is 97.0588:cpu time is 97.0588
214
] cpu time is 97.0588
[2022-12-11 22:34:57.292848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:34:57.332959: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 22:34:57.333039: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 22:34:57.334097: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 22:34:57.409654: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 22:34:57.805880: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 22:34:57.805979: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 22:35:04.578669: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 22:35:04.578760: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 22:35:06.297198: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 22:35:06.297299: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 22:35:06.300152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 22:35:06.300226: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 22:35:06.746032: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 22:35:06.777841: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 22:35:06.779234: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 22:35:06.799313: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 22:35:07.360981: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 22:35:07.364312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 22:35:07.368532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 22:35:07.372515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 22:35:07.376436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 22:35:07.380112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 22:35:07.382968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 22:35:07.385793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 22:35:07.388620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 22:35:17.504819: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 22:35:17.512440: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 22:35:17.513523: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 22:35:17.560659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:35:17.560756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:35:17.560787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:35:17.560815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:35:17.561448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:35:17.561502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.562672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.563353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.576116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 22:35:17.576194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:35:17.576636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:35:17.576690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.577581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.578223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 22:35:172022-12-11 22:35:17..578513578511: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved3 solved

[2022-12-11 22:35:17[.2022-12-11 22:35:17578637.: 578641E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 4 initing device 4] 
worker 0 thread 3 initing device 3
[2022-12-11 22:35:17.579109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:35:17.579151: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:35:17:.1815579166] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.579230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.580671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.580732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.581930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[[2022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:17...581964581981581972: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:::2021980202] ] ] 1 solvedeager alloc mem 381.47 MB2 solved


[2022-12-11 22:35:17[.2022-12-11 22:35:17582132.: 582136E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 1 initing device 1] 
worker 0 thread 2 initing device 2
[2022-12-11 22:35:17.582629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 22:35:17
.582649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 22:35:17
.582687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.582721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 22:35:172022-12-11 22:35:17..583412583416: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved5 solved

[[2022-12-11 22:35:172022-12-11 22:35:17..583537583540: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 5 initing device 5

[2022-12-11 22:35:17.584045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:35:17.584073[: 2022-12-11 22:35:17E. 584087/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.584173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.585620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.585724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.586902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.587013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.587692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.588254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.589475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.589666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:35:17.641123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:35:17.646497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.646616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.647501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.648097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.649104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.650974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.651725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.651770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 977.96 MB
[2022-12-11 22:35:17.657022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:35:17.661340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:35:17.661504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:35:17.662220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.662315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.663121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.663648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.664621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.666017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes[[
2022-12-11 22:35:172022-12-11 22:35:17..666044666048: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-11 22:35:17.666406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:35:17.666620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.666699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.666723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.666807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.667550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.668260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.668694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.668878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.669654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.669840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.670462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.671186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.671229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.53 MB
[2022-12-11 22:35:17.671461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.671537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.671565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.671648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.671747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.671822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.671853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:35:17.671931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:35:17.673657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.674174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.674678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.675225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:35:17.675451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.676151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.676190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 974.35 MB
[2022-12-11 22:35:17.676206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.676266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 22:35:17.676304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:35:17] .eager alloc mem 611.00 KB676322
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 22:35:17eager alloc mem 25.25 KB.
676368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.677062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.677104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.08 MB
[2022-12-11 22:35:17.677161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.677233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.677277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.677334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.683232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.683334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.683453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.683646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:35:17.683950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.683993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 979.88 MB
[2022-12-11 22:35:17.684037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.684080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.18 MB
[2022-12-11 22:35:17.684159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.684202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.89 MB
[2022-12-11 22:35:17.684358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:35:17.684404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 979.97 MB
[[[[[[[[2022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:172022-12-11 22:35:17........967281967281967281967281967285967285967284967283: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] Device 0 init p2p of link 3] ] ] ] ] Device 3 init p2p of link 2Device 7 init p2p of link 4
Device 2 init p2p of link 1Device 5 init p2p of link 6Device 1 init p2p of link 7Device 4 init p2p of link 5Device 6 init p2p of link 0






[[2022-12-11 22:35:172022-12-11 22:35:17..[[[967764967769[[2022-12-11 22:35:172022-12-11 22:35:17[2022-12-11 22:35:17: : 2022-12-11 22:35:172022-12-11 22:35:17..2022-12-11 22:35:17.EE..967775967777.967775  967777967777: : 967788: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : EE: E::EE  E 19801980  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KBeager alloc mem 611.00 KB::19801980:1980

19801980] ] 1980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB

eager alloc mem 611.00 KB



[[2022-12-11 22:35:172022-12-11 22:35:17..968803968808: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 22:35:17.[9688652022-12-11 22:35:17[: [.2022-12-11 22:35:17E2022-12-11 22:35:17[968869.[ .2022-12-11 22:35:17: 9688802022-12-11 22:35:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc968891.E: .:: 968898 E968902638E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : ]  E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638: 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:eager release cuda mem 625663] :] 638
eager release cuda mem 625663638eager release cuda mem 625663] 
] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 22:35:17.995368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 22:35:17.995515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.995718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 22:35:17[.2022-12-11 22:35:17995806.: 995812E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19262022-12-11 22:35:17:] .1926Device 0 init p2p of link 6995872] 
: Device 1 init p2p of link 2E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:35:172022-12-11 22:35:17..996033996035: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 22:35:17.996310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.996340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 22:35:17.996429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 22:35:17.996481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.996556: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:35:17:.1926996581] : Device 5 init p2p of link 4E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:35:172022-12-11 22:35:17..996709996712: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 22:35:17[.2022-12-11 22:35:17996887.: 996892E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663[
2022-12-11 22:35:17.996958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 22:35:17.997122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:17.997276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.997381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.997559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:17.997900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18.  8667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 22:35:18.  8780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18.  8902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 22:35:18.  9018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 22:35:18.  9030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 22:35:18.  9150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18.  9439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 22:35:18[.2022-12-11 22:35:18  9553.:   9554E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 22:35:18.  9641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 22:35:18.  9774: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18.  9815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:35:182022-12-11 22:35:18..  9920  9934: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 6 init p2p of link 4eager release cuda mem 625663

[2022-12-11 22:35:18. 10051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 22:35:18. 10097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 10147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:35:181926.]  10172Device 5 init p2p of link 7: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 10295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 10402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 10559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 10884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 10989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 11079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 23671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 22:35:18. 23785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 24564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 24759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 22:35:18. 24874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 25143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 22:35:18. 25203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 22:35:18. 25261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 25327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 25398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 22:35:18. 25509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 25649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 26052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 26123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 26194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 22:35:18. 26296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 22:35:18
. 26316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 26471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[[2022-12-11 22:35:182022-12-11 22:35:18.. 26584 26593: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 5 init p2p of link 3eager alloc mem 611.00 KB

[2022-12-11 22:35:18. 26758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:35:18. 27116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 27451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 27534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:35:18. 40211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 40329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 41178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 41457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 41722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 41965: [E2022-12-11 22:35:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 41976:: 638E]  eager release cuda mem 8399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 42457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:35:18. 42716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1997409 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5975121 / 100000000 nodes ( 5.98 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 980.18 MB | 0.460002 secs 
[2022-12-11 22:35:18. 43191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1985475 / 100000000 nodes ( 1.99 %~2.00 %) | remote 5987055 / 100000000 nodes ( 5.99 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 974.35 MB | 0.463975 secs 
[2022-12-11 22:35:18. 43747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1992862 / 100000000 nodes ( 1.99 %~2.00 %) | remote 5979668 / 100000000 nodes ( 5.98 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 977.96 MB | 0.482261 secs 
[2022-12-11 22:35:18. 44155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999261 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5973269 / 100000000 nodes ( 5.97 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 981.08 MB | 0.464996 secs 
[2022-12-11 22:35:18. 44820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1996784 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5975746 / 100000000 nodes ( 5.98 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 979.88 MB | 0.462141 secs 
[2022-12-11 22:35:18. 44887: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.27 GB
[2022-12-11 22:35:18. 45043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1996980 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5975550 / 100000000 nodes ( 5.98 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 979.97 MB | 0.460963 secs 
[2022-12-11 22:35:18. 45458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1989932 / 100000000 nodes ( 1.99 %~2.00 %) | remote 5982598 / 100000000 nodes ( 5.98 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 976.53 MB | 0.468786 secs 
[2022-12-11 22:35:18. 46091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1998867 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5973663 / 100000000 nodes ( 5.97 %) | cpu 92027470 / 100000000 nodes ( 92.03 %) | 980.89 MB | 0.461925 secs 
[2022-12-11 22:35:19.308841: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.54 GB
[2022-12-11 22:35:19.309025: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.54 GB
[2022-12-11 22:35:19.309418: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.54 GB
[2022-12-11 22:35:20.820475: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 9.80 GB
[2022-12-11 22:35:20.820762: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 9.80 GB
[2022-12-11 22:35:20.821155: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 9.80 GB
[2022-12-11 22:35:22.169296: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.02 GB
[2022-12-11 22:35:22.169565: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.02 GB
[2022-12-11 22:35:22.169877: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.02 GB
[2022-12-11 22:35:23.280978: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.23 GB
[2022-12-11 22:35:23.281309: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.23 GB
[2022-12-11 22:35:23.281868: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 10.23 GB
[2022-12-11 22:35:23.282388: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 10.23 GB
[2022-12-11 22:35:23.283846: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.23 GB
[2022-12-11 22:35:24.452744: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.43 GB
[2022-12-11 22:35:24.452908: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.43 GB
[HCTR][22:35:25.213][ERROR][RK0][tid #140644265748224]: replica 5 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][tid #140643661768448]: replica 1 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][tid #140644072814336]: replica 3 calling init per replica done, doing barrier
[HCTR][22:35:25.213][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][22:35:25.214][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][tid #140644072814336]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][main]: init per replica done
[HCTR][22:35:25.214][ERROR][RK0][tid #140643661768448]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][tid #140644265748224]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:35:25.214][ERROR][RK0][main]: init per replica done
[HCTR][22:35:25.214][ERROR][RK0][main]: init per replica done
[HCTR][22:35:25.214][ERROR][RK0][main]: init per replica done
[HCTR][22:35:25.214][ERROR][RK0][tid #140644072814336]: init per replica done
[HCTR][22:35:25.214][ERROR][RK0][tid #140643661768448]: init per replica done
[HCTR][22:35:25.214][ERROR][RK0][tid #140644265748224]: init per replica done
[HCTR][22:35:25.217][ERROR][RK0][main]: init per replica done
[HCTR][22:35:25.252][ERROR][RK0][tid #140643930203904]: 7 allocated 3276800 at 0x7fceb2238400
[HCTR][22:35:25.252][ERROR][RK0][tid #140643930203904]: 7 allocated 6553600 at 0x7fceb2558400
[HCTR][22:35:25.252][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fce3c238400
[HCTR][22:35:25.252][ERROR][RK0][tid #140643930203904]: 7 allocated 3276800 at 0x7fceb2b98400
[HCTR][22:35:25.252][ERROR][RK0][tid #140643930203904]: 7 allocated 6553600 at 0x7fceb2eb8400
[HCTR][22:35:25.252][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fce3c558400
[HCTR][22:35:25.252][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fce3cb98400
[HCTR][22:35:25.252][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fce3ceb8400
[HCTR][22:35:25.253][ERROR][RK0][tid #140643661768448]: 1 allocated 3276800 at 0x7fcd78238400
[HCTR][22:35:25.253][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fceb0238400
[HCTR][22:35:25.253][ERROR][RK0][tid #140643661768448]: 1 allocated 6553600 at 0x7fcd78558400
[HCTR][22:35:25.253][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fcdcc238400
[HCTR][22:35:25.253][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fceb0558400
[HCTR][22:35:25.253][ERROR][RK0][tid #140643661768448]: 1 allocated 3276800 at 0x7fcd78b98400
[HCTR][22:35:25.253][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fcdcc558400
[HCTR][22:35:25.253][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fceb0b98400
[HCTR][22:35:25.253][ERROR][RK0][tid #140643661768448]: 1 allocated 6553600 at 0x7fcd78eb8400
[HCTR][22:35:25.253][ERROR][RK0][main]: 2 allocated 3276800 at 0x7fcdccb98400
[HCTR][22:35:25.253][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fceb0eb8400
[HCTR][22:35:25.253][ERROR][RK0][main]: 2 allocated 6553600 at 0x7fcdcceb8400
[HCTR][22:35:25.253][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fce48238400
[HCTR][22:35:25.253][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fce48558400
[HCTR][22:35:25.253][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fce48b98400
[HCTR][22:35:25.253][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fce48eb8400
[HCTR][22:35:25.253][ERROR][RK0][tid #140645003945728]: 6 allocated 3276800 at 0x7fcd78238400
[HCTR][22:35:25.253][ERROR][RK0][tid #140645003945728]: 6 allocated 6553600 at 0x7fcd78558400
[HCTR][22:35:25.253][ERROR][RK0][tid #140645003945728]: 6 allocated 3276800 at 0x7fcd78b98400
[HCTR][22:35:25.253][ERROR][RK0][tid #140645003945728]: 6 allocated 6553600 at 0x7fcd78eb8400
[HCTR][22:35:25.256][ERROR][RK0][tid #140643863095040]: 0 allocated 3276800 at 0x7fce82320000
[HCTR][22:35:25.256][ERROR][RK0][tid #140643863095040]: 0 allocated 6553600 at 0x7fce82640000
[HCTR][22:35:25.256][ERROR][RK0][tid #140643863095040]: 0 allocated 3276800 at 0x7fce82c80000
[HCTR][22:35:25.256][ERROR][RK0][tid #140643863095040]: 0 allocated 6553600 at 0x7fce82fa0000
