2022-12-11 20:33:18.546671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.555738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.560011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.564260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.569621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.589271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.598435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.603164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.653176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.658971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.661982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.663015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.663981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.665012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.666093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.667161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.668226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.669361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.670399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.671443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.672522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.673750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.675403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.676432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.676513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.678200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.678880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.679344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.680417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.680890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.682075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.682470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.683832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.684048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.685599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.685842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.687081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.687518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.688639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.690098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.695669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.696553: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.697118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.698433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.699714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.700904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.702170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.703844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.705056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.705376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.706757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.707165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.708226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.708725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.709843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.710152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.711700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.713754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.714802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.715807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.717372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.718459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.720411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.720860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.723271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.723773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.724201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.725668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.726209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.726901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.728411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.728875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.729694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.730402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.730919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.731587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.732279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.732856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.733241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.733966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.738415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.738970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.739685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.742548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.742968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.743155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.743598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.745257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.745431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.745756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.746299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.753984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.767703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.783959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.784527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.785195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.785675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.785717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.787207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.787586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.789405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.789954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.789987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.790794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.792139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.793708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.793821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.794771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.795521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.796114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.798029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.798225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.798916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.799377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.801258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.801338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.802065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.802775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.804693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.804728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.805557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.806169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.808067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.808123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.808761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.809287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.810848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.810929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.811508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.812260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.813562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.813651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.814452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.815049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.816880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.817044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.817608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.818330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.819910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.819957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.820584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.821451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.823033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.823043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.823320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.824313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.826053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.826205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.827183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.827332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.828700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.828954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.830089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.830250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.830294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.831765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.832311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.833685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.833713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.833859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.833950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.837177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.837329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.837393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.837457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.837538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.837633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.841247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.841558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.841587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.841595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.841785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.842810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.844880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.844955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.845108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.845285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.845353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.845582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.847287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.849503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.849523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.849534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.849717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.849726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.849844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.852123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.854771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.854869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.855096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.855156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.855187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.855200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.856653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.858953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.859113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.859362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.859386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.859703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.861098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.863020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.863199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.863587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.863693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.863950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.865065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.865078: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.867196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.867346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.867593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.867799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.867959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.869093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.871170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.871479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.872066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.873110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.874484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.874641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.874892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.876035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.876472: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.877432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.877618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.877690: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.877700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.878642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.880150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.880338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.880382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.880594: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.881621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.883139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.883280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.883367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.886515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.886646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.886706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.887005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.888098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.890294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.890468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.890820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.890939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.891283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.892785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.895169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.895618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.895719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.895916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.895938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.897316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.899797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.900596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.900690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.900851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.932202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.933528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.933646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.938126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.938473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.938796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.943273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.943483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.943716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.949064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.950279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.950485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.955603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.956030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.956619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.960641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.962937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.965117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.966175: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.968621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.970751: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:18.976889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:18.980849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.001271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.005398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.005417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.012227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.012313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.012793: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:33:19.023211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.079564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.118623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.974243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.975444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.975980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.976592: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:19.976656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:33:19.994500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.995156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.995676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.996255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.996779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:19.997462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:33:20.041690: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.041864: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.103785: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:33:20.280683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.281302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.281834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.282292: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.282344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:33:20.299860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.300482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.300996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.301561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.302094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.302559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:33:20.312357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.313272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.313359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.314323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.314469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.315532: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.315593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:33:20.315690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.316362: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.316414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:33:20.330048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.330670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.331216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.332061: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.332174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:33:20.333077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.333497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.333784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.334527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.334759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.335558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.335857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.336655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.336771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.337899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.337945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:33:20.338668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:33:20.349333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.349945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.350445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.351017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.351540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.352015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:33:20.361232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.361812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.361906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.363031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.363113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.364130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.364140: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.364194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:33:20.364818: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.364870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:33:20.375825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.376429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.376960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.377420: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:33:20.377472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:33:20.381382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.381961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.381999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.382031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.382196: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.383141: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:33:20.383156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.383234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.384287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.384309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.385381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.385481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.386358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:33:20.386613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.387099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:33:20.393260: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.393386: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.394561: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:33:20.395289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.395970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.396481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.397072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.397618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:33:20.397995: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.398103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:33:20.398120: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.399107: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:33:20.431969: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.432160: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.432423: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.432591: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.433013: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.433033: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:33:20.433146: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.433673: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:33:20.434123: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:33:20.444905: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.445041: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:33:20.446059: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][20:33:21.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.705][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.712][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.712][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.712][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.713][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.713][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:33:21.713][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 93it [00:01, 78.96it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 102it [00:01, 87.93it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 101it [00:01, 86.54it/s]warmup run: 96it [00:01, 82.14it/s]warmup run: 168it [00:01, 151.71it/s]warmup run: 100it [00:01, 86.14it/s]warmup run: 95it [00:01, 80.84it/s]warmup run: 203it [00:01, 189.16it/s]warmup run: 96it [00:01, 82.28it/s]warmup run: 91it [00:01, 77.89it/s]warmup run: 203it [00:01, 188.56it/s]warmup run: 192it [00:01, 177.89it/s]warmup run: 263it [00:01, 258.50it/s]warmup run: 200it [00:01, 186.47it/s]warmup run: 189it [00:01, 174.17it/s]warmup run: 302it [00:01, 297.41it/s]warmup run: 192it [00:01, 178.22it/s]warmup run: 183it [00:01, 169.78it/s]warmup run: 305it [00:01, 300.84it/s]warmup run: 290it [00:01, 285.81it/s]warmup run: 362it [00:01, 375.75it/s]warmup run: 299it [00:01, 295.41it/s]warmup run: 283it [00:01, 276.91it/s]warmup run: 401it [00:01, 409.26it/s]warmup run: 285it [00:01, 279.61it/s]warmup run: 275it [00:01, 270.94it/s]warmup run: 407it [00:01, 416.86it/s]warmup run: 388it [00:01, 397.55it/s]warmup run: 463it [00:02, 493.56it/s]warmup run: 399it [00:01, 409.64it/s]warmup run: 374it [00:01, 378.16it/s]warmup run: 499it [00:02, 515.61it/s]warmup run: 382it [00:01, 391.03it/s]warmup run: 371it [00:01, 381.80it/s]warmup run: 477it [00:02, 488.14it/s]warmup run: 500it [00:02, 508.18it/s]warmup run: 566it [00:02, 605.26it/s]warmup run: 500it [00:02, 521.68it/s]warmup run: 466it [00:02, 478.80it/s]warmup run: 598it [00:02, 615.83it/s]warmup run: 479it [00:02, 499.11it/s]warmup run: 473it [00:02, 500.92it/s]warmup run: 565it [00:02, 567.55it/s]warmup run: 667it [00:02, 698.13it/s]warmup run: 592it [00:02, 581.22it/s]warmup run: 602it [00:02, 626.17it/s]warmup run: 560it [00:02, 574.56it/s]warmup run: 697it [00:02, 701.28it/s]warmup run: 576it [00:02, 598.30it/s]warmup run: 575it [00:02, 609.84it/s]warmup run: 669it [00:02, 674.97it/s]warmup run: 767it [00:02, 771.52it/s]warmup run: 694it [00:02, 679.18it/s]warmup run: 705it [00:02, 719.63it/s]warmup run: 660it [00:02, 671.65it/s]warmup run: 796it [00:02, 771.90it/s]warmup run: 675it [00:02, 688.00it/s]warmup run: 677it [00:02, 703.73it/s]warmup run: 773it [00:02, 763.43it/s]warmup run: 864it [00:02, 822.58it/s]warmup run: 797it [00:02, 763.04it/s]warmup run: 808it [00:02, 796.36it/s]warmup run: 759it [00:02, 749.13it/s]warmup run: 895it [00:02, 827.92it/s]warmup run: 773it [00:02, 759.90it/s]warmup run: 779it [00:02, 780.73it/s]warmup run: 874it [00:02, 827.04it/s]warmup run: 961it [00:02, 860.04it/s]warmup run: 899it [00:02, 827.55it/s]warmup run: 911it [00:02, 857.28it/s]warmup run: 856it [00:02, 804.66it/s]warmup run: 994it [00:02, 871.78it/s]warmup run: 871it [00:02, 815.57it/s]warmup run: 883it [00:02, 847.27it/s]warmup run: 975it [00:02, 875.06it/s]warmup run: 1059it [00:02, 891.03it/s]warmup run: 1000it [00:02, 876.38it/s]warmup run: 1014it [00:02, 903.83it/s]warmup run: 951it [00:02, 839.70it/s]warmup run: 1094it [00:02, 905.18it/s]warmup run: 969it [00:02, 858.32it/s]warmup run: 986it [00:02, 896.80it/s]warmup run: 1074it [00:02, 904.29it/s]warmup run: 1156it [00:02, 910.50it/s]warmup run: 1103it [00:02, 916.50it/s]warmup run: 1116it [00:02, 921.83it/s]warmup run: 1193it [00:02, 926.30it/s]warmup run: 1046it [00:02, 861.46it/s]warmup run: 1066it [00:02, 883.59it/s]warmup run: 1089it [00:02, 932.17it/s]warmup run: 1175it [00:02, 934.17it/s]warmup run: 1205it [00:02, 943.56it/s]warmup run: 1253it [00:02, 922.60it/s]warmup run: 1292it [00:02, 943.25it/s]warmup run: 1216it [00:02, 931.21it/s]warmup run: 1141it [00:02, 885.08it/s]warmup run: 1191it [00:02, 955.47it/s]warmup run: 1162it [00:02, 900.44it/s]warmup run: 1277it [00:02, 957.99it/s]warmup run: 1309it [00:02, 970.63it/s]warmup run: 1350it [00:02, 933.90it/s]warmup run: 1391it [00:02, 956.07it/s]warmup run: 1315it [00:02, 947.58it/s]warmup run: 1239it [00:02, 911.55it/s]warmup run: 1294it [00:02, 976.64it/s]warmup run: 1260it [00:02, 920.55it/s]warmup run: 1381it [00:02, 980.54it/s]warmup run: 1414it [00:02, 991.29it/s]warmup run: 1447it [00:03, 942.04it/s]warmup run: 1341it [00:02, 942.74it/s]warmup run: 1415it [00:02, 961.41it/s]warmup run: 1491it [00:03, 966.39it/s]warmup run: 1398it [00:02, 993.23it/s]warmup run: 1359it [00:02, 938.57it/s]warmup run: 1485it [00:03, 996.54it/s]warmup run: 1518it [00:03, 1004.74it/s]warmup run: 1544it [00:03, 947.99it/s]warmup run: 1445it [00:03, 969.57it/s]warmup run: 1515it [00:03, 970.09it/s]warmup run: 1591it [00:03, 974.11it/s]warmup run: 1501it [00:03, 1002.81it/s]warmup run: 1458it [00:03, 952.35it/s]warmup run: 1589it [00:03, 1007.00it/s]warmup run: 1622it [00:03, 1013.34it/s]warmup run: 1641it [00:03, 950.08it/s]warmup run: 1548it [00:03, 986.69it/s]warmup run: 1692it [00:03, 984.42it/s]warmup run: 1617it [00:03, 982.89it/s]warmup run: 1556it [00:03, 959.17it/s]warmup run: 1604it [00:03, 997.97it/s] warmup run: 1693it [00:03, 1014.45it/s]warmup run: 1725it [00:03, 1012.80it/s]warmup run: 1737it [00:03, 950.36it/s]warmup run: 1649it [00:03, 992.68it/s]warmup run: 1793it [00:03, 991.78it/s]warmup run: 1717it [00:03, 978.84it/s]warmup run: 1657it [00:03, 971.85it/s]warmup run: 1708it [00:03, 1010.25it/s]warmup run: 1797it [00:03, 1019.53it/s]warmup run: 1828it [00:03, 1013.65it/s]warmup run: 1833it [00:03, 950.07it/s]warmup run: 1752it [00:03, 1001.73it/s]warmup run: 1894it [00:03, 994.65it/s]warmup run: 1816it [00:03, 978.73it/s]warmup run: 1759it [00:03, 983.70it/s]warmup run: 1811it [00:03, 1014.39it/s]warmup run: 1901it [00:03, 1022.70it/s]warmup run: 1931it [00:03, 1015.03it/s]warmup run: 1929it [00:03, 950.42it/s]warmup run: 1855it [00:03, 1009.21it/s]warmup run: 1996it [00:03, 999.55it/s]warmup run: 1915it [00:03, 978.92it/s]warmup run: 1862it [00:03, 994.99it/s]warmup run: 1914it [00:03, 1018.05it/s]warmup run: 2005it [00:03, 1025.63it/s]warmup run: 2037it [00:03, 1026.63it/s]warmup run: 2028it [00:03, 961.13it/s]warmup run: 1958it [00:03, 1012.78it/s]warmup run: 2117it [00:03, 1061.17it/s]warmup run: 2015it [00:03, 984.24it/s]warmup run: 2019it [00:03, 1026.29it/s]warmup run: 1964it [00:03, 999.45it/s]warmup run: 2127it [00:03, 1083.01it/s]warmup run: 2155it [00:03, 1069.94it/s]warmup run: 2149it [00:03, 1033.48it/s]warmup run: 2072it [00:03, 1048.66it/s]warmup run: 2240it [00:03, 1109.69it/s]warmup run: 2134it [00:03, 1043.11it/s]warmup run: 2076it [00:03, 1034.45it/s]warmup run: 2136it [00:03, 1066.57it/s]warmup run: 2250it [00:03, 1124.17it/s]warmup run: 2268it [00:03, 1087.05it/s]warmup run: 2270it [00:03, 1084.16it/s]warmup run: 2193it [00:03, 1096.64it/s]warmup run: 2363it [00:03, 1144.30it/s]warmup run: 2253it [00:03, 1084.32it/s]warmup run: 2195it [00:03, 1079.28it/s]warmup run: 2256it [00:03, 1104.25it/s]warmup run: 2373it [00:03, 1153.68it/s]warmup run: 2386it [00:03, 1114.23it/s]warmup run: 2391it [00:03, 1119.14it/s]warmup run: 2314it [00:03, 1129.79it/s]warmup run: 2486it [00:03, 1168.99it/s]warmup run: 2371it [00:03, 1112.27it/s]warmup run: 2313it [00:03, 1108.26it/s]warmup run: 2377it [00:03, 1135.12it/s]warmup run: 2495it [00:03, 1173.08it/s]warmup run: 2500it [00:03, 1119.39it/s]warmup run: 2511it [00:04, 1143.14it/s]warmup run: 2435it [00:03, 1152.97it/s]warmup run: 2609it [00:04, 1185.21it/s]warmup run: 2490it [00:03, 1134.20it/s]warmup run: 2433it [00:03, 1135.17it/s]warmup run: 2496it [00:03, 1151.17it/s]warmup run: 2617it [00:04, 1186.50it/s]warmup run: 2615it [00:04, 1128.34it/s]warmup run: 2632it [00:04, 1160.59it/s]warmup run: 2556it [00:04, 1168.99it/s]warmup run: 2731it [00:04, 1193.35it/s]warmup run: 2610it [00:04, 1152.07it/s]warmup run: 2553it [00:04, 1154.10it/s]warmup run: 2617it [00:04, 1168.30it/s]warmup run: 2738it [00:04, 1191.29it/s]warmup run: 2731it [00:04, 1136.01it/s]warmup run: 2753it [00:04, 1172.87it/s]warmup run: 2676it [00:04, 1176.73it/s]warmup run: 2854it [00:04, 1201.45it/s]warmup run: 2729it [00:04, 1162.16it/s]warmup run: 2671it [00:04, 1161.56it/s]warmup run: 2737it [00:04, 1175.13it/s]warmup run: 2859it [00:04, 1195.58it/s]warmup run: 2872it [00:04, 1176.91it/s]warmup run: 2845it [00:04, 1130.26it/s]warmup run: 2797it [00:04, 1184.98it/s]warmup run: 2977it [00:04, 1208.43it/s]warmup run: 2849it [00:04, 1171.23it/s]warmup run: 2856it [00:04, 1178.20it/s]warmup run: 2788it [00:04, 1149.88it/s]warmup run: 3000it [00:04, 689.68it/s] warmup run: 2980it [00:04, 1196.96it/s]warmup run: 2993it [00:04, 1184.05it/s]warmup run: 2961it [00:04, 1136.85it/s]warmup run: 3000it [00:04, 687.34it/s] warmup run: 3000it [00:04, 671.30it/s] warmup run: 2917it [00:04, 1188.55it/s]warmup run: 2967it [00:04, 1162.24it/s]warmup run: 2974it [00:04, 1178.39it/s]warmup run: 2904it [00:04, 1151.92it/s]warmup run: 3000it [00:04, 682.72it/s] warmup run: 3000it [00:04, 687.15it/s] warmup run: 3000it [00:04, 686.05it/s] warmup run: 3000it [00:04, 678.09it/s] warmup run: 3000it [00:04, 677.60it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.44it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.59it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.93it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1545.11it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1602.03it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.75it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.96it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.13it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.65it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.78it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1664.79it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.79it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1609.76it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1654.52it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1601.50it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1640.91it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.53it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1614.32it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1664.86it/s]warmup should be done:  16%|█▌        | 484/3000 [00:00<00:01, 1606.96it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1650.66it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1628.32it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1647.45it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1635.06it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1668.78it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1625.02it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1663.42it/s]warmup should be done:  22%|██▏       | 645/3000 [00:00<00:01, 1604.43it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1650.38it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1625.81it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1634.50it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1642.67it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1667.84it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1622.95it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1659.10it/s]warmup should be done:  27%|██▋       | 806/3000 [00:00<00:01, 1600.50it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1646.67it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1631.09it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1631.23it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1599.19it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1663.65it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1649.75it/s]warmup should be done:  32%|███▏      | 967/3000 [00:00<00:01, 1599.16it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1652.82it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1615.04it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1624.71it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1619.36it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1588.98it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1664.02it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1650.29it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1610.09it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1655.45it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1613.14it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1624.25it/s]warmup should be done:  38%|███▊      | 1130/3000 [00:00<00:01, 1588.44it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1616.93it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:01, 1662.61it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1652.89it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1656.54it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1616.65it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1611.06it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1623.72it/s]warmup should be done:  43%|████▎     | 1291/3000 [00:00<00:01, 1593.58it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1592.84it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1654.12it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1660.65it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1657.44it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1620.31it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1623.66it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1607.34it/s]warmup should be done:  48%|████▊     | 1455/3000 [00:00<00:00, 1605.30it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1598.30it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1657.70it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1656.11it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1623.12it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1660.53it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1621.67it/s]warmup should be done:  54%|█████▍    | 1627/3000 [00:01<00:00, 1606.44it/s]warmup should be done:  54%|█████▍    | 1616/3000 [00:01<00:00, 1595.85it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1598.20it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1658.24it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1657.05it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1625.51it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1651.04it/s]warmup should be done:  60%|█████▉    | 1788/3000 [00:01<00:00, 1605.01it/s]warmup should be done:  60%|██████    | 1802/3000 [00:01<00:00, 1618.46it/s]warmup should be done:  59%|█████▉    | 1776/3000 [00:01<00:00, 1585.81it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1599.31it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1657.76it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1657.29it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1625.90it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1654.41it/s]warmup should be done:  65%|██████▍   | 1949/3000 [00:01<00:00, 1604.85it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1617.94it/s]warmup should be done:  65%|██████▍   | 1940/3000 [00:01<00:00, 1599.31it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1601.46it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1656.59it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1654.60it/s]warmup should be done:  70%|███████   | 2111/3000 [00:01<00:00, 1624.95it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1652.48it/s]warmup should be done:  70%|███████   | 2110/3000 [00:01<00:00, 1603.85it/s]warmup should be done:  70%|███████   | 2104/3000 [00:01<00:00, 1609.13it/s]warmup should be done:  71%|███████   | 2126/3000 [00:01<00:00, 1588.44it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1602.45it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1655.37it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1654.72it/s]warmup should be done:  76%|███████▌  | 2274/3000 [00:01<00:00, 1622.74it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1652.39it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1601.29it/s]warmup should be done:  76%|███████▌  | 2267/3000 [00:01<00:00, 1612.79it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1586.99it/s]warmup should be done:  76%|███████▌  | 2282/3000 [00:01<00:00, 1600.75it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1654.73it/s]warmup should be done:  81%|████████  | 2437/3000 [00:01<00:00, 1624.72it/s]warmup should be done:  83%|████████▎ | 2490/3000 [00:01<00:00, 1651.38it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1650.78it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1601.74it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1596.56it/s]warmup should be done:  81%|████████  | 2430/3000 [00:01<00:00, 1616.44it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1599.51it/s]warmup should be done:  89%|████████▊ | 2662/3000 [00:01<00:00, 1655.42it/s]warmup should be done:  87%|████████▋ | 2600/3000 [00:01<00:00, 1625.76it/s]warmup should be done:  89%|████████▉ | 2670/3000 [00:01<00:00, 1654.43it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1596.31it/s]warmup should be done:  89%|████████▊ | 2656/3000 [00:01<00:00, 1617.52it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1611.30it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1617.58it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1603.87it/s]warmup should be done:  94%|█████████▍| 2829/3000 [00:01<00:00, 1658.41it/s]warmup should be done:  92%|█████████▏| 2763/3000 [00:01<00:00, 1626.57it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1660.21it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1600.97it/s]warmup should be done:  94%|█████████▍| 2823/3000 [00:01<00:00, 1630.21it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1624.28it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1622.63it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1609.84it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1660.51it/s]warmup should be done:  98%|█████████▊| 2928/3000 [00:01<00:00, 1633.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.77it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1608.53it/s]warmup should be done: 100%|█████████▉| 2990/3000 [00:01<00:00, 1640.73it/s]warmup should be done:  98%|█████████▊| 2946/3000 [00:01<00:00, 1639.60it/s]warmup should be done:  97%|█████████▋| 2922/3000 [00:01<00:00, 1628.99it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1617.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1613.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.79it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.88it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1706.15it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1705.19it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.98it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.93it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.85it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.59it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.04it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.57it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.82it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1706.04it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1710.06it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1703.77it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1700.74it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1656.88it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1667.90it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.51it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.99it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1703.08it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1702.88it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.75it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1660.42it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1701.54it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1670.54it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1672.88it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1678.50it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1704.87it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1706.18it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1705.23it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1698.41it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1674.65it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1656.56it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1675.39it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1706.18it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1679.71it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1706.03it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1676.32it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1706.94it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1653.47it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1687.56it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1676.74it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1679.78it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1705.53it/s]warmup should be done:  34%|███▍      | 1029/3000 [00:00<00:01, 1709.58it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1674.03it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1658.90it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1695.92it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1681.23it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1677.50it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1681.84it/s]warmup should be done:  40%|████      | 1200/3000 [00:00<00:01, 1708.45it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1701.23it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1666.99it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1670.97it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1673.75it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1682.96it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1684.09it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1683.26it/s]warmup should be done:  46%|████▌     | 1373/3000 [00:00<00:00, 1712.97it/s]warmup should be done:  46%|████▌     | 1369/3000 [00:00<00:00, 1705.41it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1673.25it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:00, 1671.54it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:00, 1673.31it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1679.97it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1687.66it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1682.54it/s]warmup should be done:  51%|█████▏    | 1540/3000 [00:00<00:00, 1705.70it/s]warmup should be done:  52%|█████▏    | 1545/3000 [00:00<00:00, 1712.99it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1674.73it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1673.63it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1669.71it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1677.13it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1691.75it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1683.73it/s]warmup should be done:  57%|█████▋    | 1711/3000 [00:01<00:00, 1706.35it/s]warmup should be done:  57%|█████▋    | 1717/3000 [00:01<00:00, 1713.40it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1677.32it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1676.91it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1668.62it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1676.00it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1694.90it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1685.04it/s]warmup should be done:  63%|██████▎   | 1889/3000 [00:01<00:00, 1715.12it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1708.33it/s]warmup should be done:  62%|██████▏   | 1851/3000 [00:01<00:00, 1679.58it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1679.34it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1668.66it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1676.32it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1694.51it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1684.01it/s]warmup should be done:  69%|██████▊   | 2062/3000 [00:01<00:00, 1717.00it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1709.07it/s]warmup should be done:  67%|██████▋   | 2019/3000 [00:01<00:00, 1678.87it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1679.17it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1668.28it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1675.27it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1692.63it/s]warmup should be done:  73%|███████▎  | 2193/3000 [00:01<00:00, 1682.71it/s]warmup should be done:  74%|███████▍  | 2234/3000 [00:01<00:00, 1716.10it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1677.34it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1677.17it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1702.47it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1666.16it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1671.86it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1693.76it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1682.20it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1713.34it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1676.73it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1678.38it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1695.53it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1662.52it/s]warmup should be done:  79%|███████▉  | 2376/3000 [00:01<00:00, 1669.94it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1695.43it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1684.66it/s]warmup should be done:  86%|████████▌ | 2578/3000 [00:01<00:00, 1715.19it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1677.23it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1678.92it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1693.74it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1663.89it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1669.71it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1696.66it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1685.34it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1717.54it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1680.14it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1692.45it/s]warmup should be done:  90%|████████▉ | 2691/3000 [00:01<00:00, 1660.48it/s]warmup should be done:  90%|████████▉ | 2697/3000 [00:01<00:00, 1662.91it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1664.35it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1695.01it/s]warmup should be done:  97%|█████████▋| 2924/3000 [00:01<00:00, 1718.48it/s]warmup should be done:  96%|█████████▌| 2870/3000 [00:01<00:00, 1679.60it/s]warmup should be done:  95%|█████████▌| 2856/3000 [00:01<00:00, 1680.63it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1690.28it/s]warmup should be done:  95%|█████████▌| 2858/3000 [00:01<00:00, 1654.17it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1656.27it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1659.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1713.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.49it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.39it/s]2022-12-11 20:34:56.226916: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253b833880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:56.226981: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.259583: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253b830bb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.259644: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.333662: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2527f930b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.333726: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.507711: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f09a002e280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.507785: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.582979: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0920030a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.583048: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.658654: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253b830e60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.658730: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.688394: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253782c9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.688470: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:57.693472: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f253382c8f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:34:57.693513: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:34:58.514057: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.560132: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.627093: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.843155: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.877144: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.963753: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.979323: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:34:59.986746: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:35:01.401998: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.418519: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.517951: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.748087: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.778541: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.817745: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.857375: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:35:02.893528: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:35:46.126][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:35:46.127][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:35:46.127][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.127][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.133][ERROR][RK0][main]: coll ps creation done
[HCTR][20:35:46.134][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][20:35:46.137][ERROR][RK0][main]: coll ps creation done
[HCTR][20:35:46.137][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][20:35:46.146][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:35:46.146][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.146][ERROR][RK0][tid #139799348365056]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:35:46.146][ERROR][RK0][tid #139799348365056]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.150][ERROR][RK0][main]: coll ps creation done
[HCTR][20:35:46.150][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][20:35:46.155][ERROR][RK0][tid #139799348365056]: coll ps creation done
[HCTR][20:35:46.155][ERROR][RK0][tid #139799348365056]: replica 1 waits for coll ps creation barrier
[HCTR][20:35:46.164][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:35:46.164][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.171][ERROR][RK0][main]: coll ps creation done
[HCTR][20:35:46.171][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][20:35:46.181][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:35:46.181][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.189][ERROR][RK0][main]: coll ps creation done
[HCTR][20:35:46.189][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][20:35:46.197][ERROR][RK0][tid #139798333339392]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:35:46.197][ERROR][RK0][tid #139798333339392]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.202][ERROR][RK0][tid #139798333339392]: coll ps creation done
[HCTR][20:35:46.202][ERROR][RK0][tid #139798333339392]: replica 7 waits for coll ps creation barrier
[HCTR][20:35:46.214][ERROR][RK0][tid #139798266230528]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:35:46.214][ERROR][RK0][tid #139798266230528]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:35:46.218][ERROR][RK0][tid #139798266230528]: coll ps creation done
[HCTR][20:35:46.218][ERROR][RK0][tid #139798266230528]: replica 6 waits for coll ps creation barrier
[HCTR][20:35:46.218][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][20:35:47.077][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][20:35:47.108][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][tid #139798266230528]: replica 6 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][tid #139799348365056]: replica 1 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][tid #139798333339392]: replica 7 calling init per replica
[HCTR][20:35:47.108][ERROR][RK0][main]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][main]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][main]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][main]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][main]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][tid #139798266230528]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][tid #139799348365056]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][tid #139798333339392]: Calling build_v2
[HCTR][20:35:47.108][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][tid #139798266230528]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][tid #139799348365056]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:35:47.108][ERROR][RK0][tid #139798333339392]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 20:35:472022-12-11 20:35:472022-12-11 20:35:472022-12-11 20:35:47.2022-12-11 20:35:47.2022-12-11 20:35:472022-12-11 20:35:472022-12-11 20:35:47..108405.108406...108419108415: 108423: 108416108413108429: : E: E: : : EE E EEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136:136:::136136] 136] 136136136] ] using concurrent impl MPS] using concurrent impl MPS] ] ] using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS





[2022-12-11 20:35:47.112638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:35:47.112677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 20:35:47196.] 112683assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 20:35:472022-12-11 20:35:47..112726112732: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 20:35:47:178.196] 112757] v100x8, slow pcie: assigning 8 to cpu
[E
2022-12-11 20:35:47 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:35:47112778:.: [[212112796E2022-12-11 20:35:472022-12-11 20:35:47] :  ..build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc112821112837[
 :: : 2022-12-11 20:35:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178[EE.[:] 2022-12-11 20:35:47  1128662022-12-11 20:35:47196v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .[] 
112910::E1129112022-12-11 20:35:47assigning 8 to cpu: 178[212 : .
E] 2022-12-11 20:35:47] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 112952 v100x8, slow pcie.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
112997
178:E[:: [] 213 [2022-12-11 20:35:47178E2022-12-11 20:35:47v100x8, slow pcie] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:35:47.]  .
remote time is 8.68421:.113087v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc113098
[178113113[: 
:: 2022-12-11 20:35:47] : 2022-12-11 20:35:47E196E.[v100x8, slow pcieE. ]  1131792022-12-11 20:35:47
 113207/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:
:E113224:E2022-12-11 20:35:47212196 : 213 .] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc113274[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 8 to cpu: remote time is 8.68421:: 2022-12-11 20:35:47

196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214E.] :[]  [113340assigning 8 to cpu1962022-12-11 20:35:47cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 20:35:47: 
] .
:2022-12-11 20:35:47.Eassigning 8 to cpu113406196.113412 
: ] 113430: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu: E2022-12-11 20:35:47: 
E[ .212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 20:35:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc113506] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.:: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213:2022-12-11 20:35:47113543214E
] 212.: ]  remote time is 8.68421] 113580E[cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  2022-12-11 20:35:47
:
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.212 :2022-12-11 20:35:47113663] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212.: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:35:47:] 113702E
.212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  113727] 
E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-11 20:35:47:[E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2132022-12-11 20:35:47 :113784] .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214: remote time is 8.684211138042022-12-11 20:35:47:] E
: .213cpu time is 97.0588 E113834[] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : 2022-12-11 20:35:47remote time is 8.68421:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.
213: 113886] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: remote time is 8.68421] :2022-12-11 20:35:47E
remote time is 8.68421213. 
] 113934[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421: 2022-12-11 20:35:47[:
E.2022-12-11 20:35:47214 113962.[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 1139712022-12-11 20:35:47cpu time is 97.0588:E: .
214 E113990] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
214: ] 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588] :
cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 20:37:04.900712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:37:04.940820: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 20:37:04.940894: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 20:37:04.941955: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 20:37:05.  9609: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 20:37:05.398796: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 20:37:05.398881: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 20:37:12.125078: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 20:37:12.125172: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 20:37:13.811349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 20:37:13.811449: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 20:37:13.814397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 20:37:13.814459: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 20:37:14. 50587: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 20:37:14. 79213: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 20:37:14. 80695: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 20:37:14.101252: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 20:37:14.620865: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 20:37:27.376683: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 20:37:27.384435: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 20:37:27.385627: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 20:37:27.433255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:37:27.433354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:37:27.433385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:37:27.433413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:37:27.434000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:37:27.434053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.434946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.435693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.448760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 20:37:27.448836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 20:37:27.449226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 20:37:27.449284[: 2022-12-11 20:37:27E. 449289/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[worker 0 thread 3 initing device 3:2022-12-11 20:37:27
1815.] 449305Building Coll Cache with ... num gpu device is 8: 
[E2022-12-11 20:37:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc449336:: 202E]  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 20:37:277 solved:.
202449381] : 1 solvedE[
 [2022-12-11 20:37:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 20:37:27.:2022-12-11 20:37:27.4494031980.449390: ] 449418: Eeager alloc mem 381.47 MB: E 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205:202] 205] worker 0 thread 7 initing device 7] 4 solved
worker 0 thread 1 initing device 1

[2022-12-11 20:37:27.449574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 20:37:27.449756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:37:27.449808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.449971[: 2022-12-11 20:37:27E. 449979/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1815 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8:
1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:37:27.450034: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:37:27[:.2022-12-11 20:37:271815450049.] : 450056Building Coll Cache with ... num gpu device is 8E: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-11 20:37:27.450135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.452313[: 2022-12-11 20:37:27E. 452324/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:6 solved202
] 2 solved
[2022-12-11 20:37:27.452427[: 2022-12-11 20:37:27E. 452437/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 6 initing device 6:
205] worker 0 thread 2 initing device 2
[2022-12-11 20:37:27.452568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.452865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.452903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 20:37:27
[.2022-12-11 20:37:27452929.: 452934[E: 2022-12-11 20:37:27 E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 452969:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 1815:E] 1980 Building Coll Cache with ... num gpu device is 8] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.453079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.453417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.453484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.456989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.457172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.457223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.457292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.457363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 20:37:27eager alloc mem 381.47 MB.
457395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.457887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.461616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.461807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:37:27.514872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 20:37:27.523803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:37:27.523928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:37:27.536273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[[2022-12-11 20:37:27[2022-12-11 20:37:27.2022-12-11 20:37:27.536377.[[536382: [536378[2022-12-11 20:37:272022-12-11 20:37:27: E2022-12-11 20:37:27: 2022-12-11 20:37:27..E .E.536425536425 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu536439 536442: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: EE:1980E:E  1980]  1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::eager alloc mem 1024.00 Bytes
:eager alloc mem 1024.00 Bytes:19801980
1980
1980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-11 20:37:27.537011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.537989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.538917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.539632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:37:27.539677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.80 MB
[2022-12-11 20:37:27.543510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:37:27.543585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:37:27.543671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:37:27.543744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:37:27.543831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:37:27.543922: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:37:27:.638543921] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:37:27.543981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 20:37:27638.] 544013eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:37:27.544067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:37:27.544078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-11 20:37:272022-12-11 20:37:27.544160.: 544147E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[2022-12-11 20:37:27.544247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:37:27.544400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.545338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.546410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.546914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.547453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.547999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.548502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:37:27.549073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.549482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.549840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.549882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.549928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.549981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:37:272022-12-11 20:37:27..550020550028: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 20:37:27.550429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.550805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.550841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.550880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.550944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.550990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.551037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.551430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.551694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:37:27.551737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.15 MB
[2022-12-11 20:37:27.551917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.551960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.552136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:37:27.552181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.70 MB
[2022-12-11 20:37:27.552628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-11 20:37:272022-12-11 20:37:27..552670552673: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 493.14 MB

[2022-12-11 20:37:27.552737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.53 MB
[2022-12-11 20:37:27.554527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.555186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.555244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:37:27.555299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.93 MB
[2022-12-11 20:37:27.555444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:37:27.555896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:37:27.555948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.87 MB
[2022-12-11 20:37:27.556197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:37:27.556245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 489.06 MB
[[[[[[[[2022-12-11 20:37:272022-12-11 20:37:272022-12-11 20:37:272022-12-11 20:37:272022-12-11 20:37:272022-12-11 20:37:272022-12-11 20:37:272022-12-11 20:37:27........690533690538690531690531690531690532690533690532: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 0 init p2p of link 3Device 5 init p2p of link 6Device 7 init p2p of link 4Device 3 init p2p of link 2Device 2 init p2p of link 1Device 4 init p2p of link 5Device 6 init p2p of link 0







[[2022-12-11 20:37:272022-12-11 20:37:27..[6910286910282022-12-11 20:37:27[: : [.2022-12-11 20:37:27EE2022-12-11 20:37:27691040.  .: [[691047/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu691050[E2022-12-11 20:37:272022-12-11 20:37:27: ::: 2022-12-11 20:37:27 ..E19801980E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu691080691080 ] ]  691093:: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 1980EE:

:E]   19801980 eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
::eager alloc mem 611.00 KBeager alloc mem 611.00 KB:19801980

1980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[[2022-12-11 20:37:272022-12-11 20:37:27..692046692048: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 20:37:27.692099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.692127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-11 20:37:27
2022-12-11 20:37:27..692147[692150: 2022-12-11 20:37:27: E.E 692165 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:37:27:E:.638 638692184] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] : eager release cuda mem 625663:eager release cuda mem 625663E
638
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-11 20:37:27.715934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 20:37:27.716050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-11 20:37:27] .Device 1 init p2p of link 2716079
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.716200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.716296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 20:37:27.716447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.716490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:37:27.716599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:37:27.716646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.716758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:37:272022-12-11 20:37:27..716880716891: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 4 init p2p of link 7eager release cuda mem 625663

[2022-12-11 20:37:27.717013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.717061: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:37:27:.1926717091] : Device 0 init p2p of link 6E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.717247: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:37:27:.1980717258] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.717363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 20:37:27.717461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.717503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.717569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.717919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.718081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.718318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.732905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:37:27.733032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.733393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:37:27.733520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.733655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:37:27.733763: [E2022-12-11 20:37:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu733786:: 1926E]  Device 2 init p2p of link 0/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
[:2022-12-11 20:37:271980.] 733837eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.733944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.734011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:37:27.734147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.734271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:37:27.734328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 20:37:272022-12-11 20:37:27..734386734402: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 0 init p2p of link 1eager alloc mem 611.00 KB

[2022-12-11 20:37:27.734550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.734649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.734685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 20:37:27.734751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.734819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.734953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.735228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.735365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.735632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.748636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:37:27.748755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.749116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:37:27.749236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.749413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:37:27.749536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.749561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.749726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:37:27.749855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.750040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.750185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:37:27.750227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:37:27.750305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.750331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:37:27eager release cuda mem 625663.
750350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.750663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.750730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 20:37:27.750796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 20:37:27.750855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.750933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:37:27.751109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.751179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.751673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.751751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:37:27.764627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.765213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.765332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.765739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.766138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.766375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.766887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.767125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:37:27.768149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999403 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2990606 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 492.87 MB | 0.31811 secs 
[2022-12-11 20:37:27.768620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999511 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2990498 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 492.93 MB | 0.318576 secs 
[2022-12-11 20:37:27.769039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999959 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2990050 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 493.14 MB | 0.319678 secs 
[2022-12-11 20:37:27.769449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 997914 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2992095 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 492.15 MB | 0.316382 secs 
[2022-12-11 20:37:27.769994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 991593 / 100000000 nodes ( 0.99 %~1.00 %) | remote 2998416 / 100000000 nodes ( 3.00 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 489.06 MB | 0.317037 secs 
[2022-12-11 20:37:27.770397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999054 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2990955 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 492.70 MB | 0.32028 secs 
[2022-12-11 20:37:27.770935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999264 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2990745 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 492.80 MB | 0.336896 secs 
[2022-12-11 20:37:27.771468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 998710 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2991299 / 100000000 nodes ( 2.99 %) | cpu 96009991 / 100000000 nodes ( 96.01 %) | 492.53 MB | 0.321675 secs 
[2022-12-11 20:37:27.772038: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.12 GB
[2022-12-11 20:37:29.224256: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.38 GB
[2022-12-11 20:37:29.224619: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.38 GB
[2022-12-11 20:37:29.225795: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.38 GB
[2022-12-11 20:37:30.853349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.65 GB
[2022-12-11 20:37:30.853616: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.65 GB
[2022-12-11 20:37:30.853981: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.65 GB
[2022-12-11 20:37:32.252132: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.86 GB
[2022-12-11 20:37:32.252305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.86 GB
[2022-12-11 20:37:32.252892: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.86 GB
[2022-12-11 20:37:33.784309: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.08 GB
[2022-12-11 20:37:33.785416: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.08 GB
[2022-12-11 20:37:33.786630: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.08 GB
[2022-12-11 20:37:35.329584: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.53 GB
[2022-12-11 20:37:35.330122: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.53 GB
[2022-12-11 20:37:35.330496: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.53 GB
[2022-12-11 20:37:36.652215: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.73 GB
[2022-12-11 20:37:36.652361: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.73 GB
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][tid #139799348365056]: replica 1 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][tid #139798266230528]: replica 6 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][tid #139798333339392]: replica 7 calling init per replica done, doing barrier
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][tid #139799348365056]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][tid #139798333339392]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][tid #139798266230528]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:37:36.727][ERROR][RK0][main]: init per replica done
[HCTR][20:37:36.727][ERROR][RK0][main]: init per replica done
[HCTR][20:37:36.727][ERROR][RK0][main]: init per replica done
[HCTR][20:37:36.727][ERROR][RK0][tid #139799348365056]: init per replica done
[HCTR][20:37:36.727][ERROR][RK0][tid #139798266230528]: init per replica done
[HCTR][20:37:36.727][ERROR][RK0][tid #139798333339392]: init per replica done
[HCTR][20:37:36.727][ERROR][RK0][main]: init per replica done
[HCTR][20:37:36.730][ERROR][RK0][main]: init per replica done
[HCTR][20:37:36.733][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.733][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.733][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.733][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.733][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.733][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.733][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.733][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.733][ERROR][RK0][tid #139799348365056]: 1 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.733][ERROR][RK0][tid #139799348365056]: 1 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.733][ERROR][RK0][tid #139799348365056]: 1 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.733][ERROR][RK0][tid #139799348365056]: 1 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.733][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.733][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.733][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.733][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.733][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.733][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.733][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.734][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.734][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.734][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f2720b20000
[HCTR][20:37:36.734][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.734][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.734][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f2721000000
[HCTR][20:37:36.734][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.734][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f2721640000
[HCTR][20:37:36.734][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f2721960000
[HCTR][20:37:36.737][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f2725120000
[HCTR][20:37:36.737][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f2725600000
[HCTR][20:37:36.737][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f272630e800
[HCTR][20:37:36.737][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f272662e800








