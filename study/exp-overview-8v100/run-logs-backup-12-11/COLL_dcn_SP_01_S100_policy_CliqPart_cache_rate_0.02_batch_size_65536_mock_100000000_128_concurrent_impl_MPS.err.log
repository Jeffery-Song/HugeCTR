2022-12-11 22:04:27.096472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.101242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.107912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.112841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.124822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.133389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.137235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.149619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.198747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.202285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.204524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.205055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.205895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.206790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.207483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.208453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.209079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.210297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.210787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.211808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.212305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.213604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.213933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.215522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.215624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.217308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.217394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.219080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.220198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.221266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.222362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.223497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.225314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.226532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.227568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.228593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.229690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.230737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.232066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.233821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.234388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.235787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.236774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.238136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.239417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.239592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.240580: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.240691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.241164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.242136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.242608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.243740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.243945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.245632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.246776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.247770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.248762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.250445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.251645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.252840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.256587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.258761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.261368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.261358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.263422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.264241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.264320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.264479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.266838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.267911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.268000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.268284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.268993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.269948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.271560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.271648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.271992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.272822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.273402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.275017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.275147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.275443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.276339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.276477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.278081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.278132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.278437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.279221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.279434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.283591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.283826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.285293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.285534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.287785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.288317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.288353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.288763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.289478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.291345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.291529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.292161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.293324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.325889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.327033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.327753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.328977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.329011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.329538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.331707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.332729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.333891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.334210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.334227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.336608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.337483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.339376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.339975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.340058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.341776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.342420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.343483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.344087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.344181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.345689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.347086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.347735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.348261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.348396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.350037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.351052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.351814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.352253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.352384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.353924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.354611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.355654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.356252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.356515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.358572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.358622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.359219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.359904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.360021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.362158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.362192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.362669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.363387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.363602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.365919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.366070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.366615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.367496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.367868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.369982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.369992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.370351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.371291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.371548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.371844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.374172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.374413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.374815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.375544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.375556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.375795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.377548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.378432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.378733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.379100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.379928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.380266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.380487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.382494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.383665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.384202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.384453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.385222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.385392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.385654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.387454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.388798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.388925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.389230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.389879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.390279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.390692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.392163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.393635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.394592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.395046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.395277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.395598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.396760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.398667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.398746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.399046: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.399330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.399633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.399777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.400913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.403264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.403449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.404015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.404145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.404334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.405324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.407362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.407414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.408146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.408642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.408834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.409049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.409724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.412575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.412590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.413000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.413881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.414037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.414354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.415322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.417991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.418072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.418557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.419107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.419209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.419487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.420043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.422649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.422827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.423475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.423835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.424100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.424821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.427776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.428476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.428622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.429292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.429963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.432505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.433256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.433920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.434101: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.434360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.436598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.437705: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.438298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.438772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.440035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.441642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.441877: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.443629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.443654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.444398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.445517: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.446804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.447230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.447738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.449198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.450834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.451138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.451445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.451681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.453458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.455116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.455250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.455954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.455969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.458448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.459974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.460107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.460959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.463600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.466974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.467977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.470907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.500913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.504034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.506187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.537471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.543343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.544872: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.554041: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:04:27.554631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.559477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.563560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.565138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.574729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:27.582898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.563950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.564593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.566036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.566914: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.566975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:04:28.584910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.586120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.586792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.587614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.588142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.588798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:04:28.635775: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.635974: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.671816: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:04:28.816588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.817211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.818163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.819408: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.819475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:04:28.837148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.838367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.841188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.842639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.843998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.845554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:04:28.846072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.847159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.848353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.849291: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.849347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:04:28.867398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.868523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.869758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.870757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.871724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.872707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:04:28.875704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.878009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.879086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.879639: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.879694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:04:28.897197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.897838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.898565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.899159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.905088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.905562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.906218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.906489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:04:28.906895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.907582: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.907635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:04:28.919738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.920016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.920641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.921123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.921599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.922329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.922605: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.922684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:04:28.923405: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.923478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:04:28.925118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.925745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.926255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.926824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.927346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.927819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:04:28.933684: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.933852: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.935500: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:04:28.938243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.939458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.940598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.940841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.941296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.942168: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:04:28.942232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:04:28.942991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.943472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.944643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.945067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.946460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.946723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.948092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.948360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.949613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:04:28.949843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:04:28.953163: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.953335: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.954979: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:04:28.955917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.956077: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.957952: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:04:28.960090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.961173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.962173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.963260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.964286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:04:28.965230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:04:28.973745: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.973915: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.974808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:04:28.996556: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.996750: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.997083: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.997218: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:28.998399: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:04:28.998932: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:04:29.010930: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:29.011108: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:04:29.012792: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][22:04:30.269][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.269][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.270][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.270][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.270][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.270][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.316][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:04:30.316][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 99it [00:01, 82.30it/s]warmup run: 100it [00:01, 83.18it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 198it [00:01, 178.82it/s]warmup run: 201it [00:01, 181.67it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 100it [00:01, 85.62it/s]warmup run: 101it [00:01, 84.79it/s]warmup run: 96it [00:01, 81.29it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 296it [00:01, 284.31it/s]warmup run: 298it [00:01, 285.63it/s]warmup run: 97it [00:01, 83.41it/s]warmup run: 100it [00:01, 84.99it/s]warmup run: 200it [00:01, 185.36it/s]warmup run: 202it [00:01, 184.07it/s]warmup run: 189it [00:01, 173.05it/s]warmup run: 99it [00:01, 86.36it/s]warmup run: 396it [00:01, 397.62it/s]warmup run: 395it [00:01, 394.13it/s]warmup run: 200it [00:01, 187.07it/s]warmup run: 200it [00:01, 184.15it/s]warmup run: 300it [00:01, 295.08it/s]warmup run: 301it [00:01, 291.21it/s]warmup run: 286it [00:01, 279.57it/s]warmup run: 200it [00:01, 188.78it/s]warmup run: 493it [00:02, 502.25it/s]warmup run: 496it [00:02, 508.45it/s]warmup run: 302it [00:01, 299.65it/s]warmup run: 301it [00:01, 294.56it/s]warmup run: 402it [00:01, 411.89it/s]warmup run: 401it [00:01, 404.26it/s]warmup run: 384it [00:01, 391.78it/s]warmup run: 296it [00:01, 294.23it/s]warmup run: 597it [00:02, 614.46it/s]warmup run: 599it [00:02, 616.40it/s]warmup run: 404it [00:01, 416.01it/s]warmup run: 401it [00:01, 407.75it/s]warmup run: 494it [00:02, 492.18it/s]warmup run: 503it [00:02, 518.15it/s]warmup run: 482it [00:02, 500.77it/s]warmup run: 389it [00:01, 398.09it/s]warmup run: 701it [00:02, 711.82it/s]warmup run: 702it [00:02, 709.85it/s]warmup run: 506it [00:02, 529.05it/s]warmup run: 500it [00:02, 516.05it/s]warmup run: 592it [00:02, 591.76it/s]warmup run: 606it [00:02, 624.24it/s]warmup run: 580it [00:02, 600.40it/s]warmup run: 490it [00:01, 513.21it/s]warmup run: 803it [00:02, 787.72it/s]warmup run: 806it [00:02, 789.32it/s]warmup run: 608it [00:02, 632.77it/s]warmup run: 601it [00:02, 618.91it/s]warmup run: 694it [00:02, 687.34it/s]warmup run: 705it [00:02, 707.75it/s]warmup run: 677it [00:02, 684.07it/s]warmup run: 593it [00:02, 622.05it/s]warmup run: 904it [00:02, 845.39it/s]warmup run: 908it [00:02, 847.16it/s]warmup run: 707it [00:02, 714.70it/s]warmup run: 704it [00:02, 712.82it/s]warmup run: 792it [00:02, 757.47it/s]warmup run: 804it [00:02, 776.37it/s]warmup run: 772it [00:02, 747.87it/s]warmup run: 695it [00:02, 714.02it/s]warmup run: 1005it [00:02, 888.93it/s]warmup run: 1010it [00:02, 893.23it/s]warmup run: 806it [00:02, 781.97it/s]warmup run: 805it [00:02, 786.13it/s]warmup run: 887it [00:02, 802.75it/s]warmup run: 902it [00:02, 828.58it/s]warmup run: 797it [00:02, 789.49it/s]warmup run: 866it [00:02, 771.56it/s]warmup run: 1107it [00:02, 924.14it/s]warmup run: 1113it [00:02, 930.58it/s]warmup run: 906it [00:02, 837.32it/s]warmup run: 904it [00:02, 800.37it/s]warmup run: 986it [00:02, 851.38it/s]warmup run: 1000it [00:02, 865.81it/s]warmup run: 899it [00:02, 849.46it/s]warmup run: 1209it [00:02, 949.97it/s]warmup run: 1217it [00:02, 960.56it/s]warmup run: 957it [00:02, 792.22it/s]warmup run: 1006it [00:02, 879.88it/s]warmup run: 1005it [00:02, 853.93it/s]warmup run: 1087it [00:02, 895.04it/s]warmup run: 1102it [00:02, 907.48it/s]warmup run: 1002it [00:02, 896.47it/s]warmup run: 1310it [00:02, 962.73it/s]warmup run: 1319it [00:02, 971.00it/s]warmup run: 1105it [00:02, 910.17it/s]warmup run: 1056it [00:02, 843.69it/s]warmup run: 1107it [00:02, 897.81it/s]warmup run: 1185it [00:02, 916.28it/s]warmup run: 1204it [00:02, 938.77it/s]warmup run: 1105it [00:02, 933.29it/s]warmup run: 1412it [00:02, 977.22it/s]warmup run: 1204it [00:02, 932.62it/s]warmup run: 1421it [00:02, 979.20it/s]warmup run: 1156it [00:02, 884.71it/s]warmup run: 1210it [00:02, 934.22it/s]warmup run: 1285it [00:02, 939.53it/s]warmup run: 1207it [00:02, 956.98it/s]warmup run: 1304it [00:02, 948.53it/s]warmup run: 1515it [00:03, 991.27it/s]warmup run: 1522it [00:03, 986.17it/s]warmup run: 1256it [00:02, 915.81it/s]warmup run: 1303it [00:02, 940.88it/s]warmup run: 1312it [00:02, 956.47it/s]warmup run: 1386it [00:02, 958.39it/s]warmup run: 1310it [00:02, 975.58it/s]warmup run: 1403it [00:02, 956.66it/s]warmup run: 1617it [00:03, 999.66it/s]warmup run: 1357it [00:02, 942.05it/s]warmup run: 1623it [00:03, 989.10it/s]warmup run: 1401it [00:02, 946.83it/s]warmup run: 1415it [00:02, 977.29it/s]warmup run: 1485it [00:03, 960.16it/s]warmup run: 1412it [00:02, 987.78it/s]warmup run: 1502it [00:03, 963.81it/s]warmup run: 1719it [00:03, 1002.57it/s]warmup run: 1458it [00:03, 959.74it/s]warmup run: 1724it [00:03, 992.86it/s]warmup run: 1499it [00:03, 954.59it/s]warmup run: 1519it [00:03, 993.27it/s]warmup run: 1587it [00:03, 974.76it/s]warmup run: 1515it [00:02, 998.69it/s]warmup run: 1601it [00:03, 967.56it/s]warmup run: 1821it [00:03, 1005.49it/s]warmup run: 1559it [00:03, 971.94it/s]warmup run: 1825it [00:03, 994.71it/s]warmup run: 1599it [00:03, 965.46it/s]warmup run: 1622it [00:03, 1002.60it/s]warmup run: 1688it [00:03, 984.34it/s]warmup run: 1618it [00:03, 1006.97it/s]warmup run: 1700it [00:03, 970.17it/s]warmup run: 1923it [00:03, 1009.77it/s]warmup run: 1659it [00:03, 979.87it/s]warmup run: 1926it [00:03, 995.67it/s]warmup run: 1697it [00:03, 969.68it/s]warmup run: 1726it [00:03, 1011.40it/s]warmup run: 1791it [00:03, 994.68it/s]warmup run: 1721it [00:03, 1012.51it/s]warmup run: 1798it [00:03, 962.60it/s]warmup run: 2027it [00:03, 1018.34it/s]warmup run: 1760it [00:03, 986.77it/s]warmup run: 2031it [00:03, 1010.07it/s]warmup run: 1795it [00:03, 970.14it/s]warmup run: 1830it [00:03, 1017.78it/s]warmup run: 1894it [00:03, 1002.50it/s]warmup run: 1824it [00:03, 1012.36it/s]warmup run: 1896it [00:03, 967.62it/s]warmup run: 2146it [00:03, 1069.10it/s]warmup run: 1861it [00:03, 992.96it/s]warmup run: 2151it [00:03, 1066.50it/s]warmup run: 1893it [00:03, 971.68it/s]warmup run: 1933it [00:03, 1020.53it/s]warmup run: 1996it [00:03, 1007.40it/s]warmup run: 1926it [00:03, 1003.97it/s]warmup run: 1994it [00:03, 967.08it/s]warmup run: 2265it [00:03, 1103.57it/s]warmup run: 1961it [00:03, 993.72it/s]warmup run: 2271it [00:03, 1106.09it/s]warmup run: 1991it [00:03, 973.54it/s]warmup run: 2042it [00:03, 1039.49it/s]warmup run: 2116it [00:03, 1062.93it/s]warmup run: 2034it [00:03, 1024.46it/s]warmup run: 2115it [00:03, 1038.06it/s]warmup run: 2384it [00:03, 1127.70it/s]warmup run: 2074it [00:03, 1033.13it/s]warmup run: 2391it [00:03, 1133.68it/s]warmup run: 2110it [00:03, 1036.29it/s]warmup run: 2164it [00:03, 1092.37it/s]warmup run: 2237it [00:03, 1106.17it/s]warmup run: 2156it [00:03, 1081.56it/s]warmup run: 2239it [00:03, 1096.26it/s]warmup run: 2503it [00:03, 1145.15it/s]warmup run: 2195it [00:03, 1085.30it/s]warmup run: 2511it [00:03, 1153.01it/s]warmup run: 2232it [00:03, 1089.74it/s]warmup run: 2286it [00:03, 1129.80it/s]warmup run: 2358it [00:03, 1136.53it/s]warmup run: 2279it [00:03, 1125.09it/s]warmup run: 2363it [00:03, 1136.81it/s]warmup run: 2621it [00:04, 1155.39it/s]warmup run: 2316it [00:03, 1121.81it/s]warmup run: 2632it [00:04, 1167.58it/s]warmup run: 2354it [00:03, 1126.68it/s]warmup run: 2408it [00:03, 1155.83it/s]warmup run: 2479it [00:03, 1157.74it/s]warmup run: 2403it [00:03, 1157.77it/s]warmup run: 2485it [00:03, 1161.40it/s]warmup run: 2437it [00:03, 1147.94it/s]warmup run: 2753it [00:04, 1177.85it/s]warmup run: 2476it [00:03, 1152.80it/s]warmup run: 2737it [00:04, 1066.75it/s]warmup run: 2530it [00:03, 1173.22it/s]warmup run: 2600it [00:04, 1170.55it/s]warmup run: 2527it [00:03, 1180.89it/s]warmup run: 2608it [00:04, 1179.23it/s]warmup run: 2558it [00:04, 1165.82it/s]warmup run: 2873it [00:04, 1181.78it/s]warmup run: 2598it [00:04, 1171.80it/s]warmup run: 2846it [00:04, 1054.51it/s]warmup run: 2652it [00:04, 1185.85it/s]warmup run: 2721it [00:04, 1181.60it/s]warmup run: 2651it [00:04, 1197.92it/s]warmup run: 2732it [00:04, 1194.72it/s]warmup run: 2679it [00:04, 1178.42it/s]warmup run: 2994it [00:04, 1189.87it/s]warmup run: 2720it [00:04, 1186.07it/s]warmup run: 3000it [00:04, 681.70it/s] warmup run: 2965it [00:04, 1091.90it/s]warmup run: 2772it [00:04, 1187.37it/s]warmup run: 2842it [00:04, 1188.70it/s]warmup run: 3000it [00:04, 674.10it/s] warmup run: 2772it [00:04, 1200.44it/s]warmup run: 2853it [00:04, 1197.97it/s]warmup run: 2798it [00:04, 1179.68it/s]warmup run: 2839it [00:04, 1185.92it/s]warmup run: 2893it [00:04, 1193.68it/s]warmup run: 2965it [00:04, 1200.39it/s]warmup run: 2894it [00:04, 1204.94it/s]warmup run: 3000it [00:04, 683.39it/s] warmup run: 2976it [00:04, 1204.91it/s]warmup run: 2919it [00:04, 1187.13it/s]warmup run: 2960it [00:04, 1193.07it/s]warmup run: 3000it [00:04, 681.25it/s] warmup run: 3000it [00:04, 686.78it/s] warmup run: 3000it [00:04, 688.23it/s] warmup run: 3000it [00:04, 674.42it/s] warmup run: 3000it [00:04, 697.46it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.04it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.34it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.30it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1642.63it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.84it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.46it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.43it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.65it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1679.46it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.92it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.16it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1679.95it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1678.34it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1653.66it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1630.52it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1630.19it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1677.59it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1677.11it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1648.63it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1649.72it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1655.58it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1626.58it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1626.36it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1661.35it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1677.59it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1676.74it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1647.66it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1648.85it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1653.03it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1669.41it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1622.98it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1622.54it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1675.44it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1645.76it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1673.61it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1645.40it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1649.05it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1621.75it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1620.68it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1662.40it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1672.64it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1642.52it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1670.86it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1642.45it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1623.14it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1644.98it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1618.62it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1656.45it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1669.49it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1640.36it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1629.89it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1636.31it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1661.95it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1615.51it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1640.11it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1643.88it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1669.22it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1636.34it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1637.84it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1638.46it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1642.12it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:01, 1655.79it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:01, 1638.91it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1479.21it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1667.25it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1636.86it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1642.72it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1637.34it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1650.35it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1650.62it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1634.86it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:01, 1527.27it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1667.71it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1638.17it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1645.47it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1657.16it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1632.54it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1656.07it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1634.10it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1561.57it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1667.03it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1649.06it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1639.13it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1662.12it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1629.09it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1651.16it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1636.90it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1585.41it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1666.56it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1639.07it/s]warmup should be done:  66%|██████▌   | 1977/3000 [00:01<00:00, 1653.26it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1665.58it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1629.23it/s]warmup should be done:  67%|██████▋   | 2009/3000 [00:01<00:00, 1647.82it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1643.13it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1602.17it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1665.00it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1653.63it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1639.26it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1665.85it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1631.41it/s]warmup should be done:  72%|███████▏  | 2174/3000 [00:01<00:00, 1646.60it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1647.06it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1612.28it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1658.58it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1650.09it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1665.21it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1628.50it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1650.50it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1635.25it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1619.91it/s]warmup should be done:  84%|████████▎ | 2512/3000 [00:01<00:00, 1654.45it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1637.93it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1645.75it/s]warmup should be done:  82%|████████▏ | 2469/3000 [00:01<00:00, 1625.70it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1649.61it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1650.77it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1632.55it/s]warmup should be done:  82%|████████▏ | 2456/3000 [00:01<00:00, 1619.06it/s]warmup should be done:  89%|████████▉ | 2678/3000 [00:01<00:00, 1655.64it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1638.35it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1646.36it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1626.81it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1651.86it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1639.51it/s]warmup should be done:  89%|████████▉ | 2667/3000 [00:01<00:00, 1634.41it/s]warmup should be done:  87%|████████▋ | 2620/3000 [00:01<00:00, 1625.03it/s]warmup should be done:  95%|█████████▍| 2846/3000 [00:01<00:00, 1661.25it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1637.85it/s]warmup should be done:  94%|█████████▎| 2807/3000 [00:01<00:00, 1651.63it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1628.85it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1655.25it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1643.72it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1638.27it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1630.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.77it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1642.02it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1658.24it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1637.44it/s]warmup should be done: 100%|█████████▉| 2996/3000 [00:01<00:00, 1659.98it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1651.31it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1639.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1650.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.61it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1639.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1605.53it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.88it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.28it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.73it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.15it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.52it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1692.70it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.91it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.37it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1709.29it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.98it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1670.12it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1689.64it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.82it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1681.81it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1706.79it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1684.54it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1717.54it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1673.22it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1694.10it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1700.08it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1690.14it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.99it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1681.68it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1686.33it/s]warmup should be done:  23%|██▎       | 688/3000 [00:00<00:01, 1721.39it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1700.12it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1708.60it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1671.44it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1684.79it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1708.03it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1694.71it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1688.00it/s]warmup should be done:  29%|██▊       | 861/3000 [00:00<00:01, 1723.82it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1702.93it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1673.74it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1688.79it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1706.83it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1696.09it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1702.48it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1689.52it/s]warmup should be done:  34%|███▍      | 1034/3000 [00:00<00:01, 1724.79it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1702.53it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1696.84it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1687.06it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1676.24it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1711.37it/s]warmup should be done:  34%|███▍      | 1029/3000 [00:00<00:01, 1705.75it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1689.79it/s]warmup should be done:  40%|████      | 1207/3000 [00:00<00:01, 1723.67it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1701.15it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1696.23it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1675.07it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1682.09it/s]warmup should be done:  40%|████      | 1200/3000 [00:00<00:01, 1706.56it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1708.26it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1688.42it/s]warmup should be done:  46%|████▌     | 1381/3000 [00:00<00:00, 1726.46it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1705.15it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1675.53it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1700.70it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1684.20it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1708.41it/s]warmup should be done:  46%|████▌     | 1374/3000 [00:00<00:00, 1714.04it/s]warmup should be done:  45%|████▌     | 1359/3000 [00:00<00:00, 1692.32it/s]warmup should be done:  52%|█████▏    | 1554/3000 [00:00<00:00, 1726.03it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1705.05it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1675.73it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1701.80it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1684.48it/s]warmup should be done:  52%|█████▏    | 1546/3000 [00:00<00:00, 1715.74it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1705.85it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1693.08it/s]warmup should be done:  58%|█████▊    | 1727/3000 [00:01<00:00, 1725.17it/s]warmup should be done:  57%|█████▋    | 1707/3000 [00:01<00:00, 1705.61it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1677.73it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1702.72it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1684.94it/s]warmup should be done:  57%|█████▋    | 1718/3000 [00:01<00:00, 1716.09it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1704.87it/s]warmup should be done:  57%|█████▋    | 1699/3000 [00:01<00:00, 1659.22it/s]warmup should be done:  63%|██████▎   | 1901/3000 [00:01<00:00, 1726.96it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1708.48it/s]warmup should be done:  62%|██████▎   | 1875/3000 [00:01<00:00, 1703.08it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1674.86it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1686.65it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1717.32it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1704.60it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1668.25it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1728.53it/s]warmup should be done:  68%|██████▊   | 2051/3000 [00:01<00:00, 1709.70it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1687.26it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1702.28it/s]warmup should be done:  69%|██████▉   | 2063/3000 [00:01<00:00, 1716.72it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1671.11it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1701.45it/s]warmup should be done:  68%|██████▊   | 2036/3000 [00:01<00:00, 1670.78it/s]warmup should be done:  75%|███████▍  | 2248/3000 [00:01<00:00, 1727.54it/s]warmup should be done:  74%|███████▍  | 2222/3000 [00:01<00:00, 1708.38it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1685.40it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1699.56it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1713.33it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1670.11it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1694.12it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1674.55it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1707.67it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1685.17it/s]warmup should be done:  81%|████████  | 2421/3000 [00:01<00:00, 1720.14it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1699.37it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1713.12it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1667.20it/s]warmup should be done:  80%|███████▉  | 2395/3000 [00:01<00:00, 1690.97it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1679.27it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1709.56it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1685.84it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1701.39it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1670.35it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1710.35it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1700.65it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1697.64it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1683.89it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1711.74it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1686.46it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1700.86it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1671.50it/s]warmup should be done:  92%|█████████▏| 2766/3000 [00:01<00:00, 1706.24it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1705.73it/s]warmup should be done:  92%|█████████▏| 2750/3000 [00:01<00:00, 1691.47it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1685.87it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1711.64it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1686.12it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1695.72it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1672.14it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1707.49it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1703.03it/s]warmup should be done:  97%|█████████▋| 2920/3000 [00:01<00:00, 1668.08it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1665.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1716.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1705.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1703.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.44it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.16it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50b79a9d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50a4721c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50a464160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50b798e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50a4650d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50b79bd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50a4622b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fb50a4641f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 22:06:00.282369: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03a82bb60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:00.282431: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:00.290532: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:00.355307: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03a834020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:00.355364: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:00.364994: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:00.426811: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03ef92170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:00.426864: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:00.434398: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:00.684304: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03e8304d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:00.684366: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:00.693602: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:01.084802: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03b02e030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:01.084864: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:01.091224: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03282c520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:01.091281: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:01.094298: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:01.101256: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:01.172804: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb042834310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:01.172868: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:01.180818: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:01.222118: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fb03e833c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:06:01.222183: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:06:01.231330: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:06:07.503040: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:07.503228: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:07.638256: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:07.679922: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:07.914631: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:07.949896: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:07.959028: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:06:08.027786: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:07:09.578][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:07:09.579][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.587][ERROR][RK0][main]: coll ps creation done
[HCTR][22:07:09.587][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][22:07:09.640][ERROR][RK0][tid #140395333793536]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:07:09.640][ERROR][RK0][tid #140395333793536]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.646][ERROR][RK0][tid #140395333793536]: coll ps creation done
[HCTR][22:07:09.646][ERROR][RK0][tid #140395333793536]: replica 6 waits for coll ps creation barrier
[HCTR][22:07:09.651][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:07:09.651][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.655][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:07:09.655][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.655][ERROR][RK0][tid #140395392509696]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:07:09.655][ERROR][RK0][tid #140395392509696]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.656][ERROR][RK0][main]: coll ps creation done
[HCTR][22:07:09.656][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][22:07:09.662][ERROR][RK0][tid #140395392509696]: coll ps creation done
[HCTR][22:07:09.663][ERROR][RK0][tid #140395392509696]: replica 3 waits for coll ps creation barrier
[HCTR][22:07:09.663][ERROR][RK0][main]: coll ps creation done
[HCTR][22:07:09.663][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][22:07:09.703][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:07:09.703][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.710][ERROR][RK0][main]: coll ps creation done
[HCTR][22:07:09.710][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][22:07:09.888][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:07:09.888][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.895][ERROR][RK0][main]: coll ps creation done
[HCTR][22:07:09.895][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][22:07:09.955][ERROR][RK0][tid #140395392509696]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:07:09.955][ERROR][RK0][tid #140395392509696]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:07:09.960][ERROR][RK0][tid #140395392509696]: coll ps creation done
[HCTR][22:07:09.960][ERROR][RK0][tid #140395392509696]: replica 4 waits for coll ps creation barrier
[HCTR][22:07:09.960][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][22:07:10.825][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][22:07:10.856][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][tid #140395392509696]: replica 3 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][tid #140395392509696]: replica 4 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][tid #140395333793536]: replica 6 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][22:07:10.856][ERROR][RK0][main]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][main]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][tid #140395392509696]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][tid #140395392509696]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][tid #140395333793536]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][main]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][tid #140395392509696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][main]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][tid #140395392509696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][main]: Calling build_v2
[HCTR][22:07:10.856][ERROR][RK0][tid #140395333793536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:07:10.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-11 22:07:10[2022-12-11 22:07:102022-12-11 22:07:10[2022-12-11 22:07:10..2022-12-11 22:07:10..2022-12-11 22:07:102022-12-11 22:07:10856486856495.8564862022-12-11 22:07:10856486..: : 856496: .: 856504856508EEE: 856515E: :    E:  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  :::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::] ] ] 136:] 136136using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] 136using concurrent impl MPS] ] 


using concurrent impl MPS] 
using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS


[2022-12-11 22:07:10.860764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:07:10.860802: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:07:10:.196860808] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 22:07:102022-12-11 22:07:10..860856860853: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::2022-12-11 22:07:10196178.] ] 860883assigning 8 to cpuv100x8, slow pcie[: 

2022-12-11 22:07:10E. 860898[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 22:07:10:E.212 860923] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-11 22:07:10E[
178. 2022-12-11 22:07:10] 860939/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.v100x8, slow pcie: :860954
[E196: 2022-12-11 22:07:10 ] [E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu2022-12-11 22:07:10 860982:
[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 1782022-12-11 22:07:10860993:E] .: 212 v100x8, slow pcie
861005E] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:  [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 22:07:10:2022-12-11 22:07:10E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:07:10
.213. :.861060] 861057[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196861071: remote time is 8.68421: 2022-12-11 22:07:10:2022-12-11 22:07:10] : E
E.178.assigning 8 to cpuE  861113[] 861126
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:07:10: v100x8, slow pcie: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::.E
E:212178861180  196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [[: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 22:07:102022-12-11 22:07:10Ev100x8, slow pcie:assigning 8 to cpu178
.. 
213
] 861243861242/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [: v100x8, slow pcie[: :remote time is 8.684212022-12-11 22:07:10E
2022-12-11 22:07:10E214
. .]  861307[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[861313cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 22:07:10:2022-12-11 22:07:102022-12-11 22:07:10: 
:E.212..E196 861350] 861354861357 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu:E
EE:
213   196] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.684212022-12-11 22:07:10:::assigning 8 to cpu
.196212214
861472[] ] ] : 2022-12-11 22:07:10assigning 8 to cpu[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588E.
2022-12-11 22:07:10

 861517./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [861526:E2022-12-11 22:07:10[: 213 .2022-12-11 22:07:10E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc861571. remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: [861570
:214E[2022-12-11 22:07:10: 212]  2022-12-11 22:07:10.E] cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.861607 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8

:861639: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213: E:] E [212remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:07:10] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:212[861700
214] 2022-12-11 22:07:10: ] .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Ecpu time is 97.0588861742
[ 
: 2022-12-11 22:07:10/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.[: 8617712022-12-11 22:07:10213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : .:remote time is 8.68421E861795214
 : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588:213[
 ] 2022-12-11 22:07:10/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421.:
213861865] [remote time is 8.68421: 2022-12-11 22:07:10
E. 861930/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-11 22:07:10E214.]  861956cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:E214 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-11 22:08:27.248165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:08:27.288784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 22:08:27.425516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:08:27.425580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:08:27.549432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:08:27.549520: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:08:27.550117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:08:27.550178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.551162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.552004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.564606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 22:08:27.564667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[[2022-12-11 22:08:27[2022-12-11 22:08:27.2022-12-11 22:08:27.564961.564971: 564984: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[202:2022022-12-11 22:08:27] 202[] .2 solved] 2022-12-11 22:08:271 solved565036
7 solved.
: 
565065[[E: [2022-12-11 22:08:27[2022-12-11 22:08:27 E2022-12-11 22:08:27.2022-12-11 22:08:27./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc .565101.565090:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu565108: 565112: 202:: E: E] 1815E E 5 solved]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:
:205[:202205] 2022-12-11 22:08:27205] ] worker 0 thread 2 initing device 2[.] 3 solvedworker 0 thread 1 initing device 1
2022-12-11 22:08:27565223worker 0 thread 7 initing device 7

.: 
565245E[:  2022-12-11 22:08:27E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc. :565282/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu205: :] E1980worker 0 thread 5 initing device 5 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MB:
205] worker 0 thread 3 initing device 3
[2022-12-11 22:08:27.565640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 22:08:27
[.2022-12-11 22:08:27565662.: 565668E[:  2022-12-11 22:08:27E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. :565688/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815: [:] [E2022-12-11 22:08:271815Building Coll Cache with ... num gpu device is 82022-12-11 22:08:27 .] 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu565711Building Coll Cache with ... num gpu device is 8565733:: 
: [1980EE2022-12-11 22:08:27]   .eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu565770
:2022-12-11 22:08:27:: 1815.1815E] 565784]  Building Coll Cache with ... num gpu device is 8: Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E
: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB[1980
[2022-12-11 22:08:27] 2022-12-11 22:08:27.eager alloc mem 381.47 MB.565845
565849: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 22:08:27.568920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.569229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 22:08:27.569285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 22:08:27.569338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.569675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:08:27.569716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.569816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.569943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.569990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.570049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.573388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.573788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.573857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.574407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.574438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.574497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.574539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.578363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:08:27.631232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:08:27.636732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:08:27.636856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:08:27.637700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.638286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:27.639301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:27.641026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:08:27.641783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27.641828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[[[2022-12-11 22:08:272022-12-11 22:08:272022-12-11 22:08:272022-12-11 22:08:272022-12-11 22:08:272022-12-11 22:08:272022-12-11 22:08:27.......660428660428660427660428660428660429660453: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes






[2022-12-11 22:08:27.667430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:08:27.667479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:08:27.667545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
[2022-12-11 22:08:272022-12-11 22:08:27..667554667569: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[[2022-12-11 22:08:272022-12-11 22:08:27..667640667658: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[[2022-12-11 22:08:272022-12-11 22:08:27..667728667740: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 22:08:27.667801[: 2022-12-11 22:08:27E. 667823/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[[2022-12-11 22:08:272022-12-11 22:08:27..667881667895: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 22:08:27.667983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:08:27.673446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.673940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.674450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.674958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.675503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.676006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.676516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:08:27.690064[: 2022-12-11 22:08:27E. 690080/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:[
19802022-12-11 22:08:27] .eager alloc mem 611.00 KB690126
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:27.690185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:27.690232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:27.690278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:27.690325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:27.691004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:27.691048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:27.691092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:27.691112: [E2022-12-11 22:08:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc691169:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:[6382022-12-11 22:08:27] .eager release cuda mem 625663691210
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:27.691248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:27.697015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:08:27.697150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:08:27.697297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:08:27.697386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:08:27] .eager alloc mem 25.25 KB697406
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:08:27.697617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27.697660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 22:08:27.697747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27.697790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 22:08:27.697887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27.697929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 22:08:27.697990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:08:27] .eager release cuda mem 25855698006
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27[.2022-12-11 22:08:27698040.[: 6980312022-12-11 22:08:27E: . E698055/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980: ] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 981.44 MB] :
eager alloc mem 25.25 KB1980
] eager alloc mem 981.44 MB
[2022-12-11 22:08:27.698675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27[.2022-12-11 22:08:27698705.: 698728E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 25.25 KB] 
eager alloc mem 981.44 MB
[2022-12-11 22:08:27.699343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:08:27.699395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[[[[2022-12-11 22:08:282022-12-11 22:08:282022-12-11 22:08:282022-12-11 22:08:282022-12-11 22:08:282022-12-11 22:08:282022-12-11 22:08:282022-12-11 22:08:28........ 55428 55431 55428 55428 55434 55428 55429 55428: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 5 init p2p of link 6Device 1 init p2p of link 7Device 4 init p2p of link 5Device 3 init p2p of link 2Device 0 init p2p of link 3Device 2 init p2p of link 1Device 7 init p2p of link 4Device 6 init p2p of link 0







[[2022-12-11 22:08:28[2022-12-11 22:08:28[.2022-12-11 22:08:28.2022-12-11 22:08:28 55866[[. 55867[[.: 2022-12-11 22:08:282022-12-11 22:08:28 55871: 2022-12-11 22:08:282022-12-11 22:08:28 55872E..: E..:   55889 55884E  55886 55885E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :  :EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980  :1980  :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB::] eager alloc mem 611.00 KB::] 
19801980eager alloc mem 611.00 KB
19801980eager alloc mem 611.00 KB] ] 
] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-11 22:08:28[.2022-12-11 22:08:28 56807.:  56812E: [ E[2022-12-11 22:08:28[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 2022-12-11 22:08:28.[2022-12-11 22:08:28:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[ 568272022-12-11 22:08:28.638:[ 568312022-12-11 22:08:28: . 56834] 6382022-12-11 22:08:28: .E 56843: eager release cuda mem 625663] .E 56850 : E
eager release cuda mem 625663 56860 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638] :eager release cuda mem 625663638
] eager release cuda mem 625663638
] eager release cuda mem 625663
] eager release cuda mem 625663
eager release cuda mem 625663

[2022-12-11 22:08:28. 69677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 22:08:28. 69824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 70377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 22:08:28. 70548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 70632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 70759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 22:08:28. 70901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 70987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 22:08:28. 71154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 71351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 71406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 22:08:28. 71551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 71596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 22:08:28. 71702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 71752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 71829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 22:08:28. 71949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 71993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 72021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 22:08:28. 72184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 72344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 72544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 72763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 72955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 84066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 22:08:28. 84187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 84408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 22:08:28. 84532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 84789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 22:08:28. 84824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 22:08:28. 84912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 84942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 84969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 85004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 22:08:28. 85079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 22:08:28. 85134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 85180: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:08:28:.1926 85204] : Device 3 init p2p of link 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 85303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 22:08:28
. 85320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 85419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 22:08:28. 85538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28. 85718[: 2022-12-11 22:08:28E.  85727/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-11 22:08:28. 85919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 86008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 86069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28. 86305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.100309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 22:08:28.100420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.101000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 22:08:28.101031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 22:08:28.101116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.101151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.101210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.101364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 22:08:28.101403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 22:08:28.101479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.101519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.101659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 22:08:28.101767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.101912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.101938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.101998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 22:08:28.102123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.102264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.102279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2[
2022-12-11 22:08:28.102314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.102399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:08:28.102531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.102886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.103184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:08:28.116724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.116983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.117226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.117262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.117635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.117931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.118218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.118399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.552635 secs 
[2022-12-11 22:08:28.118618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.552779 secs 
[2022-12-11 22:08:28.118750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.552907 secs 
[2022-12-11 22:08:28.118884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.553203 secs 
[2022-12-11 22:08:28.119055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.553817 secs 
[2022-12-11 22:08:28.119178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:08:28.119322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.553545 secs 
[2022-12-11 22:08:28.119510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.549801 secs 
[2022-12-11 22:08:28.120726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.570563 secs 
[2022-12-11 22:08:28.123139: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.28 GB
[2022-12-11 22:08:29.549349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.54 GB
[2022-12-11 22:08:29.550138: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.54 GB
[2022-12-11 22:08:29.550957: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.54 GB
[2022-12-11 22:08:31. 40373: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.80 GB
[2022-12-11 22:08:31. 40865: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.80 GB
[2022-12-11 22:08:31. 41284: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.80 GB
[2022-12-11 22:08:32.477158: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.02 GB
[2022-12-11 22:08:32.477345: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.02 GB
[2022-12-11 22:08:32.478476: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.02 GB
[2022-12-11 22:08:34. 86272: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.23 GB
[2022-12-11 22:08:34. 87170: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.23 GB
[2022-12-11 22:08:34. 87874: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.23 GB
[2022-12-11 22:08:35.496514: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.69 GB
[2022-12-11 22:08:35.496923: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.69 GB
[2022-12-11 22:08:35.497256: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.69 GB
[2022-12-11 22:08:36.796344: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.89 GB
[2022-12-11 22:08:36.796516: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.89 GB
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][tid #140395333793536]: replica 6 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][tid #140395392509696]: replica 3 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][tid #140395392509696]: replica 4 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][tid #140395392509696]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][tid #140395392509696]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][tid #140395333793536]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:08:37.331][ERROR][RK0][main]: init per replica done
[HCTR][22:08:37.331][ERROR][RK0][tid #140395392509696]: init per replica done
[HCTR][22:08:37.331][ERROR][RK0][main]: init per replica done
[HCTR][22:08:37.331][ERROR][RK0][tid #140395392509696]: init per replica done
[HCTR][22:08:37.331][ERROR][RK0][tid #140395333793536]: init per replica done
[HCTR][22:08:37.331][ERROR][RK0][main]: init per replica done
[HCTR][22:08:37.331][ERROR][RK0][main]: init per replica done
[HCTR][22:08:37.334][ERROR][RK0][main]: init per replica done
[HCTR][22:08:37.370][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f94c0238400
[HCTR][22:08:37.370][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f94c0558400
[HCTR][22:08:37.370][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f94c0b98400
[HCTR][22:08:37.370][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f94c0eb8400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395392509696]: 3 allocated 3276800 at 0x7f9414238400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395392509696]: 3 allocated 6553600 at 0x7f9414558400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395392509696]: 3 allocated 3276800 at 0x7f9414b98400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395392509696]: 3 allocated 6553600 at 0x7f9414eb8400
[HCTR][22:08:37.370][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f94d0238400
[HCTR][22:08:37.370][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f94d0558400
[HCTR][22:08:37.370][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f9410238400
[HCTR][22:08:37.370][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f94d0b98400
[HCTR][22:08:37.370][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f9410558400
[HCTR][22:08:37.370][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f94d0eb8400
[HCTR][22:08:37.370][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f9410b98400
[HCTR][22:08:37.370][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f9410eb8400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395468011264]: 1 allocated 3276800 at 0x7f9438238400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395468011264]: 1 allocated 6553600 at 0x7f9438558400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395468011264]: 1 allocated 3276800 at 0x7f9438b98400
[HCTR][22:08:37.370][ERROR][RK0][tid #140395468011264]: 1 allocated 6553600 at 0x7f9438eb8400
[HCTR][22:08:37.370][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f949c238400
[HCTR][22:08:37.370][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f949c558400
[HCTR][22:08:37.370][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f949cb98400
[HCTR][22:08:37.370][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f949ceb8400
[HCTR][22:08:37.370][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f9390238400
[HCTR][22:08:37.370][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f9390558400
[HCTR][22:08:37.370][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f9390b98400
[HCTR][22:08:37.370][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f9390eb8400
[HCTR][22:08:37.373][ERROR][RK0][tid #140395392509696]: 0 allocated 3276800 at 0x7f94a2320000
[HCTR][22:08:37.373][ERROR][RK0][tid #140395392509696]: 0 allocated 6553600 at 0x7f94a2640000
[HCTR][22:08:37.373][ERROR][RK0][tid #140395392509696]: 0 allocated 3276800 at 0x7f94a2c80000
[HCTR][22:08:37.373][ERROR][RK0][tid #140395392509696]: 0 allocated 6553600 at 0x7f94a2fa0000
