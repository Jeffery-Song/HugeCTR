2022-12-11 22:53:02.045683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.050787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.057872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.062312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.068393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.079721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.086813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.098321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.149067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.153362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.154960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.155482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.156369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.157294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.157984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.158947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.159579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.160594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.161012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.162153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.162596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.163924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.164019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.165807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.165845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.167694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.167791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.169319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.170377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.171388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.172437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.173542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.175451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.176698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.177801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.178849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.179897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.180961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.181979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.183715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.188964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.189207: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.190044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.191410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.192829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.192894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.194736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.195163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.196876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.197054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.197093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.199023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.199190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.199344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.199427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.201706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.202166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.202310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.202366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.204372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.205252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.205343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.207380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.207533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.209625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.209726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.211924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.213873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.214791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.216934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.217019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.219371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.219519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.221969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.222358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.223023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.224440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.225111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.225817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.226204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.226585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.227445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.229925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.230404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.230551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.231480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.232717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.232936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.233127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.234377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.235537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.235613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.249740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.256610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.260566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.269894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.271467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.273152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.273373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.274193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.274394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.274422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.274869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.278392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.279051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.279084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.279141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.279288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.279898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.283747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.284599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.284629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.284736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.284801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.285661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.287792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.288675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.288770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.288816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.289935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.291883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.292549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.292647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.292693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.295565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.295968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.296060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.296160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.298427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.298920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.299016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.299062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.301708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.301755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.301800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.302123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.304291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.304431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.304724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.305032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.307455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.307558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.307795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.308168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.310225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.310349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.310478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.311088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.313122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.313222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.313324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.313918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.316003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.316186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.316320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.316735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.318852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.319044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.319209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.319483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.321722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.321757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.321869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.322254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.322677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.325506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.326227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.326444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.326574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.327526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.329150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.329386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.329495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.330583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.330670: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.332092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.332240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.332446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.333364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.334971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.335861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.335865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.336471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.337322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.338052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.338639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.339411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.339472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.340220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.341101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.341268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.342012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.342756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.343591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.343799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.344746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.345814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.345945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.346598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.347220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.348583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.348954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.349127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.350034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.350217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.350787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.351671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.352872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.353501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.353516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.354671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.355448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.356131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.359812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.360256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.360622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.361670: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.361670: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.361672: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.362514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.363189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.363416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.365523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.366110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.366380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.368491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.368939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.369242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.371212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.371611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.371750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.371771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.371832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.372254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.375328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.375825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.375950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.375993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.376052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.376442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.380086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.380151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.380283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.380323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.380493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.381030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.384312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.384437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.385300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.416528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.416653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.417186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.427066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.427292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.428180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.431979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.432894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.433004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.438937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.469341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.469468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.474078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.474975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.475190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.478943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.481538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.481871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.485709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.486761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.489847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.490726: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.490770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.493292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.494563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.498310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.499852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.500233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.505479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.507698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.510895: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.511830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.513916: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:53:02.520928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.523434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.597253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.597772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.604435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:02.604697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.447584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.448219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.448929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.449393: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.449466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:53:03.467081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.467751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.468258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.469309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.469844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.470317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:53:03.517068: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.517275: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.548387: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:53:03.667059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.667710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.668264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.668742: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.668806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:53:03.686161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.686801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.687329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.687956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.688722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.689190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:53:03.717939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.718577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.719110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.719725: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.719790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:53:03.736943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.739323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.739848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.740416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.740937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.741415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:53:03.745940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.745994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.752329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.752362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.753324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.753366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.754389: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.754428: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.754454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:53:03.754486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:53:03.771802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.772416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.772533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.773284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.773531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.774282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.774379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.775203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.775391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.775994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:53:03.776235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.776907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:53:03.785765: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.785981: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.787860: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:53:03.798898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.799809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.800363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.800839: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.800901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:53:03.801539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.802115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.802638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.803092: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.803153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:53:03.806537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.807113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.807660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.808115: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:53:03.808166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:53:03.820114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.820855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.821368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.821986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.822005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.822913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.823062: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.823085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.823230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.823325: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.823508: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.823924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:53:03.824137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.824721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.824851: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:53:03.825077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.825308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.825417: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:53:03.826131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.826203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:53:03.826760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.827424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.828041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:53:03.828576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:53:03.848761: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.848950: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.850789: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:53:03.869248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.869451: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.871320: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:53:03.871650: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.871788: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.873615: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 22:53:03.875087: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.875252: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:53:03.877034: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][22:53:05.127][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.127][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.127][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.128][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.129][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.131][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.170][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:53:05.170][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 96it [00:01, 79.23it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 98it [00:01, 83.78it/s]warmup run: 195it [00:01, 175.44it/s]warmup run: 101it [00:01, 85.61it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 200it [00:01, 185.88it/s]warmup run: 294it [00:01, 281.95it/s]warmup run: 203it [00:01, 186.62it/s]warmup run: 101it [00:01, 86.64it/s]warmup run: 100it [00:01, 86.13it/s]warmup run: 95it [00:01, 82.93it/s]warmup run: 95it [00:01, 81.93it/s]warmup run: 300it [00:01, 295.42it/s]warmup run: 392it [00:01, 391.45it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 305it [00:01, 297.91it/s]warmup run: 202it [00:01, 187.71it/s]warmup run: 199it [00:01, 185.41it/s]warmup run: 191it [00:01, 180.38it/s]warmup run: 192it [00:01, 179.47it/s]warmup run: 399it [00:01, 407.04it/s]warmup run: 488it [00:02, 494.90it/s]warmup run: 97it [00:01, 85.37it/s]warmup run: 405it [00:01, 410.52it/s]warmup run: 302it [00:01, 297.41it/s]warmup run: 301it [00:01, 298.46it/s]warmup run: 291it [00:01, 292.38it/s]warmup run: 290it [00:01, 287.82it/s]warmup run: 493it [00:02, 505.92it/s]warmup run: 587it [00:02, 597.05it/s]warmup run: 196it [00:01, 186.56it/s]warmup run: 505it [00:02, 519.49it/s]warmup run: 401it [00:01, 412.03it/s]warmup run: 402it [00:01, 410.50it/s]warmup run: 393it [00:01, 411.04it/s]warmup run: 389it [00:01, 401.74it/s]warmup run: 588it [00:02, 598.86it/s]warmup run: 687it [00:02, 687.84it/s]warmup run: 295it [00:01, 297.14it/s]warmup run: 607it [00:02, 624.31it/s]warmup run: 502it [00:02, 520.66it/s]warmup run: 504it [00:02, 527.27it/s]warmup run: 494it [00:01, 524.16it/s]warmup run: 489it [00:02, 513.46it/s]warmup run: 690it [00:02, 694.85it/s]warmup run: 787it [00:02, 762.60it/s]warmup run: 395it [00:01, 412.11it/s]warmup run: 710it [00:02, 716.36it/s]warmup run: 607it [00:02, 633.54it/s]warmup run: 603it [00:02, 623.15it/s]warmup run: 597it [00:02, 631.36it/s]warmup run: 590it [00:02, 617.86it/s]warmup run: 793it [00:02, 777.36it/s]warmup run: 884it [00:02, 815.87it/s]warmup run: 495it [00:01, 523.39it/s]warmup run: 812it [00:02, 790.67it/s]warmup run: 702it [00:02, 708.06it/s]warmup run: 711it [00:02, 727.21it/s]warmup run: 700it [00:02, 723.64it/s]warmup run: 692it [00:02, 710.72it/s]warmup run: 895it [00:02, 840.20it/s]warmup run: 983it [00:02, 862.49it/s]warmup run: 595it [00:02, 624.55it/s]warmup run: 915it [00:02, 852.20it/s]warmup run: 800it [00:02, 775.61it/s]warmup run: 811it [00:02, 794.29it/s]warmup run: 803it [00:02, 799.17it/s]warmup run: 794it [00:02, 785.89it/s]warmup run: 997it [00:02, 888.23it/s]warmup run: 1085it [00:02, 903.97it/s]warmup run: 691it [00:02, 698.62it/s]warmup run: 1018it [00:02, 898.40it/s]warmup run: 912it [00:02, 850.05it/s]warmup run: 903it [00:02, 849.93it/s]warmup run: 898it [00:02, 821.52it/s]warmup run: 893it [00:02, 839.16it/s]warmup run: 1100it [00:02, 926.72it/s]warmup run: 1185it [00:02, 930.77it/s]warmup run: 786it [00:02, 759.65it/s]warmup run: 1122it [00:02, 937.38it/s]warmup run: 1012it [00:02, 887.90it/s]warmup run: 1003it [00:02, 889.56it/s]warmup run: 995it [00:02, 858.41it/s]warmup run: 994it [00:02, 883.54it/s]warmup run: 1203it [00:02, 953.57it/s]warmup run: 1284it [00:02, 943.14it/s]warmup run: 881it [00:02, 808.68it/s]warmup run: 1224it [00:02, 935.19it/s]warmup run: 1104it [00:02, 922.24it/s]warmup run: 1112it [00:02, 915.86it/s]warmup run: 1094it [00:02, 893.18it/s]warmup run: 1305it [00:02, 962.72it/s]warmup run: 1383it [00:02, 946.17it/s]warmup run: 1094it [00:02, 861.29it/s]warmup run: 976it [00:02, 847.09it/s]warmup run: 1324it [00:02, 942.37it/s]warmup run: 1204it [00:02, 943.72it/s]warmup run: 1212it [00:02, 938.42it/s]warmup run: 1191it [00:02, 907.22it/s]warmup run: 1406it [00:02, 956.14it/s]warmup run: 1484it [00:03, 964.15it/s]warmup run: 1193it [00:02, 896.38it/s]warmup run: 1073it [00:02, 879.23it/s]warmup run: 1313it [00:02, 958.01it/s]warmup run: 1428it [00:02, 967.75it/s]warmup run: 1304it [00:02, 957.39it/s]warmup run: 1290it [00:02, 930.39it/s]warmup run: 1585it [00:03, 974.74it/s]warmup run: 1505it [00:03, 949.42it/s]warmup run: 1294it [00:02, 925.77it/s]warmup run: 1170it [00:02, 903.80it/s]warmup run: 1413it [00:02, 969.27it/s]warmup run: 1528it [00:03, 965.72it/s]warmup run: 1388it [00:02, 943.54it/s]warmup run: 1404it [00:02, 954.49it/s]warmup run: 1686it [00:03, 984.23it/s]warmup run: 1602it [00:03, 943.27it/s]warmup run: 1394it [00:02, 944.36it/s]warmup run: 1272it [00:02, 937.21it/s]warmup run: 1514it [00:03, 979.56it/s]warmup run: 1632it [00:03, 984.32it/s]warmup run: 1486it [00:03, 945.46it/s]warmup run: 1503it [00:03, 953.57it/s]warmup run: 1787it [00:03, 991.53it/s]warmup run: 1495it [00:03, 960.98it/s]warmup run: 1698it [00:03, 936.71it/s]warmup run: 1374it [00:02, 960.79it/s]warmup run: 1614it [00:03, 981.57it/s]warmup run: 1734it [00:03, 993.60it/s]warmup run: 1585it [00:03, 958.16it/s]warmup run: 1601it [00:03, 953.08it/s]warmup run: 1888it [00:03, 994.24it/s]warmup run: 1596it [00:03, 972.98it/s]warmup run: 1793it [00:03, 937.73it/s]warmup run: 1476it [00:02, 978.03it/s]warmup run: 1714it [00:03, 985.50it/s]warmup run: 1836it [00:03, 999.21it/s]warmup run: 1684it [00:03, 967.45it/s]warmup run: 1698it [00:03, 952.69it/s]warmup run: 1988it [00:03, 995.73it/s]warmup run: 1697it [00:03, 981.21it/s]warmup run: 1889it [00:03, 941.67it/s]warmup run: 1579it [00:03, 992.23it/s]warmup run: 1814it [00:03, 988.03it/s]warmup run: 1939it [00:03, 1006.08it/s]warmup run: 1786it [00:03, 982.74it/s]warmup run: 1795it [00:03, 949.09it/s]warmup run: 2106it [00:03, 1048.69it/s]warmup run: 1798it [00:03, 988.31it/s]warmup run: 1986it [00:03, 948.01it/s]warmup run: 1681it [00:03, 999.60it/s]warmup run: 1914it [00:03, 988.87it/s]warmup run: 2048it [00:03, 1030.84it/s]warmup run: 1885it [00:03, 984.41it/s]warmup run: 1893it [00:03, 956.27it/s]warmup run: 2226it [00:03, 1091.96it/s]warmup run: 1901it [00:03, 997.93it/s]warmup run: 2103it [00:03, 1011.78it/s]warmup run: 1784it [00:03, 1007.01it/s]warmup run: 2016it [00:03, 997.26it/s]warmup run: 2166it [00:03, 1072.67it/s]warmup run: 1984it [00:03, 979.08it/s]warmup run: 1991it [00:03, 962.82it/s]warmup run: 2345it [00:03, 1120.41it/s]warmup run: 2005it [00:03, 1009.15it/s]warmup run: 2223it [00:03, 1067.52it/s]warmup run: 1887it [00:03, 1012.53it/s]warmup run: 2136it [00:03, 1056.18it/s]warmup run: 2284it [00:03, 1104.54it/s]warmup run: 2099it [00:03, 1028.21it/s]warmup run: 2111it [00:03, 1032.79it/s]warmup run: 2467it [00:04, 1148.03it/s]warmup run: 2127it [00:03, 1069.59it/s]warmup run: 2342it [00:03, 1103.43it/s]warmup run: 1990it [00:03, 1017.06it/s]warmup run: 2256it [00:03, 1096.88it/s]warmup run: 2403it [00:03, 1128.92it/s]warmup run: 2217it [00:03, 1071.12it/s]warmup run: 2234it [00:03, 1090.30it/s]warmup run: 2589it [00:04, 1168.99it/s]warmup run: 2461it [00:03, 1129.08it/s]warmup run: 2249it [00:03, 1112.61it/s]warmup run: 2108it [00:03, 1063.40it/s]warmup run: 2376it [00:03, 1125.76it/s]warmup run: 2523it [00:03, 1148.94it/s]warmup run: 2336it [00:03, 1105.39it/s]warmup run: 2357it [00:03, 1130.43it/s]warmup run: 2711it [00:04, 1181.69it/s]warmup run: 2581it [00:04, 1148.25it/s]warmup run: 2371it [00:03, 1142.22it/s]warmup run: 2229it [00:03, 1106.31it/s]warmup run: 2496it [00:03, 1146.78it/s]warmup run: 2643it [00:04, 1163.14it/s]warmup run: 2454it [00:03, 1126.00it/s]warmup run: 2480it [00:03, 1158.38it/s]warmup run: 2834it [00:04, 1193.97it/s]warmup run: 2699it [00:04, 1155.85it/s]warmup run: 2493it [00:03, 1164.24it/s]warmup run: 2351it [00:03, 1137.85it/s]warmup run: 2616it [00:04, 1160.64it/s]warmup run: 2762it [00:04, 1169.79it/s]warmup run: 2570it [00:04, 1136.13it/s]warmup run: 2603it [00:04, 1178.01it/s]warmup run: 2957it [00:04, 1202.07it/s]warmup run: 2615it [00:04, 1179.80it/s]warmup run: 2818it [00:04, 1163.50it/s]warmup run: 2473it [00:03, 1162.30it/s]warmup run: 3000it [00:04, 674.96it/s] warmup run: 2734it [00:04, 1165.20it/s]warmup run: 2882it [00:04, 1176.38it/s]warmup run: 2687it [00:04, 1145.89it/s]warmup run: 2725it [00:04, 1188.12it/s]warmup run: 2735it [00:04, 1185.27it/s]warmup run: 2938it [00:04, 1173.99it/s]warmup run: 2595it [00:03, 1179.25it/s]warmup run: 3000it [00:04, 678.96it/s] warmup run: 2854it [00:04, 1173.83it/s]warmup run: 2807it [00:04, 1161.43it/s]warmup run: 3000it [00:04, 685.83it/s] warmup run: 2847it [00:04, 1197.38it/s]warmup run: 2856it [00:04, 1191.05it/s]warmup run: 2716it [00:04, 1187.19it/s]warmup run: 2974it [00:04, 1180.39it/s]warmup run: 2930it [00:04, 1179.79it/s]warmup run: 2970it [00:04, 1206.52it/s]warmup run: 2978it [00:04, 1198.10it/s]warmup run: 3000it [00:04, 689.06it/s] warmup run: 3000it [00:04, 690.14it/s] warmup run: 2839it [00:04, 1198.59it/s]warmup run: 3000it [00:04, 686.28it/s] warmup run: 3000it [00:04, 681.36it/s] warmup run: 2960it [00:04, 1199.84it/s]warmup run: 3000it [00:04, 693.05it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.83it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.12it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1606.38it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.82it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.72it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.06it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.61it/s]warmup should be done:   5%|         | 140/3000 [00:00<00:02, 1392.71it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1630.13it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1656.78it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.29it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1630.47it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1662.09it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1674.74it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1673.23it/s]warmup should be done:  10%|         | 305/3000 [00:00<00:01, 1539.03it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1636.41it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.15it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1648.03it/s]warmup should be done:  16%|        | 470/3000 [00:00<00:01, 1588.59it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1672.24it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1671.80it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1625.93it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1652.72it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1654.95it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1639.20it/s]warmup should be done:  21%|        | 634/3000 [00:00<00:01, 1608.20it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1646.95it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1670.34it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1670.59it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1623.23it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1648.32it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1639.29it/s]warmup should be done:  27%|       | 798/3000 [00:00<00:01, 1619.49it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1652.85it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1644.02it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1620.20it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1666.86it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1667.14it/s]warmup should be done:  28%|       | 831/3000 [00:00<00:01, 1643.80it/s]warmup should be done:  32%|      | 962/3000 [00:00<00:01, 1623.43it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1634.86it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1645.34it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1638.60it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1661.39it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1660.96it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1612.30it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1636.56it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1635.45it/s]warmup should be done:  38%|      | 1125/3000 [00:00<00:01, 1623.12it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1638.37it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1644.13it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1660.82it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1660.69it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1610.27it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1635.29it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1636.30it/s]warmup should be done:  43%|     | 1289/3000 [00:00<00:01, 1626.78it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1638.71it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1643.11it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1660.57it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1660.36it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1634.72it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1609.57it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1636.63it/s]warmup should be done:  48%|     | 1453/3000 [00:00<00:00, 1629.47it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1638.91it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1643.25it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1660.25it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1660.05it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1634.01it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1609.80it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1635.84it/s]warmup should be done:  54%|    | 1617/3000 [00:01<00:00, 1629.99it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1638.40it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1642.68it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1660.84it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1660.16it/s]warmup should be done:  55%|    | 1652/3000 [00:01<00:00, 1635.15it/s]warmup should be done:  54%|    | 1627/3000 [00:01<00:00, 1609.62it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1635.30it/s]warmup should be done:  59%|    | 1781/3000 [00:01<00:00, 1630.95it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1637.56it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1638.92it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1635.76it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1659.60it/s]warmup should be done:  60%|    | 1789/3000 [00:01<00:00, 1610.41it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1650.56it/s]warmup should be done:  66%|   | 1968/3000 [00:01<00:00, 1635.13it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1637.43it/s]warmup should be done:  65%|   | 1945/3000 [00:01<00:00, 1631.43it/s]warmup should be done:  67%|   | 2008/3000 [00:01<00:00, 1659.31it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1635.86it/s]warmup should be done:  65%|   | 1951/3000 [00:01<00:00, 1611.55it/s]warmup should be done:  66%|   | 1985/3000 [00:01<00:00, 1629.64it/s]warmup should be done:  67%|   | 2008/3000 [00:01<00:00, 1641.38it/s]warmup should be done:  71%|   | 2132/3000 [00:01<00:00, 1634.78it/s]warmup should be done:  70%|   | 2109/3000 [00:01<00:00, 1631.93it/s]warmup should be done:  71%|  | 2140/3000 [00:01<00:00, 1635.31it/s]warmup should be done:  72%|  | 2174/3000 [00:01<00:00, 1658.92it/s]warmup should be done:  71%|  | 2144/3000 [00:01<00:00, 1636.26it/s]warmup should be done:  70%|   | 2113/3000 [00:01<00:00, 1611.99it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1621.26it/s]warmup should be done:  72%|  | 2174/3000 [00:01<00:00, 1644.24it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1633.13it/s]warmup should be done:  76%|  | 2273/3000 [00:01<00:00, 1632.53it/s]warmup should be done:  77%|  | 2304/3000 [00:01<00:00, 1634.23it/s]warmup should be done:  78%|  | 2340/3000 [00:01<00:00, 1657.21it/s]warmup should be done:  77%|  | 2308/3000 [00:01<00:00, 1632.89it/s]warmup should be done:  76%|  | 2275/3000 [00:01<00:00, 1608.99it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1613.33it/s]warmup should be done:  78%|  | 2340/3000 [00:01<00:00, 1646.33it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1630.64it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1634.98it/s]warmup should be done:  81%|  | 2437/3000 [00:01<00:00, 1630.67it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1658.17it/s]warmup should be done:  82%| | 2472/3000 [00:01<00:00, 1633.63it/s]warmup should be done:  81%|  | 2436/3000 [00:01<00:00, 1608.52it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1651.48it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1609.47it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1632.82it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1634.97it/s]warmup should be done:  87%| | 2601/3000 [00:01<00:00, 1630.37it/s]warmup should be done:  89%| | 2674/3000 [00:01<00:00, 1659.16it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1633.67it/s]warmup should be done:  87%| | 2599/3000 [00:01<00:00, 1613.74it/s]warmup should be done:  89%| | 2674/3000 [00:01<00:00, 1655.19it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1607.59it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1640.78it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1635.06it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1630.35it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1661.63it/s]warmup should be done:  93%|| 2800/3000 [00:01<00:00, 1634.56it/s]warmup should be done:  92%|| 2764/3000 [00:01<00:00, 1624.29it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1659.55it/s]warmup should be done:  93%|| 2795/3000 [00:01<00:00, 1607.59it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.40it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1652.77it/s]warmup should be done:  99%|| 2962/3000 [00:01<00:00, 1639.47it/s]warmup should be done:  98%|| 2931/3000 [00:01<00:00, 1637.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.28it/s]warmup should be done:  99%|| 2966/3000 [00:01<00:00, 1639.28it/s]warmup should be done:  98%|| 2931/3000 [00:01<00:00, 1636.25it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1616.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1630.42it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.64it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1618.09it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.76it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.61it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1586.28it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1685.69it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.77it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1692.30it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1683.01it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1670.73it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1655.55it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1614.80it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1656.11it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1673.51it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1700.06it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1682.19it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1680.96it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1668.14it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1664.47it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1623.90it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1663.66it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1705.51it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1671.89it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1683.05it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1668.89it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1681.55it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1670.41it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1639.46it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1706.25it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1663.42it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1683.80it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1686.63it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1671.53it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1665.89it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1671.09it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1652.13it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1704.60it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1676.60it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1661.62it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1669.20it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1688.68it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1669.34it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1669.76it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1659.33it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1664.16it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1702.48it/s]warmup should be done:  34%|      | 1010/3000 [00:00<00:01, 1677.99it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1685.36it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1661.09it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1674.87it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1665.66it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1670.12it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1704.66it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1666.31it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1678.08it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1682.48it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1676.48it/s]warmup should be done:  39%|      | 1172/3000 [00:00<00:01, 1632.39it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1667.73it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1671.69it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:00, 1666.88it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1681.25it/s]warmup should be done:  46%|     | 1369/3000 [00:00<00:00, 1707.68it/s]warmup should be done:  45%|     | 1355/3000 [00:00<00:00, 1685.00it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1682.66it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:01, 1615.54it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1672.55it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1666.94it/s]warmup should be done:  51%|     | 1516/3000 [00:00<00:00, 1682.70it/s]warmup should be done:  51%|    | 1540/3000 [00:00<00:00, 1706.86it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1671.26it/s]warmup should be done:  51%|     | 1524/3000 [00:00<00:00, 1685.41it/s]warmup should be done:  51%|     | 1522/3000 [00:00<00:00, 1683.12it/s]warmup should be done:  50%|     | 1498/3000 [00:00<00:00, 1603.74it/s]warmup should be done:  57%|    | 1711/3000 [00:01<00:00, 1707.54it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1675.58it/s]warmup should be done:  56%|    | 1672/3000 [00:01<00:00, 1672.09it/s]warmup should be done:  56%|    | 1675/3000 [00:01<00:00, 1672.45it/s]warmup should be done:  56%|    | 1686/3000 [00:01<00:00, 1684.89it/s]warmup should be done:  56%|    | 1693/3000 [00:01<00:00, 1685.47it/s]warmup should be done:  56%|    | 1691/3000 [00:01<00:00, 1684.52it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1598.07it/s]warmup should be done:  63%|   | 1883/3000 [00:01<00:00, 1710.20it/s]warmup should be done:  61%|   | 1842/3000 [00:01<00:00, 1680.21it/s]warmup should be done:  61%|    | 1833/3000 [00:01<00:00, 1678.35it/s]warmup should be done:  62%|   | 1856/3000 [00:01<00:00, 1687.89it/s]warmup should be done:  62%|   | 1862/3000 [00:01<00:00, 1686.21it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1662.57it/s]warmup should be done:  62%|   | 1861/3000 [00:01<00:00, 1687.85it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1592.57it/s]warmup should be done:  68%|   | 2055/3000 [00:01<00:00, 1710.85it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1677.48it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1683.44it/s]warmup should be done:  68%|   | 2025/3000 [00:01<00:00, 1687.08it/s]warmup should be done:  68%|   | 2031/3000 [00:01<00:00, 1686.47it/s]warmup should be done:  67%|   | 2011/3000 [00:01<00:00, 1665.63it/s]warmup should be done:  68%|   | 2031/3000 [00:01<00:00, 1689.24it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1588.42it/s]warmup should be done:  72%|  | 2169/3000 [00:01<00:00, 1677.94it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1685.12it/s]warmup should be done:  73%|  | 2194/3000 [00:01<00:00, 1686.00it/s]warmup should be done:  74%|  | 2227/3000 [00:01<00:00, 1707.96it/s]warmup should be done:  73%|  | 2200/3000 [00:01<00:00, 1685.43it/s]warmup should be done:  73%|  | 2200/3000 [00:01<00:00, 1688.57it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1667.13it/s]warmup should be done:  71%|  | 2138/3000 [00:01<00:00, 1579.56it/s]warmup should be done:  78%|  | 2338/3000 [00:01<00:00, 1681.03it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1687.94it/s]warmup should be done:  79%|  | 2363/3000 [00:01<00:00, 1686.50it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1706.81it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1685.27it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1688.90it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1669.23it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1571.13it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1682.21it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1690.34it/s]warmup should be done:  84%| | 2533/3000 [00:01<00:00, 1688.14it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1708.43it/s]warmup should be done:  85%| | 2538/3000 [00:01<00:00, 1686.05it/s]warmup should be done:  85%| | 2539/3000 [00:01<00:00, 1690.49it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1671.46it/s]warmup should be done:  82%| | 2454/3000 [00:01<00:00, 1563.38it/s]warmup should be done:  90%| | 2691/3000 [00:01<00:00, 1691.21it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1681.00it/s]warmup should be done:  90%| | 2703/3000 [00:01<00:00, 1688.94it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1710.98it/s]warmup should be done:  90%| | 2707/3000 [00:01<00:00, 1679.07it/s]warmup should be done:  90%| | 2709/3000 [00:01<00:00, 1691.17it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1671.42it/s]warmup should be done:  87%| | 2611/3000 [00:01<00:00, 1557.08it/s]warmup should be done:  95%|| 2861/3000 [00:01<00:00, 1689.08it/s]warmup should be done:  95%|| 2845/3000 [00:01<00:00, 1678.96it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1709.85it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1680.73it/s]warmup should be done:  96%|| 2879/3000 [00:01<00:00, 1692.90it/s]warmup should be done:  96%|| 2875/3000 [00:01<00:00, 1672.42it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1669.73it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1555.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1707.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.63it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1683.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.65it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1668.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1668.19it/s]warmup should be done:  97%|| 2923/3000 [00:01<00:00, 1553.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1596.11it/s]2022-12-11 22:54:41.905221: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fdd77830e90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:41.905288: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.060132: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fdd778316d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.060200: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.065152: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc0cc029ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.065196: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.125335: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fdd738335e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.125410: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.125496: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc19c0300d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.125545: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.139067: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fdd6f7960a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.139154: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.343460: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc1a00295a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.343535: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:42.357182: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fc1b002df50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:54:42.357254: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:54:44.260499: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.406199: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.431279: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.475148: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.508706: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.518154: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.694794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:44.732977: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:54:47.204899: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.341447: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.351326: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.433097: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.463279: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.480610: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.564902: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:54:47.614309: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:55:25.337][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:55:25.337][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.343][ERROR][RK0][main]: coll ps creation done
[HCTR][22:55:25.343][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][22:55:25.351][ERROR][RK0][tid #140589244872448]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:55:25.351][ERROR][RK0][tid #140589244872448]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.359][ERROR][RK0][tid #140589244872448]: coll ps creation done
[HCTR][22:55:25.359][ERROR][RK0][tid #140589244872448]: replica 6 waits for coll ps creation barrier
[HCTR][22:55:25.468][ERROR][RK0][tid #140589311981312]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:55:25.468][ERROR][RK0][tid #140589311981312]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.474][ERROR][RK0][tid #140589311981312]: coll ps creation done
[HCTR][22:55:25.474][ERROR][RK0][tid #140589311981312]: replica 4 waits for coll ps creation barrier
[HCTR][22:55:25.501][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:55:25.501][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.504][ERROR][RK0][tid #140589362304768]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:55:25.504][ERROR][RK0][tid #140589362304768]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.509][ERROR][RK0][main]: coll ps creation done
[HCTR][22:55:25.509][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][22:55:25.511][ERROR][RK0][tid #140589362304768]: coll ps creation done
[HCTR][22:55:25.511][ERROR][RK0][tid #140589362304768]: replica 5 waits for coll ps creation barrier
[HCTR][22:55:25.559][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:55:25.559][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.566][ERROR][RK0][main]: coll ps creation done
[HCTR][22:55:25.566][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][22:55:25.633][ERROR][RK0][tid #140589303588608]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:55:25.633][ERROR][RK0][tid #140589303588608]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.638][ERROR][RK0][tid #140589303588608]: coll ps creation done
[HCTR][22:55:25.638][ERROR][RK0][tid #140589303588608]: replica 0 waits for coll ps creation barrier
[HCTR][22:55:25.670][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:55:25.670][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:55:25.675][ERROR][RK0][main]: coll ps creation done
[HCTR][22:55:25.675][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][22:55:25.675][ERROR][RK0][tid #140589303588608]: replica 0 preparing frequency
[HCTR][22:55:26.575][ERROR][RK0][tid #140589303588608]: replica 0 preparing frequency done
[HCTR][22:55:26.609][ERROR][RK0][tid #140589303588608]: replica 0 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][tid #140589311981312]: replica 4 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][tid #140589244872448]: replica 6 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][tid #140589362304768]: replica 5 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][22:55:26.609][ERROR][RK0][tid #140589303588608]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][tid #140589311981312]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][main]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][main]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][tid #140589244872448]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][tid #140589362304768]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][main]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][tid #140589303588608]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][main]: Calling build_v2
[HCTR][22:55:26.609][ERROR][RK0][tid #140589311981312]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][tid #140589244872448]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][tid #140589362304768]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:55:26.609][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[2022-12-11 22:55:26.2022-12-11 22:55:26[2022-12-11 22:55:26609488[[[2022-12-11 22:55:26[..: 2022-12-11 22:55:26.2022-12-11 22:55:266094872022-12-11 22:55:26609488E.609516.2022-12-11 22:55:26: .:  609520: 609512.E609516E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: E: 609514 :  :E E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E: :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136using concurrent impl MPS:136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] :] 
136] 136:using concurrent impl MPS136using concurrent impl MPS] using concurrent impl MPS] 136
] 
using concurrent impl MPS
using concurrent impl MPS] using concurrent impl MPS

using concurrent impl MPS

[2022-12-11 22:55:26.613782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:55:26.613823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-11 22:55:26.613878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1782022-12-11 22:55:26] .v100x8, slow pcie613899
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 22:55:26[212.2022-12-11 22:55:26] 613921.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 613922
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196:] [178assigning 8 to cpu2022-12-11 22:55:26] 
.[v100x8, slow pcie6139672022-12-11 22:55:26
: .E[613974 2022-12-11 22:55:26: [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E2022-12-11 22:55:262022-12-11 22:55:26:614010 ..213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[614030614017] E:2022-12-11 22:55:26: : remote time is 8.68421 178.E[E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 614065 2022-12-11 22:55:26 :v100x8, slow pcie[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196
2022-12-11 22:55:26E:6141192022-12-11 22:55:26:] .[ 212: .178assigning 8 to cpu6141572022-12-11 22:55:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E614165] 
: .:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 : v100x8, slow pcieE614214178
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
 : ] :[ [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[Ev100x8, slow pcie1782022-12-11 22:55:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:55:26:2022-12-11 22:55:26 
] .:.214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie614322[178614331] 614335:
: 2022-12-11 22:55:26] : cpu time is 97.0588: 196E.[v100x8, slow pcieE
E]  6143962022-12-11 22:55:26
  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:E614446::2022-12-11 22:55:26212 : 213196.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [] 614515build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: remote time is 8.684212022-12-11 22:55:26assigning 8 to cpu: 
196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.
E] :[614581[ assigning 8 to cpu1962022-12-11 22:55:26: 2022-12-11 22:55:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] .E.:assigning 8 to cpu[614648 614653196
[2022-12-11 22:55:26: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 2022-12-11 22:55:26.E:Eassigning 8 to cpu.614702 212 
614723: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E2022-12-11 22:55:26:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:[E .213
2142022-12-11 22:55:26 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc614795] ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:: remote time is 8.68421cpu time is 97.0588614833:2022-12-11 22:55:26212E

: 212.]  E[] 614889build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 22:55:26build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
E212:[614947[ ] 2122022-12-11 22:55:26: 2022-12-11 22:55:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] .E.:
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8615022 615007213
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: ] E:2022-12-11 22:55:26E[remote time is 8.68421 214. 2022-12-11 22:55:26
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 615096/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:cpu time is 97.0588[: :615117213
2022-12-11 22:55:26E213: ] . ] Eremote time is 8.68421615170/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 
: :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE213[: [] 2022-12-11 22:55:26213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:55:26remote time is 8.68421.] :.
615255remote time is 8.68421214615264: 
] [: Ecpu time is 97.05882022-12-11 22:55:26[E 
.2022-12-11 22:55:26 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc615319./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 615332:214E: 214]  E] cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 22:56:45.998159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:56:46. 38242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 22:56:46. 38314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 2999999
[2022-12-11 22:56:46.153316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:56:46.153408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:56:46.153441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:56:46.153472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:56:46.153934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.160789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.165692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.168659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 22:56:46.168717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 22:56:46.168929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 22:56:46.168989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 22:56:46.169029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 22:56:46.169110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 22:56:46205.] 169121worker 0 thread 4 initing device 4: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.169390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.169583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.171404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.171461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.171514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.171598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[[2022-12-11 22:56:462022-12-11 22:56:46..171629171651: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 6 solvedworker 0 thread 5 initing device 5

[2022-12-11 22:56:46.171742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:56:46.172108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.172162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 22:56:462022-12-11 22:56:46..172679172688: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved7 solved

[[2022-12-11 22:56:462022-12-11 22:56:46..172803172805: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 7 initing device 7

[2022-12-11 22:56:46.173269: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:56:46:.1980173283] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.175234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.175572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.175625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.176140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.176358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.177346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.177835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.180064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.180178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.180239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.180296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:56:46.243829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:56:46.244253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:56:46.249716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:56:46.249817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:56:46.249865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.250758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.251530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.252516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.252609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:56:46.253285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:56:46.253327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[2022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:46....258634258637258634258636: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] eager alloc mem 2.00 Bytes] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes
eager alloc mem 2.00 Bytes


[2022-12-11 22:56:46.259047: [[E2022-12-11 22:56:462022-12-11 22:56:46 .[./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2590552022-12-11 22:56:46259054:: .: 1980E259066E]  :  eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:] ] 1980eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes] 

eager alloc mem 1024.00 Bytes
[[2022-12-11 22:56:462022-12-11 22:56:46..264216264215: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980[1980] 2022-12-11 22:56:46] eager alloc mem 2.00 Bytes.eager alloc mem 2.00 Bytes
264288
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:56:46.[2645922022-12-11 22:56:46: .E[264597 2022-12-11 22:56:46: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.E:264608 1980: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] E:eager alloc mem 1024.00 Bytes 1980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 1024.00 Bytes1980
] eager alloc mem 1024.00 Bytes
[2022-12-11 22:56:46.273368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:56:46.273444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:56:46.273486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.273599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:56:46.273669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:56:46.273682: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46:.638273713] : Eeager release cuda mem 1024 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.273763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:56:46.273808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.274074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:56:46.274138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:56:46.274182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.274220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:56:46.274291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[[2022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:46...274314274327274335: : : EEE [  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.::6382743761980638] : ] ] eager release cuda mem 1024Eeager alloc mem 11.83 MBeager release cuda mem 400000000
 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:56:46.274502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:56:46.274524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2[
2022-12-11 22:56:46.274545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.274572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:56:46.275656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.276274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.276888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.277861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.278452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.278955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:56:46.279263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.280299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:56:46.280458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.280969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 22:56:46eager release cuda mem 25855.
280987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 22:56:46.281022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 22:56:46.281061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 22:56:46
.[2810832022-12-11 22:56:46: .E281092 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 25.25 KB638
] eager release cuda mem 625663
[2022-12-11 22:56:46.281148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:56:46.281192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:56:46.281412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.281442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.281498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:56:46.281528[: 2022-12-11 22:56:46E. 281533/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 25.25 KB:
638] eager release cuda mem 625663
[2022-12-11 22:56:46.281634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:56:46.281783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:56:46.281815: [E2022-12-11 22:56:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc281825:: 638E]  eager release cuda mem 25855/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1.43 GB
[2022-12-11 22:56:46[.2022-12-11 22:56:46281866.: 281870E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 25855] 
eager alloc mem 1.43 GB
[2022-12-11 22:56:46.281927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 22:56:46.282170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:56:46.282213[: 2022-12-11 22:56:46E. 282218/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 1.43 GB:
638] eager release cuda mem 25855
[2022-12-11 22:56:46.282276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 22:56:46.282308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:56:46.282348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[2022-12-11 22:56:46[[[2022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:46.2022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:46....564827...564828: 564837564835564835: 564846564845564844E: : : E: : :  EEE EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:::1980:::] 198019801980] 198019801980eager alloc mem 611.00 KB] ] ] eager alloc mem 611.00 KB] ] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB





[2022-12-11 22:56:46.565969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.566013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[[[2022-12-11 22:56:46638[[2022-12-11 22:56:462022-12-11 22:56:462022-12-11 22:56:46[.] 2022-12-11 22:56:462022-12-11 22:56:46...2022-12-11 22:56:46566028eager release cuda mem 625663..566030566030566032.: 
566039566040: : : 566054E: : EEE:  EE   E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 2022-12-11 22:56:46:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.638::638638638:566191] 638638] ] ] 1980: eager release cuda mem 625663] ] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663] E
eager release cuda mem 625663eager release cuda mem 625663


eager alloc mem 611.00 KB 


/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:56:462022-12-11 22:56:46[..2022-12-11 22:56:46[566356566359.2022-12-11 22:56:46[[: : 566363.2022-12-11 22:56:462022-12-11 22:56:46EE: 566368..  E: 566379566380/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E: : ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu EE19801980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  ] ] 1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBeager alloc mem 611.00 KB] 1980::

eager alloc mem 611.00 KB] 19801980
eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 22:56:46.567057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.567096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.567137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.567180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.567267: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46:.638567282] [: [eager release cuda mem 6256632022-12-11 22:56:46E[2022-12-11 22:56:46
. 2022-12-11 22:56:46.567295/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.567296: :2022-12-11 22:56:46567305: E638.: E ] [567332E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 6256632022-12-11 22:56:46:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638567374 :638] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] eager release cuda mem 625663E:] eager release cuda mem 625663
[ 638eager release cuda mem 625663
2022-12-11 22:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
.:eager release cuda mem 6256635674661980
: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:56:461980.] 567526eager alloc mem 611.00 KB: [
E[2022-12-11 22:56:46 2022-12-11 22:56:46[./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-11 22:56:46567542:567552.: 1980: 567560E] E:  eager alloc mem 611.00 KB E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:] ] 1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 

eager alloc mem 611.00 KB
[2022-12-11 22:56:46.567889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46[.2022-12-11 22:56:46567958.: 567961E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 22:56:46.568058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.568257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.568295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.568328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.568363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.568387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46[.2022-12-11 22:56:46568434.: [568439E2022-12-11 22:56:46:  .E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc568456 2022-12-11 22:56:46:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.638E:568479]  638: eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] E
:eager release cuda mem 625663 1980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-11 22:56:46.568604: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:56:46:.1980[568617] 2022-12-11 22:56:46: eager alloc mem 611.00 KB.E
568633 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.568735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.568803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 22:56:46.568824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.568892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.569079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.569127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 22:56:46.569149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.569197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.569325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.569391: [E2022-12-11 22:56:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu569399:: [1980E2022-12-11 22:56:46]  .eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc569426[
:: 2022-12-11 22:56:46638E.]  569458eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 
:E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-11 22:56:46[.2022-12-11 22:56:46569556.: 569561E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc [2022-12-11 22:56:46:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:56:46.638:.569582] 1980569586: eager release cuda mem 625663] : [E
eager alloc mem 611.00 KBE2022-12-11 22:56:46 
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu569641::: [19801980E2022-12-11 22:56:46] ]  .eager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc569710

:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.569827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.569904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.569959: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46:.638569972] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.570042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.570215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.570281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.570411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.570478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:56:461980.] 570489eager alloc mem 611.00 KB: [
E[2022-12-11 22:56:46 2022-12-11 22:56:46./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.570518:570522: 638: E] E[ eager release cuda mem 625663 2022-12-11 22:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.::570576638638: ] ] Eeager release cuda mem 625663eager release cuda mem 625663 
[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46:.638570658] : eager release cuda mem 625663E
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 22:56:46:2022-12-11 22:56:46.[1980.5707072022-12-11 22:56:46] 570714[: .eager alloc mem 611.00 KB: 2022-12-11 22:56:46E570739
E. :  570753/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : 2022-12-11 22:56:46:E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.1980 ] :570803] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB638: eager alloc mem 611.00 KB:
] E
1980eager release cuda mem 625663 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-11 22:56:46.570952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 22:56:46
.570972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.571042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.571108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.571249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.571318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:56:46.571512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.571583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:56:46] .eager alloc mem 611.00 KB571608
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[[] 2022-12-11 22:56:462022-12-11 22:56:46eager release cuda mem 625663..
571648571650: : E[E 2022-12-11 22:56:46 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46:571698[:.638: 2022-12-11 22:56:46638571711] E.] : eager release cuda mem 625663 571742eager release cuda mem 625663E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 
 :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638 [:[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:466382022-12-11 22:56:46eager release cuda mem 12399996:[.] .
6382022-12-11 22:56:46571844eager release cuda mem 625663571862] .: 
: eager release cuda mem 625663571878EE
: [  E2022-12-11 22:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu .:2022-12-11 22:56:46:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc571953638.1980:: ] 571970] 638Eeager release cuda mem 12399996: eager alloc mem 611.00 KB]  
E
eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 
[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:56:46638:[.] 6382022-12-11 22:56:46572065eager release cuda mem 12399996] .: 
eager release cuda mem 12399996572080E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 12399996
[2022-12-11 22:56:46.572197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:56:46.572366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.572405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:56:46.572815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:56:46.572851: E[ 2022-12-11 22:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:572855638: ] Eeager release cuda mem 12399996 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.403282 secs 
[2022-12-11 22:56:46.574045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.400782 secs 
[2022-12-11 22:56:46.574464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.402361 secs 
[2022-12-11 22:56:46.574867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.405754 secs 
[2022-12-11 22:56:46.575438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.406054 secs 
[2022-12-11 22:56:46.575627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.403473 secs 
[2022-12-11 22:56:46.576060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.422132 secs 
[2022-12-11 22:56:46.577055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 97000001 / 100000000 nodes ( 97.00 %) | 1.43 GB | 0.403778 secs 
[2022-12-11 22:56:46.578167: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.07 GB
[2022-12-11 22:56:47.992604: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.33 GB
[2022-12-11 22:56:47.992960: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.33 GB
[2022-12-11 22:56:47.993486: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.33 GB
[2022-12-11 22:56:49.369124: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.59 GB
[2022-12-11 22:56:49.369360: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.59 GB
[2022-12-11 22:56:49.370420: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.59 GB
[2022-12-11 22:56:50.709775: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.81 GB
[2022-12-11 22:56:50.709923: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.81 GB
[2022-12-11 22:56:50.710208: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.81 GB
[2022-12-11 22:56:52.889414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.02 GB
[2022-12-11 22:56:52.891079: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.02 GB
[2022-12-11 22:56:52.896122: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.02 GB
[2022-12-11 22:56:54.551212: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.48 GB
[2022-12-11 22:56:54.551494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.48 GB
[2022-12-11 22:56:54.554120: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.48 GB
[2022-12-11 22:56:55.950872: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.68 GB
[2022-12-11 22:56:55.951040: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.68 GB
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][tid #140589244872448]: replica 6 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][tid #140589303588608]: replica 0 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][tid #140589311981312]: replica 4 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][tid #140589362304768]: replica 5 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][22:56:55.951][ERROR][RK0][tid #140589244872448]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][tid #140589303588608]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][tid #140589311981312]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][tid #140589362304768]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:56:55.951][ERROR][RK0][tid #140589244872448]: init per replica done
[HCTR][22:56:55.951][ERROR][RK0][main]: init per replica done
[HCTR][22:56:55.951][ERROR][RK0][main]: init per replica done
[HCTR][22:56:55.951][ERROR][RK0][main]: init per replica done
[HCTR][22:56:55.951][ERROR][RK0][main]: init per replica done
[HCTR][22:56:55.951][ERROR][RK0][tid #140589311981312]: init per replica done
[HCTR][22:56:55.951][ERROR][RK0][tid #140589362304768]: init per replica done
[HCTR][22:56:55.953][ERROR][RK0][tid #140589303588608]: init per replica done
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 4 allocated 3276800 at 0x7fdf66d20000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 4 allocated 6553600 at 0x7fdf67200000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 4 allocated 3276800 at 0x7fdf67840000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589362304768]: 5 allocated 3276800 at 0x7fdf5cd20000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 4 allocated 6553600 at 0x7fdf67b60000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589362304768]: 5 allocated 6553600 at 0x7fdf5d200000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589362304768]: 5 allocated 3276800 at 0x7fdf5d840000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589362304768]: 5 allocated 6553600 at 0x7fdf5db60000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589504915200]: 7 allocated 3276800 at 0x7fdf58d20000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589504915200]: 7 allocated 6553600 at 0x7fdf59200000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589504915200]: 7 allocated 3276800 at 0x7fdf59840000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589504915200]: 7 allocated 6553600 at 0x7fdf59b60000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589437806336]: 2 allocated 3276800 at 0x7fdf68d20000
[HCTR][22:56:55.957][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fdf66d20000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589437806336]: 2 allocated 6553600 at 0x7fdf69200000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589437806336]: 2 allocated 3276800 at 0x7fdf69840000
[HCTR][22:56:55.957][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fdf67200000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589437806336]: 2 allocated 6553600 at 0x7fdf69b60000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 1 allocated 3276800 at 0x7fdf68d20000
[HCTR][22:56:55.957][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fdf67840000
[HCTR][22:56:55.957][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fdf67b60000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 1 allocated 6553600 at 0x7fdf69200000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 1 allocated 3276800 at 0x7fdf69840000
[HCTR][22:56:55.957][ERROR][RK0][tid #140589311981312]: 1 allocated 6553600 at 0x7fdf69b60000
[HCTR][22:56:55.957][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fdf66d20000
[HCTR][22:56:55.957][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fdf67200000
[HCTR][22:56:55.957][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fdf67840000
[HCTR][22:56:55.957][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fdf67b60000
[HCTR][22:56:55.960][ERROR][RK0][main]: 0 allocated 3276800 at 0x7fdf69920000
[HCTR][22:56:55.960][ERROR][RK0][main]: 0 allocated 6553600 at 0x7fdf69e00000
[HCTR][22:56:55.960][ERROR][RK0][main]: 0 allocated 3276800 at 0x7fdf6ab0e800
[HCTR][22:56:55.960][ERROR][RK0][main]: 0 allocated 6553600 at 0x7fdf6ae2e800








