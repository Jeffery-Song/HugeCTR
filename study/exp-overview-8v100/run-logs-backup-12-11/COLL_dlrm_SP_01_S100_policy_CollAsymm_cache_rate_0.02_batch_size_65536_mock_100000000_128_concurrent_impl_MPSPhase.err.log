2022-12-11 21:52:41.168641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.182363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.187774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.193135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.206288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.211229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.223923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.229350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.267403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.268836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.269900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.270913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.272047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.273147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.274177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.275221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.279955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.281125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.282141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.282674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.283404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.284267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.284977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.286021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.286591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.287438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.288367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.289024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.290340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.290731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.292178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.293130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.294058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.295096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.296383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.298030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.298745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.299219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.300455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.300685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.302039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.302113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.303630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.303691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.305313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.306006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.306661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.307564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.308372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.309076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.310013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.310489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.311903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.313223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.314996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.315895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.317069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.318199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.318469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.320672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.320768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.320903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.320910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.323301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.323393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.323517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.323583: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.323661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.326134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.326191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.326331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.326452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.326554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.328609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.328727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.329118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.329233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.329581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.331464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.331711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.332186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.332424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.332875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.333396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.334016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.334607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.335207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.335642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.335737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.336263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.336746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.337391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.338488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.339272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.339679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.340145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.340241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.340816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.342354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.342973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.343307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.343655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.344932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.346072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.346241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.347171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.348283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.348375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.349140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.350415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.351015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.352010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.352561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.353545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.354099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.355061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.355076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.355828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.357262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.357307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.358005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.359434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.359833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.360188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.360789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.361266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.362156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.367341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.369020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.369941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.370337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.370996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.372316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.372877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.373570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.383333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.385296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.389480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.393465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.408095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.408886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.411022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.411385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.412052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.412096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.412133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.412525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.413510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.416486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.416619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.416855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.416901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.417043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.417355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.419025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.422082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.422287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.422544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.422592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.422686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.423013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.424058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.426598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.426815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.427112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.427172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.427354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.428531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.431324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.431436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.431613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.431657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.431760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.432187: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.432735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.435702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.435778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.435883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.436037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.436144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.437084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.439797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.440025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.440184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.440261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.440332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.440929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.441975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.444110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.444402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.444628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.444787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.444920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.445865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.446949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.448907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.449451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.449791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.449918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.451515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.451948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.452994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.453481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.453721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.453758: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.453887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.456028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.457026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.457430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.457615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.457753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.460540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.461254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.461726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.461867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.462055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.463204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.465067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.465706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.466275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.466514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.466866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.467959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.469672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.470211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.470742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.471062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.471277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.472398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.474064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.474621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.475155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.475460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.476013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.479093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.480296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.481231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.481622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.481623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.483436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.484571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.486249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.486940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.486996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.489591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.490435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.491057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.491112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.492008: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.493466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.494095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.495079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.495142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.497609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.500449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.501639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.502400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.502414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.505080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.506181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.506821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.507240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.507377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.510874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.510885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.511258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.511726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.512253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.544076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.544229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.545180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.545486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.552999: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.553001: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.553999: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.554006: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:52:41.562774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.562807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.564123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.564264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.599126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.599178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.599857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.600027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.604534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.604573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.606303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:41.606509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.679391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.680828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.682356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.683582: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.683642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:52:42.702557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.704094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.705856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.707285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.708678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.709892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:52:42.759844: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.760040: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.791758: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:52:42.794695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.796007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.796973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.798118: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.798173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:52:42.816409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.817904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.818972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.820163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.821186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.822254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:52:42.858032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.858651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.859405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.860026: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.860082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:52:42.878204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.878846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.879845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.880606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.881289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.882074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:52:42.909045: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.909234: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.910226: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:52:42.919357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.920665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.921981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.923076: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.923142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:52:42.928759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.928759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.930420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.930452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.931975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.932016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.934939: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.934978: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.934994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:52:42.935022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:52:42.941240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.942085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.945120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.945756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.946288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.946584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.947788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:52:42.948333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.949154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.950267: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.950314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:52:42.952862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.952932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.953914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.954036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.954757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.954972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.955864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.955936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.956839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.956993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.957962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:52:42.958040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:52:42.964713: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.964898: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.965831: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:52:42.968265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.968900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.969417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.970010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.970523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.970998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:52:42.982451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.983116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.983661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:42.984129: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:52:42.984181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:52:42.993767: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.993937: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:42.995444: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:52:43.002283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:43.003486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:43.003673: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.003848: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.004223: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.004360: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.004639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:43.004946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:52:43.005400: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:52:43.005987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:43.007242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:52:43.008358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:52:43.018047: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.018220: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.019360: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:52:43.056594: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.056791: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:52:43.057726: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:52:44.336][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 92it [00:01, 77.63it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 97it [00:01, 82.95it/s]warmup run: 189it [00:01, 173.76it/s]warmup run: 95it [00:01, 81.19it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.60s/it]warmup run: 98it [00:01, 83.55it/s]warmup run: 194it [00:01, 179.72it/s]warmup run: 286it [00:01, 279.69it/s]warmup run: 173it [00:01, 157.24it/s]warmup run: 1it [00:01,  1.62s/it]warmup run: 76it [00:01, 64.66it/s]warmup run: 96it [00:01, 78.45it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 196it [00:01, 181.02it/s]warmup run: 291it [00:01, 286.12it/s]warmup run: 382it [00:01, 388.46it/s]warmup run: 269it [00:01, 265.28it/s]warmup run: 97it [00:01, 78.32it/s]warmup run: 172it [00:01, 161.91it/s]warmup run: 193it [00:01, 171.87it/s]warmup run: 85it [00:01, 74.51it/s]warmup run: 294it [00:01, 288.48it/s]warmup run: 389it [00:01, 397.56it/s]warmup run: 475it [00:02, 488.97it/s]warmup run: 367it [00:01, 380.06it/s]warmup run: 194it [00:01, 170.79it/s]warmup run: 269it [00:01, 270.46it/s]warmup run: 288it [00:01, 273.12it/s]warmup run: 185it [00:01, 177.83it/s]warmup run: 392it [00:01, 399.36it/s]warmup run: 487it [00:02, 505.65it/s]warmup run: 570it [00:02, 585.62it/s]warmup run: 292it [00:01, 275.15it/s]warmup run: 465it [00:02, 491.43it/s]warmup run: 362it [00:01, 376.31it/s]warmup run: 386it [00:02, 383.43it/s]warmup run: 286it [00:01, 292.13it/s]warmup run: 490it [00:02, 507.49it/s]warmup run: 583it [00:02, 600.99it/s]warmup run: 666it [00:02, 671.82it/s]warmup run: 391it [00:02, 386.13it/s]warmup run: 563it [00:02, 593.99it/s]warmup run: 455it [00:02, 479.32it/s]warmup run: 387it [00:01, 409.85it/s]warmup run: 484it [00:02, 492.00it/s]warmup run: 589it [00:02, 608.10it/s]warmup run: 679it [00:02, 683.70it/s]warmup run: 763it [00:02, 745.23it/s]warmup run: 663it [00:02, 687.76it/s]warmup run: 492it [00:02, 498.99it/s]warmup run: 549it [00:02, 575.72it/s]warmup run: 584it [00:02, 596.94it/s]warmup run: 488it [00:01, 523.34it/s]warmup run: 689it [00:02, 697.26it/s]warmup run: 777it [00:02, 756.31it/s]warmup run: 857it [00:02, 793.79it/s]warmup run: 593it [00:02, 604.08it/s]warmup run: 764it [00:02, 767.18it/s]warmup run: 643it [00:02, 659.32it/s]warmup run: 686it [00:02, 692.54it/s]warmup run: 591it [00:02, 630.61it/s]warmup run: 790it [00:02, 773.35it/s]warmup run: 873it [00:02, 807.78it/s]warmup run: 951it [00:02, 831.76it/s]warmup run: 865it [00:02, 830.45it/s]warmup run: 695it [00:02, 697.78it/s]warmup run: 737it [00:02, 727.76it/s]warmup run: 787it [00:02, 770.07it/s]warmup run: 694it [00:02, 722.43it/s]warmup run: 890it [00:02, 832.06it/s]warmup run: 968it [00:02, 836.41it/s]warmup run: 1046it [00:02, 862.79it/s]warmup run: 966it [00:02, 877.97it/s]warmup run: 797it [00:02, 775.51it/s]warmup run: 831it [00:02, 781.67it/s]warmup run: 888it [00:02, 832.17it/s]warmup run: 797it [00:02, 798.46it/s]warmup run: 990it [00:02, 875.50it/s]warmup run: 1140it [00:02, 882.42it/s]warmup run: 1067it [00:02, 914.56it/s]warmup run: 898it [00:02, 835.86it/s]warmup run: 1062it [00:02, 856.65it/s]warmup run: 923it [00:02, 815.43it/s]warmup run: 989it [00:02, 878.74it/s]warmup run: 899it [00:02, 856.30it/s]warmup run: 1089it [00:02, 906.84it/s]warmup run: 1234it [00:02, 898.16it/s]warmup run: 1167it [00:02, 937.43it/s]warmup run: 999it [00:02, 882.13it/s]warmup run: 1155it [00:02, 868.80it/s]warmup run: 1015it [00:02, 838.41it/s]warmup run: 1090it [00:02, 914.42it/s]warmup run: 1000it [00:02, 897.29it/s]warmup run: 1188it [00:02, 925.44it/s]warmup run: 1330it [00:02, 915.57it/s]warmup run: 1268it [00:02, 957.80it/s]warmup run: 1103it [00:02, 924.66it/s]warmup run: 1249it [00:02, 887.46it/s]warmup run: 1107it [00:02, 861.44it/s]warmup run: 1192it [00:02, 942.17it/s]warmup run: 1101it [00:02, 927.97it/s]warmup run: 1287it [00:02, 939.61it/s]warmup run: 1425it [00:03, 921.29it/s]warmup run: 1370it [00:02, 973.75it/s]warmup run: 1206it [00:02, 953.35it/s]warmup run: 1342it [00:02, 892.28it/s]warmup run: 1199it [00:02, 874.14it/s]warmup run: 1293it [00:02, 958.64it/s]warmup run: 1202it [00:02, 920.43it/s]warmup run: 1385it [00:02, 950.69it/s]warmup run: 1472it [00:03, 985.81it/s]warmup run: 1309it [00:02, 972.89it/s]warmup run: 1520it [00:03, 923.04it/s]warmup run: 1434it [00:03, 894.51it/s]warmup run: 1291it [00:02, 885.28it/s]warmup run: 1394it [00:03, 972.86it/s]warmup run: 1483it [00:03, 957.56it/s]warmup run: 1300it [00:02, 890.74it/s]warmup run: 1574it [00:03, 993.44it/s]warmup run: 1411it [00:03, 982.45it/s]warmup run: 1614it [00:03, 925.25it/s]warmup run: 1526it [00:03, 894.34it/s]warmup run: 1383it [00:03, 890.01it/s]warmup run: 1497it [00:03, 986.97it/s]warmup run: 1581it [00:03, 962.06it/s]warmup run: 1401it [00:02, 921.36it/s]warmup run: 1675it [00:03, 995.79it/s]warmup run: 1712it [00:03, 941.06it/s]warmup run: 1513it [00:03, 990.78it/s]warmup run: 1617it [00:03, 896.50it/s]warmup run: 1474it [00:03, 892.51it/s]warmup run: 1599it [00:03, 994.09it/s]warmup run: 1679it [00:03, 966.57it/s]warmup run: 1503it [00:03, 947.53it/s]warmup run: 1777it [00:03, 1001.00it/s]warmup run: 1811it [00:03, 953.71it/s]warmup run: 1615it [00:03, 993.81it/s]warmup run: 1709it [00:03, 902.23it/s]warmup run: 1566it [00:03, 898.32it/s]warmup run: 1700it [00:03, 995.74it/s]warmup run: 1781it [00:03, 981.09it/s]warmup run: 1605it [00:03, 966.63it/s]warmup run: 1879it [00:03, 1003.69it/s]warmup run: 1907it [00:03, 952.06it/s]warmup run: 1718it [00:03, 1002.58it/s]warmup run: 1803it [00:03, 911.61it/s]warmup run: 1660it [00:03, 909.64it/s]warmup run: 1802it [00:03, 1002.79it/s]warmup run: 1880it [00:03, 971.55it/s]warmup run: 1706it [00:03, 978.93it/s]warmup run: 1983it [00:03, 1012.54it/s]warmup run: 2003it [00:03, 952.92it/s]warmup run: 1820it [00:03, 1006.67it/s]warmup run: 1896it [00:03, 916.66it/s]warmup run: 1754it [00:03, 916.72it/s]warmup run: 1904it [00:03, 1004.48it/s]warmup run: 1806it [00:03, 982.15it/s]warmup run: 1978it [00:03, 960.28it/s]warmup run: 2102it [00:03, 1062.85it/s]warmup run: 2124it [00:03, 1027.04it/s]warmup run: 1922it [00:03, 1009.45it/s]warmup run: 1990it [00:03, 920.76it/s]warmup run: 1847it [00:03, 919.29it/s]warmup run: 2005it [00:03, 1004.62it/s]warmup run: 2091it [00:03, 1009.64it/s]warmup run: 1907it [00:03, 987.87it/s]warmup run: 2225it [00:03, 1110.28it/s]warmup run: 2026it [00:03, 1018.38it/s]warmup run: 2245it [00:03, 1080.33it/s]warmup run: 2104it [00:03, 984.75it/s]warmup run: 1940it [00:03, 916.79it/s]warmup run: 2123it [00:03, 1055.46it/s]warmup run: 2210it [00:03, 1061.75it/s]warmup run: 2009it [00:03, 995.24it/s]warmup run: 2348it [00:03, 1144.37it/s]warmup run: 2141it [00:03, 1056.76it/s]warmup run: 2366it [00:03, 1117.69it/s]warmup run: 2221it [00:03, 1037.45it/s]warmup run: 2038it [00:03, 934.67it/s]warmup run: 2244it [00:03, 1099.66it/s]warmup run: 2328it [00:03, 1096.23it/s]warmup run: 2127it [00:03, 1048.86it/s]warmup run: 2471it [00:03, 1168.09it/s]warmup run: 2256it [00:03, 1083.68it/s]warmup run: 2486it [00:04, 1140.47it/s]warmup run: 2338it [00:03, 1075.59it/s]warmup run: 2153it [00:03, 996.40it/s]warmup run: 2365it [00:03, 1130.39it/s]warmup run: 2446it [00:03, 1120.41it/s]warmup run: 2244it [00:03, 1084.86it/s]warmup run: 2594it [00:04, 1185.25it/s]warmup run: 2372it [00:03, 1103.67it/s]warmup run: 2605it [00:04, 1153.53it/s]warmup run: 2455it [00:04, 1103.04it/s]warmup run: 2268it [00:03, 1040.75it/s]warmup run: 2486it [00:04, 1151.68it/s]warmup run: 2564it [00:04, 1136.91it/s]warmup run: 2363it [00:03, 1113.64it/s]warmup run: 2716it [00:04, 1193.10it/s]warmup run: 2725it [00:04, 1166.04it/s]warmup run: 2491it [00:04, 1127.63it/s]warmup run: 2572it [00:04, 1122.60it/s]warmup run: 2388it [00:04, 1086.58it/s]warmup run: 2607it [00:04, 1166.26it/s]warmup run: 2681it [00:04, 1144.10it/s]warmup run: 2481it [00:03, 1133.00it/s]warmup run: 2837it [00:04, 1197.63it/s]warmup run: 2846it [00:04, 1177.24it/s]warmup run: 2610it [00:04, 1143.85it/s]warmup run: 2687it [00:04, 1130.20it/s]warmup run: 2508it [00:04, 1117.78it/s]warmup run: 2727it [00:04, 1174.96it/s]warmup run: 2799it [00:04, 1153.22it/s]warmup run: 2599it [00:04, 1146.04it/s]warmup run: 2957it [00:04, 1197.43it/s]warmup run: 2967it [00:04, 1185.12it/s]warmup run: 2728it [00:04, 1153.55it/s]warmup run: 2804it [00:04, 1139.84it/s]warmup run: 2627it [00:04, 1138.27it/s]warmup run: 3000it [00:04, 666.97it/s] warmup run: 3000it [00:04, 684.92it/s] warmup run: 2846it [00:04, 1178.78it/s]warmup run: 2917it [00:04, 1159.00it/s]warmup run: 2716it [00:04, 1150.95it/s]warmup run: 2848it [00:04, 1165.64it/s]warmup run: 2921it [00:04, 1147.44it/s]warmup run: 2745it [00:04, 1149.03it/s]warmup run: 2968it [00:04, 1190.30it/s]warmup run: 3000it [00:04, 677.33it/s] warmup run: 3000it [00:04, 662.80it/s] warmup run: 2833it [00:04, 1154.00it/s]warmup run: 2968it [00:04, 1173.45it/s]warmup run: 3000it [00:04, 674.43it/s] warmup run: 2860it [00:04, 1142.97it/s]warmup run: 3000it [00:04, 671.45it/s] warmup run: 2952it [00:04, 1163.29it/s]warmup run: 2980it [00:04, 1158.79it/s]warmup run: 3000it [00:04, 655.39it/s] warmup run: 3000it [00:04, 686.38it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1645.73it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.19it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.44it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1644.52it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.91it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1624.20it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1601.54it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1642.03it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1647.75it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1622.53it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.03it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1671.03it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.07it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.97it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1615.50it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.42it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1642.36it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1612.04it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1643.55it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1615.76it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1649.78it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1665.75it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1624.93it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1633.32it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1640.18it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1646.50it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1664.52it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1613.57it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1625.25it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1639.65it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1629.83it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1601.09it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1624.47it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1636.69it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1611.42it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1642.21it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1660.97it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1635.45it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1626.93it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1594.03it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1658.99it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1618.77it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1621.17it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1635.16it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1627.90it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1603.31it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1615.03it/s]warmup should be done:  32%|      | 969/3000 [00:00<00:01, 1583.49it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1617.48it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1654.49it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1627.28it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1634.44it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1620.43it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1602.74it/s]warmup should be done:  38%|      | 1128/3000 [00:00<00:01, 1580.30it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1599.78it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:01, 1654.69it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1622.40it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1616.94it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1598.69it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1627.74it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1603.62it/s]warmup should be done:  43%|     | 1287/3000 [00:00<00:01, 1573.68it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1585.34it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1653.69it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1624.36it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1629.59it/s]warmup should be done:  49%|     | 1456/3000 [00:00<00:00, 1593.92it/s]warmup should be done:  49%|     | 1472/3000 [00:00<00:00, 1612.04it/s]warmup should be done:  48%|     | 1445/3000 [00:00<00:00, 1571.35it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1597.40it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1557.10it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1625.45it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1648.16it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1629.66it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1608.71it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1588.35it/s]warmup should be done:  53%|    | 1603/3000 [00:01<00:00, 1573.06it/s]warmup should be done:  55%|    | 1636/3000 [00:01<00:00, 1603.11it/s]warmup should be done:  54%|    | 1619/3000 [00:01<00:00, 1538.40it/s]warmup should be done:  60%|    | 1804/3000 [00:01<00:00, 1625.52it/s]warmup should be done:  61%|    | 1834/3000 [00:01<00:00, 1649.15it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1630.73it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1609.07it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1583.49it/s]warmup should be done:  59%|    | 1762/3000 [00:01<00:00, 1575.96it/s]warmup should be done:  60%|    | 1799/3000 [00:01<00:00, 1610.29it/s]warmup should be done:  59%|    | 1773/3000 [00:01<00:00, 1527.42it/s]warmup should be done:  66%|   | 1967/3000 [00:01<00:00, 1625.48it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1631.93it/s]warmup should be done:  67%|   | 2000/3000 [00:01<00:00, 1650.30it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1607.32it/s]warmup should be done:  64%|   | 1920/3000 [00:01<00:00, 1573.80it/s]warmup should be done:  65%|   | 1962/3000 [00:01<00:00, 1614.61it/s]warmup should be done:  64%|   | 1934/3000 [00:01<00:00, 1569.79it/s]warmup should be done:  64%|   | 1926/3000 [00:01<00:00, 1518.35it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1625.17it/s]warmup should be done:  71%|  | 2140/3000 [00:01<00:00, 1630.97it/s]warmup should be done:  72%|  | 2166/3000 [00:01<00:00, 1649.88it/s]warmup should be done:  71%|   | 2117/3000 [00:01<00:00, 1604.80it/s]warmup should be done:  71%|   | 2125/3000 [00:01<00:00, 1617.58it/s]warmup should be done:  69%|   | 2078/3000 [00:01<00:00, 1558.74it/s]warmup should be done:  70%|   | 2092/3000 [00:01<00:00, 1533.25it/s]warmup should be done:  69%|   | 2078/3000 [00:01<00:00, 1511.16it/s]warmup should be done:  76%|  | 2293/3000 [00:01<00:00, 1624.12it/s]warmup should be done:  77%|  | 2304/3000 [00:01<00:00, 1629.60it/s]warmup should be done:  78%|  | 2331/3000 [00:01<00:00, 1641.97it/s]warmup should be done:  76%|  | 2278/3000 [00:01<00:00, 1600.41it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1613.85it/s]warmup should be done:  74%|  | 2234/3000 [00:01<00:00, 1554.45it/s]warmup should be done:  75%|  | 2249/3000 [00:01<00:00, 1543.71it/s]warmup should be done:  74%|  | 2230/3000 [00:01<00:00, 1505.68it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1625.49it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1629.90it/s]warmup should be done:  81%| | 2439/3000 [00:01<00:00, 1600.23it/s]warmup should be done:  83%| | 2496/3000 [00:01<00:00, 1635.99it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1610.23it/s]warmup should be done:  80%|  | 2393/3000 [00:01<00:00, 1562.08it/s]warmup should be done:  80%|  | 2406/3000 [00:01<00:00, 1549.02it/s]warmup should be done:  79%|  | 2381/3000 [00:01<00:00, 1501.81it/s]warmup should be done:  87%| | 2619/3000 [00:01<00:00, 1616.28it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1630.60it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1602.14it/s]warmup should be done:  89%| | 2661/3000 [00:01<00:00, 1637.21it/s]warmup should be done:  87%| | 2612/3000 [00:01<00:00, 1614.62it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1564.20it/s]warmup should be done:  85%| | 2563/3000 [00:01<00:00, 1553.33it/s]warmup should be done:  84%| | 2533/3000 [00:01<00:00, 1506.01it/s]warmup should be done:  93%|| 2796/3000 [00:01<00:00, 1631.48it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1602.95it/s]warmup should be done:  94%|| 2827/3000 [00:01<00:00, 1642.01it/s]warmup should be done:  92%|| 2775/3000 [00:01<00:00, 1617.14it/s]warmup should be done:  90%| | 2709/3000 [00:01<00:00, 1568.28it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1570.65it/s]warmup should be done:  90%| | 2686/3000 [00:01<00:00, 1510.30it/s]warmup should be done:  93%|| 2781/3000 [00:01<00:00, 1438.78it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1636.85it/s]warmup should be done:  97%|| 2923/3000 [00:01<00:00, 1607.46it/s]warmup should be done: 100%|| 2993/3000 [00:01<00:00, 1646.74it/s]warmup should be done:  98%|| 2939/3000 [00:01<00:00, 1623.23it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.29it/s]warmup should be done:  96%|| 2869/3000 [00:01<00:00, 1577.33it/s]warmup should be done:  96%|| 2889/3000 [00:01<00:00, 1589.01it/s]warmup should be done:  95%|| 2840/3000 [00:01<00:00, 1518.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1634.89it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1484.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.19it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1589.49it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1585.36it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1578.58it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1526.03it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1549.76it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.92it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.53it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1684.52it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1582.46it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.68it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.39it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1661.89it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1620.44it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1670.96it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1663.01it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.50it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1680.14it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1695.08it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1669.30it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1579.14it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1619.73it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1679.14it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1669.99it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1687.04it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1694.80it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1692.52it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1625.31it/s]warmup should be done:  16%|        | 477/3000 [00:00<00:01, 1580.01it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1658.33it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1680.11it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1666.01it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1701.14it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1628.07it/s]warmup should be done:  21%|       | 640/3000 [00:00<00:01, 1596.57it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1678.77it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1682.46it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1613.79it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1679.27it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1700.58it/s]warmup should be done:  27%|       | 802/3000 [00:00<00:01, 1604.40it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1658.92it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1624.34it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1663.64it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1673.85it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1631.84it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1675.02it/s]warmup should be done:  32%|      | 964/3000 [00:00<00:01, 1608.83it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1658.89it/s]warmup should be done:  34%|      | 1023/3000 [00:00<00:01, 1698.65it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1670.97it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1668.65it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1589.79it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1636.43it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1658.87it/s]warmup should be done:  38%|      | 1127/3000 [00:00<00:01, 1614.93it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1677.81it/s]warmup should be done:  40%|      | 1193/3000 [00:00<00:01, 1690.73it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1685.01it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1664.96it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1599.14it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1634.57it/s]warmup should be done:  44%|     | 1333/3000 [00:00<00:01, 1659.15it/s]warmup should be done:  45%|     | 1348/3000 [00:00<00:00, 1683.07it/s]warmup should be done:  43%|     | 1289/3000 [00:00<00:01, 1612.91it/s]warmup should be done:  45%|     | 1363/3000 [00:00<00:00, 1688.85it/s]warmup should be done:  45%|     | 1351/3000 [00:00<00:00, 1665.73it/s]warmup should be done:  45%|     | 1354/3000 [00:00<00:00, 1674.76it/s]warmup should be done:  43%|     | 1303/3000 [00:00<00:01, 1601.98it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1633.17it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1662.99it/s]warmup should be done:  48%|     | 1453/3000 [00:00<00:00, 1620.68it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1685.84it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1685.35it/s]warmup should be done:  51%|     | 1518/3000 [00:00<00:00, 1665.02it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1676.89it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1609.02it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1634.71it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1667.81it/s]warmup should be done:  54%|    | 1617/3000 [00:01<00:00, 1625.43it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1688.15it/s]warmup should be done:  57%|    | 1701/3000 [00:01<00:00, 1684.01it/s]warmup should be done:  56%|    | 1692/3000 [00:01<00:00, 1678.10it/s]warmup should be done:  54%|    | 1629/3000 [00:01<00:00, 1614.89it/s]warmup should be done:  56%|    | 1685/3000 [00:01<00:00, 1648.35it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1634.41it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1671.35it/s]warmup should be done:  62%|   | 1858/3000 [00:01<00:00, 1689.08it/s]warmup should be done:  59%|    | 1780/3000 [00:01<00:00, 1621.92it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1683.37it/s]warmup should be done:  62%|   | 1860/3000 [00:01<00:00, 1673.75it/s]warmup should be done:  60%|    | 1792/3000 [00:01<00:00, 1618.02it/s]warmup should be done:  61%|    | 1820/3000 [00:01<00:00, 1635.95it/s]warmup should be done:  62%|   | 1853/3000 [00:01<00:00, 1656.01it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1665.59it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1620.26it/s]warmup should be done:  68%|   | 2039/3000 [00:01<00:00, 1679.14it/s]warmup should be done:  68%|   | 2027/3000 [00:01<00:00, 1667.67it/s]warmup should be done:  68%|   | 2030/3000 [00:01<00:00, 1679.91it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1615.49it/s]warmup should be done:  67%|   | 2020/3000 [00:01<00:00, 1657.41it/s]warmup should be done:  66%|   | 1984/3000 [00:01<00:00, 1632.87it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1664.47it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1616.11it/s]warmup should be done:  73%|  | 2195/3000 [00:01<00:00, 1668.53it/s]warmup should be done:  74%|  | 2207/3000 [00:01<00:00, 1672.85it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1675.13it/s]warmup should be done:  71%|   | 2116/3000 [00:01<00:00, 1611.49it/s]warmup should be done:  73%|  | 2187/3000 [00:01<00:00, 1659.47it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1633.14it/s]warmup should be done:  78%|  | 2339/3000 [00:01<00:00, 1664.32it/s]warmup should be done:  76%|  | 2269/3000 [00:01<00:00, 1619.64it/s]warmup should be done:  79%|  | 2364/3000 [00:01<00:00, 1673.74it/s]warmup should be done:  79%|  | 2375/3000 [00:01<00:00, 1668.51it/s]warmup should be done:  79%|  | 2366/3000 [00:01<00:00, 1675.41it/s]warmup should be done:  76%|  | 2279/3000 [00:01<00:00, 1616.19it/s]warmup should be done:  78%|  | 2353/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  77%|  | 2312/3000 [00:01<00:00, 1632.07it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1660.66it/s]warmup should be done:  81%|  | 2431/3000 [00:01<00:00, 1617.76it/s]warmup should be done:  84%| | 2533/3000 [00:01<00:00, 1677.17it/s]warmup should be done:  85%| | 2542/3000 [00:01<00:00, 1668.24it/s]warmup should be done:  84%| | 2535/3000 [00:01<00:00, 1678.56it/s]warmup should be done:  81%| | 2441/3000 [00:01<00:00, 1616.99it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1635.88it/s]warmup should be done:  84%| | 2519/3000 [00:01<00:00, 1471.51it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1661.69it/s]warmup should be done:  86%| | 2594/3000 [00:01<00:00, 1619.89it/s]warmup should be done:  90%| | 2702/3000 [00:01<00:00, 1679.30it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1679.94it/s]warmup should be done:  87%| | 2604/3000 [00:01<00:00, 1619.52it/s]warmup should be done:  88%| | 2642/3000 [00:01<00:00, 1637.58it/s]warmup should be done:  90%| | 2709/3000 [00:01<00:00, 1539.12it/s]warmup should be done:  90%| | 2686/3000 [00:01<00:00, 1526.14it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1632.43it/s]warmup should be done:  96%|| 2870/3000 [00:01<00:00, 1678.97it/s]warmup should be done:  96%|| 2872/3000 [00:01<00:00, 1678.36it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1622.34it/s]warmup should be done:  95%|| 2840/3000 [00:01<00:00, 1631.11it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1636.88it/s]warmup should be done:  96%|| 2865/3000 [00:01<00:00, 1531.63it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1566.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.31it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.30it/s]warmup should be done:  98%|| 2928/3000 [00:01<00:00, 1641.50it/s]warmup should be done:  98%|| 2931/3000 [00:01<00:00, 1626.60it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1654.13it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1636.24it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1641.72it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1635.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1619.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1613.69it/s]2022-12-11 21:54:19.369955: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7e802fbd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:19.370020: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:20.511252: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13b78307a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:20.511320: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:20.520327: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef7e802fa40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:20.520387: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:20.888678: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13b7830a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:20.888761: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:20.915545: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13abf93150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:20.915612: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:21.091754: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13af82cb20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:21.091825: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:21.092186: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f13b782bce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:21.092251: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:21.142249: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef81c02d5a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:54:21.142336: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:54:21.586972: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:22.787318: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:22.824103: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:23.215636: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:23.215866: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:23.375964: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:23.384045: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:23.401507: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:54:24.451251: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:25.678306: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:25.771234: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:26.100650: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:26.134317: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:26.220786: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:26.256408: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:54:26.269932: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:55:05.936][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:55:05.936][ERROR][RK0][tid #139723037193984]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:55:05.936][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:05.936][ERROR][RK0][tid #139723037193984]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:05.945][ERROR][RK0][tid #139723037193984]: coll ps creation done
[HCTR][21:55:05.945][ERROR][RK0][main]: coll ps creation done
[HCTR][21:55:05.945][ERROR][RK0][tid #139723037193984]: replica 1 waits for coll ps creation barrier
[HCTR][21:55:05.945][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][21:55:05.950][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:55:05.951][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:05.956][ERROR][RK0][main]: coll ps creation done
[HCTR][21:55:05.956][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][21:55:05.988][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:55:05.988][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:05.993][ERROR][RK0][main]: coll ps creation done
[HCTR][21:55:05.993][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:55:06.053][ERROR][RK0][tid #139722911368960]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:55:06.053][ERROR][RK0][tid #139722911368960]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:06.058][ERROR][RK0][tid #139722911368960]: coll ps creation done
[HCTR][21:55:06.058][ERROR][RK0][tid #139722911368960]: replica 6 waits for coll ps creation barrier
[HCTR][21:55:06.067][ERROR][RK0][tid #139723179804416]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:55:06.068][ERROR][RK0][tid #139723179804416]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:06.072][ERROR][RK0][tid #139723179804416]: coll ps creation done
[HCTR][21:55:06.072][ERROR][RK0][tid #139723179804416]: replica 7 waits for coll ps creation barrier
[HCTR][21:55:06.130][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:55:06.130][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:06.135][ERROR][RK0][main]: coll ps creation done
[HCTR][21:55:06.135][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][21:55:06.147][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:55:06.147][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:55:06.154][ERROR][RK0][main]: coll ps creation done
[HCTR][21:55:06.154][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:55:06.154][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][21:55:06.970][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][21:55:07.000][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][tid #139722911368960]: replica 6 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][tid #139723179804416]: replica 7 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][tid #139723037193984]: replica 1 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:55:07.000][ERROR][RK0][main]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][main]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][main]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][tid #139722911368960]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][tid #139723179804416]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][main]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][tid #139723037193984]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][main]: Calling build_v2
[HCTR][21:55:07.000][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][tid #139722911368960]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][tid #139723179804416]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][tid #139723037193984]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:55:07.000][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-11 21:55:07[2022-12-11 21:55:072022-12-11 21:55:072022-12-11 21:55:07.2022-12-11 21:55:072022-12-11 21:55:07.[2022-12-11 21:55:07..   700..   715.   700   716:    720   716: 2022-12-11 21:55:07   723: : E: : E.: EE EE    757E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136::136 :136136] 136136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] ] using concurrent impl MPSPhase] ] using concurrent impl MPSPhase:] using concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhaseusing concurrent impl MPSPhase
136using concurrent impl MPSPhase



] 
using concurrent impl MPSPhase
[2022-12-11 21:55:07.  5584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:55:07.  5623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196[] 2022-12-11 21:55:07assigning 8 to cpu.
  5631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 21:55:072022-12-11 21:55:07..  5679  5675: : E[E 2022-12-11 21:55:07 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  5699:196: 178] E[] assigning 8 to cpu 2022-12-11 21:55:07v100x8, slow pcie
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
:  5722212: ] [Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:55:07 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  5758[:: 2022-12-11 21:55:07178E.[] [   57692022-12-11 21:55:07v100x8, slow pcie2022-12-11 21:55:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .
.:E  5789  5791196[ [: : ] 2022-12-11 21:55:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:55:07EEassigning 8 to cpu.:.  
[  5815178  5822/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:55:07: ] : ::.Ev100x8, slow pcieE212213[  5858 
[ ] ] 2022-12-11 21:55:07: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 21:55:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421.E:2022-12-11 21:55:07:  5913

  5925 178.196: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] [  5946] EE:2022-12-11 21:55:07v100x8, slow pcie2022-12-11 21:55:07: assigning 8 to cpu  178.
.E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]   6019  6023 [::v100x8, slow pcie: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:55:07178212[
EE:.] ] 2022-12-11 21:55:07  [196  6111v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:55:07] : 

  6147::.assigning 8 to cpuE: 214213  6181[[
 E] ] : 2022-12-11 21:55:072022-12-11 21:55:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588remote time is 8.68421E..:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

 [  6256  6251196:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 21:55:07: ] : 212:2022-12-11 21:55:07.Eassigning 8 to cpuE] 196.  6330 
 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]   6353: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
assigning 8 to cpu: E::
[E 213[1962022-12-11 21:55:07 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 21:55:07] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:remote time is 8.68421.[assigning 8 to cpu  6475:212
  64832022-12-11 21:55:07
: 214] : .[E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E  65262022-12-11 21:55:07 cpu time is 97.0588
[ : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 21:55:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E  6564:.:2022-12-11 21:55:07 : 212  6595213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] ] :   6627: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421E: 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

 E] :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82142022-12-11 21:55:07:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:55:07
] .212:.cpu time is 97.0588  6769] [213  6774
: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:55:07] : E
.remote time is 8.68421E   6836
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 21:55:07E:2022-12-11 21:55:07214. 213.]   6910/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]   6925cpu time is 97.0588: :remote time is 8.68421: 
E213
E ]  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:55:07:
:.213214  7021[] ] : 2022-12-11 21:55:07remote time is 8.68421cpu time is 97.0588E.

   7056/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :[E2142022-12-11 21:55:07 ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588  7112:
: 214] Ecpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 21:56:24.312401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:56:24.352602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:56:24.352692: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:56:24.353686: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 21:56:24.423747: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 21:56:24.817316: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 21:56:24.817410: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 21:56:32.399387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-11 21:56:32.399479: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 21:56:34.109586: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 21:56:34.109702: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-11 21:56:34.113278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:56:34.113352: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-11 21:56:34.331860: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 21:56:34.360819: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 21:56:34.362316: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 21:56:34.383069: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 21:56:34.902118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 21:56:34.904391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 21:56:34.907408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 21:56:34.910335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 21:56:34.913263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 21:56:34.916195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 21:56:34.919095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 21:56:34.922017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 21:56:34.924934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 21:56:45.863690: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 21:56:45.871639: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 21:56:45.875511: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 21:56:45.921908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:56:45.922003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:56:45.922034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:56:45.922063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:56:45.922580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:56:45.922629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.923541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.924189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.937389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 21:56:45.937466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 21:56:45.937656[: 2022-12-11 21:56:45E. 937679/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1 solved:
202] 4 solved
[2022-12-11 21:56:45.937739: [E2022-12-11 21:56:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc937747:: 205E]  worker 0 thread 1 initing device 1/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 4 initing device 4
[[2022-12-11 21:56:452022-12-11 21:56:45..[9378749378742022-12-11 21:56:45: : .EE937927  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE:: 202202/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] :6 solved3 solved1815

] Building Coll Cache with ... num gpu device is 8[
[2022-12-11 21:56:452022-12-11 21:56:45..938040938044: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[2022-12-11 21:56:45::2022-12-11 21:56:45.205205.938070] ] [938066: worker 0 thread 6 initing device 6worker 0 thread 3 initing device 32022-12-11 21:56:45: E

.E 938109 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 21:56:45:E:.1980[ 202938174] 2022-12-11 21:56:45/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] : eager alloc mem 381.47 MB.:5 solvedE
938204: 202
 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [2 solved:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:56:45
1815:.] 1815[938289Building Coll Cache with ... num gpu device is 8] 2022-12-11 21:56:45: 
Building Coll Cache with ... num gpu device is 8.E
938325 : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE2022-12-11 21:56:45[: .2022-12-11 21:56:45205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc938376.] :: 938390worker 0 thread 5 initing device 5205E: 
]  Eworker 0 thread 2 initing device 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[[2022-12-11 21:56:452022-12-11 21:56:45..938595938601: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 21:56:452022-12-11 21:56:45..938670938670: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 21:56:45.938844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:56:45.938884: [E2022-12-11 21:56:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu938891:: 1980E]  eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:56:45.938949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.942729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.942949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.942996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.943064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.943121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.943179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.943697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:45.947509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:56:46.  1381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 21:56:46.  6785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 21:56:46.  6919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:56:46.  7794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46.  8794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.  9793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 11621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 12364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:56:46. 12409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 977.87 MB
[[[[[[[2022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:46...... 28570. 28569 28569 28569 28569 28570:  28569: : : : : E: EEEEE E     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::1980:19801980198019801980] 1980] ] ] ] ] eager alloc mem 1023.00 Bytes] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes
eager alloc mem 1023.00 Bytes





[2022-12-11 21:56:46. 35392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 21:56:46[.2022-12-11 21:56:46 35456.:  35476E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1023] 
eager release cuda mem 400000000
[[2022-12-11 21:56:462022-12-11 21:56:46.. 35539 35557: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[[2022-12-11 21:56:462022-12-11 21:56:46.. 35633 35653: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-11 21:56:46. 35721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 21:56:46. 35773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:56:46. 35803: E[ 2022-12-11 21:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 35801638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-11 21:56:462022-12-11 21:56:46.. 35882 35895: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-11 21:56:46. 35986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:56:46. 36529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 37172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 38321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 38973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 39504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 40080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 40657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:56:46. 41254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 41440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 41802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 41973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 42015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 42060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 42108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46. 42229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 42401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 42769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 42943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:56:46. 42967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 43023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 43063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46. 48421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 49047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:56:46. 49092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.56 MB
[2022-12-11 21:56:46. 49212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 49299[: 2022-12-11 21:56:46E.  49313/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 25.25 KB:
1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 49645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 49938: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:56:46:.638 49952] : eager release cuda mem 25855E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:56:46. 50010: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:56:46:.1980 50022] : eager alloc mem 979.58 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:56:461980.]  50064eager alloc mem 981.00 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:56:46. 50130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.17 MB
[2022-12-11 21:56:46. 50164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 50304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:56:46eager release cuda mem 25855.
 50313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:56:46. 50368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.18 MB
[2022-12-11 21:56:46. 50890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:56:46. 50936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 978.40 MB
[2022-12-11 21:56:46. 51044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:56:46. 51087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.09 MB
[[[[[[[[2022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:462022-12-11 21:56:46........350819350819350819350819350819350820350820350819: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 5 init p2p of link 6Device 1 init p2p of link 7Device 3 init p2p of link 2Device 4 init p2p of link 5Device 2 init p2p of link 1Device 0 init p2p of link 3Device 6 init p2p of link 0







[[2022-12-11 21:56:462022-12-11 21:56:46..[3514103514102022-12-11 21:56:46[: : [.[2022-12-11 21:56:46EE2022-12-11 21:56:463514242022-12-11 21:56:46.[  [.: .3514312022-12-11 21:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:56:46351429E351435: .::.:  : E35145819801980351466E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : ] ] :  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEeager alloc mem 611.00 KBeager alloc mem 611.00 KBE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 

 :] :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB1980] ::] 
] eager alloc mem 611.00 KB19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] ] 

eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 21:56:46[.2022-12-11 21:56:46352417.: 352423E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 21:56:46.[352469[2022-12-11 21:56:46[: 2022-12-11 21:56:46.[[2022-12-11 21:56:46E.3524742022-12-11 21:56:462022-12-11 21:56:46. 352479: ..352482/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E352491352494: :E : : E638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
638] ::638] eager release cuda mem 625663638638] eager release cuda mem 625663
] ] eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 21:56:46.365690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 21:56:46.[3658102022-12-11 21:56:46: .E365804 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1926
] Device 7 init p2p of link 1
[2022-12-11 21:56:46.365973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.366204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 21:56:46.366290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 21:56:46.366360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.366434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.366526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 21:56:46.366556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 21:56:46.366618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.366669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.366696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.366750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.366929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 21:56:46.367010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 21:56:46.367084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:56:462022-12-11 21:56:46..367164367167: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::[63819802022-12-11 21:56:46] ] .eager release cuda mem 625663eager alloc mem 611.00 KB367222

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.367458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.367488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.367883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.368032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 21:56:462022-12-11 21:56:46..379086379088: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 1 init p2p of link 3Device 7 init p2p of link 6

[2022-12-11 21:56:46.379232: [E2022-12-11 21:56:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu379240:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.379777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 21:56:46.379892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.380036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] [2022-12-11 21:56:46eager release cuda mem 6256632022-12-11 21:56:46.
.380046380058: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 7eager release cuda mem 625663

[2022-12-11 21:56:46.380170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 21:56:46.380224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.380293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.380332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 21:56:46.380445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.380567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[[2022-12-11 21:56:462022-12-11 21:56:46..380650380659: [: E2022-12-11 21:56:46E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu380684/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: :1926E638]  ] Device 0 init p2p of link 1/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663
:
1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.380852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.381014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.381090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.381215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.381548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.381632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.394677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 21:56:46.394796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.394826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 21:56:46.394939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.395196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 21:56:46.395314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.395574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.395730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.395993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-11 21:56:46Device 3 init p2p of link 1.
396017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 21:56:46.396108[: 2022-12-11 21:56:46E. 396117[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 2022-12-11 21:56:46:E.638 396135] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: eager release cuda mem 625663:E
1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.396460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 21:56:46.396572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.396818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 21:56:46.396935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:56:462022-12-11 21:56:46..396980396981: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 21:56:46.397132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 21:56:46.397251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:56:46.397353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.397710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.398033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:56:46.410589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.411176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.411256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.411569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.412218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.412322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.412494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.412798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1998179 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5990320 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 980.56 MB | 0.474428 secs 
[2022-12-11 21:56:46.412942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999095 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5989404 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 981.00 MB | 0.474558 secs 
[2022-12-11 21:56:46.413032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1996183 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5992316 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 979.58 MB | 0.474151 secs 
[2022-12-11 21:56:46.413057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:56:46.413168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999463 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5989036 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 981.18 MB | 0.475109 secs 
[2022-12-11 21:56:46.413587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999442 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5989057 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 981.17 MB | 0.47493 secs 
[2022-12-11 21:56:46.413693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999274 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5989225 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 981.09 MB | 0.474752 secs 
[2022-12-11 21:56:46.413786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1993758 / 100000000 nodes ( 1.99 %~2.00 %) | remote 5994741 / 100000000 nodes ( 5.99 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 978.40 MB | 0.475127 secs 
[2022-12-11 21:56:46.414301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1992671 / 100000000 nodes ( 1.99 %~2.00 %) | remote 5995828 / 100000000 nodes ( 6.00 %) | cpu 92011501 / 100000000 nodes ( 92.01 %) | 977.87 MB | 0.491684 secs 
[2022-12-11 21:56:46.416008: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 7.59 GB
[2022-12-11 21:56:47.767971: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 7.86 GB
[2022-12-11 21:56:47.768342: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 7.86 GB
[2022-12-11 21:56:47.769305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 7.86 GB
[2022-12-11 21:56:48.986716: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.12 GB
[2022-12-11 21:56:48.986849: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.12 GB
[2022-12-11 21:56:48.987204: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.12 GB
[2022-12-11 21:56:50.231451: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.33 GB
[2022-12-11 21:56:50.231581: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.33 GB
[2022-12-11 21:56:50.231947: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.33 GB
[2022-12-11 21:56:51.454886: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.55 GB
[2022-12-11 21:56:51.455615: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.55 GB
[2022-12-11 21:56:51.457162: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 8.55 GB
[2022-12-11 21:56:51.457662: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 8.55 GB
[2022-12-11 21:56:51.458201: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.55 GB
[2022-12-11 21:56:52.572915: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.75 GB
[2022-12-11 21:56:52.573806: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.75 GB
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][tid #139722911368960]: replica 6 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][tid #139723037193984]: replica 1 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][tid #139723179804416]: replica 7 calling init per replica done, doing barrier
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][tid #139723179804416]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][tid #139722911368960]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][tid #139723037193984]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:56:53.622][ERROR][RK0][main]: init per replica done
[HCTR][21:56:53.622][ERROR][RK0][main]: init per replica done
[HCTR][21:56:53.622][ERROR][RK0][main]: init per replica done
[HCTR][21:56:53.622][ERROR][RK0][main]: init per replica done
[HCTR][21:56:53.622][ERROR][RK0][tid #139723179804416]: init per replica done
[HCTR][21:56:53.622][ERROR][RK0][tid #139722911368960]: init per replica done
[HCTR][21:56:53.622][ERROR][RK0][tid #139723037193984]: init per replica done
[HCTR][21:56:53.625][ERROR][RK0][main]: init per replica done
[HCTR][21:56:53.628][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f159ff20000
[HCTR][21:56:53.628][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f15a0400000
[HCTR][21:56:53.628][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f15a0a40000
[HCTR][21:56:53.628][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f15a0d60000
[HCTR][21:56:53.628][ERROR][RK0][tid #139723246913280]: 4 allocated 3276800 at 0x7f159df20000
[HCTR][21:56:53.628][ERROR][RK0][tid #139723246913280]: 4 allocated 6553600 at 0x7f159e400000
[HCTR][21:56:53.628][ERROR][RK0][tid #139723246913280]: 4 allocated 3276800 at 0x7f159ea40000
[HCTR][21:56:53.628][ERROR][RK0][tid #139723246913280]: 4 allocated 6553600 at 0x7f159ed60000
[HCTR][21:56:53.628][ERROR][RK0][tid #139722911368960]: 5 allocated 3276800 at 0x7f159df20000
[HCTR][21:56:53.628][ERROR][RK0][tid #139722911368960]: 6 allocated 3276800 at 0x7f159df20000
[HCTR][21:56:53.628][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f159df20000
[HCTR][21:56:53.628][ERROR][RK0][tid #139722911368960]: 5 allocated 6553600 at 0x7f159e400000
[HCTR][21:56:53.628][ERROR][RK0][tid #139722911368960]: 6 allocated 6553600 at 0x7f159e400000
[HCTR][21:56:53.629][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f159e400000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 5 allocated 3276800 at 0x7f159ea40000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 6 allocated 3276800 at 0x7f159ea40000
[HCTR][21:56:53.629][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f159ea40000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 5 allocated 6553600 at 0x7f159ed60000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 6 allocated 6553600 at 0x7f159ed60000
[HCTR][21:56:53.629][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f159ed60000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 3 allocated 3276800 at 0x7f159df20000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 3 allocated 6553600 at 0x7f159e400000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 3 allocated 3276800 at 0x7f159ea40000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 3 allocated 6553600 at 0x7f159ed60000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 2 allocated 3276800 at 0x7f159ff20000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 2 allocated 6553600 at 0x7f15a0400000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 2 allocated 3276800 at 0x7f15a0a40000
[HCTR][21:56:53.629][ERROR][RK0][tid #139722911368960]: 2 allocated 6553600 at 0x7f15a0d60000
[HCTR][21:56:53.631][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f15a0520000
[HCTR][21:56:53.631][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f15a0a00000
[HCTR][21:56:53.631][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f15a170e800
[HCTR][21:56:53.631][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f15a1a2e800








