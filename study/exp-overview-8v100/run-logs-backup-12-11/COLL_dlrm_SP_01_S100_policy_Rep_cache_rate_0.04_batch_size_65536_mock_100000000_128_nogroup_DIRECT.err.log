2022-12-11 23:59:05.962494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:05.972545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:05.978374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:05.986337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:05.991097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.002340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.009605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.014101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.079980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.080981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.082082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.083154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.084495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.086308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.087139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.087539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.088971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.089214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.090568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.090694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.092214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.092269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.093848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.093909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.095491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.095587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.097248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.097615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.098689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.099438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.100407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.101569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.103372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.104524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.105438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.106443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.107489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.108544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.109501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.110407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.113881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.115011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.115739: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.115999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.116971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.117993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.119502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.120987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.121219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.122823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.123305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.124538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.125574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.125655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.126626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.127889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.128061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.128758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.130234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.130394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.132442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.132665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.135304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.135572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.137388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.137872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.138083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.140562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.140929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.143244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.144044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.145799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.146543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.147530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.148604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.149455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.150478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.151014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.151155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.152127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.153165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.153699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.153824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.154732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.155932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.161653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.161998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.163862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.164967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.165112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.165907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.168194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.168216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.168833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.169326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.184531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.190806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.204558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.205083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.205811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.206262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.206306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.206499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.208248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.209831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.210663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.210789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.210877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.210921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.211872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.213389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.216615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.216705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.216802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.216852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.217760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.218205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.220730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.220771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.220818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.221796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.222106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.224346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.224390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.224435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.225295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.225958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.227999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.228044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.228132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.229130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.229467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.231525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.231568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.231617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.232494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.232832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.234731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.234817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.234936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.235768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.236278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.238136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.238176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.238239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.239292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.240411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.241676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.241721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.241858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.242674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.243940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.245075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.245162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.245266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.246199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.247653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.248528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.248629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.248719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.249616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.250792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.252045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.252091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.252187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.252952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.254313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.255289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.255333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.255505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.256272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.257672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.258586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.258687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.258733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.259149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.260014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.261588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.263274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.263623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.263663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.263697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.264229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.265013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.265248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.266962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.268271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.268624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.268709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.269531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.270379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.272136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.272652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.272886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.273543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.274123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.274465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.275010: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.275609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.276163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.276435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.277119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.277694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.278640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.279427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.280332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.280420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.281166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.281933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.282784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.283449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.283609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.284464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.284629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.285425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.286171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.287554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.288374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.288477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.289397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.290318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.290386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.291317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.292139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.292922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.293164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.294062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.295051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.295343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.295980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.296613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.297659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.300231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.300810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.301595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.303064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.303165: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.303303: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.303303: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.303569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.304356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.306203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.307050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.308597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.309494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.310351: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.310744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.311672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.312021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.312057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.312134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.314141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.315692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.316025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.316160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.316231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.317797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.317931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.320544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.320638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.320712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.320874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.322740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.322886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.325662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.327513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.328281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.358568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.364302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.367565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.370789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.372998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.375637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.379148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.409685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.413221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.417017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.420081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.424273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.428416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.429758: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.435607: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:59:06.437868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.443421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.451406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.451761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.456770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:06.456884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.420349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.421176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.421718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.422647: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.422710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:59:07.440662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.441840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.442559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.443150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.443683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.444353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:59:07.488871: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.489074: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.531851: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:59:07.637881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.639121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.640415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.641206: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.641262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:59:07.657923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.658563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.659064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.659673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.660619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.661093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:59:07.669515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.670131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.671068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.671586: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.671650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:59:07.681911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.682710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.683248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.683723: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.683776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:59:07.689620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.690224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.691374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.691959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.692475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.692948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:59:07.696798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.696798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.697959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.697964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.698959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.698996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.699901: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.699939: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.699958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:59:07.699990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:59:07.701859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.702557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.703078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.703671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.704247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.705003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:59:07.717343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.717343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.718719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.718746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.719722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.719783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.720939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.721030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.721842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.721971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.722718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:59:07.722835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:59:07.724443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.725009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.725537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.725995: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.726043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:59:07.726504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.727068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.727627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.728096: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:59:07.728141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:59:07.744180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.744859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.745362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.745953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.746478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.746697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.747240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:59:07.747628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.748142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.748410: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.748566: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.748727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.749237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:59:07.749699: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:59:07.749710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:59:07.757901: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.758086: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.759999: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:59:07.767175: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.767351: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.767582: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.767747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.769108: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 23:59:07.769583: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:59:07.779527: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.779719: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.780662: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:59:07.789638: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.789806: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.791653: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:59:07.794362: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.794538: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:59:07.795510: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][23:59:09.022][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.032][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.032][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.044][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.047][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.047][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.088][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:59:09.088][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 97it [00:01, 81.77it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 98it [00:01, 83.11it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 194it [00:01, 177.39it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 100it [00:01, 85.76it/s]warmup run: 103it [00:01, 89.15it/s]warmup run: 198it [00:01, 182.45it/s]warmup run: 96it [00:01, 83.22it/s]warmup run: 72it [00:01, 61.91it/s]warmup run: 97it [00:01, 85.39it/s]warmup run: 292it [00:01, 284.03it/s]warmup run: 97it [00:01, 84.98it/s]warmup run: 201it [00:01, 186.71it/s]warmup run: 206it [00:01, 192.79it/s]warmup run: 299it [00:01, 293.29it/s]warmup run: 194it [00:01, 182.25it/s]warmup run: 162it [00:01, 153.82it/s]warmup run: 194it [00:01, 184.38it/s]warmup run: 389it [00:01, 393.59it/s]warmup run: 199it [00:01, 189.13it/s]warmup run: 301it [00:01, 296.53it/s]warmup run: 309it [00:01, 306.53it/s]warmup run: 399it [00:01, 406.83it/s]warmup run: 294it [00:01, 293.61it/s]warmup run: 254it [00:01, 257.63it/s]warmup run: 292it [00:01, 293.72it/s]warmup run: 485it [00:02, 498.43it/s]warmup run: 299it [00:01, 300.44it/s]warmup run: 401it [00:01, 409.75it/s]warmup run: 412it [00:01, 423.69it/s]warmup run: 500it [00:02, 519.19it/s]warmup run: 397it [00:01, 413.16it/s]warmup run: 345it [00:01, 362.96it/s]warmup run: 391it [00:01, 407.75it/s]warmup run: 583it [00:02, 598.79it/s]warmup run: 399it [00:01, 415.07it/s]warmup run: 501it [00:02, 520.37it/s]warmup run: 516it [00:02, 539.14it/s]warmup run: 603it [00:02, 625.64it/s]warmup run: 500it [00:02, 529.03it/s]warmup run: 435it [00:02, 463.17it/s]warmup run: 489it [00:01, 516.14it/s]warmup run: 680it [00:02, 683.32it/s]warmup run: 500it [00:01, 527.80it/s]warmup run: 603it [00:02, 625.84it/s]warmup run: 621it [00:02, 647.01it/s]warmup run: 707it [00:02, 719.98it/s]warmup run: 605it [00:02, 639.14it/s]warmup run: 525it [00:02, 554.36it/s]warmup run: 589it [00:02, 618.44it/s]warmup run: 777it [00:02, 753.07it/s]warmup run: 599it [00:02, 625.49it/s]warmup run: 705it [00:02, 716.73it/s]warmup run: 726it [00:02, 739.63it/s]warmup run: 810it [00:02, 795.55it/s]warmup run: 709it [00:02, 732.21it/s]warmup run: 622it [00:02, 649.23it/s]warmup run: 703it [00:02, 721.61it/s]warmup run: 684it [00:02, 666.61it/s]warmup run: 872it [00:02, 764.97it/s]warmup run: 808it [00:02, 792.79it/s]warmup run: 827it [00:02, 806.75it/s]warmup run: 912it [00:02, 853.81it/s]warmup run: 812it [00:02, 804.73it/s]warmup run: 718it [00:02, 725.83it/s]warmup run: 807it [00:02, 799.66it/s]warmup run: 781it [00:02, 739.20it/s]warmup run: 969it [00:02, 818.49it/s]warmup run: 908it [00:02, 841.56it/s]warmup run: 928it [00:02, 852.99it/s]warmup run: 1014it [00:02, 897.97it/s]warmup run: 915it [00:02, 862.49it/s]warmup run: 813it [00:02, 783.21it/s]warmup run: 910it [00:02, 859.75it/s]warmup run: 879it [00:02, 800.13it/s]warmup run: 1067it [00:02, 860.69it/s]warmup run: 1009it [00:02, 885.53it/s]warmup run: 1115it [00:02, 924.27it/s]warmup run: 1029it [00:02, 883.73it/s]warmup run: 1018it [00:02, 907.44it/s]warmup run: 905it [00:02, 820.21it/s]warmup run: 1013it [00:02, 904.47it/s]warmup run: 978it [00:02, 848.69it/s]warmup run: 1165it [00:02, 893.24it/s]warmup run: 1111it [00:02, 921.53it/s]warmup run: 1217it [00:02, 951.39it/s]warmup run: 1128it [00:02, 909.26it/s]warmup run: 1120it [00:02, 926.64it/s]warmup run: 997it [00:02, 842.95it/s]warmup run: 1117it [00:02, 941.54it/s]warmup run: 1075it [00:02, 881.32it/s]warmup run: 1263it [00:02, 915.99it/s]warmup run: 1212it [00:02, 945.59it/s]warmup run: 1320it [00:02, 971.68it/s]warmup run: 1227it [00:02, 931.81it/s]warmup run: 1221it [00:02, 940.54it/s]warmup run: 1089it [00:02, 862.93it/s]warmup run: 1220it [00:02, 966.52it/s]warmup run: 1173it [00:02, 907.85it/s]warmup run: 1362it [00:02, 935.72it/s]warmup run: 1313it [00:02, 963.48it/s]warmup run: 1422it [00:02, 982.52it/s]warmup run: 1329it [00:02, 955.30it/s]warmup run: 1321it [00:02, 949.84it/s]warmup run: 1182it [00:02, 880.41it/s]warmup run: 1324it [00:02, 985.52it/s]warmup run: 1272it [00:02, 929.90it/s]warmup run: 1460it [00:03, 946.24it/s]warmup run: 1414it [00:02, 969.65it/s]warmup run: 1431it [00:02, 972.53it/s]warmup run: 1524it [00:03, 974.34it/s]warmup run: 1421it [00:02, 963.53it/s]warmup run: 1277it [00:02, 898.74it/s]warmup run: 1429it [00:02, 1002.84it/s]warmup run: 1373it [00:02, 950.58it/s]warmup run: 1559it [00:03, 958.29it/s]warmup run: 1514it [00:03, 972.52it/s]warmup run: 1534it [00:03, 987.25it/s]warmup run: 1624it [00:03, 970.56it/s]warmup run: 1521it [00:03, 971.43it/s]warmup run: 1370it [00:03, 906.23it/s]warmup run: 1533it [00:02, 1013.56it/s]warmup run: 1476it [00:03, 973.00it/s]warmup run: 1657it [00:03, 964.11it/s]warmup run: 1614it [00:03, 977.16it/s]warmup run: 1636it [00:03, 996.86it/s]warmup run: 1723it [00:03, 962.92it/s]warmup run: 1466it [00:03, 921.61it/s]warmup run: 1621it [00:03, 965.08it/s]warmup run: 1638it [00:03, 1021.83it/s]warmup run: 1576it [00:03, 975.43it/s]warmup run: 1755it [00:03, 967.97it/s]warmup run: 1714it [00:03, 981.38it/s]warmup run: 1739it [00:03, 1005.40it/s]warmup run: 1821it [00:03, 960.39it/s]warmup run: 1562it [00:03, 932.48it/s]warmup run: 1719it [00:03, 955.26it/s]warmup run: 1742it [00:03, 1024.37it/s]warmup run: 1675it [00:03, 978.53it/s]warmup run: 1854it [00:03, 972.75it/s]warmup run: 1814it [00:03, 985.22it/s]warmup run: 1842it [00:03, 1011.49it/s]warmup run: 1918it [00:03, 956.12it/s]warmup run: 1658it [00:03, 939.67it/s]warmup run: 1819it [00:03, 966.90it/s]warmup run: 1846it [00:03, 1024.46it/s]warmup run: 1776it [00:03, 986.99it/s]warmup run: 1953it [00:03, 977.52it/s]warmup run: 1916it [00:03, 992.85it/s]warmup run: 1945it [00:03, 1016.88it/s]warmup run: 2017it [00:03, 965.49it/s]warmup run: 1757it [00:03, 953.41it/s]warmup run: 1917it [00:03, 969.53it/s]warmup run: 1950it [00:03, 1022.88it/s]warmup run: 1876it [00:03, 990.46it/s]warmup run: 2063it [00:03, 1012.98it/s]warmup run: 2020it [00:03, 1006.63it/s]warmup run: 2057it [00:03, 1045.29it/s]warmup run: 2137it [00:03, 1033.38it/s]warmup run: 1858it [00:03, 969.98it/s]warmup run: 2017it [00:03, 978.14it/s]warmup run: 2059it [00:03, 1041.49it/s]warmup run: 1976it [00:03, 991.96it/s]warmup run: 2185it [00:03, 1073.87it/s]warmup run: 2143it [00:03, 1072.95it/s]warmup run: 2179it [00:03, 1096.45it/s]warmup run: 2259it [00:03, 1086.53it/s]warmup run: 1959it [00:03, 980.03it/s]warmup run: 2137it [00:03, 1041.79it/s]warmup run: 2177it [00:03, 1080.54it/s]warmup run: 2093it [00:03, 1044.30it/s]warmup run: 2307it [00:03, 1117.14it/s]warmup run: 2267it [00:03, 1120.71it/s]warmup run: 2302it [00:03, 1135.84it/s]warmup run: 2381it [00:03, 1123.96it/s]warmup run: 2071it [00:03, 1019.30it/s]warmup run: 2257it [00:03, 1087.08it/s]warmup run: 2295it [00:03, 1108.70it/s]warmup run: 2216it [00:03, 1098.53it/s]warmup run: 2429it [00:03, 1147.00it/s]warmup run: 2391it [00:03, 1154.49it/s]warmup run: 2426it [00:03, 1165.99it/s]warmup run: 2502it [00:03, 1143.98it/s]warmup run: 2190it [00:03, 1068.63it/s]warmup run: 2377it [00:03, 1119.46it/s]warmup run: 2413it [00:03, 1128.37it/s]warmup run: 2339it [00:03, 1137.43it/s]warmup run: 2551it [00:04, 1167.35it/s]warmup run: 2515it [00:03, 1178.27it/s]warmup run: 2550it [00:03, 1186.51it/s]warmup run: 2622it [00:04, 1159.08it/s]warmup run: 2309it [00:03, 1102.62it/s]warmup run: 2497it [00:03, 1141.68it/s]warmup run: 2530it [00:03, 1140.15it/s]warmup run: 2462it [00:03, 1163.56it/s]warmup run: 2672it [00:04, 1177.68it/s]warmup run: 2637it [00:04, 1190.33it/s]warmup run: 2672it [00:04, 1188.68it/s]warmup run: 2741it [00:04, 1166.60it/s]warmup run: 2428it [00:04, 1127.77it/s]warmup run: 2617it [00:04, 1156.96it/s]warmup run: 2648it [00:03, 1149.81it/s]warmup run: 2586it [00:04, 1183.82it/s]warmup run: 2794it [00:04, 1189.69it/s]warmup run: 2761it [00:04, 1203.42it/s]warmup run: 2796it [00:04, 1203.02it/s]warmup run: 2861it [00:04, 1176.16it/s]warmup run: 2547it [00:04, 1145.83it/s]warmup run: 2735it [00:04, 1163.33it/s]warmup run: 2765it [00:04, 1153.50it/s]warmup run: 2709it [00:04, 1195.63it/s]warmup run: 2916it [00:04, 1198.53it/s]warmup run: 2885it [00:04, 1212.83it/s]warmup run: 2920it [00:04, 1213.54it/s]warmup run: 2981it [00:04, 1182.94it/s]warmup run: 2665it [00:04, 1154.78it/s]warmup run: 2855it [00:04, 1172.72it/s]warmup run: 3000it [00:04, 683.03it/s] warmup run: 3000it [00:04, 673.79it/s] warmup run: 2883it [00:04, 1158.67it/s]warmup run: 2832it [00:04, 1203.96it/s]warmup run: 3000it [00:04, 698.46it/s] warmup run: 3000it [00:04, 691.75it/s] warmup run: 2783it [00:04, 1160.46it/s]warmup run: 2973it [00:04, 1170.72it/s]warmup run: 3000it [00:04, 687.80it/s] warmup run: 2953it [00:04, 1200.37it/s]warmup run: 3000it [00:04, 697.05it/s] warmup run: 3000it [00:04, 688.48it/s] warmup run: 2901it [00:04, 1164.83it/s]warmup run: 3000it [00:04, 664.15it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.76it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.32it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1625.48it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.79it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.23it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.31it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.36it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.42it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1668.12it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.42it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1623.91it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1680.62it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1652.70it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1629.64it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1636.17it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1640.86it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1653.82it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1660.86it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1635.20it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1637.05it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1618.13it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1618.57it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1631.83it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1648.88it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1653.71it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1639.77it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1662.21it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1634.45it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1614.41it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1610.05it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1627.90it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1653.04it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1641.44it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1660.53it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1631.06it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1611.96it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1639.77it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1623.04it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1605.32it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1629.61it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1637.60it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1655.17it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1628.33it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1604.82it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1630.61it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1600.22it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1616.58it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1615.26it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1638.73it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1650.03it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1604.12it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1618.92it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1627.67it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1600.44it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1616.08it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1625.00it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1640.50it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1602.58it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1601.06it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1641.86it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1611.93it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1614.05it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1624.58it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1631.25it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1644.15it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1600.43it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1616.03it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1601.50it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1624.86it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1613.56it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1635.52it/s]warmup should be done:  50%|████▉     | 1497/3000 [00:00<00:00, 1635.39it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1646.52it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1622.65it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1625.59it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1598.24it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1601.60it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1612.69it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1631.34it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1640.06it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1647.83it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1598.31it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1632.00it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1624.84it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1600.85it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1612.92it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1627.56it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1641.32it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1647.90it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1637.39it/s]warmup should be done:  66%|██████▌   | 1972/3000 [00:01<00:00, 1625.17it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1612.94it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1591.55it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1600.02it/s]warmup should be done:  66%|██████▋   | 1990/3000 [00:01<00:00, 1624.57it/s]warmup should be done:  66%|██████▋   | 1993/3000 [00:01<00:00, 1641.80it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1648.88it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1641.98it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1625.23it/s]warmup should be done:  71%|███████   | 2119/3000 [00:01<00:00, 1612.21it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1594.97it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1600.75it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1622.77it/s]warmup should be done:  72%|███████▏  | 2159/3000 [00:01<00:00, 1646.96it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1645.53it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1643.28it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1623.37it/s]warmup should be done:  76%|███████▌  | 2265/3000 [00:01<00:00, 1609.42it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1609.62it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1595.69it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1619.27it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1643.92it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1643.74it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1641.29it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1623.20it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1621.79it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1611.72it/s]warmup should be done:  81%|████████  | 2423/3000 [00:01<00:00, 1596.50it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1618.33it/s]warmup should be done:  83%|████████▎ | 2489/3000 [00:01<00:00, 1643.62it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1644.58it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1644.70it/s]warmup should be done:  87%|████████▋ | 2624/3000 [00:01<00:00, 1623.16it/s]warmup should be done:  87%|████████▋ | 2597/3000 [00:01<00:00, 1632.38it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1612.80it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1597.13it/s]warmup should be done:  88%|████████▊ | 2640/3000 [00:01<00:00, 1618.56it/s]warmup should be done:  88%|████████▊ | 2654/3000 [00:01<00:00, 1643.78it/s]warmup should be done:  94%|█████████▎| 2807/3000 [00:01<00:00, 1645.51it/s]warmup should be done:  93%|█████████▎| 2797/3000 [00:01<00:00, 1646.38it/s]warmup should be done:  93%|█████████▎| 2787/3000 [00:01<00:00, 1622.34it/s]warmup should be done:  92%|█████████▏| 2763/3000 [00:01<00:00, 1640.54it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1613.52it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1598.18it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1617.49it/s]warmup should be done:  94%|█████████▍| 2821/3000 [00:01<00:00, 1651.05it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1650.84it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1652.53it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1630.93it/s]warmup should be done:  98%|█████████▊| 2931/3000 [00:01<00:00, 1649.81it/s]warmup should be done:  98%|█████████▊| 2931/3000 [00:01<00:00, 1618.90it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1602.98it/s]warmup should be done:  99%|█████████▉| 2967/3000 [00:01<00:00, 1625.50it/s]warmup should be done: 100%|█████████▉| 2989/3000 [00:01<00:00, 1658.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.51it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1636.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1617.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.98it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.57it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1705.32it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.93it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.06it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.61it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.62it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.01it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.11it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1644.42it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1713.21it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.62it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1683.12it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.42it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1701.66it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1690.49it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1670.38it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1646.57it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1712.19it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.00it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.48it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1678.66it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1699.26it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1650.49it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1664.42it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1645.46it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1713.45it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1701.31it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1694.41it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1673.75it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1653.69it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1658.75it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1667.38it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1641.64it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1710.02it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1703.70it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1695.05it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1669.38it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1672.14it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1675.64it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1639.93it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1643.24it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1695.49it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1703.32it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1677.30it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1674.60it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1677.46it/s]warmup should be done:  34%|███▍      | 1031/3000 [00:00<00:01, 1698.53it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1647.52it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1645.25it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1690.81it/s]warmup should be done:  40%|███▉      | 1197/3000 [00:00<00:01, 1699.14it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1674.49it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1671.15it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1676.11it/s]warmup should be done:  40%|████      | 1203/3000 [00:00<00:01, 1702.48it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1652.78it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1643.70it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1698.83it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1691.49it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1674.05it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1672.70it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1684.67it/s]warmup should be done:  46%|████▌     | 1376/3000 [00:00<00:00, 1708.77it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1655.07it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1646.02it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1693.53it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1674.40it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1671.90it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1691.78it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1655.92it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1673.29it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1687.28it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1646.24it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1669.37it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1668.66it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1684.31it/s]warmup should be done:  57%|█████▋    | 1707/3000 [00:01<00:00, 1685.52it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1674.84it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1649.03it/s]warmup should be done:  57%|█████▋    | 1716/3000 [00:01<00:00, 1668.96it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1664.38it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1667.18it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1682.87it/s]warmup should be done:  63%|██████▎   | 1877/3000 [00:01<00:00, 1687.73it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1677.06it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1674.37it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1641.36it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1662.38it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1674.29it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1692.59it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1663.20it/s]warmup should be done:  68%|██████▊   | 2047/3000 [00:01<00:00, 1689.88it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1679.21it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1666.78it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1635.28it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1654.07it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1682.38it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1697.01it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1690.29it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1659.52it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1683.34it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1659.79it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1632.34it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1648.60it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1686.02it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1694.32it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1659.99it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1688.72it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1676.50it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1655.49it/s]warmup should be done:  78%|███████▊  | 2326/3000 [00:01<00:00, 1639.09it/s]warmup should be done:  79%|███████▉  | 2381/3000 [00:01<00:00, 1645.59it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1690.25it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1689.96it/s]warmup should be done:  85%|████████▌ | 2556/3000 [00:01<00:00, 1688.67it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1659.51it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1680.56it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1654.71it/s]warmup should be done:  83%|████████▎ | 2492/3000 [00:01<00:00, 1644.02it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1644.24it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1691.37it/s]warmup should be done:  91%|█████████ | 2725/3000 [00:01<00:00, 1689.01it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1689.25it/s]warmup should be done:  89%|████████▉ | 2680/3000 [00:01<00:00, 1660.06it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1687.55it/s]warmup should be done:  89%|████████▊ | 2657/3000 [00:01<00:00, 1645.31it/s]warmup should be done:  90%|█████████ | 2703/3000 [00:01<00:00, 1654.75it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1642.52it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1691.16it/s]warmup should be done:  96%|█████████▋| 2894/3000 [00:01<00:00, 1687.55it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1686.64it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1657.58it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1689.48it/s]warmup should be done:  94%|█████████▍| 2822/3000 [00:01<00:00, 1644.65it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1651.04it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1639.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.20it/s]warmup should be done: 100%|█████████▉| 2987/3000 [00:01<00:00, 1645.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.11it/s]2022-12-12 00:00:44.153783: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fac1f8334e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:44.153846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.030615: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f90300299b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.030688: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.142294: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fac33831550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.142388: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.142718: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fac238315a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.142777: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.159239: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8fc802df50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.159294: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.164711: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f740305c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.164774: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.182905: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fac2ff929c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.182968: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:45.347955: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fac2b82c6c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 00:00:45.348026: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 00:00:46.411758: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.346200: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.410184: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.422984: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.443621: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.452292: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.544755: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:47.664740: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 00:00:49.328842: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.226815: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.259274: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.306034: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.348739: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.381986: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.438093: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 00:00:50.592895: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][00:01:26.940][ERROR][RK0][tid #140377969383168]: replica 6 reaches 1000, calling init pre replica
[HCTR][00:01:26.940][ERROR][RK0][tid #140377969383168]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:26.946][ERROR][RK0][tid #140377969383168]: coll ps creation done
[HCTR][00:01:26.946][ERROR][RK0][tid #140377969383168]: replica 6 waits for coll ps creation barrier
[HCTR][00:01:26.982][ERROR][RK0][tid #140378028099328]: replica 7 reaches 1000, calling init pre replica
[HCTR][00:01:26.982][ERROR][RK0][tid #140378028099328]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:26.990][ERROR][RK0][tid #140378028099328]: coll ps creation done
[HCTR][00:01:26.990][ERROR][RK0][tid #140378028099328]: replica 7 waits for coll ps creation barrier
[HCTR][00:01:27.111][ERROR][RK0][tid #140377835165440]: replica 0 reaches 1000, calling init pre replica
[HCTR][00:01:27.111][ERROR][RK0][tid #140377835165440]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:27.116][ERROR][RK0][tid #140377893881600]: replica 3 reaches 1000, calling init pre replica
[HCTR][00:01:27.116][ERROR][RK0][tid #140377893881600]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:27.120][ERROR][RK0][tid #140377835165440]: coll ps creation done
[HCTR][00:01:27.120][ERROR][RK0][tid #140377835165440]: replica 0 waits for coll ps creation barrier
[HCTR][00:01:27.124][ERROR][RK0][tid #140377893881600]: coll ps creation done
[HCTR][00:01:27.124][ERROR][RK0][tid #140377893881600]: replica 3 waits for coll ps creation barrier
[HCTR][00:01:27.133][ERROR][RK0][tid #140378397181696]: replica 1 reaches 1000, calling init pre replica
[HCTR][00:01:27.133][ERROR][RK0][tid #140378397181696]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:27.134][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][00:01:27.134][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:27.138][ERROR][RK0][tid #140378397181696]: coll ps creation done
[HCTR][00:01:27.138][ERROR][RK0][tid #140378397181696]: replica 1 waits for coll ps creation barrier
[HCTR][00:01:27.139][ERROR][RK0][main]: coll ps creation done
[HCTR][00:01:27.139][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][00:01:27.177][ERROR][RK0][tid #140378296534784]: replica 5 reaches 1000, calling init pre replica
[HCTR][00:01:27.177][ERROR][RK0][tid #140378296534784]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:27.181][ERROR][RK0][tid #140378296534784]: coll ps creation done
[HCTR][00:01:27.181][ERROR][RK0][tid #140378296534784]: replica 5 waits for coll ps creation barrier
[HCTR][00:01:27.204][ERROR][RK0][tid #140377969383168]: replica 4 reaches 1000, calling init pre replica
[HCTR][00:01:27.204][ERROR][RK0][tid #140377969383168]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][00:01:27.209][ERROR][RK0][tid #140377969383168]: coll ps creation done
[HCTR][00:01:27.209][ERROR][RK0][tid #140377969383168]: replica 4 waits for coll ps creation barrier
[HCTR][00:01:27.209][ERROR][RK0][tid #140377835165440]: replica 0 preparing frequency
[HCTR][00:01:28.144][ERROR][RK0][tid #140377835165440]: replica 0 preparing frequency done
[HCTR][00:01:28.185][ERROR][RK0][tid #140377835165440]: replica 0 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140378028099328]: replica 7 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140377893881600]: replica 3 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140378397181696]: replica 1 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140377969383168]: replica 6 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140378296534784]: replica 5 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140377969383168]: replica 4 calling init per replica
[HCTR][00:01:28.185][ERROR][RK0][tid #140377835165440]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][main]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][tid #140378028099328]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][tid #140377893881600]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][tid #140378397181696]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140377969383168]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][tid #140378296534784]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][tid #140377835165440]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140377969383168]: Calling build_v2
[HCTR][00:01:28.185][ERROR][RK0][tid #140378296534784]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140378028099328]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140377893881600]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140378397181696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140377969383168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][00:01:28.185][ERROR][RK0][tid #140377969383168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 00:01:28.189951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie
2022-12-12 00:01:28[.2022-12-12 00:01:28189994.: 190026E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:[] 196v100x8, slow pcie] 
assigning 0 to cpu2022-12-12 00:01:28
.[1900472022-12-12 00:01:28: [.E190094 2022-12-12 00:01:28: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:190096 [178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:01:28[] E:.v100x8, slow pcie 1961901512022-12-12 00:01:28
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : .:assigning 0 to cpu[E2022-12-12 00:01:28190147178
2022-12-12 00:01:28 .: ] [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc190198Ev100x8, slow pcie[190221:: 2022-12-12 00:01:28 
[2022-12-12 00:01:28: 212E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[E]  190247:1902842022-12-12 00:01:282022-12-12 00:01:28 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 178: ../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:E] E190331190291:178 v100x8, slow pcie : [: 196] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 00:01:28E] v100x8, slow pcie:: .[ assigning 0 to cpu
178212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1904302022-12-12 00:01:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] ] :[: .:v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 81962022-12-12 00:01:28E190477178

] . : ] [assigning 0 to cpu190523[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[v100x8, slow pcie2022-12-12 00:01:28
: 2022-12-12 00:01:28: 2022-12-12 00:01:28
.E.213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.190586 [190596] :[190605: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:01:28: remote time is 8.684211962022-12-12 00:01:28: E:.E
] .E 196190672 assigning 0 to cpu[190687 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 00:01:28: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:assigning 0 to cpuE:.E:212
 196190774 213] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:assigning 0 to cpu[E:remote time is 8.684212022-12-12 00:01:28
196
2022-12-12 00:01:28 212
.] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [190871assigning 0 to cpu[190887:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 00:01:28: 
2022-12-12 00:01:28: 214
[.E.E] 2022-12-12 00:01:28190939 190948[ cpu time is 97.0588[.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 00:01:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 00:01:28190974E:E.:.:  212 191004212191011E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :  :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
214
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] ] :[[:212remote time is 8.68421cpu time is 97.05882132022-12-12 00:01:282022-12-12 00:01:28212] 

] ..] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421[191162191168build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8

2022-12-12 00:01:28: : 
.E[E[191233 2022-12-12 00:01:28[ 2022-12-12 00:01:28: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-12 00:01:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:191280.:191283 213: 191298213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E: ] E:remote time is 8.68421 Eremote time is 8.68421 214
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:cpu time is 97.0588[214:2022-12-12 00:01:28213
2022-12-12 00:01:28] 213.] .cpu time is 97.0588] 191417remote time is 8.68421191425
remote time is 8.68421: 
: 
EE[ [ 2022-12-12 00:01:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 00:01:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:.:191508214191515214: ] : ] Ecpu time is 97.0588Ecpu time is 97.0588 
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 00:02:46.295441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 00:02:46.335344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 00:02:46.335403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 3999999
[2022-12-12 00:02:46.456832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 00:02:46.456921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 00:02:46.465353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 00:02:46.465387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 00:02:46.465826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.466802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.467649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.480566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 00:02:46.480631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 00:02:46.480653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 00:02:46.480728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 00:02:46.481037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.481148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.481539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [2 solved2022-12-12 00:02:46
.481565: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:481614202: ] E6 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 2 initing device 22022-12-12 00:02:46
.481675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 00:02:46.482051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.482099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.483104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 00:02:46.483171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 00:02:46.483556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 00:02:462022-12-12 00:02:46..[4836994837132022-12-12 00:02:46: : .EE483723  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:: 2021980/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] ] :7 solvedeager alloc mem 381.47 MB202

] 3 solved[
2022-12-12 00:02:46.483860: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:483875205: ] E[worker 0 thread 7 initing device 7 2022-12-12 00:02:46
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:483902205: ] Eworker 0 thread 3 initing device 3 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.484305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.484339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.484567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.486518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.488022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.488370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.488432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.488942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.489002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.489139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.490617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.492447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.492883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.493457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 00:02:46.545755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 00:02:46.546141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:02:46.551478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:02:46.551565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:02:46.551611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:02:46.552410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.552920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.553907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.553995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:02:46.554670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:02:46.554713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:02:46.571259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 00:02:46.571622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:02:46.576856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:02:46.576953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:02:46.576999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:02:46.577837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[[[2022-12-12 00:02:462022-12-12 00:02:462022-12-12 00:02:46...578064578064578064: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes


[2022-12-12 00:02:46.578309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 00:02:46[2022-12-12 00:02:46.2022-12-12 00:02:46.578466.578469: 578471: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980] 1980] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes

[2022-12-12 00:02:46.578995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[2022-12-12 00:02:462022-12-12 00:02:46..579070579070: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 00:02:46.579277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.579320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 00:02:46.579366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:02:46.[5794032022-12-12 00:02:46: .E579410 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 1024.00 Bytes1980
] eager alloc mem 1024.00 Bytes
[2022-12-12 00:02:46.580038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:02:46.580079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:02:46.586068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 00:02:46.586137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 00:02:46.586148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.] 586186eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 00:02:46[.2022-12-12 00:02:46586241.: 586231E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 1024
[2022-12-12 00:02:46.[586315[2022-12-12 00:02:46: 2022-12-12 00:02:46.E.586321 586310: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E:E [638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:02:46] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.eager release cuda mem 400000000:638586377
638] : ] [eager release cuda mem 2Eeager release cuda mem 10242022-12-12 00:02:46
 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc586466:: 638E]  [eager release cuda mem 1024/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 00:02:46
:2022-12-12 00:02:46.638.586523] 586531eager release cuda mem 1024: [: 
E2022-12-12 00:02:46E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc586569/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:: :2022-12-12 00:02:46638E638.]  ] 586613eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2: 
:
E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 2:
638] [eager release cuda mem 22022-12-12 00:02:46
.586705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.[] 5867222022-12-12 00:02:46eager release cuda mem 400000000: .
E586732 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 400000000
[2022-12-12 00:02:46.586996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.588407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.588914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.589425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.589928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.590432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 15.64 MB
[2022-12-12 00:02:46.591042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.591795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.591836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.591889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.591921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.591965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.592000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.592087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:02:46.592743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.] 592758eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:02:46.592792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.] 592810eager release cuda mem 625663: 
E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[:5928362022-12-12 00:02:461980: [.] E2022-12-12 00:02:46592844eager alloc mem 1.91 GB .[: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu5928662022-12-12 00:02:46[E:: .2022-12-12 00:02:46 1980E592896./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  : 592913:eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638
: E] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager release cuda mem 625663] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
eager release cuda mem 6256631980:
] 638eager alloc mem 25.25 KB] 
eager release cuda mem 625663
[2022-12-12 00:02:46.593121: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:5931331980: ] Eeager alloc mem 25.25 KB [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:02:46:.1980593161] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 00:02:46.593670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:02:46.593711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:02:46.593749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 00:02:46.593792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[2022-12-12 00:02:46.593816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 00:02:46] .eager release cuda mem 25855593835
: [E2022-12-12 00:02:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc593852:[: 6382022-12-12 00:02:46E] . eager release cuda mem 25855593868/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 25855:[
19802022-12-12 00:02:46] .eager alloc mem 1.91 GB593910
: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:5939301980: ] Eeager alloc mem 1.91 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.91 GB
[[[[[[2022-12-12 00:02:46[[2022-12-12 00:02:462022-12-12 00:02:462022-12-12 00:02:462022-12-12 00:02:462022-12-12 00:02:46.2022-12-12 00:02:462022-12-12 00:02:46.....979666..979671979692979671979671979672: 979686979686: : : : : E: : EEEEE EE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::1980::19801980198019801980] 19801980] ] ] ] ] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB






[2022-12-12 00:02:46.980794: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:980808638: [] [E[2022-12-12 00:02:46[[eager release cuda mem 6256632022-12-12 00:02:46 [2022-12-12 00:02:46.2022-12-12 00:02:462022-12-12 00:02:46
./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:02:46.980824..980824:.980827: 980828980829: [638980837: E: : E2022-12-12 00:02:46] : E EE .eager release cuda mem 625663E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc980931
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638::638E:638] 638638]  638[] eager release cuda mem 625663] ] eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-12 00:02:46eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663
:eager release cuda mem 625663.


1980
981079] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:02:46eager alloc mem 611.00 KB[.
2022-12-12 00:02:46981197.: 981203[E: 2022-12-12 00:02:46[[[ E.2022-12-12 00:02:462022-12-12 00:02:462022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 981219...:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 9812319812239812261980:E: : : ] 1980 EEEeager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   
eager alloc mem 611.00 KB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
1980:::] 198019801980eager alloc mem 611.00 KB] ] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 00:02:46.981919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.981945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.981988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.982015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.982078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.] 982091eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.982139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[[2022-12-12 00:02:46[6382022-12-12 00:02:462022-12-12 00:02:46.[2022-12-12 00:02:46] ..9821542022-12-12 00:02:46.eager release cuda mem 625663982158982158: .982164
: : E982177: EE : E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[::638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-12 00:02:46638638] :1980.] ] eager release cuda mem 6256631980] 982280eager release cuda mem 625663eager release cuda mem 625663
] eager alloc mem 611.00 KB: 

eager alloc mem 611.00 KB
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.982403: [E2022-12-12 00:02:46[ .2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu982414.:: 9824171980E: ]  Eeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 00:02:46.982735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.982762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.982803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.982829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 00:02:462022-12-12 00:02:46..983102983103: [: E2022-12-12 00:02:46E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc983117/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: :638E638]  ] eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
:
638[] 2022-12-12 00:02:46eager release cuda mem 625663.[
[9831832022-12-12 00:02:462022-12-12 00:02:46: ..E983202[[983207 : 2022-12-12 00:02:462022-12-12 00:02:46: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE..E2022-12-12 00:02:46: 983229983231 .638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc983264] :EE:: eager release cuda mem 625663638  638E
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  eager release cuda mem 625663::eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
19801980
:[] ] 19802022-12-12 00:02:46eager alloc mem 611.00 KBeager alloc mem 611.00 KB] .

eager alloc mem 611.00 KB983428[
[: 2022-12-12 00:02:462022-12-12 00:02:46E.. 983471983480/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : :EE1980  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB::
19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 00:02:46.983552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.983588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.983621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.983658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 00:02:462022-12-12 00:02:46..984198984199[: : 2022-12-12 00:02:46EE.  984213/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ::E638638 ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663eager release cuda mem 625663:

[6382022-12-12 00:02:46[] [.2022-12-12 00:02:46eager release cuda mem 6256632022-12-12 00:02:46984264.
.: 984279[[984280E: 2022-12-12 00:02:462022-12-12 00:02:46:  E..E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc [984312[984316 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:02:46: 2022-12-12 00:02:46: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:.E.[E:] 638984361 9843652022-12-12 00:02:46 638eager release cuda mem 625663] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 
eager release cuda mem 625663E:E984401:eager release cuda mem 625663
 1980 : 1980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE] :eager alloc mem 611.00 KB[: eager alloc mem 611.00 KB1980
2022-12-12 00:02:46638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
[] .] :[2022-12-12 00:02:46eager alloc mem 611.00 KB984515eager release cuda mem 6256636382022-12-12 00:02:46.
: 
] .984538Eeager release cuda mem 625663984554:  
: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:02:46:] :.1980eager alloc mem 611.00 KB1980[984655] 
] 2022-12-12 00:02:46: eager alloc mem 611.00 KBeager alloc mem 611.00 KB.E

984684 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.985267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.] 985283eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.985332: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 00:02:46:.638985346] : [eager release cuda mem 625663E2022-12-12 00:02:46
 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu985360:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.985416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 00:02:46eager alloc mem 611.00 KB.
985431: [E2022-12-12 00:02:46 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc9854602022-12-12 00:02:46:: .638E985468]  : eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE
:[[ 6382022-12-12 00:02:462022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ..:eager release cuda mem 625663985500985502638[
: : ] 2022-12-12 00:02:46EEeager release cuda mem 625663.  
985547/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :[:E6382022-12-12 00:02:46638 ] .[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 6256639856082022-12-12 00:02:46eager release cuda mem 625663:
: .
1980E985644]  : eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE
: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB[1980
2022-12-12 00:02:46[] .2022-12-12 00:02:46eager alloc mem 611.00 KB985733.
: 985744E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 00:02:46.986120: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:986135638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.986183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.986206: E[ 2022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9862181980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 00:02:46] .eager alloc mem 611.00 KB986248
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 00:02:46.986447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.986474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.[9865082022-12-12 00:02:46: .E986516 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE[: [[2022-12-12 00:02:46638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 00:02:462022-12-12 00:02:46.] :..986534eager release cuda mem 6256631980986540986541: 
] : : Eeager alloc mem 611.00 KBEE 
  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2022-12-12 00:02:46:638638.1980] ] 986665] eager release cuda mem 625663eager release cuda mem 625663: eager alloc mem 611.00 KB

E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[[19802022-12-12 00:02:462022-12-12 00:02:46] ..eager alloc mem 611.00 KB986748986751
: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 16399996eager release cuda mem 16399996

[2022-12-12 00:02:46.986967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.986990: E[[ 2022-12-12 00:02:462022-12-12 00:02:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc..:987000987003638: : ] EEeager release cuda mem 625663  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] [] eager release cuda mem 6256632022-12-12 00:02:46eager release cuda mem 16399996
.
987058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 00:02:46638.] 987084eager release cuda mem 16399996: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:02:46.987389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.987430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:02:46.987469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.987506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:02:46.987531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 00:02:46.987569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 16399996
[2022-12-12 00:02:46.987688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.503353 secs 
[2022-12-12 00:02:46.987887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.505843 secs 
[2022-12-12 00:02:46.988747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.522926 secs 
[2022-12-12 00:02:46.989451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.505901 secs 
[2022-12-12 00:02:46.989863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.508835 secs 
[2022-12-12 00:02:46.990287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.508194 secs 
[2022-12-12 00:02:46.990695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.509559 secs 
[2022-12-12 00:02:46.991110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 3999999 / 100000000 nodes ( 4.00 %~4.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 96000001 / 100000000 nodes ( 96.00 %) | 1.91 GB | 0.506813 secs 
[HCTR][00:02:46.991][ERROR][RK0][tid #140377893881600]: replica 3 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][tid #140377969383168]: replica 4 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][tid #140377969383168]: replica 6 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][tid #140377835165440]: replica 0 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][tid #140378397181696]: replica 1 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][tid #140378028099328]: replica 7 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][tid #140378296534784]: replica 5 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][00:02:46.991][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377835165440]: replica 0 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140378028099328]: replica 7 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140378296534784]: replica 5 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][main]: init per replica done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377969383168]: replica 4 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377893881600]: replica 3 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140378397181696]: replica 1 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140378296534784]: init per replica done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377969383168]: replica 6 calling init per replica done, doing barrier done
[HCTR][00:02:46.991][ERROR][RK0][tid #140378028099328]: init per replica done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377969383168]: init per replica done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377893881600]: init per replica done
[HCTR][00:02:46.991][ERROR][RK0][tid #140378397181696]: init per replica done
[HCTR][00:02:46.991][ERROR][RK0][tid #140377969383168]: init per replica done
[HCTR][00:02:46.993][ERROR][RK0][tid #140377835165440]: init per replica done








