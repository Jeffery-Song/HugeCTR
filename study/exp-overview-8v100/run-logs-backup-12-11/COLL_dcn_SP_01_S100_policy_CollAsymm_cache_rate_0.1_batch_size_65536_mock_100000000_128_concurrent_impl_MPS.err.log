2022-12-12 03:10:16.128201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.142851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.146335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.153131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.164089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.170357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.181890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.189341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.242988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.244127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.245134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.245276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.246747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.247064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.248513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.248624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.250109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.250160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.251708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.251762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.253508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.253613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.255228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.256258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.257184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.258098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.259113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.260172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.261229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.262275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.263313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.264339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.266211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.267347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.268414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.269451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.270473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.271522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.272520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.273792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.276059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.277357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.278846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.279292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.279837: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.280118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.280804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.281657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.282344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.283248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.283787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.284791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.285269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.286467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.286865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.288400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.289476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.289714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.291569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.293195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.293637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.295295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.297427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.299879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.299913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.300023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.302908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.303121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.303358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.303516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.305722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.306278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.306606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.306834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.307239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.308589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.309373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.309875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.310108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.310413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.311449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.312764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.313265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.313485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.313838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.314596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.317153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.317289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.317556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.317952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.319592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.319722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.320041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.320196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.323346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.323484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.323705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.323921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.325890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.326048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.326158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.328268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.332026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.334663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.360937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.362999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.365305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.365338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.365400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.365469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.368454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.368499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.368590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.368721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.369779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.371512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.372007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.372054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.372227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.372324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.374822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.375889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.376173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.376309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.376445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.376597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.378486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.379954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.380378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.380506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.380593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.380679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.382331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.383839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.384599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.384686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.384867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.385408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.386350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.387380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.387801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.388208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.388427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.389181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.389954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.391174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.391579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.391816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.392029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.392830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.393450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.394709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.395164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.395398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.395619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.396367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.397174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.398313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.399068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.399360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.399990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.401039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.402153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.403179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.403690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.403905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.404287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.404990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.405628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.406968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.407963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.408039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.408273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.409163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.409167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.409615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.410952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.411593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.411841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.412199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.413408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.413433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.413710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.415018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.415843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.415970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.416243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.417542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.417798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.419034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.419508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.419731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.419986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.420956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.421415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.421894: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.422657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.423883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.424727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.425033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.425492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.425813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.427490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.428485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.429350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.429706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.429727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.430183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.431464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.431857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.432556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.433461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.433780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.433941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.435353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.436199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.436830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.437000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.437981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.438186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.438400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.439763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.440405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.441374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.441530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.442335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.442482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.442838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.445059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.447231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.447272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.448126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.448232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.448672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.450227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.451604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.451973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.452743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.452746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.453418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.454797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.456191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.456472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.457287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.457290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.458225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.459660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.461018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.461654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.463872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.464799: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.464891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.465035: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.465581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.466393: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.467468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.468390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.468691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.471552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.474017: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.474361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.474658: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.474762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.475049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.476527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.478624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.479229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.479814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.481271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.483487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.483692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.483804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.484240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.484777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.486584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.488733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.488983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.489055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.522733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.522990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.522990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.555728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.562462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.569120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.578611: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:10:16.588359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.602323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:16.607113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.639555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.640160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.640820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.641524: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.641580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:10:17.659758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.660609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.661319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.661915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.662445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.662921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:10:17.708823: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:17.709032: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:17.735747: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:10:17.843115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.843750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.844274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.844754: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.844806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:10:17.862609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.863247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.863765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.864347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.864873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.865767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:10:17.938808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.939440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.939962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.940443: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.940498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:10:17.946044: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:17.946293: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:17.947444: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:10:17.951375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.952158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.952689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.953158: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.953206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:10:17.958275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.959425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.959953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.960547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.961287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.961762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:10:17.966049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.966657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.967187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.967558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.968167: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.968222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:10:17.969015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.969575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.970049: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.970090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:10:17.971548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.972146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.972665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.973124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.973244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.973324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.974488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.974780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.974872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.975821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.976200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.976276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:10:17.976912: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.976957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:10:17.977065: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:10:17.977108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:10:17.985699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.986355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.986861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.987308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.987551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.988298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.988539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.989282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.989623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:10:17.990077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.990609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.991078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:10:17.994079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.994509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.994802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.995588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.995866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.996426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.996927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.997283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.997835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.998207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:10:17.998711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:10:17.998937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:10:18.021524: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.021745: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.022797: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 03:10:18.035094: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.035321: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.036205: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.036283: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 03:10:18.036408: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.037404: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 03:10:18.039692: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.039859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.040723: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:10:18.044002: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.044152: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.044335: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.044475: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:10:18.045155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 03:10:18.046231: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][03:10:19.273][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.273][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.301][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.301][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.304][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.304][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:10:19.307][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 96it [00:01, 80.65it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 192it [00:01, 175.10it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 100it [00:01, 86.54it/s]warmup run: 99it [00:01, 85.14it/s]warmup run: 98it [00:01, 85.19it/s]warmup run: 95it [00:01, 81.94it/s]warmup run: 95it [00:01, 81.86it/s]warmup run: 288it [00:01, 279.43it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 98it [00:01, 85.23it/s]warmup run: 198it [00:01, 185.00it/s]warmup run: 199it [00:01, 185.45it/s]warmup run: 197it [00:01, 185.35it/s]warmup run: 191it [00:01, 178.51it/s]warmup run: 193it [00:01, 180.43it/s]warmup run: 384it [00:01, 387.83it/s]warmup run: 99it [00:01, 87.49it/s]warmup run: 197it [00:01, 185.41it/s]warmup run: 298it [00:01, 294.40it/s]warmup run: 300it [00:01, 298.34it/s]warmup run: 293it [00:01, 291.11it/s]warmup run: 289it [00:01, 287.07it/s]warmup run: 292it [00:01, 289.96it/s]warmup run: 483it [00:02, 498.57it/s]warmup run: 199it [00:01, 189.84it/s]warmup run: 296it [00:01, 294.93it/s]warmup run: 397it [00:01, 407.11it/s]warmup run: 401it [00:01, 413.96it/s]warmup run: 390it [00:01, 401.17it/s]warmup run: 388it [00:01, 401.13it/s]warmup run: 390it [00:01, 401.38it/s]warmup run: 582it [00:02, 600.96it/s]warmup run: 299it [00:01, 301.75it/s]warmup run: 388it [00:01, 396.42it/s]warmup run: 497it [00:02, 517.80it/s]warmup run: 499it [00:02, 520.33it/s]warmup run: 486it [00:01, 505.87it/s]warmup run: 486it [00:02, 509.79it/s]warmup run: 490it [00:02, 513.45it/s]warmup run: 682it [00:02, 691.86it/s]warmup run: 399it [00:01, 416.29it/s]warmup run: 597it [00:02, 619.25it/s]warmup run: 594it [00:02, 610.86it/s]warmup run: 476it [00:02, 479.16it/s]warmup run: 583it [00:02, 603.67it/s]warmup run: 587it [00:02, 614.52it/s]warmup run: 591it [00:02, 617.37it/s]warmup run: 781it [00:02, 765.07it/s]warmup run: 499it [00:01, 526.61it/s]warmup run: 696it [00:02, 703.68it/s]warmup run: 690it [00:02, 690.73it/s]warmup run: 577it [00:02, 589.45it/s]warmup run: 681it [00:02, 690.13it/s]warmup run: 687it [00:02, 703.55it/s]warmup run: 692it [00:02, 708.21it/s]warmup run: 880it [00:02, 822.31it/s]warmup run: 601it [00:02, 631.95it/s]warmup run: 785it [00:02, 754.53it/s]warmup run: 794it [00:02, 771.32it/s]warmup run: 679it [00:02, 687.09it/s]warmup run: 777it [00:02, 756.84it/s]warmup run: 787it [00:02, 775.89it/s]warmup run: 792it [00:02, 779.94it/s]warmup run: 979it [00:02, 865.99it/s]warmup run: 703it [00:02, 721.38it/s]warmup run: 882it [00:02, 809.76it/s]warmup run: 777it [00:02, 757.72it/s]warmup run: 891it [00:02, 818.52it/s]warmup run: 877it [00:02, 820.34it/s]warmup run: 887it [00:02, 834.52it/s]warmup run: 892it [00:02, 836.95it/s]warmup run: 1077it [00:02, 891.78it/s]warmup run: 806it [00:02, 797.17it/s]warmup run: 984it [00:02, 866.95it/s]warmup run: 879it [00:02, 824.28it/s]warmup run: 990it [00:02, 863.31it/s]warmup run: 979it [00:02, 874.02it/s]warmup run: 989it [00:02, 882.84it/s]warmup run: 992it [00:02, 880.16it/s]warmup run: 1177it [00:02, 919.84it/s]warmup run: 908it [00:02, 853.54it/s]warmup run: 1086it [00:02, 907.45it/s]warmup run: 980it [00:02, 874.42it/s]warmup run: 1090it [00:02, 900.86it/s]warmup run: 1082it [00:02, 915.85it/s]warmup run: 1091it [00:02, 919.62it/s]warmup run: 1092it [00:02, 913.54it/s]warmup run: 1276it [00:02, 939.57it/s]warmup run: 1009it [00:02, 895.56it/s]warmup run: 1188it [00:02, 937.32it/s]warmup run: 1081it [00:02, 910.19it/s]warmup run: 1189it [00:02, 924.88it/s]warmup run: 1185it [00:02, 946.71it/s]warmup run: 1194it [00:02, 948.43it/s]warmup run: 1193it [00:02, 939.18it/s]warmup run: 1376it [00:02, 956.96it/s]warmup run: 1112it [00:02, 931.51it/s]warmup run: 1290it [00:02, 959.04it/s]warmup run: 1180it [00:02, 932.15it/s]warmup run: 1287it [00:02, 940.77it/s]warmup run: 1287it [00:02, 966.48it/s]warmup run: 1295it [00:02, 964.93it/s]warmup run: 1293it [00:02, 949.76it/s]warmup run: 1477it [00:03, 971.84it/s]warmup run: 1215it [00:02, 957.50it/s]warmup run: 1392it [00:02, 976.49it/s]warmup run: 1279it [00:02, 948.66it/s]warmup run: 1388it [00:02, 958.52it/s]warmup run: 1391it [00:02, 985.30it/s]warmup run: 1397it [00:02, 979.11it/s]warmup run: 1396it [00:02, 971.53it/s]warmup run: 1577it [00:03, 975.98it/s]warmup run: 1317it [00:02, 974.48it/s]warmup run: 1494it [00:03, 988.79it/s]warmup run: 1381it [00:02, 969.01it/s]warmup run: 1488it [00:03, 970.50it/s]warmup run: 1493it [00:03, 985.21it/s]warmup run: 1498it [00:03, 976.73it/s]warmup run: 1499it [00:03, 986.01it/s]warmup run: 1677it [00:03, 979.40it/s]warmup run: 1421it [00:02, 993.53it/s]warmup run: 1484it [00:03, 986.09it/s]warmup run: 1597it [00:03, 998.76it/s]warmup run: 1588it [00:03, 967.83it/s]warmup run: 1594it [00:03, 983.16it/s]warmup run: 1601it [00:03, 991.88it/s]warmup run: 1602it [00:03, 997.67it/s]warmup run: 1776it [00:03, 981.04it/s]warmup run: 1526it [00:02, 1008.93it/s]warmup run: 1586it [00:03, 995.28it/s]warmup run: 1699it [00:03, 995.98it/s]warmup run: 1688it [00:03, 977.03it/s]warmup run: 1702it [00:03, 996.91it/s]warmup run: 1694it [00:03, 962.54it/s]warmup run: 1704it [00:03, 1001.26it/s]warmup run: 1875it [00:03, 981.84it/s]warmup run: 1630it [00:03, 1016.99it/s]warmup run: 1688it [00:03, 1001.19it/s]warmup run: 1790it [00:03, 986.92it/s]warmup run: 1800it [00:03, 991.13it/s]warmup run: 1803it [00:03, 999.62it/s]warmup run: 1797it [00:03, 980.57it/s]warmup run: 1974it [00:03, 982.83it/s]warmup run: 1806it [00:03, 999.34it/s] warmup run: 1734it [00:03, 1019.60it/s]warmup run: 1791it [00:03, 1007.72it/s]warmup run: 1900it [00:03, 984.53it/s]warmup run: 1890it [00:03, 973.00it/s]warmup run: 1905it [00:03, 1003.48it/s]warmup run: 1899it [00:03, 990.42it/s]warmup run: 2088it [00:03, 1028.41it/s]warmup run: 1907it [00:03, 1001.64it/s]warmup run: 1838it [00:03, 1023.98it/s]warmup run: 1893it [00:03, 1008.85it/s]warmup run: 1999it [00:03, 985.30it/s]warmup run: 1988it [00:03, 968.25it/s]warmup run: 2011it [00:03, 1017.90it/s]warmup run: 2002it [00:03, 1001.89it/s]warmup run: 2009it [00:03, 1006.96it/s]warmup run: 2209it [00:03, 1080.00it/s]warmup run: 1943it [00:03, 1029.93it/s]warmup run: 1995it [00:03, 1007.31it/s]warmup run: 2118it [00:03, 1044.99it/s]warmup run: 2104it [00:03, 1024.51it/s]warmup run: 2135it [00:03, 1082.03it/s]warmup run: 2126it [00:03, 1071.51it/s]warmup run: 2117it [00:03, 1026.77it/s]warmup run: 2329it [00:03, 1113.87it/s]warmup run: 2056it [00:03, 1058.12it/s]warmup run: 2114it [00:03, 1060.56it/s]warmup run: 2238it [00:03, 1090.28it/s]warmup run: 2224it [00:03, 1076.36it/s]warmup run: 2259it [00:03, 1127.07it/s]warmup run: 2250it [00:03, 1121.10it/s]warmup run: 2449it [00:03, 1139.36it/s]warmup run: 2236it [00:03, 1073.12it/s]warmup run: 2179it [00:03, 1108.77it/s]warmup run: 2236it [00:03, 1107.13it/s]warmup run: 2358it [00:03, 1122.68it/s]warmup run: 2345it [00:03, 1114.00it/s]warmup run: 2382it [00:03, 1157.71it/s]warmup run: 2373it [00:03, 1152.65it/s]warmup run: 2355it [00:03, 1107.34it/s]warmup run: 2570it [00:04, 1157.93it/s]warmup run: 2302it [00:03, 1144.61it/s]warmup run: 2359it [00:03, 1141.39it/s]warmup run: 2478it [00:03, 1145.06it/s]warmup run: 2465it [00:03, 1138.30it/s]warmup run: 2497it [00:03, 1177.75it/s]warmup run: 2506it [00:03, 1179.49it/s]warmup run: 2472it [00:03, 1123.42it/s]warmup run: 2691it [00:04, 1170.57it/s]warmup run: 2425it [00:03, 1169.33it/s]warmup run: 2482it [00:03, 1166.14it/s]warmup run: 2598it [00:04, 1160.29it/s]warmup run: 2585it [00:04, 1155.28it/s]warmup run: 2621it [00:04, 1195.15it/s]warmup run: 2630it [00:04, 1195.36it/s]warmup run: 2591it [00:04, 1142.26it/s]warmup run: 2810it [00:04, 1175.07it/s]warmup run: 2548it [00:03, 1185.64it/s]warmup run: 2605it [00:04, 1183.55it/s]warmup run: 2718it [00:04, 1171.05it/s]warmup run: 2706it [00:04, 1169.22it/s]warmup run: 2743it [00:04, 1200.83it/s]warmup run: 2753it [00:04, 1204.33it/s]warmup run: 2930it [00:04, 1182.08it/s]warmup run: 2712it [00:04, 1159.86it/s]warmup run: 2672it [00:03, 1199.28it/s]warmup run: 2728it [00:04, 1195.91it/s]warmup run: 2837it [00:04, 1176.52it/s]warmup run: 2826it [00:04, 1177.26it/s]warmup run: 3000it [00:04, 676.59it/s] warmup run: 2877it [00:04, 1212.16it/s]warmup run: 2864it [00:04, 1197.19it/s]warmup run: 2830it [00:04, 1165.08it/s]warmup run: 2794it [00:04, 1203.54it/s]warmup run: 2849it [00:04, 1199.84it/s]warmup run: 2958it [00:04, 1183.80it/s]warmup run: 2948it [00:04, 1189.84it/s]warmup run: 3000it [00:04, 687.44it/s] warmup run: 3000it [00:04, 1217.30it/s]warmup run: 2984it [00:04, 1194.92it/s]warmup run: 3000it [00:04, 692.51it/s] warmup run: 2949it [00:04, 1170.05it/s]warmup run: 3000it [00:04, 683.85it/s] warmup run: 3000it [00:04, 689.65it/s] warmup run: 2916it [00:04, 1207.36it/s]warmup run: 2971it [00:04, 1204.57it/s]warmup run: 3000it [00:04, 685.14it/s] warmup run: 3000it [00:04, 689.80it/s] warmup run: 3000it [00:04, 704.49it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.55it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.56it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.18it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.66it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.48it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1595.21it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1612.87it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.62it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.23it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1677.21it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1621.98it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.99it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.17it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1680.00it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.25it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1604.69it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1615.91it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1653.31it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1632.74it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1680.43it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1672.05it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1637.79it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1617.85it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1646.67it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1633.39it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.91it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1680.62it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1635.16it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1669.98it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1614.77it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1644.09it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1586.82it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1631.82it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1666.20it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1648.17it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1611.35it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1630.81it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1640.32it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1590.89it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1630.91it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1631.04it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1665.18it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1645.32it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1609.68it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1627.58it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1630.93it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1599.44it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1644.49it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1661.87it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1640.77it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1602.66it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1621.69it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1615.70it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1594.83it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1623.67it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1649.91it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1663.07it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1639.30it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1622.52it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1620.92it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1598.06it/s]warmup should be done:  43%|████▎     | 1289/3000 [00:00<00:01, 1593.55it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1621.63it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1656.32it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1663.64it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1638.75it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1623.38it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1624.57it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1595.55it/s]warmup should be done:  48%|████▊     | 1451/3000 [00:00<00:00, 1600.74it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1619.40it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1659.72it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1664.49it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1638.59it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1623.60it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1626.74it/s]warmup should be done:  54%|█████▍    | 1616/3000 [00:01<00:00, 1593.81it/s]warmup should be done:  54%|█████▎    | 1612/3000 [00:01<00:00, 1598.64it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1619.13it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1662.92it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1663.85it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1638.94it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1622.74it/s]warmup should be done:  60%|██████    | 1803/3000 [00:01<00:00, 1628.36it/s]warmup should be done:  59%|█████▉    | 1776/3000 [00:01<00:00, 1594.17it/s]warmup should be done:  59%|█████▉    | 1772/3000 [00:01<00:00, 1597.39it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1623.97it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1662.84it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1663.81it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1638.45it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1621.71it/s]warmup should be done:  66%|██████▌   | 1966/3000 [00:01<00:00, 1626.86it/s]warmup should be done:  65%|██████▍   | 1936/3000 [00:01<00:00, 1594.09it/s]warmup should be done:  64%|██████▍   | 1932/3000 [00:01<00:00, 1595.45it/s]warmup should be done:  66%|██████▌   | 1972/3000 [00:01<00:00, 1626.08it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1660.64it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1638.05it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1662.92it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1622.92it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1627.72it/s]warmup should be done:  70%|██████▉   | 2097/3000 [00:01<00:00, 1596.98it/s]warmup should be done:  70%|██████▉   | 2092/3000 [00:01<00:00, 1594.58it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1627.28it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1662.41it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1660.34it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1632.90it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1628.98it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1626.64it/s]warmup should be done:  75%|███████▌  | 2258/3000 [00:01<00:00, 1599.51it/s]warmup should be done:  75%|███████▌  | 2252/3000 [00:01<00:00, 1593.94it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1629.32it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1661.55it/s]warmup should be done:  84%|████████▎ | 2509/3000 [00:01<00:00, 1660.52it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1625.56it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1624.79it/s]warmup should be done:  81%|████████  | 2418/3000 [00:01<00:00, 1596.89it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1621.90it/s]warmup should be done:  80%|████████  | 2414/3000 [00:01<00:00, 1599.64it/s]warmup should be done:  82%|████████▏ | 2463/3000 [00:01<00:00, 1629.25it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1665.12it/s]warmup should be done:  89%|████████▉ | 2676/3000 [00:01<00:00, 1661.18it/s]warmup should be done:  87%|████████▋ | 2620/3000 [00:01<00:00, 1631.70it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1624.15it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1598.42it/s]warmup should be done:  88%|████████▊ | 2627/3000 [00:01<00:00, 1630.70it/s]warmup should be done:  86%|████████▌ | 2580/3000 [00:01<00:00, 1615.49it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1614.87it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1666.68it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1666.30it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1637.13it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1626.49it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1600.57it/s]warmup should be done:  91%|█████████▏| 2743/3000 [00:01<00:00, 1617.76it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1625.55it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1598.76it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1669.44it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.27it/s]warmup should be done:  98%|█████████▊| 2952/3000 [00:01<00:00, 1646.44it/s]warmup should be done:  98%|█████████▊| 2947/3000 [00:01<00:00, 1633.33it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1607.36it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1623.15it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1630.45it/s]warmup should be done:  99%|█████████▊| 2960/3000 [00:01<00:00, 1604.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1603.01it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.38it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.13it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1703.70it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.78it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1722.98it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.63it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.30it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.80it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.72it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1682.25it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.51it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1702.29it/s]warmup should be done:  12%|█▏        | 346/3000 [00:00<00:01, 1720.46it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1660.39it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1688.13it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1647.42it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1693.91it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1676.97it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1661.02it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1690.45it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1719.49it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1648.86it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1692.42it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1638.95it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1695.06it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1663.67it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1675.70it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1692.72it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1652.75it/s]warmup should be done:  23%|██▎       | 692/3000 [00:00<00:01, 1719.88it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1698.92it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1649.26it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1672.92it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1691.35it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1654.91it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1696.25it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1673.14it/s]warmup should be done:  29%|██▉       | 865/3000 [00:00<00:01, 1722.13it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1706.38it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1653.15it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1682.55it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1675.20it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1653.25it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1688.77it/s]warmup should be done:  35%|███▍      | 1038/3000 [00:00<00:01, 1722.96it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1708.80it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1693.60it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1655.27it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1684.85it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1676.03it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1687.84it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1652.55it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1706.73it/s]warmup should be done:  40%|████      | 1211/3000 [00:00<00:01, 1719.44it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1690.63it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1653.63it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1692.36it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1677.55it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1688.02it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1710.64it/s]warmup should be done:  46%|████▌     | 1384/3000 [00:00<00:00, 1722.48it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1693.26it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1650.63it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1637.56it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1694.23it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1687.01it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1676.06it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1712.25it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1693.88it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1654.93it/s]warmup should be done:  52%|█████▏    | 1557/3000 [00:00<00:00, 1721.09it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1644.14it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1696.23it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1687.66it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1676.26it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1662.62it/s]warmup should be done:  57%|█████▋    | 1715/3000 [00:01<00:00, 1710.49it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1696.33it/s]warmup should be done:  58%|█████▊    | 1730/3000 [00:01<00:00, 1718.64it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1650.08it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1697.16it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1677.10it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1688.97it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1667.99it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1700.58it/s]warmup should be done:  63%|██████▎   | 1903/3000 [00:01<00:00, 1719.47it/s]warmup should be done:  63%|██████▎   | 1887/3000 [00:01<00:00, 1703.08it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1654.72it/s]warmup should be done:  67%|██████▋   | 2018/3000 [00:01<00:00, 1676.38it/s]warmup should be done:  68%|██████▊   | 2033/3000 [00:01<00:00, 1687.56it/s]warmup should be done:  68%|██████▊   | 2029/3000 [00:01<00:00, 1691.30it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1668.66it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1699.96it/s]warmup should be done:  69%|██████▉   | 2076/3000 [00:01<00:00, 1721.29it/s]warmup should be done:  69%|██████▊   | 2058/3000 [00:01<00:00, 1695.33it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1654.85it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1675.38it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1683.53it/s]warmup should be done:  73%|███████▎  | 2199/3000 [00:01<00:00, 1687.60it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1668.70it/s]warmup should be done:  75%|███████▍  | 2249/3000 [00:01<00:00, 1721.53it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1692.41it/s]warmup should be done:  74%|███████▍  | 2228/3000 [00:01<00:00, 1693.49it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1643.08it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1676.67it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1682.93it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1670.36it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1686.09it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1720.04it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1685.90it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1690.77it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1663.70it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1677.89it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1685.69it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1686.28it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1673.74it/s]warmup should be done:  86%|████████▋ | 2595/3000 [00:01<00:00, 1717.00it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1682.14it/s]warmup should be done:  86%|████████▌ | 2568/3000 [00:01<00:00, 1683.08it/s]warmup should be done:  84%|████████▎ | 2507/3000 [00:01<00:00, 1681.30it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1676.69it/s]warmup should be done:  90%|█████████ | 2712/3000 [00:01<00:00, 1692.51it/s]warmup should be done:  89%|████████▉ | 2672/3000 [00:01<00:00, 1674.79it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1684.60it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1716.29it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1679.31it/s]warmup should be done:  91%|█████████▏| 2739/3000 [00:01<00:00, 1689.03it/s]warmup should be done:  89%|████████▉ | 2679/3000 [00:01<00:00, 1691.81it/s]warmup should be done:  96%|█████████▌| 2883/3000 [00:01<00:00, 1697.30it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1675.09it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1673.93it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1680.97it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1715.04it/s]warmup should be done:  96%|█████████▋| 2891/3000 [00:01<00:00, 1676.07it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1692.64it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1697.97it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1718.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1698.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1690.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.44it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.80it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7e54e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7b51190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7e569d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7b610d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7b521f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7b601c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7e57d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7face7b532b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 03:11:48.246079: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa81b028be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:48.246143: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:48.254942: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:48.690747: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa81e830120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:48.690815: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:48.698730: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:49.348046: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa81e830740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:49.348110: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:49.356196: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:49.390937: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa82682ef20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:49.390997: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:49.399512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:49.428515: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa81a830430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:49.428575: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:49.436134: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:49.455819: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa826838510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:49.455879: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:49.456090: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa817031c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:49.456133: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:49.457844: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fa81af92740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:11:49.457889: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:11:49.465219: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:49.465218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:49.465218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:11:55.646887: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:55.792517: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:56.109974: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:56.209208: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:56.220114: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:56.298815: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:56.528103: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:11:56.569545: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:12:49.897][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:12:49.897][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:49.905][ERROR][RK0][main]: coll ps creation done
[HCTR][03:12:49.905][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][03:12:49.917][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:12:49.917][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:49.924][ERROR][RK0][main]: coll ps creation done
[HCTR][03:12:49.924][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][03:12:50.051][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:12:50.051][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:50.057][ERROR][RK0][main]: coll ps creation done
[HCTR][03:12:50.057][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][03:12:50.157][ERROR][RK0][tid #140360437184256]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:12:50.158][ERROR][RK0][tid #140360437184256]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:50.162][ERROR][RK0][tid #140360437184256]: coll ps creation done
[HCTR][03:12:50.162][ERROR][RK0][tid #140360437184256]: replica 3 waits for coll ps creation barrier
[HCTR][03:12:50.292][ERROR][RK0][tid #140360437184256]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:12:50.292][ERROR][RK0][tid #140360437184256]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:50.297][ERROR][RK0][tid #140360437184256]: coll ps creation done
[HCTR][03:12:50.297][ERROR][RK0][tid #140360437184256]: replica 4 waits for coll ps creation barrier
[HCTR][03:12:50.340][ERROR][RK0][tid #140360781121280]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:12:50.340][ERROR][RK0][tid #140360781121280]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:50.345][ERROR][RK0][tid #140360781121280]: coll ps creation done
[HCTR][03:12:50.345][ERROR][RK0][tid #140360781121280]: replica 2 waits for coll ps creation barrier
[HCTR][03:12:50.426][ERROR][RK0][tid #140360915339008]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:12:50.426][ERROR][RK0][tid #140360915339008]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:50.433][ERROR][RK0][tid #140360915339008]: coll ps creation done
[HCTR][03:12:50.433][ERROR][RK0][tid #140360915339008]: replica 1 waits for coll ps creation barrier
[HCTR][03:12:50.476][ERROR][RK0][tid #140361376708352]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:12:50.476][ERROR][RK0][tid #140361376708352]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:12:50.484][ERROR][RK0][tid #140361376708352]: coll ps creation done
[HCTR][03:12:50.484][ERROR][RK0][tid #140361376708352]: replica 0 waits for coll ps creation barrier
[HCTR][03:12:50.484][ERROR][RK0][tid #140361376708352]: replica 0 preparing frequency
[HCTR][03:12:51.393][ERROR][RK0][tid #140361376708352]: replica 0 preparing frequency done
[HCTR][03:12:51.447][ERROR][RK0][tid #140361376708352]: replica 0 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][tid #140360915339008]: replica 1 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][tid #140360437184256]: replica 4 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][tid #140360437184256]: replica 3 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][tid #140360781121280]: replica 2 calling init per replica
[HCTR][03:12:51.447][ERROR][RK0][tid #140361376708352]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][main]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][tid #140360915339008]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][main]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][main]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][tid #140360437184256]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][tid #140360437184256]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][tid #140360781121280]: Calling build_v2
[HCTR][03:12:51.447][ERROR][RK0][tid #140361376708352]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][tid #140360915339008]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][tid #140360437184256]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][tid #140360781121280]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:12:51.447][ERROR][RK0][tid #140360437184256]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 03:12:512022-12-12 03:12:512022-12-12 03:12:512022-12-12 03:12:51.2022-12-12 03:12:51.2022-12-12 03:12:51..2022-12-12 03:12:51447281..447281447296447281.: 447296447281: : : 447297E[: : EEE:  EE   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :2022-12-12 03:12:51./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136447354::136136136] :: 136136] ] ] using concurrent impl MPS136E] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
]  using concurrent impl MPSusing concurrent impl MPS


using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc


:136] using concurrent impl MPS
[2022-12-12 03:12:51.451800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:12:51.451839: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:12:51:.196451845] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 03:12:512022-12-12 03:12:51.[.4518882022-12-12 03:12:51451893: .: E451901E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196] :2022-12-12 03:12:51] v100x8, slow pcie212.assigning 8 to cpu
] 451934
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [
E2022-12-12 03:12:51 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc4519722022-12-12 03:12:51:: .[178E4519782022-12-12 03:12:51]  : .[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE4519972022-12-12 03:12:51
: : .196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[452013] :[ 2022-12-12 03:12:51: assigning 8 to cpu1782022-12-12 03:12:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E
] .:452049 v100x8, slow pcie452040213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: ] E[:Eremote time is 8.68421 [2022-12-12 03:12:51212 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 03:12:51.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 03:12:51.[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8452096:196[.4521172022-12-12 03:12:51
: 178] 2022-12-12 03:12:51452133: .E] assigning 8 to cpu[.: E452154 
v100x8, slow pcie2022-12-12 03:12:51452155E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:452215E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: [178:  :2022-12-12 03:12:51196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:12:51] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212.] :. v100x8, slow pcie:] 452281assigning 8 to cpu214452289/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
] : :] 
[Ecpu time is 97.0588E213v100x8, slow pcie2022-12-12 03:12:51 
[ ] 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:12:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421[452400:.:[
2022-12-12 03:12:51: 1964524342122022-12-12 03:12:51.E[] : ] .452462 2022-12-12 03:12:51assigning 8 to cpuEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8452472: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. 

: E:452505/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE [196: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:12:51] E213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.assigning 8 to cpu ] [:212452589
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-12 03:12:51196] : :
.] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E214452629[assigning 8 to cpu
 ] : 2022-12-12 03:12:51
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588E.[2022-12-12 03:12:51:
 4526942022-12-12 03:12:51.213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .452715] :E452726: remote time is 8.68421[ 212: E
2022-12-12 03:12:51/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E]  .:2022-12-12 03:12:51 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc452772214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:: ] 452816:212Ecpu time is 97.0588: 213[]  
E] 2022-12-12 03:12:51build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc remote time is 8.68421.452884
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: [212:E[2022-12-12 03:12:51] 2142022-12-12 03:12:51 .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc452968
cpu time is 97.0588452971: :: 
E213E []  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:12:51remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.
:214453055213] : [] cpu time is 97.0588remote time is 8.68421E2022-12-12 03:12:51

 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc453125[:: 2022-12-12 03:12:51213E.]  453177remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
:E214 ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.05882022-12-12 03:12:51:
.214453264] : cpu time is 97.0588E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 03:14:08.397970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:14:08.450349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:14:08.450462: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:14:08.451900: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 03:14:08.525685: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 03:14:08.908009: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 03:14:08.908102: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 03:14:15.955987: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 03:14:15.956076: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 03:14:17.705127: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 03:14:17.705228: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 03:14:17.707871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 03:14:17.707937: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 03:14:17.954937: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 03:14:17.983739: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 03:14:17.985177: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 03:14:18.  6374: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 03:14:18.567235: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 03:16:12.310116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 03:16:12.318412: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 03:16:12.322853: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 03:16:12.369944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:16:12.370056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:16:12.370090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:16:12.370134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:16:12.370723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:16:12.370797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.371772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.372446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.386041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 03:16:12.386118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 03:16:12.[386543: 2022-12-12 03:16:12E. 386563/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:3 solved202
] 7 solved
[[2022-12-12 03:16:122022-12-12 03:16:12.[.3866262022-12-12 03:16:12386627: .: E386635E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205] :] Building Coll Cache with ... num gpu device is 8205worker 0 thread 3 initing device 3
] 
worker 0 thread 7 initing device 7
[2022-12-12 03:16:12.386719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12[.2022-12-12 03:16:12387168.: 387173E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815:] 1815Building Coll Cache with ... num gpu device is 8] 
Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:16:12[.2022-12-12 03:16:12387233.: 387236E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-12 03:16:12.388664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.388747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 03:16:12.388800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 03:16:12.388843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 03:16:12.388915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 03:16:12.389277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 03:16:122022-12-12 03:16:12..389312389320: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 03:16:12.389371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.389442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:16:12.389492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.390887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.392449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.392485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 03:16:12202.] 3925225 solved: 
E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[2022-12-12 03:16:12:2022-12-12 03:16:122022-12-12 03:16:12.1980..392542] 392556392562: eager alloc mem 381.47 MB: : E
EE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:::2021980205] ] ] eager alloc mem 381.47 MB6 solvedworker 0 thread 5 initing device 5


[[2022-12-12 03:16:122022-12-12 03:16:12..392703392721: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1980205] ] eager alloc mem 381.47 MBworker 0 thread 6 initing device 6

[2022-12-12 03:16:12.393176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:16:12.393219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.393266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:16:12.393312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.396337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.396459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.396531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.396598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.398989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.399053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:16:12.454755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 03:16:12.460106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 03:16:12.460236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:16:12.461064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.461682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.462721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:12.462770: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.08 MB
[[[[2022-12-12 03:16:122022-12-12 03:16:122022-12-12 03:16:122022-12-12 03:16:12....470212470208470212470211: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes



[2022-12-12 03:16:12.474541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 03:16:12.474989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 03:16:12.476400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 03:16:122022-12-12 03:16:12..476468476483: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 03:16:12.476556: [E2022-12-12 03:16:12 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc476584:: 638E]  eager release cuda mem 1023/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[[2022-12-12 03:16:122022-12-12 03:16:12..476640476655: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 03:16:12.476741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:16:12.477542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.478152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.479174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.479807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.480703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 03:16:12.481270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.481517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.481674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.481717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.482279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 03:16:122022-12-12 03:16:12..482300482324: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 1023WORKER[0] alloc host memory 38.10 MB

[2022-12-12 03:16:12.482449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:16:12.482517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:12.482565: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.10 MB
[2022-12-12 03:16:12.482685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:12.[4827212022-12-12 03:16:12: .E482729 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :eager release cuda mem 62566343
] WORKER[0] alloc host memory 38.13 MB
[2022-12-12 03:16:12.482786: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.14 MB
[2022-12-12 03:16:12.482922: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 03:16:12.483083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:16:12.483267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.484713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.485105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.485610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.486127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:12.486175: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.09 MB
[2022-12-12 03:16:12.486524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 03:16:12.486606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:16:12.486780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:12.486866: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.14 MB
[2022-12-12 03:16:12.487443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:16:12.488094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:12.489112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:12.489157: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.10 MB
[2022-12-12 03:16:12.497220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.497971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.498081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.76 GB
[2022-12-12 03:16:12.508119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.508283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.508713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.508753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:16:12.508870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.508912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:16:12.509004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.509091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.509605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.509646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:16:12.509693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.509735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:16:12.511732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.512337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.512380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:16:12.515077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.515691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.515734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:16:12.517333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:16:12.517970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:16:12.518031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:14........170944170949170944170944170944170944170944170945: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 0 init p2p of link 3Device 2 init p2p of link 1Device 3 init p2p of link 2Device 5 init p2p of link 6Device 1 init p2p of link 7Device 6 init p2p of link 0Device 4 init p2p of link 5







[[[[2022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:14[[....2022-12-12 03:16:142022-12-12 03:16:14171538171538171546171538..: : : : 171550171550EEE[[E: :    2022-12-12 03:16:142022-12-12 03:16:14 EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu../hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :::171600171605:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu198019801980: : 1980::] ] ] EE] 19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB  eager alloc mem 611.00 KB] ] 


/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager alloc mem 611.00 KBeager alloc mem 611.00 KB::

19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[[[[[2022-12-12 03:16:142022-12-12 03:16:14[2022-12-12 03:16:142022-12-12 03:16:142022-12-12 03:16:14..2022-12-12 03:16:14...172648172648.172647172658172648: : 172659[: : : EE: 2022-12-12 03:16:14E[EE  E. 2022-12-12 03:16:14  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 172719/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :172754::638638:E638] eager release cuda mem 625663: 638638] ] 638 
E] ] eager release cuda mem 625663eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663eager release cuda mem 625663

eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc


638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 03:16:14.189006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 03:16:14.189160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.189559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 03:16:14.189719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.189788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 03:16:14.189930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.190022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:16:14638.] 190028eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 03:16:14.190188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.190249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 03:16:14.190388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.190541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 03:16:14.190613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.190659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 03:16:14.190724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.190791: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:16:14:.638190804] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.191023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.191206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.191297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 03:16:14.191475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.191620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.191675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.192331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.205725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 03:16:14.205846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.205948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 03:16:14.206065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.206333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 03:16:14.206402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[[2022-12-12 03:16:14[2022-12-12 03:16:14.2022-12-12 03:16:14.206457.206450: 206458: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1926] 1926] [eager alloc mem 611.00 KB] Device 6 init p2p of link 42022-12-12 03:16:14
Device 5 init p2p of link 7
.
206566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.206671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-12 03:16:14
2022-12-12 03:16:14..206690206692: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 03:16:14.206924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.207289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 03:16:14.207403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.207420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 03:16:141926.] 207445Device 3 init p2p of link 5: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.207526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.207568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 03:16:141980.[] 2075822022-12-12 03:16:14eager alloc mem 611.00 KB: .
E207598 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-12 03:16:14.208334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.208393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.225127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 03:16:14.225243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.226089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.226128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 03:16:14.226240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.227085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.227472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 03:16:14.227585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.227854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[[2022-12-12 03:16:142022-12-12 03:16:14..227961227976: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 6 init p2p of link 7eager alloc mem 611.00 KB

[2022-12-12 03:16:14[.2022-12-12 03:16:14228138.: 228130E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 4 init p2p of link 6
[2022-12-12 03:16:14.228221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 03:16:14.228303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.228364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.228426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.228845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.228911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 03:16:14.[2290142022-12-12 03:16:14: .E229022 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-12 03:16:14.229181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.229248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.229876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:16:14.240997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.242997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9998719 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29958160 / 100000000 nodes ( 29.96 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.85577 secs 
[2022-12-12 03:16:14.244491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.244605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.245707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9988249 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29968630 / 100000000 nodes ( 29.97 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.85623 secs 
[2022-12-12 03:16:14.245961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9996720 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29960159 / 100000000 nodes ( 29.96 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.85926 secs 
[2022-12-12 03:16:14.246235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.246713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9986787 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29970092 / 100000000 nodes ( 29.97 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.8535 secs 
[2022-12-12 03:16:14.246933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.248182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.248491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.248579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9986753 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29970126 / 100000000 nodes ( 29.97 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.86135 secs 
[2022-12-12 03:16:14.248808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:16:14.250786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9998707 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29958172 / 100000000 nodes ( 29.96 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.86147 secs 
[2022-12-12 03:16:14.252724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9985709 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29971170 / 100000000 nodes ( 29.97 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.77 GB | 1.85942 secs 
[2022-12-12 03:16:14.253031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9982884 / 100000000 nodes ( 9.98 %~10.00 %) | remote 29973995 / 100000000 nodes ( 29.97 %) | cpu 60043121 / 100000000 nodes ( 60.04 %) | 4.76 GB | 1.88225 secs 
[2022-12-12 03:16:14.254900: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.08 GB
[2022-12-12 03:16:15.680773: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.35 GB
[2022-12-12 03:16:15.681094: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.35 GB
[2022-12-12 03:16:15.681517: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.35 GB
[2022-12-12 03:16:16.876183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.61 GB
[2022-12-12 03:16:16.876380: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.61 GB
[2022-12-12 03:16:16.876691: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.61 GB
[2022-12-12 03:16:18.116653: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.83 GB
[2022-12-12 03:16:18.116799: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.83 GB
[2022-12-12 03:16:18.117121: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.83 GB
[2022-12-12 03:16:19. 36772: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.04 GB
[2022-12-12 03:16:19. 38345: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.04 GB
[2022-12-12 03:16:19. 39637: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.04 GB
[2022-12-12 03:16:20.440375: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.50 GB
[2022-12-12 03:16:20.444781: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.50 GB
[2022-12-12 03:16:20.445810: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.50 GB
[2022-12-12 03:16:21.950750: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.70 GB
[2022-12-12 03:16:21.951889: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.70 GB
[HCTR][03:16:23.207][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][tid #140360781121280]: replica 2 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][tid #140360437184256]: replica 3 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][tid #140360437184256]: replica 4 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][tid #140360915339008]: replica 1 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][tid #140361376708352]: replica 0 calling init per replica done, doing barrier
[HCTR][03:16:23.207][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][03:16:23.208][ERROR][RK0][tid #140360437184256]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360437184256]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][tid #140361376708352]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360781121280]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360915339008]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360437184256]: init per replica done
[HCTR][03:16:23.208][ERROR][RK0][main]: init per replica done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360437184256]: init per replica done
[HCTR][03:16:23.208][ERROR][RK0][main]: init per replica done
[HCTR][03:16:23.208][ERROR][RK0][main]: init per replica done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360781121280]: init per replica done
[HCTR][03:16:23.208][ERROR][RK0][tid #140360915339008]: init per replica done
[HCTR][03:16:23.211][ERROR][RK0][tid #140361376708352]: init per replica done
[HCTR][03:16:23.246][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f8bbc238400
[HCTR][03:16:23.246][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f8bbc558400
[HCTR][03:16:23.246][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f8bbcb98400
[HCTR][03:16:23.246][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f8bbceb8400
[HCTR][03:16:23.246][ERROR][RK0][tid #140360571401984]: 7 allocated 3276800 at 0x7f8b90238400
[HCTR][03:16:23.246][ERROR][RK0][tid #140360571401984]: 7 allocated 6553600 at 0x7f8b90558400
[HCTR][03:16:23.246][ERROR][RK0][tid #140360571401984]: 7 allocated 3276800 at 0x7f8b90b98400
[HCTR][03:16:23.246][ERROR][RK0][tid #140360571401984]: 7 allocated 6553600 at 0x7f8b90eb8400
[HCTR][03:16:23.246][ERROR][RK0][tid #140360437184256]: 3 allocated 3276800 at 0x7f8bac238400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360437184256]: 3 allocated 6553600 at 0x7f8bac558400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360437184256]: 3 allocated 3276800 at 0x7f8bacb98400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360437184256]: 3 allocated 6553600 at 0x7f8baceb8400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360445576960]: 6 allocated 3276800 at 0x7f8bc0238400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360445576960]: 6 allocated 6553600 at 0x7f8bc0558400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360445576960]: 6 allocated 3276800 at 0x7f8bc0b98400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360445576960]: 6 allocated 6553600 at 0x7f8bc0eb8400
[HCTR][03:16:23.247][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f8b48238400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360781121280]: 2 allocated 3276800 at 0x7f8af0238400
[HCTR][03:16:23.247][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f8b48558400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360781121280]: 2 allocated 6553600 at 0x7f8af0558400
[HCTR][03:16:23.247][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f8b48b98400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360781121280]: 2 allocated 3276800 at 0x7f8af0b98400
[HCTR][03:16:23.247][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f8b48eb8400
[HCTR][03:16:23.247][ERROR][RK0][tid #140360781121280]: 2 allocated 6553600 at 0x7f8af0eb8400
[HCTR][03:16:23.247][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f8bb8238400
[HCTR][03:16:23.247][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f8bb8558400
[HCTR][03:16:23.247][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f8bb8b98400
[HCTR][03:16:23.247][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f8bb8eb8400
[HCTR][03:16:23.250][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f8b8e320000
[HCTR][03:16:23.250][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f8b8e640000
[HCTR][03:16:23.250][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f8b8ec80000
[HCTR][03:16:23.250][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f8b8efa0000
