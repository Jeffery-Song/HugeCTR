2022-12-11 21:04:55.654705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.662237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.670961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.676389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.681145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.693583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.699855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.711872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.761972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.771077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.773944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.775215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.776557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.778442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.779282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.779645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.781159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.781190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.782854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.782926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.784285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.784571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.786087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.786333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.787719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.788137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.789350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.790009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.790857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.791914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.792420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.794149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.795973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.797154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.798228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.799302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.800405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.801463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.802497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.803432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.808708: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:55.810347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.811909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.813371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.813484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.814907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.815098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.816412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.816720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.817867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.818243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.818790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.820054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.820485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.821327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.823070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.823598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.824197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.824362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.826784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.828001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.828232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.829545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.831102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.831323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.834189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.834448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.837126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.837523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.838174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.838688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.840428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.840855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.841504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.842657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.843479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.843986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.844321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.844721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.846082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.847573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.847904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.848333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.848430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.850271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.851505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.851847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.852089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.853680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.854317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.854391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.855973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.857015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.857219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.859615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.860057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.860642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.861707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.862192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.863137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.874899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.875971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.895663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.898151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.898470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.900167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.900684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.900781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.900810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.901814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.902090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.904935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.904942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.904987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.905522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.905727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.909659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.909884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.909903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.910111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.910403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.913387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.913693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.913716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.913732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.913966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.917572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.917711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.917801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.917918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.918043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.921545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.921649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.921737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.921888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.921924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.925259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.925300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.925497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.925797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.926654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.928608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.928752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.928912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.929388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.930453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.932535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.932789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.933000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.933802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.934439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.936889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.937822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.937868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.939048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.939511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.941121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.941512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.941730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.942140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.942797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.943102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.945272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.945854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.946149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.946468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.947163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.947526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.949505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.949936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.950277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.950778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.951668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.951806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.952277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.953647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.954321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.954649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.955029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.956199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.956249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.957071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.958134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.958922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.959007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.959555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.961281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.961984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.963846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.964937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.965112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.965162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.965810: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:55.967115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.967796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.967960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.969003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.969120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.969153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.971079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.972029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.972103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.973001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.973010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.973363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.974725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.975073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.976078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.976322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.977842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.977860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.978133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.979529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.979709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.981182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.981459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.982587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.982672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.983264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.985250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.985977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.987659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.987796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.988745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.989010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.989134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.990957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.992348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.992557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.993457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.993683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.994552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.995691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.997401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.997420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.998530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.998857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:55.999695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.000742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.002113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.002942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.004630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.006302: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:56.007064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.007098: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:56.007418: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:56.008598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.009778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.009901: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:56.011452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.012979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.013996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.015269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.015611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.016112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.016682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.016687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.018619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.019140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.020493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.021920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.023096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.024089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.024402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.024656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.025577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.026591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.027666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.028769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.028790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.029170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.039306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.039726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.071878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.072622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.104962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.105744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.111178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.115924: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:56.118030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.123453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.124778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.130180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.133441: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:04:56.135809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.142716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.150639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:56.158419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.141099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.141716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.142253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.142946: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.143012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:04:57.161559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.162199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.162935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.163944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.164469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.164935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:04:57.209995: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.210198: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.259773: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:04:57.400689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.401708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.402574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.403054: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.403108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:04:57.421702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.422580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.423141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.423739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.424290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.424821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:04:57.446127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.446728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.447666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.448143: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.448198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:04:57.461543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.462170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.462690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.463404: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.463462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:04:57.468506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.469146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.470010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.470918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.471746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.472236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:04:57.475341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.475938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.476471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.477117: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.477165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:04:57.481051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.481694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.482212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.482785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.483334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.483807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:04:57.491319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.491912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.492439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.492892: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.492944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:04:57.495374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.495988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.496526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.497254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.497779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.498260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:04:57.505434: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.505589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.507498: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:04:57.507762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.508400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.508924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.509398: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.509451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:04:57.510407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.511040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.511146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.512057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.512213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.513161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.513282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.514162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.514298: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:04:57.514356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:04:57.514912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:04:57.517916: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.518080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.519928: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:04:57.527580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.528258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.528804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.529405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.529914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.530390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:04:57.532557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.533221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.533730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.534315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.534843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:04:57.535331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:04:57.544838: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.545029: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.546909: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:04:57.561009: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.561206: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.563016: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:04:57.563744: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.563929: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.565808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:04:57.575878: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.576053: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.577791: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:04:57.581744: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.581910: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:04:57.583712: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][21:04:58.844][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.850][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.850][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.850][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.850][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.850][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.857][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:04:58.858][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 96it [00:01, 79.79it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 102it [00:01, 86.66it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 193it [00:01, 174.44it/s]warmup run: 75it [00:01, 62.59it/s]warmup run: 100it [00:01, 85.71it/s]warmup run: 203it [00:01, 186.69it/s]warmup run: 95it [00:01, 81.27it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 100it [00:01, 83.37it/s]warmup run: 96it [00:01, 82.83it/s]warmup run: 290it [00:01, 279.05it/s]warmup run: 173it [00:01, 160.53it/s]warmup run: 200it [00:01, 185.60it/s]warmup run: 304it [00:01, 296.98it/s]warmup run: 192it [00:01, 178.19it/s]warmup run: 96it [00:01, 84.35it/s]warmup run: 199it [00:01, 180.08it/s]warmup run: 194it [00:01, 181.37it/s]warmup run: 386it [00:01, 386.63it/s]warmup run: 274it [00:01, 272.70it/s]warmup run: 300it [00:01, 295.50it/s]warmup run: 405it [00:01, 411.35it/s]warmup run: 290it [00:01, 286.03it/s]warmup run: 193it [00:01, 183.20it/s]warmup run: 299it [00:01, 288.28it/s]warmup run: 294it [00:01, 292.11it/s]warmup run: 482it [00:02, 491.90it/s]warmup run: 376it [00:01, 390.86it/s]warmup run: 400it [00:01, 409.16it/s]warmup run: 506it [00:02, 522.86it/s]warmup run: 388it [00:01, 397.75it/s]warmup run: 291it [00:01, 292.83it/s]warmup run: 385it [00:01, 378.43it/s]warmup run: 394it [00:01, 406.93it/s]warmup run: 479it [00:02, 508.10it/s]warmup run: 501it [00:02, 521.02it/s]warmup run: 609it [00:02, 628.98it/s]warmup run: 573it [00:02, 555.49it/s]warmup run: 486it [00:02, 506.70it/s]warmup run: 389it [00:01, 405.29it/s]warmup run: 471it [00:02, 455.09it/s]warmup run: 493it [00:02, 516.62it/s]warmup run: 580it [00:02, 612.70it/s]warmup run: 603it [00:02, 625.01it/s]warmup run: 712it [00:02, 720.84it/s]warmup run: 659it [00:02, 613.38it/s]warmup run: 584it [00:02, 606.06it/s]warmup run: 487it [00:01, 514.11it/s]warmup run: 565it [00:02, 553.98it/s]warmup run: 594it [00:02, 620.21it/s]warmup run: 678it [00:02, 695.70it/s]warmup run: 812it [00:02, 789.84it/s]warmup run: 705it [00:02, 715.01it/s]warmup run: 759it [00:02, 704.56it/s]warmup run: 684it [00:02, 696.31it/s]warmup run: 589it [00:02, 620.74it/s]warmup run: 664it [00:02, 650.89it/s]warmup run: 695it [00:02, 710.39it/s]warmup run: 776it [00:02, 764.51it/s]warmup run: 912it [00:02, 843.62it/s]warmup run: 806it [00:02, 786.38it/s]warmup run: 855it [00:02, 768.99it/s]warmup run: 784it [00:02, 770.96it/s]warmup run: 691it [00:02, 712.46it/s]warmup run: 764it [00:02, 734.46it/s]warmup run: 796it [00:02, 784.02it/s]warmup run: 874it [00:02, 820.07it/s]warmup run: 1013it [00:02, 887.97it/s]warmup run: 905it [00:02, 830.29it/s]warmup run: 956it [00:02, 831.40it/s]warmup run: 883it [00:02, 827.88it/s]warmup run: 793it [00:02, 788.04it/s]warmup run: 866it [00:02, 807.74it/s]warmup run: 897it [00:02, 841.92it/s]warmup run: 974it [00:02, 867.04it/s]warmup run: 1114it [00:02, 920.85it/s]warmup run: 1003it [00:02, 863.24it/s]warmup run: 1057it [00:02, 880.12it/s]warmup run: 985it [00:02, 878.08it/s]warmup run: 894it [00:02, 844.58it/s]warmup run: 966it [00:02, 858.29it/s]warmup run: 996it [00:02, 881.46it/s]warmup run: 1076it [00:02, 907.28it/s]warmup run: 1216it [00:02, 947.45it/s]warmup run: 1100it [00:02, 889.61it/s]warmup run: 1158it [00:02, 915.97it/s]warmup run: 1086it [00:02, 912.43it/s]warmup run: 996it [00:02, 892.07it/s]warmup run: 1067it [00:02, 898.01it/s]warmup run: 1096it [00:02, 912.47it/s]warmup run: 1178it [00:02, 937.09it/s]warmup run: 1197it [00:02, 909.98it/s]warmup run: 1317it [00:02, 936.44it/s]warmup run: 1258it [00:02, 938.30it/s]warmup run: 1188it [00:02, 940.35it/s]warmup run: 1097it [00:02, 924.57it/s]warmup run: 1168it [00:02, 928.67it/s]warmup run: 1195it [00:02, 932.76it/s]warmup run: 1278it [00:02, 953.14it/s]warmup run: 1416it [00:02, 951.50it/s]warmup run: 1294it [00:02, 922.21it/s]warmup run: 1360it [00:03, 961.76it/s]warmup run: 1288it [00:02, 948.35it/s]warmup run: 1198it [00:02, 948.21it/s]warmup run: 1267it [00:02, 945.88it/s]warmup run: 1294it [00:02, 945.59it/s]warmup run: 1379it [00:02, 967.50it/s]warmup run: 1391it [00:02, 935.03it/s]warmup run: 1515it [00:03, 960.92it/s]warmup run: 1463it [00:03, 979.64it/s]warmup run: 1387it [00:02, 954.72it/s]warmup run: 1299it [00:02, 962.13it/s]warmup run: 1369it [00:02, 966.50it/s]warmup run: 1393it [00:02, 955.63it/s]warmup run: 1479it [00:03, 976.31it/s]warmup run: 1615it [00:03, 971.69it/s]warmup run: 1488it [00:03, 943.04it/s]warmup run: 1565it [00:03, 989.23it/s]warmup run: 1486it [00:03, 963.62it/s]warmup run: 1400it [00:02, 975.34it/s]warmup run: 1471it [00:03, 980.68it/s]warmup run: 1492it [00:03, 964.33it/s]warmup run: 1580it [00:03, 984.94it/s]warmup run: 1716it [00:03, 981.09it/s]warmup run: 1585it [00:03, 950.34it/s]warmup run: 1666it [00:03, 988.58it/s]warmup run: 1586it [00:03, 971.54it/s]warmup run: 1502it [00:02, 986.15it/s]warmup run: 1573it [00:03, 992.10it/s]warmup run: 1591it [00:03, 962.50it/s]warmup run: 1681it [00:03, 991.74it/s]warmup run: 1684it [00:03, 961.94it/s]warmup run: 1818it [00:03, 991.98it/s]warmup run: 1767it [00:03, 994.51it/s]warmup run: 1685it [00:03, 976.26it/s]warmup run: 1603it [00:03, 991.75it/s]warmup run: 1674it [00:03, 981.42it/s]warmup run: 1689it [00:03, 958.12it/s]warmup run: 1783it [00:03, 999.12it/s]warmup run: 1786it [00:03, 977.24it/s]warmup run: 1921it [00:03, 1001.23it/s]warmup run: 1869it [00:03, 1001.00it/s]warmup run: 1784it [00:03, 979.35it/s]warmup run: 1704it [00:03, 992.39it/s]warmup run: 1774it [00:03, 969.74it/s]warmup run: 1786it [00:03, 952.79it/s]warmup run: 1885it [00:03, 1004.53it/s]warmup run: 2027it [00:03, 1017.52it/s]warmup run: 1888it [00:03, 987.84it/s]warmup run: 1972it [00:03, 1006.75it/s]warmup run: 1883it [00:03, 979.73it/s]warmup run: 1805it [00:03, 993.30it/s]warmup run: 1872it [00:03, 962.98it/s]warmup run: 1882it [00:03, 952.75it/s]warmup run: 1987it [00:03, 1007.10it/s]warmup run: 2149it [00:03, 1076.76it/s]warmup run: 1989it [00:03, 992.74it/s]warmup run: 2085it [00:03, 1042.86it/s]warmup run: 1985it [00:03, 988.82it/s]warmup run: 1905it [00:03, 992.24it/s]warmup run: 1969it [00:03, 960.23it/s]warmup run: 1978it [00:03, 953.97it/s]warmup run: 2104it [00:03, 1054.16it/s]warmup run: 2271it [00:03, 1118.48it/s]warmup run: 2106it [00:03, 1043.75it/s]warmup run: 2203it [00:03, 1081.89it/s]warmup run: 2100it [00:03, 1034.62it/s]warmup run: 2007it [00:03, 999.93it/s]warmup run: 2080it [00:03, 1002.42it/s]warmup run: 2092it [00:03, 1007.13it/s]warmup run: 2224it [00:03, 1097.45it/s]warmup run: 2390it [00:03, 1137.40it/s]warmup run: 2226it [00:03, 1089.29it/s]warmup run: 2321it [00:03, 1108.52it/s]warmup run: 2218it [00:03, 1077.21it/s]warmup run: 2127it [00:03, 1057.48it/s]warmup run: 2199it [00:03, 1057.06it/s]warmup run: 2213it [00:03, 1064.98it/s]warmup run: 2344it [00:03, 1127.75it/s]warmup run: 2510it [00:03, 1154.75it/s]warmup run: 2340it [00:03, 1102.47it/s]warmup run: 2439it [00:04, 1127.11it/s]warmup run: 2336it [00:03, 1107.17it/s]warmup run: 2247it [00:03, 1097.88it/s]warmup run: 2318it [00:03, 1095.37it/s]warmup run: 2332it [00:03, 1101.35it/s]warmup run: 2464it [00:03, 1148.11it/s]warmup run: 2628it [00:04, 1162.11it/s]warmup run: 2456it [00:03, 1119.11it/s]warmup run: 2557it [00:04, 1141.01it/s]warmup run: 2455it [00:03, 1129.86it/s]warmup run: 2366it [00:03, 1125.24it/s]warmup run: 2437it [00:04, 1123.16it/s]warmup run: 2451it [00:03, 1126.65it/s]warmup run: 2584it [00:04, 1162.72it/s]warmup run: 2748it [00:04, 1172.59it/s]warmup run: 2574it [00:04, 1136.92it/s]warmup run: 2675it [00:04, 1150.80it/s]warmup run: 2573it [00:04, 1144.43it/s]warmup run: 2486it [00:03, 1146.26it/s]warmup run: 2556it [00:04, 1141.55it/s]warmup run: 2571it [00:04, 1147.56it/s]warmup run: 2705it [00:04, 1174.03it/s]warmup run: 2867it [00:04, 1175.03it/s]warmup run: 2690it [00:04, 1141.75it/s]warmup run: 2791it [00:04, 1151.96it/s]warmup run: 2692it [00:04, 1156.78it/s]warmup run: 2606it [00:03, 1160.35it/s]warmup run: 2674it [00:04, 1151.29it/s]warmup run: 2691it [00:04, 1162.45it/s]warmup run: 2824it [00:04, 1177.82it/s]warmup run: 2808it [00:04, 1152.88it/s]warmup run: 2986it [00:04, 1177.44it/s]warmup run: 2908it [00:04, 1157.02it/s]warmup run: 3000it [00:04, 685.90it/s] warmup run: 2809it [00:04, 1158.74it/s]warmup run: 2725it [00:04, 1168.08it/s]warmup run: 2791it [00:04, 1155.89it/s]warmup run: 2810it [00:04, 1169.22it/s]warmup run: 2944it [00:04, 1182.67it/s]warmup run: 3000it [00:04, 667.02it/s] warmup run: 2925it [00:04, 1157.56it/s]warmup run: 2927it [00:04, 1163.63it/s]warmup run: 2843it [00:04, 1170.88it/s]warmup run: 3000it [00:04, 677.30it/s] warmup run: 3000it [00:04, 680.24it/s] warmup run: 2909it [00:04, 1162.37it/s]warmup run: 2929it [00:04, 1173.59it/s]warmup run: 3000it [00:04, 680.46it/s] warmup run: 2963it [00:04, 1177.13it/s]warmup run: 3000it [00:04, 682.16it/s] warmup run: 3000it [00:04, 668.21it/s] warmup run: 3000it [00:04, 692.25it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1589.51it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.25it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.14it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1635.77it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1612.61it/s]warmup should be done:   5%|▌         | 153/3000 [00:00<00:01, 1521.84it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1601.78it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1581.61it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1599.15it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.26it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.17it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1604.86it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.90it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1589.84it/s]warmup should be done:  11%|█         | 317/3000 [00:00<00:01, 1585.72it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1571.66it/s]warmup should be done:  16%|█▌        | 480/3000 [00:00<00:01, 1595.18it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1600.07it/s]warmup should be done:  16%|█▌        | 478/3000 [00:00<00:01, 1584.65it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1600.03it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1625.38it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1610.15it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1602.74it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1590.10it/s]warmup should be done:  21%|██▏       | 641/3000 [00:00<00:01, 1605.50it/s]warmup should be done:  21%|██▏       | 640/3000 [00:00<00:01, 1590.64it/s]warmup should be done:  21%|██        | 637/3000 [00:00<00:01, 1583.09it/s]warmup should be done:  21%|██▏       | 644/3000 [00:00<00:01, 1590.35it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1609.42it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1600.41it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1576.99it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1580.88it/s]warmup should be done:  27%|██▋       | 803/3000 [00:00<00:01, 1608.72it/s]warmup should be done:  27%|██▋       | 796/3000 [00:00<00:01, 1578.73it/s]warmup should be done:  27%|██▋       | 800/3000 [00:00<00:01, 1582.63it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1620.01it/s]warmup should be done:  27%|██▋       | 804/3000 [00:00<00:01, 1581.02it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1596.85it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1582.60it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1589.58it/s]warmup should be done:  32%|███▏      | 964/3000 [00:00<00:01, 1608.95it/s]warmup should be done:  32%|███▏      | 954/3000 [00:00<00:01, 1576.29it/s]warmup should be done:  32%|███▏      | 959/3000 [00:00<00:01, 1579.05it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1622.73it/s]warmup should be done:  32%|███▏      | 963/3000 [00:00<00:01, 1579.75it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1602.26it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1595.93it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1581.58it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1607.33it/s]warmup should be done:  37%|███▋      | 1112/3000 [00:00<00:01, 1572.47it/s]warmup should be done:  37%|███▋      | 1117/3000 [00:00<00:01, 1575.70it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1617.85it/s]warmup should be done:  37%|███▋      | 1121/3000 [00:00<00:01, 1578.33it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1604.65it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1596.42it/s]warmup should be done:  38%|███▊      | 1128/3000 [00:00<00:01, 1576.39it/s]warmup should be done:  43%|████▎     | 1288/3000 [00:00<00:01, 1612.04it/s]warmup should be done:  42%|████▏     | 1270/3000 [00:00<00:01, 1572.57it/s]warmup should be done:  42%|████▎     | 1275/3000 [00:00<00:01, 1576.94it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1618.18it/s]warmup should be done:  43%|████▎     | 1280/3000 [00:00<00:01, 1580.07it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1612.47it/s]warmup should be done:  43%|████▎     | 1293/3000 [00:00<00:01, 1596.73it/s]warmup should be done:  43%|████▎     | 1286/3000 [00:00<00:01, 1569.40it/s]warmup should be done:  48%|████▊     | 1450/3000 [00:00<00:00, 1607.48it/s]warmup should be done:  48%|████▊     | 1428/3000 [00:00<00:00, 1573.48it/s]warmup should be done:  48%|████▊     | 1433/3000 [00:00<00:00, 1577.57it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1618.26it/s]warmup should be done:  48%|████▊     | 1439/3000 [00:00<00:00, 1581.47it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1613.48it/s]warmup should be done:  48%|████▊     | 1453/3000 [00:00<00:00, 1595.85it/s]warmup should be done:  48%|████▊     | 1443/3000 [00:00<00:00, 1564.51it/s]warmup should be done:  54%|█████▍    | 1615/3000 [00:01<00:00, 1618.79it/s]warmup should be done:  53%|█████▎    | 1586/3000 [00:01<00:00, 1573.35it/s]warmup should be done:  53%|█████▎    | 1592/3000 [00:01<00:00, 1578.38it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1617.82it/s]warmup should be done:  53%|█████▎    | 1598/3000 [00:01<00:00, 1582.17it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1613.78it/s]warmup should be done:  54%|█████▍    | 1613/3000 [00:01<00:00, 1594.20it/s]warmup should be done:  53%|█████▎    | 1600/3000 [00:01<00:00, 1563.09it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1625.89it/s]warmup should be done:  58%|█████▊    | 1750/3000 [00:01<00:00, 1578.55it/s]warmup should be done:  58%|█████▊    | 1744/3000 [00:01<00:00, 1572.19it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1617.26it/s]warmup should be done:  59%|█████▊    | 1757/3000 [00:01<00:00, 1582.25it/s]warmup should be done:  59%|█████▉    | 1781/3000 [00:01<00:00, 1613.47it/s]warmup should be done:  59%|█████▉    | 1773/3000 [00:01<00:00, 1594.02it/s]warmup should be done:  59%|█████▊    | 1757/3000 [00:01<00:00, 1558.97it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1629.26it/s]warmup should be done:  64%|██████▎   | 1908/3000 [00:01<00:00, 1577.84it/s]warmup should be done:  63%|██████▎   | 1902/3000 [00:01<00:00, 1572.63it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1616.75it/s]warmup should be done:  65%|██████▍   | 1943/3000 [00:01<00:00, 1612.91it/s]warmup should be done:  64%|██████▍   | 1916/3000 [00:01<00:00, 1581.62it/s]warmup should be done:  64%|██████▍   | 1933/3000 [00:01<00:00, 1591.63it/s]warmup should be done:  64%|██████▍   | 1913/3000 [00:01<00:00, 1553.85it/s]warmup should be done:  70%|███████   | 2108/3000 [00:01<00:00, 1632.16it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1577.87it/s]warmup should be done:  69%|██████▊   | 2060/3000 [00:01<00:00, 1560.60it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1611.29it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1607.25it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1578.39it/s]warmup should be done:  70%|██████▉   | 2093/3000 [00:01<00:00, 1580.97it/s]warmup should be done:  69%|██████▉   | 2069/3000 [00:01<00:00, 1549.61it/s]warmup should be done:  76%|███████▌  | 2272/3000 [00:01<00:00, 1633.91it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1577.85it/s]warmup should be done:  76%|███████▌  | 2280/3000 [00:01<00:00, 1609.30it/s]warmup should be done:  74%|███████▍  | 2233/3000 [00:01<00:00, 1577.94it/s]warmup should be done:  76%|███████▌  | 2267/3000 [00:01<00:00, 1607.82it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1556.16it/s]warmup should be done:  75%|███████▌  | 2253/3000 [00:01<00:00, 1585.64it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1544.85it/s]warmup should be done:  81%|████████  | 2436/3000 [00:01<00:00, 1632.89it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1574.09it/s]warmup should be done:  81%|████████▏ | 2441/3000 [00:01<00:00, 1607.15it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1576.17it/s]warmup should be done:  81%|████████  | 2428/3000 [00:01<00:00, 1607.13it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1557.35it/s]warmup should be done:  80%|████████  | 2415/3000 [00:01<00:00, 1593.84it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1540.85it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1635.30it/s]warmup should be done:  85%|████████▍ | 2540/3000 [00:01<00:00, 1574.91it/s]warmup should be done:  87%|████████▋ | 2603/3000 [00:01<00:00, 1610.42it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1577.46it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1561.75it/s]warmup should be done:  86%|████████▋ | 2589/3000 [00:01<00:00, 1592.78it/s]warmup should be done:  86%|████████▌ | 2577/3000 [00:01<00:00, 1599.22it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1539.61it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1636.53it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1575.10it/s]warmup should be done:  92%|█████████▏| 2765/3000 [00:01<00:00, 1612.23it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1576.90it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1562.89it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1599.04it/s]warmup should be done:  92%|█████████▏| 2749/3000 [00:01<00:00, 1577.57it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1536.92it/s]warmup should be done:  98%|█████████▊| 2931/3000 [00:01<00:00, 1642.51it/s]warmup should be done:  95%|█████████▌| 2857/3000 [00:01<00:00, 1577.91it/s]warmup should be done:  98%|█████████▊| 2927/3000 [00:01<00:00, 1612.97it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1579.60it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1567.85it/s]warmup should be done:  97%|█████████▋| 2897/3000 [00:01<00:00, 1598.32it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1572.77it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1542.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1592.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1580.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1579.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1570.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1560.93it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.47it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.58it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.76it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.37it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.84it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.05it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.18it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.87it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.49it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1639.17it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1644.34it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1664.50it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1640.94it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1592.33it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.07it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1619.36it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1689.51it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1671.36it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1648.53it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1652.49it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1640.38it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1596.71it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1621.59it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1681.94it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1691.21it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1675.21it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1658.83it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1660.64it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1639.32it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1599.64it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1682.69it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1625.04it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1691.92it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1676.50it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1665.18it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1665.63it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1597.91it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1625.90it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1682.51it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1632.87it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1676.06it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1664.76it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1665.77it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1688.66it/s]warmup should be done:  32%|███▏      | 962/3000 [00:00<00:01, 1595.92it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1624.17it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1678.12it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1626.06it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1676.07it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1666.05it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1665.84it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1624.50it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1674.00it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1623.54it/s]warmup should be done:  37%|███▋      | 1122/3000 [00:00<00:01, 1576.33it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1643.27it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1678.52it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:00, 1671.95it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1664.92it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1620.81it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1672.79it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1619.29it/s]warmup should be done:  43%|████▎     | 1281/3000 [00:00<00:01, 1578.46it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1658.48it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1677.20it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1665.91it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1678.95it/s]warmup should be done:  49%|████▉     | 1468/3000 [00:00<00:00, 1622.88it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1669.70it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1620.46it/s]warmup should be done:  48%|████▊     | 1442/3000 [00:00<00:00, 1586.95it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1661.71it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1680.37it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1677.88it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1667.43it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1624.21it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1670.38it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1622.72it/s]warmup should be done:  53%|█████▎    | 1604/3000 [00:01<00:00, 1595.61it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1664.36it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1674.49it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1668.33it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1624.35it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1665.66it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1622.16it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1668.12it/s]warmup should be done:  59%|█████▉    | 1764/3000 [00:01<00:00, 1595.52it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1661.20it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1666.69it/s]warmup should be done:  67%|██████▋   | 2016/3000 [00:01<00:00, 1669.55it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1622.79it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1662.40it/s]warmup should be done:  64%|██████▍   | 1924/3000 [00:01<00:00, 1595.59it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1619.70it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1663.17it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1661.34it/s]warmup should be done:  72%|███████▏  | 2169/3000 [00:01<00:00, 1666.10it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1624.72it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1663.64it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  70%|██████▉   | 2085/3000 [00:01<00:00, 1597.78it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1620.21it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1659.50it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1662.45it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1667.44it/s]warmup should be done:  76%|███████▌  | 2284/3000 [00:01<00:00, 1626.46it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1658.98it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1656.07it/s]warmup should be done:  75%|███████▍  | 2245/3000 [00:01<00:00, 1596.99it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1621.52it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1660.12it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1664.97it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1667.88it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1624.32it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1655.84it/s]warmup should be done:  84%|████████▎ | 2508/3000 [00:01<00:00, 1659.26it/s]warmup should be done:  80%|████████  | 2405/3000 [00:01<00:00, 1594.95it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1620.42it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1658.84it/s]warmup should be done:  84%|████████▍ | 2528/3000 [00:01<00:00, 1667.83it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1664.62it/s]warmup should be done:  87%|████████▋ | 2610/3000 [00:01<00:00, 1622.24it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1655.71it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1650.71it/s]warmup should be done:  86%|████████▌ | 2566/3000 [00:01<00:00, 1597.03it/s]warmup should be done:  87%|████████▋ | 2617/3000 [00:01<00:00, 1623.19it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1651.70it/s]warmup should be done:  90%|████████▉ | 2696/3000 [00:01<00:00, 1668.88it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1663.74it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1623.95it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1653.43it/s]warmup should be done:  91%|█████████ | 2728/3000 [00:01<00:00, 1603.16it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1628.24it/s]warmup should be done:  95%|█████████▍| 2848/3000 [00:01<00:00, 1643.21it/s]warmup should be done:  95%|█████████▌| 2863/3000 [00:01<00:00, 1669.10it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1643.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.65it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1625.89it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1606.49it/s]warmup should be done:  98%|█████████▊| 2946/3000 [00:01<00:00, 1631.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.71it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1596.76it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c2adccd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c29a680d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c29a732b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c29a661c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c29a651f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c2adc9e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c29a65160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5c2adca730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 21:06:29.746808: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f575302ccd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:29.746871: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:29.757156: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:29.815995: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f575e82fe70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:29.816051: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:29.823929: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:30.009795: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f574e799200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:30.009852: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:30.017923: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:30.347238: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5752834430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:30.347301: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:30.357478: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:30.369151: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f575e833ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:30.369227: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:30.377224: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:30.512999: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5752830480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:30.513075: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:30.521109: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:30.522430: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f575e82c0b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:30.522472: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:30.532419: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:30.542650: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5743031570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:06:30.542719: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:06:30.552729: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:06:36.951810: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:36.951867: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:37.194248: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:37.210771: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:37.233003: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:37.277220: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:37.360124: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:06:37.468472: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:07:41.977][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:07:41.977][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:41.986][ERROR][RK0][main]: coll ps creation done
[HCTR][21:07:41.986][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][21:07:42.182][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:07:42.182][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.189][ERROR][RK0][main]: coll ps creation done
[HCTR][21:07:42.189][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:07:42.351][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:07:42.351][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.352][ERROR][RK0][tid #140013375317760]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:07:42.352][ERROR][RK0][tid #140013375317760]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.356][ERROR][RK0][main]: coll ps creation done
[HCTR][21:07:42.356][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:07:42.356][ERROR][RK0][tid #140013375317760]: coll ps creation done
[HCTR][21:07:42.356][ERROR][RK0][tid #140013375317760]: replica 2 waits for coll ps creation barrier
[HCTR][21:07:42.455][ERROR][RK0][tid #140013912188672]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:07:42.456][ERROR][RK0][tid #140013912188672]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.461][ERROR][RK0][tid #140013912188672]: coll ps creation done
[HCTR][21:07:42.461][ERROR][RK0][tid #140013912188672]: replica 3 waits for coll ps creation barrier
[HCTR][21:07:42.580][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:07:42.580][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.587][ERROR][RK0][main]: coll ps creation done
[HCTR][21:07:42.587][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][21:07:42.775][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:07:42.775][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.783][ERROR][RK0][main]: coll ps creation done
[HCTR][21:07:42.783][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][21:07:42.790][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:07:42.791][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:07:42.794][ERROR][RK0][main]: coll ps creation done
[HCTR][21:07:42.794][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][21:07:42.794][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][21:07:43.674][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][21:07:43.706][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][tid #140013912188672]: replica 3 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][tid #140013375317760]: replica 2 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][21:07:43.706][ERROR][RK0][main]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][tid #140013912188672]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][main]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][main]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][tid #140013375317760]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][main]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][main]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][main]: Calling build_v2
[HCTR][21:07:43.706][ERROR][RK0][tid #140013912188672]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][tid #140013375317760]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:07:43.706][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-11 21:07:432022-12-11 21:07:432022-12-11 21:07:43[.2022-12-11 21:07:43.[2022-12-11 21:07:43.706680.7066682022-12-11 21:07:43.2022-12-11 21:07:43706677: 7066822022-12-11 21:07:43: .706677.: E: .E706695: 706684E E706694 : E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :136: 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] :136:] using concurrent impl MPS] :using concurrent impl MPS136] 136using concurrent impl MPS
using concurrent impl MPS136
] using concurrent impl MPS] 

] using concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS


[2022-12-11 21:07:43.710846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:07:43.710886: E[ 2022-12-11 21:07:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:710889196: ] Eassigning 8 to cpu 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:07:43[.2022-12-11 21:07:43710932.: 710941E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[178:2022-12-11 21:07:43] 196.v100x8, slow pcie] [710966
assigning 8 to cpu2022-12-11 21:07:43: 
.E[710977 2022-12-11 21:07:43: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E:711000 212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] E:2022-12-11 21:07:43build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[ 178.
2022-12-11 21:07:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 711022.:v100x8, slow pcie[: [711038196
2022-12-11 21:07:43E2022-12-11 21:07:43: ] . .[Eassigning 8 to cpu711066/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc7110722022-12-11 21:07:43 
: E:: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [178E711096:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:07:43]  : 212:.[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 178711119[2022-12-11 21:07:43
: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] : 2022-12-11 21:07:43.213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[v100x8, slow pcieE.711168] :2022-12-11 21:07:43
 [711187: remote time is 8.68421196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:07:43[: E
] 711225:.2022-12-11 21:07:43E assigning 8 to cpu: [178711260. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E2022-12-11 21:07:43] .: 711282/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 711325v100x8, slow pcieE: :178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
 E[212] :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 21:07:43[] v100x8, slow pcie196 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 21:07:43build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213:711409.
assigning 8 to cpu:] [196: 711431
214remote time is 8.684212022-12-11 21:07:43[] E: ] 
.2022-12-11 21:07:43assigning 8 to cpu[ Ecpu time is 97.0588711495.
2022-12-11 21:07:43[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
: 711519.2022-12-11 21:07:43:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 711571.212:[ E: 711577] 1962022-12-11 21:07:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E
assigning 8 to cpu711625196[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
: ] 2022-12-11 21:07:43213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu.] 214: 
711718remote time is 8.68421] 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
cpu time is 97.0588[] :E
[2022-12-11 21:07:43build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212 2022-12-11 21:07:43.
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.711781[[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:711796: 2022-12-11 21:07:432022-12-11 21:07:43
213: E..] E 711829[711842remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 21:07:43: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E.E:212[ 711878 214] 2022-12-11 21:07:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.:E:cpu time is 97.0588
711923212 213
: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:remote time is 8.684212022-12-11 21:07:43 
213
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 712016:remote time is 8.68421[[: 214
2022-12-11 21:07:432022-12-11 21:07:43E] .[. cpu time is 97.05887120752022-12-11 21:07:43712075/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: .: :E712114E213 :  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421: :
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214] :[] remote time is 8.684212142022-12-11 21:07:43cpu time is 97.0588
] .
cpu time is 97.0588[712249
2022-12-11 21:07:43: .E712295 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 21:09:01.310787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:09:01.350714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:09:01.350776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 999999
[2022-12-11 21:09:01.466858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:09:01.466947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:09:01.466979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:09:01.467010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:09:01.467450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.468332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.469045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.482138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 21:09:01.482201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 21:09:01.482612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 21:09:012022-12-11 21:09:01..482684482692: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved3 solved

[[2022-12-11 21:09:012022-12-11 21:09:01..482772482773: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 3 initing device 3

[2022-12-11 21:09:01.[4829162022-12-11 21:09:01: .E482931 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :4 solved202
] 6 solved[
2022-12-11 21:09:01.483000: [E2022-12-11 21:09:01 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc483012:: 205E]  worker 0 thread 4 initing device 4/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 6 initing device 6
[[2022-12-11 21:09:012022-12-11 21:09:01..483120483140: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::[2022022022-12-11 21:09:01[] ] .2022-12-11 21:09:012 solved5 solved483188.

: 483197E: [ [E2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:09:01 .:./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu4832321980483234:: ] : 1980Eeager alloc mem 381.47 MBE]  
 eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
::205205] ] worker 0 thread 2 initing device 2worker 0 thread 5 initing device 5

[2022-12-11 21:09:01.483422[: 2022-12-11 21:09:01E. 483433/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.483675[: 2022-12-11 21:09:01E. 483684/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.484390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.487931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.491840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.491890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.491938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.491990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.492039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.492088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:09:01.546387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:09:01.546769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:09:01.552349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:09:01.552448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:09:01.552497: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:09:01.553554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.554328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.555334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.555448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.556138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:09:01.556185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:09:01.572261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:09:01.572643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[[[[2022-12-11 21:09:012022-12-11 21:09:012022-12-11 21:09:012022-12-11 21:09:012022-12-11 21:09:012022-12-11 21:09:01......577667577668577670577667577667577667: : : : : : EEEEEE      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::198019801980198019801980] ] ] ] ] eager alloc mem 2.00 Bytes] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes
eager alloc mem 2.00 Bytes




[[[[[2022-12-11 21:09:012022-12-11 21:09:01[2022-12-11 21:09:012022-12-11 21:09:012022-12-11 21:09:01..2022-12-11 21:09:01...578130578130.578131578132578134: : 578139: : : EE: EEE  E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::19801980:198019801980] ] 1980] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

eager alloc mem 1024.00 Bytes



[2022-12-11 21:09:01.589887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:09:01.589974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:09:01.590021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:09:01.590472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:09:01.590541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 21:09:01] .eager release cuda mem 2590542
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:09:01.590597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 21:09:01.590617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 21:09:01] .eager release cuda mem 2590624
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:09:01.590676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 21:09:01.590696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[[2022-12-11 21:09:012022-12-11 21:09:01..590724590742: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-11 21:09:01.[5908112022-12-11 21:09:01: .E590802 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 2638
] eager release cuda mem 1024
[2022-12-11 21:09:01.590880: E[ 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:[5908906382022-12-11 21:09:01: ] .Eeager release cuda mem 400000000590897 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 21980
] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.590972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:09:01.591108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:09:01.591218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:09:01.591265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:09:01.592320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.592920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.593427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.593965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.594713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.595048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.595475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:09:01.595991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.596076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.596316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.596390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.596503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.596552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:09:012022-12-11 21:09:01..596727596744: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] [] eager alloc mem 611.00 KB2022-12-11 21:09:01eager release cuda mem 25855
.
596775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 21:09:01
.596818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:09:01.597260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.597318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:09:01.597341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.597405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.597434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.597480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.597517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.597560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.597702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:09:01.597723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.597787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.597810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:09:01.598022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:09:01.598062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:09:011980.] 598073eager alloc mem 488.28 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:09:01.598123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:09:01.598181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:09:01.598220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 21:09:01] .eager alloc mem 488.28 MB598233
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:09:01.598281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:09:01.598459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:09:01.[5984922022-12-11 21:09:01: .E598499 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 258551980
] eager alloc mem 488.28 MB
[2022-12-11 21:09:01.598552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[[[2022-12-11 21:09:01[[2022-12-11 21:09:01[2022-12-11 21:09:01.[2022-12-11 21:09:012022-12-11 21:09:01.2022-12-11 21:09:01.6909312022-12-11 21:09:01..690931.690934: .690939690944: 690950: E690953: : [E: E : EE2022-12-11 21:09:01 E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu691009:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::: 1980:1980] :19801980E] 1980] eager alloc mem 611.00 KB1980] ]  ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB

eager alloc mem 611.00 KB

:

1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.691934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 21:09:01638.] 691948eager release cuda mem 625663: 
E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:09:01:.638691970] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[638[2022-12-11 21:09:01[] [2022-12-11 21:09:01.[2022-12-11 21:09:01eager release cuda mem 6256632022-12-11 21:09:01[.6919992022-12-11 21:09:01.
.2022-12-11 21:09:01692001: .692001692007.[: E692012: : 6920202022-12-11 21:09:01E : EE: . /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE  E692048/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc [: :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:09:01E638] :638638:. ] eager release cuda mem 625663638] ] 1980692100/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663
] eager release cuda mem 625663eager release cuda mem 625663] : :
eager release cuda mem 625663

eager alloc mem 611.00 KBE1980

 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.692214: E[ 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[:[6922282022-12-11 21:09:011980[2022-12-11 21:09:01: .] 2022-12-11 21:09:01.E692235eager alloc mem 611.00 KB.692238 : 
692246: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: E: E 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB1980:1980
] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-11 21:09:01.692950: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:09:01:.638692961] : eager release cuda mem 625663E[
 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:692980638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.693033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.693057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 21:09:01] .eager alloc mem 611.00 KB693072[
: 2022-12-11 21:09:01E. 693083[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 2022-12-11 21:09:01:E.1980 693098] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager alloc mem 611.00 KB:E[
638 2022-12-11 21:09:01] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager release cuda mem 625663:[693128
638[2022-12-11 21:09:01: ] 2022-12-11 21:09:01.Eeager release cuda mem 625663.693148 
693155: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E:E 638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 6256632022-12-11 21:09:01:638
.638] 693203] eager release cuda mem 625663[: eager release cuda mem 625663
2022-12-11 21:09:01E
. 693228/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[eager alloc mem 611.00 KB:2022-12-11 21:09:01
1980.] 693267eager alloc mem 611.00 KB: 
[E[2022-12-11 21:09:01 2022-12-11 21:09:01./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.693288:693293: 1980: E] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 21:09:01.693812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.693845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.693880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 21:09:01
.693897: E[ 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:693913638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.693985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.694045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.694079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.694113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.[6941382022-12-11 21:09:01: .E694144 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE[:[ 2022-12-11 21:09:016382022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.] .:694161eager release cuda mem 6256636941631980: 
: ] EEeager alloc mem 611.00 KB  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 21:09:01.694253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01[.2022-12-11 21:09:01694289.: 694292E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 21:09:01.694656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.694704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:09:01eager release cuda mem 625663.
694722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.694765: E[ 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:694778638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.694848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.694894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.694960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.695077: E[ 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:695089638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695141: E[ 2022-12-11 21:09:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:695151638[: ] 2022-12-11 21:09:01Eeager release cuda mem 625663. 
695166/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01[.2022-12-11 21:09:01695570.: 695575E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.695659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.695803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.695937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.696002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:09:011980.] 696016eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:09:01eager release cuda mem 625663.
696050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 21:09:01
.696076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.696105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 21:09:01.696126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 21:09:01
.696146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.696376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01[.2022-12-11 21:09:01696437.: 696443E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB[
2022-12-11 21:09:01.696482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.696524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.696551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.696581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.696647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.696848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.696912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:09:01eager alloc mem 611.00 KB.
696928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:09:01.696956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:09:01eager release cuda mem 625663.
696975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 21:09:01
.697002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.697030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 21:09:01.697051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:09:01.697242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.697276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:09:01.697299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 21:09:012022-12-11 21:09:01..697330697333: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 4399996

[2022-12-11 21:09:01.697393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:09:01.697422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.697458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:09:01.697740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.697780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:09:01.697827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:09:01.697855[: 2022-12-11 21:09:01E. 697862/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: [:E2022-12-11 21:09:01638 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc697876eager release cuda mem 625663:: 
638E]  eager release cuda mem 4399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
[:2022-12-11 21:09:01638.] 697918eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996[
2022-12-11 21:09:01.697949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:09:01.703047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.219367 secs 
[2022-12-11 21:09:01.714330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.230663 secs 
[2022-12-11 21:09:01.714974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.231792 secs 
[2022-12-11 21:09:01.715643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.248198 secs 
[2022-12-11 21:09:01.716083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.232667 secs 
[2022-12-11 21:09:01.717041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.23385 secs 
[2022-12-11 21:09:01.717433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.234006 secs 
[2022-12-11 21:09:01.717837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.235233 secs 
[2022-12-11 21:09:01.717934: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.80 GB
[2022-12-11 21:09:03.323378: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.06 GB
[2022-12-11 21:09:03.324060: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.06 GB
[2022-12-11 21:09:03.325289: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.06 GB
[2022-12-11 21:09:05. 91182: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.32 GB
[2022-12-11 21:09:05. 92167: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.32 GB
[2022-12-11 21:09:05. 93334: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.32 GB
[2022-12-11 21:09:06.318341: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.54 GB
[2022-12-11 21:09:06.319921: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.54 GB
[2022-12-11 21:09:06.320799: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.54 GB
[2022-12-11 21:09:07.954988: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.75 GB
[2022-12-11 21:09:07.956591: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.75 GB
[2022-12-11 21:09:07.958728: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.75 GB
[2022-12-11 21:09:09.782031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.21 GB
[2022-12-11 21:09:09.782443: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.21 GB
[2022-12-11 21:09:09.808017: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.21 GB
[2022-12-11 21:09:11.358494: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.41 GB
[2022-12-11 21:09:11.358646: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.41 GB
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][tid #140013375317760]: replica 2 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][tid #140013912188672]: replica 3 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][tid #140013375317760]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][tid #140013912188672]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:09:11.358][ERROR][RK0][main]: init per replica done
[HCTR][21:09:11.358][ERROR][RK0][main]: init per replica done
[HCTR][21:09:11.358][ERROR][RK0][main]: init per replica done
[HCTR][21:09:11.358][ERROR][RK0][main]: init per replica done
[HCTR][21:09:11.358][ERROR][RK0][main]: init per replica done
[HCTR][21:09:11.358][ERROR][RK0][tid #140013375317760]: init per replica done
[HCTR][21:09:11.358][ERROR][RK0][tid #140013912188672]: init per replica done
[HCTR][21:09:11.361][ERROR][RK0][main]: init per replica done
[HCTR][21:09:11.397][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3c0a238400
[HCTR][21:09:11.397][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3c0a558400
[HCTR][21:09:11.397][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3c0ab98400
[HCTR][21:09:11.397][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3c0aeb8400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 5 allocated 3276800 at 0x7f3be8238400
[HCTR][21:09:11.397][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3c0a238400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 5 allocated 6553600 at 0x7f3be8558400
[HCTR][21:09:11.397][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f3c0a558400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 5 allocated 3276800 at 0x7f3be8b98400
[HCTR][21:09:11.397][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3c0ab98400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 5 allocated 6553600 at 0x7f3be8eb8400
[HCTR][21:09:11.397][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f3c0aeb8400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 7 allocated 3276800 at 0x7f3af4238400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 7 allocated 6553600 at 0x7f3af4558400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 7 allocated 3276800 at 0x7f3af4b98400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013375317760]: 7 allocated 6553600 at 0x7f3af4eb8400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013434033920]: 4 allocated 3276800 at 0x7f3b78238400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013509535488]: 6 allocated 3276800 at 0x7f3ae0238400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013434033920]: 4 allocated 6553600 at 0x7f3b78558400
[HCTR][21:09:11.397][ERROR][RK0][tid #140013777970944]: 1 allocated 3276800 at 0x7f3acc238400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013509535488]: 6 allocated 6553600 at 0x7f3ae0558400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013434033920]: 4 allocated 3276800 at 0x7f3b78b98400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013777970944]: 1 allocated 6553600 at 0x7f3acc558400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013509535488]: 6 allocated 3276800 at 0x7f3ae0b98400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013434033920]: 4 allocated 6553600 at 0x7f3b78eb8400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013777970944]: 1 allocated 3276800 at 0x7f3accb98400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013509535488]: 6 allocated 6553600 at 0x7f3ae0eb8400
[HCTR][21:09:11.398][ERROR][RK0][tid #140013777970944]: 1 allocated 6553600 at 0x7f3acceb8400
[HCTR][21:09:11.400][ERROR][RK0][tid #140013702469376]: 0 allocated 3276800 at 0x7f3bda320000
[HCTR][21:09:11.401][ERROR][RK0][tid #140013702469376]: 0 allocated 6553600 at 0x7f3bda640000
[HCTR][21:09:11.401][ERROR][RK0][tid #140013702469376]: 0 allocated 3276800 at 0x7f3bdac80000
[HCTR][21:09:11.401][ERROR][RK0][tid #140013702469376]: 0 allocated 6553600 at 0x7f3bdafa0000
