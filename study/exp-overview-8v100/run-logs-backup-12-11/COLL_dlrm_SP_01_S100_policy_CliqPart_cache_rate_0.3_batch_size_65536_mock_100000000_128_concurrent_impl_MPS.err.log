2022-12-12 07:26:41.755407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.760907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.768121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.772869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.780046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.792353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.797877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.809791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.855327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.857798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.860729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.866632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.867635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.870884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.871021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.872562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.872653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.874134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.874228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.875886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.875911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.877700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.877756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.879193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.880289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.881246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.882412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.883436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.884391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.885327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.886344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.887424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.889216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.890357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.891308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.892254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.893278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.894328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.895282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.896218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.901482: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:41.902346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.903397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.903667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.904919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.905242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.906429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.906857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.908211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.908581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.909857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.910184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.911195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.911522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.912095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.913250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.913869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.914594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.915877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.916535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.922294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.923774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.924534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.925009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.926687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.927976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.928286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.928305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.929413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.931092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.931496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.931589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.933287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.935203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.935618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.935678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.936376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.936798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.938396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.938845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.938898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.939361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.940060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.941554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.941831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.942070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.942254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.954482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.956938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.957307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.957389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.957494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.958783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.960097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.960504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.960886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.961930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.962318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.962483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.996842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.997601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.998684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.998778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.998908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:41.999901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.002690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.002723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.002827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.003325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.004850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.006348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.007368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.007502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.007891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.008722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.010482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.010576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.011199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.011879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.013926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.013971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.014555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.015008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.017253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.017301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.017705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.018332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.020072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.020188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.021077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.021495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.022586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.022769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.023723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.024607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.025460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.025510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.026617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.027467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.028954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.029111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.030739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.031431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.032360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.032525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.033227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.034393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.035540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.035759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.036371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.037421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.038445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.038543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.039259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.040194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.040634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.041294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.041438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.042368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.043722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.044083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.044663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.044866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.045387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.045610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.047435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.049868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.049963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.050025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.050102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.051598: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.051714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.053352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.053430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.053475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.053652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.054245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.055151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.057226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.057622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.057871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.058263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.058370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.059029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.061333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.061500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.061739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.061937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.062494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.062743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.063418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.065852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.066040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.066209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.066892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.068050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.068296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.069230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.072049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.072058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.072236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.072583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.073489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.073764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.074313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.077421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.077639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.077958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.078584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.078861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.079304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.082198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.082466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.082791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.083449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.083843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.084141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.087070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.087990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.088184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.088384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.090993: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.091286: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.091532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.092676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.092840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.092994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.095144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.096196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.096632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.098516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.099033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.099536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.099837: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.100858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.101250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.101471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.102142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.102973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.104721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.105734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.105849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.106799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.109037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.109544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.109890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.110438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.110619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.111388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.113515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.114028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.115741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.115911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.129120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.129705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.132525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.133142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.164805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.166833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.167165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.169607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.199765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.200112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.203388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.206588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.206876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.208598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.213009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.213444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.216710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.220481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.220529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.225359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.225465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.226069: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.231072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.234097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.234281: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.235926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.244568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.266735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.266875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.269559: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:26:42.271391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.271529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.279242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.339297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:42.375309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.292089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.293225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.294227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.295144: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.295204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:26:43.313695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.315205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.316563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.317155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.317880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.318417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:26:43.364787: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.365008: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.391758: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:26:43.543766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.544378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.545092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.545775: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.545832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:26:43.563745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.564371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.565085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.565675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.566200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.566684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:26:43.585698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.587016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.588458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.589517: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.589572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:26:43.608101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.609524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.610612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.612024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.613245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.614792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:26:43.625558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.627667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.629976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.631941: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.631985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:26:43.633601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.634215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.635272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.635781: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.635827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:26:43.645311: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.645506: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.646434: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:26:43.649718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.650343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.651083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.651690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.652278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.652785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:26:43.653571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.654152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.654882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.655474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.656005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.656481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:26:43.676827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.677441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.678195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.678674: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.678728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:26:43.691936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.693012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.694044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.694257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.695347: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.695407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:26:43.695847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.696416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.697151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.698037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.698486: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:26:43.698542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:26:43.698753: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.698959: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.699427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.699796: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.699876: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 07:26:43.699956: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.700494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.701495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.701661: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 07:26:43.702441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:26:43.702767: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.702932: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.704733: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:26:43.713597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.714713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.715703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.716516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.716910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.718007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.718345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.719507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.719785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:26:43.720730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.721729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:26:43.722690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:26:43.748075: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.748274: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.750083: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:26:43.765698: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.765891: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.767497: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 07:26:43.768895: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.769065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:26:43.770787: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][07:26:45.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.037][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.059][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:26:45.059][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 102it [00:01, 85.27it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 204it [00:01, 185.17it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 97it [00:01, 82.57it/s]warmup run: 304it [00:01, 292.82it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 97it [00:01, 81.12it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 191it [00:01, 175.74it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 402it [00:01, 401.81it/s]warmup run: 96it [00:01, 82.81it/s]warmup run: 99it [00:01, 84.08it/s]warmup run: 91it [00:01, 77.56it/s]warmup run: 194it [00:01, 176.08it/s]warmup run: 98it [00:01, 84.26it/s]warmup run: 293it [00:01, 289.10it/s]warmup run: 101it [00:01, 86.75it/s]warmup run: 502it [00:02, 511.98it/s]warmup run: 194it [00:01, 181.45it/s]warmup run: 198it [00:01, 182.29it/s]warmup run: 182it [00:01, 168.14it/s]warmup run: 290it [00:01, 279.76it/s]warmup run: 199it [00:01, 185.69it/s]warmup run: 391it [00:01, 399.79it/s]warmup run: 199it [00:01, 184.55it/s]warmup run: 605it [00:02, 619.60it/s]warmup run: 294it [00:01, 292.38it/s]warmup run: 297it [00:01, 290.37it/s]warmup run: 273it [00:01, 267.91it/s]warmup run: 386it [00:01, 387.94it/s]warmup run: 298it [00:01, 294.31it/s]warmup run: 484it [00:02, 498.30it/s]warmup run: 299it [00:01, 294.63it/s]warmup run: 706it [00:02, 708.35it/s]warmup run: 394it [00:01, 406.88it/s]warmup run: 390it [00:01, 392.53it/s]warmup run: 364it [00:01, 371.19it/s]warmup run: 485it [00:02, 499.12it/s]warmup run: 396it [00:01, 405.04it/s]warmup run: 587it [00:02, 609.62it/s]warmup run: 399it [00:01, 408.62it/s]warmup run: 806it [00:02, 780.29it/s]warmup run: 494it [00:02, 518.14it/s]warmup run: 457it [00:02, 475.07it/s]warmup run: 585it [00:02, 603.67it/s]warmup run: 478it [00:02, 464.34it/s]warmup run: 494it [00:02, 512.14it/s]warmup run: 692it [00:02, 710.03it/s]warmup run: 499it [00:02, 518.70it/s]warmup run: 907it [00:02, 839.17it/s]warmup run: 597it [00:02, 626.34it/s]warmup run: 550it [00:02, 570.71it/s]warmup run: 685it [00:02, 693.79it/s]warmup run: 579it [00:02, 574.89it/s]warmup run: 592it [00:02, 610.84it/s]warmup run: 795it [00:02, 788.44it/s]warmup run: 597it [00:02, 614.87it/s]warmup run: 697it [00:02, 712.35it/s]warmup run: 1007it [00:02, 843.85it/s]warmup run: 644it [00:02, 655.56it/s]warmup run: 788it [00:02, 775.56it/s]warmup run: 682it [00:02, 676.31it/s]warmup run: 692it [00:02, 699.74it/s]warmup run: 896it [00:02, 845.54it/s]warmup run: 696it [00:02, 701.24it/s]warmup run: 800it [00:02, 789.55it/s]warmup run: 1102it [00:02, 868.14it/s]warmup run: 739it [00:02, 728.54it/s]warmup run: 890it [00:02, 838.46it/s]warmup run: 783it [00:02, 756.81it/s]warmup run: 794it [00:02, 778.39it/s]warmup run: 997it [00:02, 889.85it/s]warmup run: 796it [00:02, 774.81it/s]warmup run: 903it [00:02, 850.54it/s]warmup run: 1202it [00:02, 902.52it/s]warmup run: 833it [00:02, 783.50it/s]warmup run: 994it [00:02, 891.34it/s]warmup run: 882it [00:02, 816.10it/s]warmup run: 896it [00:02, 840.51it/s]warmup run: 1100it [00:02, 927.04it/s]warmup run: 894it [00:02, 827.66it/s]warmup run: 1004it [00:02, 893.71it/s]warmup run: 1303it [00:02, 930.46it/s]warmup run: 927it [00:02, 825.76it/s]warmup run: 1099it [00:02, 934.12it/s]warmup run: 980it [00:02, 860.00it/s]warmup run: 997it [00:02, 886.36it/s]warmup run: 1201it [00:02, 950.60it/s]warmup run: 995it [00:02, 874.99it/s]warmup run: 1105it [00:02, 920.38it/s]warmup run: 1403it [00:02, 948.48it/s]warmup run: 1022it [00:02, 858.23it/s]warmup run: 1203it [00:02, 963.41it/s]warmup run: 1079it [00:02, 895.00it/s]warmup run: 1099it [00:02, 923.21it/s]warmup run: 1303it [00:02, 970.07it/s]warmup run: 1095it [00:02, 909.57it/s]warmup run: 1205it [00:02, 934.49it/s]warmup run: 1503it [00:03, 962.29it/s]warmup run: 1117it [00:02, 883.14it/s]warmup run: 1306it [00:02, 977.87it/s]warmup run: 1177it [00:02, 917.56it/s]warmup run: 1200it [00:02, 946.05it/s]warmup run: 1405it [00:02, 982.72it/s]warmup run: 1305it [00:02, 950.70it/s]warmup run: 1194it [00:02, 879.18it/s]warmup run: 1604it [00:03, 975.41it/s]warmup run: 1217it [00:02, 916.07it/s]warmup run: 1409it [00:02, 990.35it/s]warmup run: 1276it [00:02, 937.33it/s]warmup run: 1301it [00:02, 963.90it/s]warmup run: 1507it [00:03, 993.37it/s]warmup run: 1404it [00:02, 959.87it/s]warmup run: 1294it [00:02, 911.90it/s]warmup run: 1704it [00:03, 979.72it/s]warmup run: 1321it [00:02, 951.15it/s]warmup run: 1512it [00:03, 999.29it/s]warmup run: 1375it [00:02, 951.06it/s]warmup run: 1403it [00:02, 977.97it/s]warmup run: 1609it [00:03, 1000.97it/s]warmup run: 1503it [00:03, 966.07it/s]warmup run: 1396it [00:02, 940.29it/s]warmup run: 1804it [00:03, 983.73it/s]warmup run: 1424it [00:03, 972.60it/s]warmup run: 1614it [00:03, 1002.87it/s]warmup run: 1473it [00:03, 949.13it/s]warmup run: 1712it [00:03, 1007.91it/s]warmup run: 1504it [00:03, 970.93it/s]warmup run: 1602it [00:03, 969.36it/s]warmup run: 1494it [00:03, 944.85it/s]warmup run: 1904it [00:03, 987.94it/s]warmup run: 1524it [00:03, 980.00it/s]warmup run: 1716it [00:03, 992.26it/s] warmup run: 1571it [00:03, 957.16it/s]warmup run: 1814it [00:03, 1009.59it/s]warmup run: 1606it [00:03, 984.79it/s]warmup run: 1701it [00:03, 974.67it/s]warmup run: 1597it [00:03, 968.70it/s]warmup run: 2004it [00:03, 990.33it/s]warmup run: 1624it [00:03, 978.99it/s]warmup run: 1671it [00:03, 967.46it/s]warmup run: 1817it [00:03, 976.23it/s]warmup run: 1917it [00:03, 1013.42it/s]warmup run: 1709it [00:03, 996.27it/s]warmup run: 1801it [00:03, 980.08it/s]warmup run: 1699it [00:03, 983.26it/s]warmup run: 2123it [00:03, 1048.56it/s]warmup run: 1723it [00:03, 979.87it/s]warmup run: 1771it [00:03, 976.78it/s]warmup run: 1916it [00:03, 963.84it/s]warmup run: 2023it [00:03, 1026.49it/s]warmup run: 1812it [00:03, 1004.84it/s]warmup run: 1905it [00:03, 995.74it/s]warmup run: 2241it [00:03, 1087.56it/s]warmup run: 1802it [00:03, 994.67it/s]warmup run: 1824it [00:03, 987.32it/s]warmup run: 1871it [00:03, 981.69it/s]warmup run: 2013it [00:03, 960.97it/s]warmup run: 2145it [00:03, 1083.21it/s]warmup run: 1914it [00:03, 1008.63it/s]warmup run: 2009it [00:03, 1007.31it/s]warmup run: 2362it [00:03, 1122.35it/s]warmup run: 1904it [00:03, 1001.29it/s]warmup run: 1925it [00:03, 991.32it/s]warmup run: 1971it [00:03, 985.37it/s]warmup run: 2133it [00:03, 1029.37it/s]warmup run: 2268it [00:03, 1126.32it/s]warmup run: 2019it [00:03, 1020.64it/s]warmup run: 2131it [00:03, 1069.51it/s]warmup run: 2006it [00:03, 1006.77it/s]warmup run: 2484it [00:03, 1150.15it/s]warmup run: 2031it [00:03, 1010.15it/s]warmup run: 2085it [00:03, 1031.14it/s]warmup run: 2253it [00:03, 1078.14it/s]warmup run: 2389it [00:03, 1150.56it/s]warmup run: 2142it [00:03, 1081.55it/s]warmup run: 2253it [00:03, 1113.77it/s]warmup run: 2606it [00:04, 1170.58it/s]warmup run: 2127it [00:03, 1065.76it/s]warmup run: 2154it [00:03, 1075.39it/s]warmup run: 2206it [00:03, 1084.17it/s]warmup run: 2373it [00:03, 1114.01it/s]warmup run: 2510it [00:03, 1167.14it/s]warmup run: 2265it [00:03, 1123.68it/s]warmup run: 2376it [00:03, 1146.11it/s]warmup run: 2247it [00:03, 1105.55it/s]warmup run: 2727it [00:04, 1180.38it/s]warmup run: 2278it [00:03, 1122.13it/s]warmup run: 2327it [00:03, 1120.35it/s]warmup run: 2493it [00:03, 1138.28it/s]warmup run: 2630it [00:04, 1174.49it/s]warmup run: 2388it [00:03, 1153.72it/s]warmup run: 2499it [00:03, 1168.77it/s]warmup run: 2848it [00:04, 1188.63it/s]warmup run: 2368it [00:03, 1134.16it/s]warmup run: 2401it [00:03, 1154.14it/s]warmup run: 2448it [00:03, 1146.45it/s]warmup run: 2613it [00:04, 1155.12it/s]warmup run: 2751it [00:04, 1184.44it/s]warmup run: 2511it [00:03, 1175.72it/s]warmup run: 2621it [00:04, 1181.93it/s]warmup run: 2489it [00:03, 1156.26it/s]warmup run: 2971it [00:04, 1198.10it/s]warmup run: 3000it [00:04, 678.36it/s] warmup run: 2524it [00:04, 1174.80it/s]warmup run: 2570it [00:04, 1165.63it/s]warmup run: 2734it [00:04, 1169.76it/s]warmup run: 2870it [00:04, 1185.78it/s]warmup run: 2633it [00:04, 1186.94it/s]warmup run: 2740it [00:04, 1181.70it/s]warmup run: 2610it [00:04, 1170.03it/s]warmup run: 2646it [00:04, 1187.38it/s]warmup run: 2690it [00:04, 1173.70it/s]warmup run: 2856it [00:04, 1182.05it/s]warmup run: 2989it [00:04, 1186.81it/s]warmup run: 2756it [00:04, 1199.49it/s]warmup run: 3000it [00:04, 688.61it/s] warmup run: 2859it [00:04, 1184.04it/s]warmup run: 2730it [00:04, 1176.74it/s]warmup run: 2769it [00:04, 1199.99it/s]warmup run: 2810it [00:04, 1179.38it/s]warmup run: 2977it [00:04, 1189.68it/s]warmup run: 2879it [00:04, 1206.43it/s]warmup run: 3000it [00:04, 678.83it/s] warmup run: 2979it [00:04, 1187.57it/s]warmup run: 2851it [00:04, 1184.56it/s]warmup run: 3000it [00:04, 689.28it/s] warmup run: 2892it [00:04, 1206.82it/s]warmup run: 2929it [00:04, 1181.36it/s]warmup run: 3000it [00:04, 691.86it/s] warmup run: 2971it [00:04, 1187.17it/s]warmup run: 3000it [00:04, 675.96it/s] warmup run: 3000it [00:04, 676.40it/s] warmup run: 3000it [00:04, 684.59it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.64it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.17it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.07it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.15it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.41it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.68it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.06it/s]warmup should be done:   5%|▌         | 151/3000 [00:00<00:01, 1502.58it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1674.52it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1673.71it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1631.45it/s]warmup should be done:  10%|█         | 305/3000 [00:00<00:01, 1524.14it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.94it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.43it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.85it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.76it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1639.70it/s]warmup should be done:  15%|█▌        | 458/3000 [00:00<00:01, 1524.24it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.35it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1668.96it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1630.18it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.45it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1679.53it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1623.15it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1637.97it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1658.96it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1671.28it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1666.39it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1678.90it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1630.58it/s]warmup should be done:  20%|██        | 611/3000 [00:00<00:01, 1520.70it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1620.77it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1634.29it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1663.94it/s]warmup should be done:  25%|██▌       | 764/3000 [00:00<00:01, 1521.61it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1654.33it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1673.75it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1660.60it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1621.49it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1616.83it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1630.18it/s]warmup should be done:  31%|███       | 917/3000 [00:00<00:01, 1518.41it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1658.12it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1647.43it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1668.04it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1610.16it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1608.84it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1646.90it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1645.07it/s]warmup should be done:  36%|███▌      | 1069/3000 [00:00<00:01, 1513.54it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1623.29it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1650.37it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1608.94it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1645.69it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1606.05it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1666.40it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1623.98it/s]warmup should be done:  41%|████      | 1221/3000 [00:00<00:01, 1512.52it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1641.80it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1608.68it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1644.02it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1610.14it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1645.00it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1622.59it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1642.00it/s]warmup should be done:  46%|████▌     | 1373/3000 [00:00<00:01, 1511.39it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1659.32it/s]warmup should be done:  49%|████▉     | 1463/3000 [00:00<00:00, 1608.89it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1612.96it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1640.99it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1644.30it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1642.48it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1621.05it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1608.51it/s]warmup should be done:  51%|█████     | 1525/3000 [00:01<00:00, 1510.52it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1614.86it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1648.93it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1644.54it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1639.30it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1620.57it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1640.94it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1607.23it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1511.55it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1641.94it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1636.73it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1642.34it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1587.94it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1621.65it/s]warmup should be done:  65%|██████▍   | 1946/3000 [00:01<00:00, 1606.10it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1511.29it/s]warmup should be done:  66%|██████▋   | 1988/3000 [00:01<00:00, 1638.08it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1636.27it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1639.49it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1636.89it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1611.69it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1606.06it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1512.27it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1620.34it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1640.07it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1634.96it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1638.36it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1634.28it/s]warmup should be done:  71%|███████   | 2125/3000 [00:01<00:00, 1627.82it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1621.92it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1512.38it/s]warmup should be done:  76%|███████▌  | 2268/3000 [00:01<00:00, 1602.55it/s]warmup should be done:  77%|███████▋  | 2318/3000 [00:01<00:00, 1634.41it/s]warmup should be done:  77%|███████▋  | 2324/3000 [00:01<00:00, 1634.22it/s]warmup should be done:  77%|███████▋  | 2322/3000 [00:01<00:00, 1622.70it/s]warmup should be done:  78%|███████▊  | 2336/3000 [00:01<00:00, 1630.11it/s]warmup should be done:  76%|███████▋  | 2291/3000 [00:01<00:00, 1634.50it/s]warmup should be done:  82%|████████▏ | 2453/3000 [00:01<00:00, 1621.06it/s]warmup should be done:  81%|████████  | 2429/3000 [00:01<00:00, 1602.49it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1510.68it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1632.55it/s]warmup should be done:  83%|████████▎ | 2488/3000 [00:01<00:00, 1630.17it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1628.66it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1612.76it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1629.35it/s]warmup should be done:  86%|████████▋ | 2590/3000 [00:01<00:00, 1602.97it/s]warmup should be done:  81%|████████  | 2437/3000 [00:01<00:00, 1511.26it/s]warmup should be done:  87%|████████▋ | 2616/3000 [00:01<00:00, 1616.65it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1630.96it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1627.27it/s]warmup should be done:  88%|████████▊ | 2652/3000 [00:01<00:00, 1622.99it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1616.08it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1621.64it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1603.74it/s]warmup should be done:  86%|████████▋ | 2589/3000 [00:01<00:00, 1511.46it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1616.04it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1635.67it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1628.98it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1622.62it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1618.32it/s]warmup should be done:  93%|█████████▎| 2781/3000 [00:01<00:00, 1609.86it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1610.44it/s]warmup should be done:  91%|█████████▏| 2741/3000 [00:01<00:00, 1512.33it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1622.21it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1639.24it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1630.72it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1635.41it/s]warmup should be done:  99%|█████████▉| 2974/3000 [00:01<00:00, 1621.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.31it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1614.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1609.76it/s]warmup should be done:  96%|█████████▋| 2895/3000 [00:01<00:00, 1518.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1515.26it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1548.35it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.78it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.66it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.37it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1704.25it/s]warmup should be done:   5%|▌         | 154/3000 [00:00<00:01, 1533.97it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.43it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1670.34it/s]warmup should be done:  10%|█         | 311/3000 [00:00<00:01, 1551.61it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1705.78it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1611.40it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1679.99it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1649.03it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1663.63it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1671.03it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1654.40it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1707.26it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1641.73it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.42it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1680.54it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1677.69it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1654.44it/s]warmup should be done:  16%|█▌        | 467/3000 [00:00<00:01, 1545.66it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1659.76it/s]warmup should be done:  23%|██▎       | 685/3000 [00:00<00:01, 1711.07it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1655.89it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1686.80it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1667.36it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1671.86it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1672.47it/s]warmup should be done:  21%|██        | 622/3000 [00:00<00:01, 1540.64it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1675.33it/s]warmup should be done:  29%|██▊       | 858/3000 [00:00<00:01, 1715.32it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1659.59it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1687.33it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1672.69it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1669.62it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1676.77it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1680.08it/s]warmup should be done:  26%|██▌       | 779/3000 [00:00<00:01, 1549.31it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1716.61it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1664.70it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1696.29it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1675.43it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1667.71it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1682.07it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1686.04it/s]warmup should be done:  31%|███       | 935/3000 [00:00<00:01, 1550.06it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1678.61it/s]warmup should be done:  40%|████      | 1202/3000 [00:00<00:01, 1711.95it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1701.97it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1671.39it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1677.17it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1685.73it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1691.88it/s]warmup should be done:  36%|███▋      | 1095/3000 [00:00<00:01, 1563.81it/s]warmup should be done:  44%|████▍     | 1331/3000 [00:00<00:00, 1678.60it/s]warmup should be done:  46%|████▌     | 1375/3000 [00:00<00:00, 1714.91it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1708.41it/s]warmup should be done:  45%|████▍     | 1341/3000 [00:00<00:00, 1672.88it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1679.73it/s]warmup should be done:  45%|████▌     | 1351/3000 [00:00<00:00, 1686.00it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1696.46it/s]warmup should be done:  42%|████▏     | 1254/3000 [00:00<00:01, 1569.97it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1674.90it/s]warmup should be done:  52%|█████▏    | 1547/3000 [00:00<00:00, 1716.20it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1710.87it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1678.20it/s]warmup should be done:  50%|█████     | 1509/3000 [00:00<00:00, 1671.65it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1684.37it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1693.86it/s]warmup should be done:  47%|████▋     | 1415/3000 [00:00<00:01, 1580.89it/s]warmup should be done:  57%|█████▋    | 1719/3000 [00:01<00:00, 1715.22it/s]warmup should be done:  57%|█████▋    | 1706/3000 [00:01<00:00, 1711.12it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1679.28it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1671.09it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1684.78it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1692.30it/s]warmup should be done:  52%|█████▎    | 1575/3000 [00:01<00:00, 1585.74it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1640.20it/s]warmup should be done:  63%|██████▎   | 1891/3000 [00:01<00:00, 1716.26it/s]warmup should be done:  63%|██████▎   | 1878/3000 [00:01<00:00, 1710.37it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1685.51it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1672.78it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1684.33it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1695.02it/s]warmup should be done:  58%|█████▊    | 1735/3000 [00:01<00:00, 1587.12it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1629.75it/s]warmup should be done:  69%|██████▉   | 2063/3000 [00:01<00:00, 1716.00it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1687.75it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1710.58it/s]warmup should be done:  68%|██████▊   | 2027/3000 [00:01<00:00, 1683.50it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1671.01it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1695.73it/s]warmup should be done:  63%|██████▎   | 1895/3000 [00:01<00:00, 1589.85it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1632.97it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1714.54it/s]warmup should be done:  74%|███████▍  | 2222/3000 [00:01<00:00, 1708.92it/s]warmup should be done:  73%|███████▎  | 2189/3000 [00:01<00:00, 1682.63it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1671.41it/s]warmup should be done:  74%|███████▎  | 2206/3000 [00:01<00:00, 1697.25it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1591.36it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1678.56it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1642.38it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1713.73it/s]warmup should be done:  79%|███████▊  | 2358/3000 [00:01<00:00, 1683.92it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1672.80it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1698.54it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1680.54it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1701.20it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1584.90it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1651.43it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1715.12it/s]warmup should be done:  84%|████████▍ | 2528/3000 [00:01<00:00, 1687.78it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1676.37it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1701.47it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1687.73it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1704.67it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1581.07it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1659.40it/s]warmup should be done:  92%|█████████▏| 2751/3000 [00:01<00:00, 1715.68it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1685.55it/s]warmup should be done:  91%|█████████ | 2719/3000 [00:01<00:00, 1703.39it/s]warmup should be done:  90%|█████████ | 2706/3000 [00:01<00:00, 1690.13it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1707.08it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1580.17it/s]warmup should be done:  90%|████████▉ | 2697/3000 [00:01<00:00, 1656.74it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1635.85it/s]warmup should be done:  97%|█████████▋| 2923/3000 [00:01<00:00, 1714.91it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1703.69it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1688.76it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1709.78it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1685.11it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1580.28it/s]warmup should be done:  95%|█████████▌| 2863/3000 [00:01<00:00, 1640.77it/s]warmup should be done:  94%|█████████▍| 2833/3000 [00:01<00:00, 1645.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1712.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1703.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.07it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1581.93it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1649.71it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1648.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1575.14it/s]2022-12-12 07:28:21.294508: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d70029fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.294574: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.301700: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5c8f8307e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.301758: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.305299: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d58031220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.305346: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.322562: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5c937965f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.322619: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.611837: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5c97831400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.611898: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.680360: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3c900312f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.680435: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.729133: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5c8b830a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.729208: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:21.735714: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5c8f82cf10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:28:21.735775: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:28:23.549626: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:23.574096: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:23.619810: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:23.650992: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:23.927005: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:23.988674: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:24.033122: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:24.049665: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:28:26.432907: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.489216: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.527998: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.708473: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.890716: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.924115: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.961534: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:28:26.982450: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:28:49.991][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:28:49.991][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:49.997][ERROR][RK0][tid #140036360087296]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:28:49.997][ERROR][RK0][tid #140036360087296]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.001][ERROR][RK0][main]: coll ps creation done
[HCTR][07:28:50.001][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][07:28:50.005][ERROR][RK0][tid #140036360087296]: coll ps creation done
[HCTR][07:28:50.005][ERROR][RK0][tid #140036360087296]: replica 2 waits for coll ps creation barrier
[HCTR][07:28:50.032][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:28:50.033][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.041][ERROR][RK0][main]: coll ps creation done
[HCTR][07:28:50.041][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][07:28:50.048][ERROR][RK0][tid #140035965826816]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:28:50.048][ERROR][RK0][tid #140035965826816]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.056][ERROR][RK0][tid #140035965826816]: coll ps creation done
[HCTR][07:28:50.056][ERROR][RK0][tid #140035965826816]: replica 1 waits for coll ps creation barrier
[HCTR][07:28:50.059][ERROR][RK0][tid #140035965826816]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:28:50.059][ERROR][RK0][tid #140035965826816]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.067][ERROR][RK0][tid #140035965826816]: coll ps creation done
[HCTR][07:28:50.067][ERROR][RK0][tid #140035965826816]: replica 6 waits for coll ps creation barrier
[HCTR][07:28:50.071][ERROR][RK0][tid #140036234262272]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:28:50.072][ERROR][RK0][tid #140036234262272]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.074][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:28:50.074][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.076][ERROR][RK0][tid #140036234262272]: coll ps creation done
[HCTR][07:28:50.076][ERROR][RK0][tid #140036234262272]: replica 0 waits for coll ps creation barrier
[HCTR][07:28:50.078][ERROR][RK0][tid #140036032935680]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:28:50.078][ERROR][RK0][tid #140036032935680]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][07:28:50.082][ERROR][RK0][tid #140036032935680]: coll ps creation done
[HCTR][07:28:50.082][ERROR][RK0][tid #140036032935680]: replica 3 waits for coll ps creation barrier
[HCTR][07:28:50.083][ERROR][RK0][main]: coll ps creation done
[HCTR][07:28:50.083][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][07:28:50.083][ERROR][RK0][tid #140036234262272]: replica 0 preparing frequency
[HCTR][07:28:50.919][ERROR][RK0][tid #140036234262272]: replica 0 preparing frequency done
[HCTR][07:28:50.979][ERROR][RK0][tid #140036234262272]: replica 0 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][tid #140035965826816]: replica 6 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][tid #140036360087296]: replica 2 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][tid #140036032935680]: replica 3 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][tid #140035965826816]: replica 1 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][07:28:50.979][ERROR][RK0][tid #140036234262272]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][tid #140035965826816]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][tid #140036360087296]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][main]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][main]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][tid #140036032935680]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][tid #140035965826816]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][main]: Calling build_v2
[HCTR][07:28:50.979][ERROR][RK0][tid #140036234262272]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][tid #140035965826816]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][tid #140036360087296]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][tid #140036032935680]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][tid #140035965826816]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:28:50.979][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 07:28:50[[.[2022-12-12 07:28:502022-12-12 07:28:502022-12-12 07:28:502022-12-12 07:28:50[9796082022-12-12 07:28:50..2022-12-12 07:28:50..: .979624979631.979629979625E2022-12-12 07:28:50979635: : 979632: :  .: EE: EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc979651E  E  ::  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136]  :136136:136] using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136] ] 136] using concurrent impl MPS
] :using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS
using concurrent impl MPS136

using concurrent impl MPS

] 
using concurrent impl MPS
[2022-12-12 07:28:50.984059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:28:50.984100: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:28:50:.196984105] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:28:50.[9841562022-12-12 07:28:50: .E984154[ : 2022-12-12 07:28:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.: 984173196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :Eassigning 8 to cpu178 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[v100x8, slow pcie:2022-12-12 07:28:50
212.] [984207build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-12 07:28:50: 
2022-12-12 07:28:50[.E.2022-12-12 07:28:50984243 984249.[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 9842542022-12-12 07:28:50E[:E: . 2022-12-12 07:28:50178 E984288/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : :984299v100x8, slow pcie:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE196: 
212[: ] E] [2022-12-12 07:28:50178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 07:28:50.] :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:28:50
.984365v100x8, slow pcie213:.984395: 
] [178984405: E[remote time is 8.684212022-12-12 07:28:50[] : E 2022-12-12 07:28:50
.2022-12-12 07:28:50v100x8, slow pcieE /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.984478.[
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:984493: 9844992022-12-12 07:28:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[178: E: .:1962022-12-12 07:28:50] E E984544178] .v100x8, slow pcie /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : ] assigning 8 to cpu984574
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie
: :213: [
E212] 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:28:50[ ] remote time is 8.68421[] :.2022-12-12 07:28:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
2022-12-12 07:28:50assigning 8 to cpu214984687.:
.
[] : 9847171969847222022-12-12 07:28:50[cpu time is 97.0588E: ] : .2022-12-12 07:28:50
 [Eassigning 8 to cpuE984774./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:28:50 
 : 984802:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 196984834[:: E] : 2022-12-12 07:28:50212196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc assigning 8 to cpuE.] ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 984919build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 8 to cpu214:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 

] 213:Ecpu time is 97.0588] 212[ 
remote time is 8.68421] 2022-12-12 07:28:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[[[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.:2022-12-12 07:28:502022-12-12 07:28:502022-12-12 07:28:50
985060212...: ] [985079985072985093Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 07:28:50: : :  
.EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc985137:  [  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:28:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 :.::] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212985211212214remote time is 8.68421:] : ] ] 
213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588] [
 

remote time is 8.684212022-12-12 07:28:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.:[[985358[2132022-12-12 07:28:502022-12-12 07:28:50: 2022-12-12 07:28:50] ..E.remote time is 8.68421985403985401 985415
: : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: EE2022-12-12 07:28:50:E  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc985487:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 213cpu time is 97.0588:213E] 
214]  remote time is 8.68421] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588
:
214[] [2022-12-12 07:28:50cpu time is 97.05882022-12-12 07:28:50.
.985641985648: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 07:30:09.747000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:30:09.786929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 07:30:09.902184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:30:09.902247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:30:10. 66335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:30:10. 66374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:30:10. 66855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10. 66911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 67932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 68751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 81587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 07:30:10. 81641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 07:30:10. 82063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10. 82109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 82139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [5 solved2022-12-12 07:30:10
. 82161: E[ 2022-12-12 07:30:10/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.: 82217202: ] E6 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5[
2022-12-12 07:30:10. 82279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 07:30:10. 82686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10.[ 827372022-12-12 07:30:10: .E 82739 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10. 82852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 84075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 84509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 07:30:10. 84561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 07:30:10. 84585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 07:30:10. 84646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 07:30:10. 84974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10. 85024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 07:30:102022-12-12 07:30:10.. 85061 85058: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1815[1980] 2022-12-12 07:30:10] Building Coll Cache with ... num gpu device is 8.eager alloc mem 381.47 MB
 85119
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 07:30:10. 85184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 85763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 88333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 88485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 88524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 88608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 90015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 07:30:10. 90068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 07:30:10. 90108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 07:30:10. 90164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 07:30:10. 90475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10. 90521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 90566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:30:10. 90611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 92071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 92391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 93567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 93670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 95303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10. 95407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:30:10.150188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:30:10.155536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:30:10.155662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:30:10.156492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.157200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.158312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.158361: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:30:10.169335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:30:10.172786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:30:10.174767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:30:10.174872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:30:10.175678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.176717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.177829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.177879: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:30:10.178118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:30:10.178211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:30:10.179019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.179553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 07:30:10.179630: [E2022-12-12 07:30:10 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu179667:: 1980E]  eager alloc mem 5.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.180800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.180851: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[[[2022-12-12 07:30:102022-12-12 07:30:102022-12-12 07:30:10...182289182300182302: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes


[2022-12-12 07:30:10.184751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 07:30:102022-12-12 07:30:10..184826184841: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 07:30:10.184926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:30:10.186042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.186988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.188942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.189005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.189972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 07:30:10.190025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.190056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 07:30:10eager release cuda mem 400000000.[
1900752022-12-12 07:30:10: .W190086 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccE: 43/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :WORKER[0] alloc host memory 95.37 MB638
] eager release cuda mem 625663
[2022-12-12 07:30:10.190134: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 07:30:10:.638190166] : eager release cuda mem 5W
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:30:10.190220[: 2022-12-12 07:30:10E. 190244/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5:
638] eager release cuda mem 400000000
[2022-12-12 07:30:10.190319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:30:10.191123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.192031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.192541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:30:10.193562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.194489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:10.194605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 07:30:10
.194633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.194693: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:30:10.195562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.195609: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:30:10.195689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:10.195735: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 07:30:10.224021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.224659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.224701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.241686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.242309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.242354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.245268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.245884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.245926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.252864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.253495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.253538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.253670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.254280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.254323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.258835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.259442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.259485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.259719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.260326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.260368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 07:30:10.261143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:30:10.261746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:30:10.261787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:14........624640624639624637624637624639624638624637624637: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 6 init p2p of link 0Device 2 init p2p of link 1Device 4 init p2p of link 5Device 0 init p2p of link 3Device 7 init p2p of link 4Device 5 init p2p of link 6Device 3 init p2p of link 2







[[2022-12-12 07:30:14[[[[2022-12-12 07:30:14[[.2022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:142022-12-12 07:30:14.2022-12-12 07:30:142022-12-12 07:30:14625207....625207..: 625208625208625209625212: 625219625221E: : : : E: :  EEEE EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::::1980::] 1980198019801980] 19801980eager alloc mem 611.00 KB] ] ] ] eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB





[2022-12-12 07:30:14.626281: [E2022-12-12 07:30:14 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc626293:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[[2022-12-12 07:30:142022-12-12 07:30:14[[..2022-12-12 07:30:142022-12-12 07:30:14[626450626453[..2022-12-12 07:30:14: : 2022-12-12 07:30:14626459626459.EE.: : 626469  626472EE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:   E::E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638638 ::638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] :eager release cuda mem 625663eager release cuda mem 625663:] eager release cuda mem 625663638

638eager release cuda mem 625663
] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 07:30:14.639967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:30:14.640124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.640267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:30:14.640406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.641097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.641357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.649446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 07:30:14.649622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.649786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 07:30:14.649857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 07:30:14.649957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.650032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.650087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:30:14.[650269[2022-12-12 07:30:14: 2022-12-12 07:30:14.E.650263 650270: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E:E 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB:1926
1926] ] Device 0 init p2p of link 6Device 2 init p2p of link 3

[2022-12-12 07:30:14.650420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.[6504822022-12-12 07:30:14: .E650487 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.650929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.650967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.651256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.651295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.651476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.654284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 07:30:14.654411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.654586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 07:30:14.654709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.655346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.655633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.662916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 07:30:14.663055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.664008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.664409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:30:14.664557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.665543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.672013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:30:14.672158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.672384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 07:30:14.672535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.672563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 07:30:14.672716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.673091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.673315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.673503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.673627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 07:30:14.673760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.674694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.680826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:30:14.680946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.681155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:30:14.681274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.681904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.682230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.684429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:30:14.684556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.684809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:30:14.684926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.685508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.685872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.687236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:30:14.687364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.687604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 07:30:14.687731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.688311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.688677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.698437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 07:30:14.698566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.699343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.699503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:30:14.699637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:30:14.700413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:30:14.702980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.703259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.703851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62112 secs 
[2022-12-12 07:30:14.704199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61919 secs 
[2022-12-12 07:30:14.704482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.704933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61442 secs 
[2022-12-12 07:30:14.705134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.706786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62469 secs 
[2022-12-12 07:30:14.710505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.710975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.712607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.713046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:30:14.713267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.63043 secs 
[2022-12-12 07:30:14.713478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62831 secs 
[2022-12-12 07:30:14.714493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62389 secs 
[2022-12-12 07:30:14.714938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~30.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64804 secs 
[2022-12-12 07:30:14.716367: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.56 GB
[2022-12-12 07:30:16.128505: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.83 GB
[2022-12-12 07:30:16.128955: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.83 GB
[2022-12-12 07:30:16.129885: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.83 GB
[2022-12-12 07:30:17.557626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.09 GB
[2022-12-12 07:30:17.558405: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.09 GB
[2022-12-12 07:30:17.559673: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.09 GB
[2022-12-12 07:30:18.845219: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.30 GB
[2022-12-12 07:30:18.845571: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.30 GB
[2022-12-12 07:30:18.847047: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.30 GB
[2022-12-12 07:30:19.868111: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.52 GB
[2022-12-12 07:30:19.868923: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.52 GB
[2022-12-12 07:30:19.869888: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.52 GB
[2022-12-12 07:30:20.958351: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.98 GB
[2022-12-12 07:30:20.958755: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.98 GB
[2022-12-12 07:30:20.959063: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.98 GB
[2022-12-12 07:30:22.610471: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.18 GB
[2022-12-12 07:30:22.610622: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.18 GB
[HCTR][07:30:24.384][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][tid #140036032935680]: replica 3 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][tid #140036234262272]: replica 0 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][tid #140036360087296]: replica 2 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][tid #140035965826816]: replica 1 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][tid #140035965826816]: replica 6 calling init per replica done, doing barrier
[HCTR][07:30:24.384][ERROR][RK0][tid #140035965826816]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][tid #140036032935680]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][tid #140035965826816]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][tid #140036360087296]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][tid #140036234262272]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:30:24.384][ERROR][RK0][tid #140035965826816]: init per replica done
[HCTR][07:30:24.384][ERROR][RK0][main]: init per replica done
[HCTR][07:30:24.384][ERROR][RK0][tid #140036032935680]: init per replica done
[HCTR][07:30:24.384][ERROR][RK0][tid #140035965826816]: init per replica done
[HCTR][07:30:24.384][ERROR][RK0][main]: init per replica done
[HCTR][07:30:24.384][ERROR][RK0][main]: init per replica done
[HCTR][07:30:24.384][ERROR][RK0][tid #140036360087296]: init per replica done
[HCTR][07:30:24.387][ERROR][RK0][tid #140036234262272]: init per replica done
[HCTR][07:30:24.390][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f461b920000
[HCTR][07:30:24.390][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4082000000
[HCTR][07:30:24.390][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4082640000
[HCTR][07:30:24.390][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4082960000
[HCTR][07:30:24.390][ERROR][RK0][tid #140035965826816]: 6 allocated 3276800 at 0x7f4617920000
[HCTR][07:30:24.390][ERROR][RK0][tid #140035965826816]: 6 allocated 6553600 at 0x7f3fde000000
[HCTR][07:30:24.390][ERROR][RK0][tid #140035965826816]: 6 allocated 3276800 at 0x7f3fde640000
[HCTR][07:30:24.390][ERROR][RK0][tid #140035965826816]: 6 allocated 6553600 at 0x7f3fde960000
[HCTR][07:30:24.390][ERROR][RK0][tid #140036360087296]: 2 allocated 3276800 at 0x7f4617920000
[HCTR][07:30:24.390][ERROR][RK0][tid #140036360087296]: 2 allocated 6553600 at 0x7f416c000000
[HCTR][07:30:24.390][ERROR][RK0][tid #140036360087296]: 2 allocated 3276800 at 0x7f416c640000
[HCTR][07:30:24.390][ERROR][RK0][tid #140036360087296]: 2 allocated 6553600 at 0x7f416c960000
[HCTR][07:30:24.390][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f4617920000
[HCTR][07:30:24.391][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f40e2000000
[HCTR][07:30:24.391][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f40e2640000
[HCTR][07:30:24.391][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f40e2960000
[HCTR][07:30:24.391][ERROR][RK0][tid #140035965826816]: 4 allocated 3276800 at 0x7f4617920000
[HCTR][07:30:24.391][ERROR][RK0][tid #140035965826816]: 4 allocated 6553600 at 0x7f408a000000
[HCTR][07:30:24.391][ERROR][RK0][tid #140035965826816]: 4 allocated 3276800 at 0x7f408a640000
[HCTR][07:30:24.391][ERROR][RK0][tid #140035965826816]: 4 allocated 6553600 at 0x7f408a960000
[HCTR][07:30:24.391][ERROR][RK0][tid #140036301371136]: 5 allocated 3276800 at 0x7f4617920000
[HCTR][07:30:24.391][ERROR][RK0][tid #140036301371136]: 5 allocated 6553600 at 0x7f4166000000
[HCTR][07:30:24.391][ERROR][RK0][tid #140036301371136]: 5 allocated 3276800 at 0x7f4166640000
[HCTR][07:30:24.391][ERROR][RK0][tid #140036301371136]: 5 allocated 6553600 at 0x7f4166960000
[HCTR][07:30:24.391][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f4613920000
[HCTR][07:30:24.391][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f416a000000
[HCTR][07:30:24.391][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f416a640000
[HCTR][07:30:24.391][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f416a960000
[HCTR][07:30:24.393][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f40ee320000
[HCTR][07:30:24.393][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f43d9200000
[HCTR][07:30:24.393][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f40eed0e800
[HCTR][07:30:24.393][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f40ef02e800








