2022-12-12 05:42:42.107588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.115194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.119415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.126027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.130448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.142701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.149033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.160184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.218271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.220703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.222626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.224695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.226429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.228187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.229076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.229252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.230747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.230772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.232724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.232896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.234560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.235005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.236273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.236618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.237939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.238404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.239415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.240359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.241543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.242633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.243517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.244993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.246850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.247886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.248860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.250240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.251982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.253692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.254238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.254961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.255883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.256502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.257535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.259368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.260638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.261678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.262746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.262777: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.263876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.270994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.272456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.272519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.274846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.275325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.277206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.278307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.280449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.280802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.281053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.283117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.283603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.283724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.286069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.286766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.286820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.289277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.289830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.290591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.290772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.292219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.293236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.294115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.294995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.295081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.296647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.297352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.297726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.298375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.299210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.300479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.300643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.301441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.301964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.302571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.303944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.304006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.305113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.306131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.306890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.307170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.307981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.309216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.310585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.311224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.312148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.312940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.313544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.314493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.315523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.315922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.316594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.343725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.349277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.352517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.353633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.353885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.354089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.354523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.355791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.356219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.358376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.358748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.358862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.359192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.360285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.360498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.363867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.364176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.364252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.366214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.366397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.367908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.368096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.368297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.369773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.369817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.371983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.372220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.372358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.373331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.373465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.375617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.375995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.376185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.377271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.377362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.379090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.379319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.379414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.380774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.380955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.382865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.383001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.383996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.384155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.384251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.386323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.386382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.387681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.387697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.387789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.389534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.389684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.391263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.391396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.391402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.393035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.393132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.394542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.394716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.394771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.396508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.396555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.398122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.398393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.398503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.400484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.400596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.401549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.402160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.402672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.402676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.405136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.405568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.406349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.407009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.407561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.407614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.407862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.410158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.410575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.411171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.411956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.413216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.413358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.413958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.416655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.417489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.418093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.419997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.420299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.420382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.421918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.422009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.422375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.423894: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.424047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.424717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.424876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.426467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.426686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.426825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.428815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.429260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.429506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.431161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.431346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.431430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.432356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.433936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.434380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.434710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.436961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.437118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.437206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.437676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.439238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.439674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.440235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.442245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.442527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.442528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.442972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.444163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.444968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.445486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.447540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.447932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.449301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.449727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.450858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.451674: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.452138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.452393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.453811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.454301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.455758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.456900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.457279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.458414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.460011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.461571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.462166: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.462500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.463450: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.464265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.464745: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.465498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.466351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.467872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.469030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.469971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.470542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.471823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.473114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.473158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.474112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.475115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.476836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.478277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.478997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.479033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.480305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.481974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.483386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.484311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.485615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.489116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.490409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.495413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.497281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.530492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.532190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.564838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.566266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.570738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.572553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.578281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.579441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.587743: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.596030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.599734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.606927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.610279: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:42:42.611818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.618425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.651109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:42.659723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.680640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.681978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.683459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.684425: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:43.684484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:42:43.702850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.704404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.705275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.705942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.706459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.706937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:42:43.753476: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:43.753683: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:43.784500: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:42:43.898925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.902280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.903710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.904720: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:43.904782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:42:43.916714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.917328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.917860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.918526: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:43.918595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:42:43.923332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.924039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.924546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.925309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.925834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.926308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:42:43.936520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.937176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.937681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.938645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.939201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.939682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:42:43.942473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.943302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.943836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.944292: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:43.944343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:42:43.961752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.962839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.964023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.965083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.966023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.967373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:42:43.967411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.968547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.969581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.970550: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:43.970615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:42:43.973731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.974813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.975817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.976740: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:43.976789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:42:43.988753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.990890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.991902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.993033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.994025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.995074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:42:43.995116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.996265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.997443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.998574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:43.999606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.000512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:42:44.002324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.003598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.004997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.006068: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:44.006125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:42:44.011726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.012894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.013086: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.013264: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.013682: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.013907: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.013985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.015176: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:42:44.015319: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:42:44.015376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:42:44.015767: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:42:44.024658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.026872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.028170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.029306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.030332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.031865: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.032007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:42:44.032018: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.033843: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:42:44.034396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.035030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.035557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.036144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.036655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:42:44.037128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:42:44.042605: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.042785: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.044855: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:42:44.046321: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.046459: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.048243: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:42:44.078947: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.079165: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.081100: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 05:42:44.082664: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.082855: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:42:44.084603: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:42:45.364][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 94it [00:01, 79.41it/s]warmup run: 100it [00:01, 83.34it/s]warmup run: 191it [00:01, 175.48it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 201it [00:01, 182.01it/s]warmup run: 288it [00:01, 281.56it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 73it [00:01, 62.21it/s]warmup run: 101it [00:01, 87.51it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 302it [00:01, 291.24it/s]warmup run: 385it [00:01, 391.33it/s]warmup run: 99it [00:01, 86.69it/s]warmup run: 101it [00:01, 86.23it/s]warmup run: 160it [00:01, 150.16it/s]warmup run: 199it [00:01, 186.04it/s]warmup run: 99it [00:01, 85.91it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 403it [00:01, 405.26it/s]warmup run: 485it [00:02, 503.80it/s]warmup run: 196it [00:01, 185.12it/s]warmup run: 202it [00:01, 186.78it/s]warmup run: 248it [00:01, 248.06it/s]warmup run: 283it [00:01, 274.87it/s]warmup run: 198it [00:01, 185.80it/s]warmup run: 100it [00:01, 88.52it/s]warmup run: 505it [00:02, 518.60it/s]warmup run: 588it [00:02, 613.28it/s]warmup run: 293it [00:01, 292.65it/s]warmup run: 304it [00:01, 298.63it/s]warmup run: 337it [00:01, 350.91it/s]warmup run: 371it [00:01, 373.35it/s]warmup run: 297it [00:01, 295.34it/s]warmup run: 200it [00:01, 191.10it/s]warmup run: 608it [00:02, 625.10it/s]warmup run: 688it [00:02, 702.05it/s]warmup run: 392it [00:01, 406.41it/s]warmup run: 406it [00:01, 414.52it/s]warmup run: 425it [00:02, 449.43it/s]warmup run: 467it [00:02, 483.35it/s]warmup run: 396it [00:01, 408.37it/s]warmup run: 301it [00:01, 304.40it/s]warmup run: 711it [00:02, 716.61it/s]warmup run: 791it [00:02, 781.43it/s]warmup run: 490it [00:01, 515.07it/s]warmup run: 508it [00:02, 527.36it/s]warmup run: 513it [00:02, 539.89it/s]warmup run: 569it [00:02, 595.90it/s]warmup run: 496it [00:02, 519.26it/s]warmup run: 400it [00:01, 417.35it/s]warmup run: 812it [00:02, 787.26it/s]warmup run: 893it [00:02, 843.39it/s]warmup run: 589it [00:02, 616.16it/s]warmup run: 611it [00:02, 633.19it/s]warmup run: 601it [00:02, 618.13it/s]warmup run: 666it [00:02, 682.42it/s]warmup run: 597it [00:02, 621.92it/s]warmup run: 499it [00:01, 525.56it/s]warmup run: 912it [00:02, 842.22it/s]warmup run: 994it [00:02, 886.71it/s]warmup run: 688it [00:02, 702.69it/s]warmup run: 714it [00:02, 724.23it/s]warmup run: 688it [00:02, 679.31it/s]warmup run: 767it [00:02, 762.57it/s]warmup run: 697it [00:02, 709.44it/s]warmup run: 597it [00:02, 622.47it/s]warmup run: 1012it [00:02, 883.80it/s]warmup run: 1094it [00:02, 914.89it/s]warmup run: 789it [00:02, 778.33it/s]warmup run: 816it [00:02, 796.77it/s]warmup run: 775it [00:02, 727.74it/s]warmup run: 869it [00:02, 828.71it/s]warmup run: 795it [00:02, 775.50it/s]warmup run: 695it [00:02, 705.74it/s]warmup run: 1112it [00:02, 910.72it/s]warmup run: 890it [00:02, 839.03it/s]warmup run: 1194it [00:02, 933.73it/s]warmup run: 918it [00:02, 854.52it/s]warmup run: 861it [00:02, 763.38it/s]warmup run: 970it [00:02, 877.69it/s]warmup run: 892it [00:02, 826.42it/s]warmup run: 791it [00:02, 769.34it/s]warmup run: 1212it [00:02, 935.64it/s]warmup run: 989it [00:02, 875.52it/s]warmup run: 1293it [00:02, 945.62it/s]warmup run: 1021it [00:02, 901.42it/s]warmup run: 948it [00:02, 791.18it/s]warmup run: 1073it [00:02, 918.48it/s]warmup run: 991it [00:02, 869.64it/s]warmup run: 887it [00:02, 812.72it/s]warmup run: 1312it [00:02, 953.65it/s]warmup run: 1089it [00:02, 908.23it/s]warmup run: 1392it [00:02, 953.80it/s]warmup run: 1126it [00:02, 942.80it/s]warmup run: 1034it [00:02, 809.77it/s]warmup run: 1174it [00:02, 943.94it/s]warmup run: 1089it [00:02, 895.29it/s]warmup run: 982it [00:02, 845.51it/s]warmup run: 1412it [00:02, 921.38it/s]warmup run: 1191it [00:02, 937.44it/s]warmup run: 1491it [00:03, 960.23it/s]warmup run: 1231it [00:02, 971.32it/s]warmup run: 1120it [00:02, 824.02it/s]warmup run: 1274it [00:02, 959.19it/s]warmup run: 1186it [00:02, 916.19it/s]warmup run: 1079it [00:02, 878.05it/s]warmup run: 1291it [00:02, 954.77it/s]warmup run: 1590it [00:03, 968.41it/s]warmup run: 1334it [00:02, 987.86it/s]warmup run: 1508it [00:03, 875.86it/s]warmup run: 1207it [00:02, 837.25it/s]warmup run: 1375it [00:02, 973.65it/s]warmup run: 1288it [00:02, 944.97it/s]warmup run: 1178it [00:02, 907.67it/s]warmup run: 1392it [00:02, 969.06it/s]warmup run: 1689it [00:03, 968.56it/s]warmup run: 1438it [00:02, 1002.90it/s]warmup run: 1610it [00:03, 914.67it/s]warmup run: 1477it [00:03, 985.72it/s]warmup run: 1294it [00:03, 837.85it/s]warmup run: 1392it [00:02, 970.23it/s]warmup run: 1275it [00:02, 923.75it/s]warmup run: 1494it [00:02, 981.27it/s]warmup run: 1543it [00:03, 1015.24it/s]warmup run: 1787it [00:03, 956.00it/s]warmup run: 1713it [00:03, 945.61it/s]warmup run: 1578it [00:03, 991.58it/s]warmup run: 1381it [00:03, 845.76it/s]warmup run: 1494it [00:03, 984.09it/s]warmup run: 1372it [00:02, 937.03it/s]warmup run: 1596it [00:03, 990.80it/s]warmup run: 1648it [00:03, 1025.50it/s]warmup run: 1816it [00:03, 968.81it/s]warmup run: 1679it [00:03, 995.43it/s]warmup run: 1468it [00:03, 852.86it/s]warmup run: 1599it [00:03, 1000.80it/s]warmup run: 1884it [00:03, 885.25it/s]warmup run: 1470it [00:02, 947.60it/s]warmup run: 1699it [00:03, 999.49it/s]warmup run: 1753it [00:03, 1030.75it/s]warmup run: 1919it [00:03, 986.45it/s]warmup run: 1780it [00:03, 997.47it/s]warmup run: 1555it [00:03, 857.61it/s]warmup run: 1701it [00:03, 989.92it/s] warmup run: 1984it [00:03, 916.07it/s]warmup run: 1567it [00:03, 953.51it/s]warmup run: 1801it [00:03, 1005.01it/s]warmup run: 1858it [00:03, 1035.42it/s]warmup run: 2024it [00:03, 1003.16it/s]warmup run: 1646it [00:03, 872.93it/s]warmup run: 1883it [00:03, 1005.26it/s]warmup run: 1801it [00:03, 990.15it/s]warmup run: 2100it [00:03, 984.56it/s]warmup run: 1666it [00:03, 962.87it/s]warmup run: 1905it [00:03, 1013.50it/s]warmup run: 1963it [00:03, 1035.86it/s]warmup run: 2144it [00:03, 1058.64it/s]warmup run: 1741it [00:03, 894.50it/s]warmup run: 1985it [00:03, 993.73it/s] warmup run: 1901it [00:03, 991.21it/s]warmup run: 2220it [00:03, 1044.77it/s]warmup run: 1766it [00:03, 973.49it/s]warmup run: 2010it [00:03, 1023.55it/s]warmup run: 2077it [00:03, 1066.19it/s]warmup run: 2264it [00:03, 1098.61it/s]warmup run: 1837it [00:03, 914.02it/s]warmup run: 2102it [00:03, 1044.88it/s]warmup run: 2001it [00:03, 992.91it/s]warmup run: 2341it [00:03, 1091.40it/s]warmup run: 1866it [00:03, 980.19it/s]warmup run: 2130it [00:03, 1074.39it/s]warmup run: 2198it [00:03, 1108.70it/s]warmup run: 2385it [00:03, 1130.53it/s]warmup run: 1931it [00:03, 921.48it/s]warmup run: 2223it [00:03, 1092.88it/s]warmup run: 2121it [00:03, 1051.76it/s]warmup run: 2462it [00:03, 1125.82it/s]warmup run: 1965it [00:03, 982.38it/s]warmup run: 2250it [00:03, 1110.81it/s]warmup run: 2319it [00:03, 1138.60it/s]warmup run: 2506it [00:04, 1153.29it/s]warmup run: 2031it [00:03, 943.66it/s]warmup run: 2344it [00:03, 1127.23it/s]warmup run: 2241it [00:03, 1094.85it/s]warmup run: 2582it [00:04, 1146.76it/s]warmup run: 2077it [00:03, 1021.67it/s]warmup run: 2370it [00:03, 1136.99it/s]warmup run: 2440it [00:03, 1159.87it/s]warmup run: 2627it [00:04, 1168.88it/s]warmup run: 2142it [00:03, 991.40it/s]warmup run: 2465it [00:03, 1150.77it/s]warmup run: 2361it [00:03, 1125.70it/s]warmup run: 2703it [00:04, 1162.80it/s]warmup run: 2197it [00:03, 1073.66it/s]warmup run: 2490it [00:03, 1155.62it/s]warmup run: 2562it [00:03, 1175.19it/s]warmup run: 2748it [00:04, 1179.45it/s]warmup run: 2252it [00:04, 1023.66it/s]warmup run: 2586it [00:04, 1166.59it/s]warmup run: 2481it [00:03, 1147.43it/s]warmup run: 2317it [00:03, 1110.01it/s]warmup run: 2820it [00:04, 1139.13it/s]warmup run: 2610it [00:03, 1167.06it/s]warmup run: 2684it [00:04, 1185.95it/s]warmup run: 2867it [00:04, 1182.51it/s]warmup run: 2362it [00:04, 1045.41it/s]warmup run: 2707it [00:04, 1178.66it/s]warmup run: 2601it [00:04, 1161.74it/s]warmup run: 2437it [00:03, 1135.16it/s]warmup run: 2938it [00:04, 1149.04it/s]warmup run: 2728it [00:04, 1170.62it/s]warmup run: 3000it [00:04, 672.22it/s] warmup run: 2803it [00:04, 1178.07it/s]warmup run: 2987it [00:04, 1185.55it/s]warmup run: 2471it [00:04, 1057.91it/s]warmup run: 2825it [00:04, 1178.25it/s]warmup run: 2720it [00:04, 1168.83it/s]warmup run: 3000it [00:04, 676.41it/s] warmup run: 2557it [00:03, 1152.40it/s]warmup run: 2846it [00:04, 1169.19it/s]warmup run: 2924it [00:04, 1185.34it/s]warmup run: 2583it [00:04, 1075.74it/s]warmup run: 2945it [00:04, 1184.40it/s]warmup run: 2839it [00:04, 1174.26it/s]warmup run: 2676it [00:04, 1162.81it/s]warmup run: 3000it [00:04, 696.36it/s] warmup run: 3000it [00:04, 686.52it/s] warmup run: 2700it [00:04, 1103.76it/s]warmup run: 2963it [00:04, 1066.93it/s]warmup run: 2958it [00:04, 1178.86it/s]warmup run: 2794it [00:04, 1165.43it/s]warmup run: 3000it [00:04, 686.51it/s] warmup run: 3000it [00:04, 688.31it/s] warmup run: 2816it [00:04, 1119.26it/s]warmup run: 2913it [00:04, 1170.98it/s]warmup run: 3000it [00:04, 687.74it/s] warmup run: 2933it [00:04, 1134.25it/s]warmup run: 3000it [00:04, 637.38it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.86it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.00it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.27it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1581.08it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.16it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.50it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1602.08it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.84it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1612.45it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1653.08it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1591.14it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1624.87it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.19it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1628.83it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.34it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1624.58it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1626.50it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1592.37it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.21it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1629.90it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1620.95it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1608.58it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1623.35it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1645.65it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1627.85it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1632.59it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1590.53it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1609.84it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1640.57it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1618.64it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1643.62it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1613.31it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1622.29it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1609.61it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1631.40it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1618.51it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1587.25it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1638.13it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1638.22it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1613.49it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1621.65it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1630.01it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1615.83it/s]warmup should be done:  32%|███▏      | 958/3000 [00:00<00:01, 1584.02it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1598.96it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1611.71it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1616.34it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1615.29it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1619.02it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1626.52it/s]warmup should be done:  38%|███▊      | 1129/3000 [00:00<00:01, 1596.95it/s]warmup should be done:  37%|███▋      | 1117/3000 [00:00<00:01, 1579.09it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1606.58it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1609.82it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1618.69it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1610.53it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1618.82it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1626.29it/s]warmup should be done:  43%|████▎     | 1290/3000 [00:00<00:01, 1599.13it/s]warmup should be done:  42%|████▎     | 1275/3000 [00:00<00:01, 1578.44it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1608.77it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1622.81it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1587.09it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1606.09it/s]warmup should be done:  49%|████▉     | 1465/3000 [00:00<00:00, 1618.05it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1625.13it/s]warmup should be done:  48%|████▊     | 1433/3000 [00:00<00:00, 1577.72it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1610.75it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1628.13it/s]warmup should be done:  48%|████▊     | 1450/3000 [00:00<00:00, 1575.90it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1613.59it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1583.70it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1618.94it/s]warmup should be done:  53%|█████▎    | 1591/3000 [00:01<00:00, 1578.24it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1626.91it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1610.38it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1631.04it/s]warmup should be done:  54%|█████▍    | 1613/3000 [00:01<00:00, 1590.91it/s]warmup should be done:  54%|█████▍    | 1620/3000 [00:01<00:00, 1592.15it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1599.97it/s]warmup should be done:  60%|█████▉    | 1791/3000 [00:01<00:00, 1619.70it/s]warmup should be done:  58%|█████▊    | 1749/3000 [00:01<00:00, 1578.44it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1627.87it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1612.12it/s]warmup should be done:  59%|█████▉    | 1776/3000 [00:01<00:00, 1602.13it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1626.32it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1593.63it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1590.10it/s]warmup should be done:  65%|██████▌   | 1954/3000 [00:01<00:00, 1619.97it/s]warmup should be done:  64%|██████▎   | 1908/3000 [00:01<00:00, 1579.36it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1629.06it/s]warmup should be done:  65%|██████▍   | 1947/3000 [00:01<00:00, 1607.66it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1608.22it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1598.51it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1620.94it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1587.31it/s]warmup should be done:  71%|███████   | 2117/3000 [00:01<00:00, 1620.03it/s]warmup should be done:  69%|██████▉   | 2067/3000 [00:01<00:00, 1579.71it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1629.39it/s]warmup should be done:  70%|███████   | 2109/3000 [00:01<00:00, 1610.02it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1613.57it/s]warmup should be done:  70%|███████   | 2103/3000 [00:01<00:00, 1602.33it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1616.24it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1600.56it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1579.45it/s]warmup should be done:  76%|███████▌  | 2280/3000 [00:01<00:00, 1619.65it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1629.35it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1610.12it/s]warmup should be done:  76%|███████▌  | 2265/3000 [00:01<00:00, 1616.86it/s]warmup should be done:  75%|███████▌  | 2264/3000 [00:01<00:00, 1603.67it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1610.89it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1608.01it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1575.86it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1625.98it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1612.26it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1607.97it/s]warmup should be done:  81%|████████  | 2425/3000 [00:01<00:00, 1601.24it/s]warmup should be done:  81%|████████  | 2427/3000 [00:01<00:00, 1603.54it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1605.29it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1609.46it/s]warmup should be done:  85%|████████▍ | 2541/3000 [00:01<00:00, 1575.88it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1626.14it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1607.31it/s]warmup should be done:  87%|████████▋ | 2604/3000 [00:01<00:00, 1595.58it/s]warmup should be done:  86%|████████▋ | 2589/3000 [00:01<00:00, 1610.24it/s]warmup should be done:  86%|████████▋ | 2590/3000 [00:01<00:00, 1609.55it/s]warmup should be done:  87%|████████▋ | 2610/3000 [00:01<00:00, 1607.40it/s]warmup should be done:  87%|████████▋ | 2622/3000 [00:01<00:00, 1588.90it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1577.14it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1627.47it/s]warmup should be done:  92%|█████████▏| 2756/3000 [00:01<00:00, 1610.04it/s]warmup should be done:  92%|█████████▏| 2764/3000 [00:01<00:00, 1596.80it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1616.65it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1614.77it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1613.26it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1589.79it/s]warmup should be done:  98%|█████████▊| 2947/3000 [00:01<00:00, 1633.77it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1581.26it/s]warmup should be done:  97%|█████████▋| 2919/3000 [00:01<00:00, 1614.61it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1600.98it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1625.34it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1622.43it/s]warmup should be done:  98%|█████████▊| 2938/3000 [00:01<00:00, 1623.37it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1597.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1615.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1614.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1613.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1611.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1608.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1581.31it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1668.88it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.75it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.77it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.58it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.84it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.52it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.61it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.48it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1668.08it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1659.07it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1696.35it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1632.51it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1673.29it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1668.66it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1660.44it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1657.95it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1670.73it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1665.10it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1698.34it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.41it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1683.26it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1671.61it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1658.33it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1661.33it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1674.04it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1636.17it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1693.75it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1701.66it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1676.42it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1661.27it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1661.30it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1675.40it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1698.85it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1703.55it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1671.89it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1634.43it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1683.56it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1663.72it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1654.55it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1673.61it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1700.24it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1672.24it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1702.46it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1684.93it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1632.45it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1661.70it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1641.13it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1671.73it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1634.76it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1684.15it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1698.82it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1669.80it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1699.33it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1659.70it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1634.84it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1702.24it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1673.07it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1687.85it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1673.68it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1701.54it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1631.54it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1660.97it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1636.02it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1703.21it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1673.10it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1633.72it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1679.69it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1683.55it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1701.01it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1661.17it/s]warmup should be done:  50%|████▉     | 1494/3000 [00:00<00:00, 1632.38it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1703.70it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1674.09it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1637.01it/s]warmup should be done:  56%|█████▌    | 1684/3000 [00:01<00:00, 1686.23it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1682.15it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1700.88it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1664.27it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1633.29it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1704.43it/s]warmup should be done:  61%|██████▏   | 1844/3000 [00:01<00:00, 1674.25it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1638.18it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1690.60it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1701.69it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1680.61it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1666.10it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1637.75it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1705.00it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1673.46it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1635.62it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1702.90it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1679.97it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1664.30it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1685.36it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1626.29it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1704.28it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1672.31it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1635.85it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1701.87it/s]warmup should be done:  73%|███████▎  | 2196/3000 [00:01<00:00, 1683.32it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1662.51it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1679.44it/s]warmup should be done:  72%|███████▏  | 2152/3000 [00:01<00:00, 1630.52it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1703.74it/s]warmup should be done:  78%|███████▊  | 2348/3000 [00:01<00:00, 1673.04it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1632.46it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1687.66it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1663.89it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1696.90it/s]warmup should be done:  79%|███████▊  | 2362/3000 [00:01<00:00, 1661.79it/s]warmup should be done:  77%|███████▋  | 2317/3000 [00:01<00:00, 1636.28it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1674.66it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1702.07it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1693.83it/s]warmup should be done:  82%|████████▏ | 2462/3000 [00:01<00:00, 1631.90it/s]warmup should be done:  84%|████████▎ | 2508/3000 [00:01<00:00, 1670.71it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1691.91it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1659.73it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1635.63it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1702.97it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1672.18it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1698.22it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1644.94it/s]warmup should be done:  89%|████████▉ | 2677/3000 [00:01<00:00, 1675.41it/s]warmup should be done:  91%|█████████ | 2732/3000 [00:01<00:00, 1688.18it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1656.86it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1637.48it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1702.71it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1670.99it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1698.83it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1655.68it/s]warmup should be done:  95%|█████████▍| 2846/3000 [00:01<00:00, 1676.99it/s]warmup should be done:  97%|█████████▋| 2901/3000 [00:01<00:00, 1684.54it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1652.56it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1634.41it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.45it/s]warmup should be done:  99%|█████████▉| 2968/3000 [00:01<00:00, 1665.54it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1638.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.32it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f94439800d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9443ca6d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9443ca4730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9443ca3e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f944397f2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f944397f1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9443971190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f94439710d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 05:44:16.177396: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f7a82fb20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:16.177453: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:16.187668: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:16.227795: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f7a830540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:16.227860: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:16.230306: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f72833e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:16.230351: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:16.237865: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:16.238398: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:16.327073: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f730303c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:16.327142: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:16.334813: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:16.982876: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f7302d2e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:16.982948: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:16.992365: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:16.996695: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f7b028cd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:16.996740: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:17.006808: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:17.016379: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f7b030a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:17.016444: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:17.025953: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:17.028647: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8f72f91ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:44:17.028692: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:44:17.036507: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:44:23.331493: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:23.438048: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:23.475761: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:23.577026: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:23.780147: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:23.905503: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:24.076109: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:44:24.076771: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:45:14.374][ERROR][RK0][tid #140255260808960]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:45:14.375][ERROR][RK0][tid #140255260808960]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.383][ERROR][RK0][tid #140255260808960]: coll ps creation done
[HCTR][05:45:14.383][ERROR][RK0][tid #140255260808960]: replica 7 waits for coll ps creation barrier
[HCTR][05:45:14.451][ERROR][RK0][tid #140254304532224]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:45:14.451][ERROR][RK0][tid #140254304532224]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.456][ERROR][RK0][tid #140254304532224]: coll ps creation done
[HCTR][05:45:14.456][ERROR][RK0][tid #140254304532224]: replica 5 waits for coll ps creation barrier
[HCTR][05:45:14.550][ERROR][RK0][tid #140254791046912]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:45:14.551][ERROR][RK0][tid #140254791046912]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.556][ERROR][RK0][tid #140254791046912]: coll ps creation done
[HCTR][05:45:14.556][ERROR][RK0][tid #140254791046912]: replica 4 waits for coll ps creation barrier
[HCTR][05:45:14.630][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:45:14.630][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.638][ERROR][RK0][main]: coll ps creation done
[HCTR][05:45:14.638][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:45:14.741][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:45:14.741][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.743][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:45:14.743][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.746][ERROR][RK0][main]: coll ps creation done
[HCTR][05:45:14.746][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][05:45:14.746][ERROR][RK0][main]: coll ps creation done
[HCTR][05:45:14.746][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][05:45:14.778][ERROR][RK0][tid #140255260808960]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:45:14.778][ERROR][RK0][tid #140255260808960]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.784][ERROR][RK0][tid #140254304532224]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:45:14.784][ERROR][RK0][tid #140254304532224]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:45:14.785][ERROR][RK0][tid #140255260808960]: coll ps creation done
[HCTR][05:45:14.785][ERROR][RK0][tid #140255260808960]: replica 3 waits for coll ps creation barrier
[HCTR][05:45:14.791][ERROR][RK0][tid #140254304532224]: coll ps creation done
[HCTR][05:45:14.791][ERROR][RK0][tid #140254304532224]: replica 2 waits for coll ps creation barrier
[HCTR][05:45:14.791][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][05:45:15.653][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][05:45:15.697][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][tid #140255260808960]: replica 3 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][tid #140254791046912]: replica 4 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][tid #140254304532224]: replica 2 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][tid #140255260808960]: replica 7 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][tid #140254304532224]: replica 5 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:45:15.697][ERROR][RK0][main]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140255260808960]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140254791046912]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140254304532224]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140255260808960]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140254304532224]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140254304532224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][main]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][main]: Calling build_v2
[HCTR][05:45:15.697][ERROR][RK0][tid #140255260808960]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][tid #140254791046912]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][tid #140255260808960]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][tid #140254304532224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:45:15.697][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 05:45:152022-12-12 05:45:15.[[[2022-12-12 05:45:15.2022-12-12 05:45:156974812022-12-12 05:45:15[.2022-12-12 05:45:156974822022-12-12 05:45:15.: .697483.: .697477E6975002022-12-12 05:45:15: 697500E697502:  : .E:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE697527 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] : 136:] :136using concurrent impl MPS136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] 136using concurrent impl MPS136] 
] :using concurrent impl MPS] 
] using concurrent impl MPSusing concurrent impl MPS136
using concurrent impl MPSusing concurrent impl MPS

] 

using concurrent impl MPS
[2022-12-12 05:45:15.701844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:45:15.701880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 05:45:15196.] 701891assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:45:15.701938[: 2022-12-12 05:45:15E.[ 7019382022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .:E701952196 : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu: [
178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:45:15] :.v100x8, slow pcie[212701981
2022-12-12 05:45:15] : .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[702018
[ 2022-12-12 05:45:15: 2022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E[.:702032 2022-12-12 05:45:15702031[178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 2022-12-12 05:45:15] E:702062E.v100x8, slow pcie 212:  [702075
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:45:15: [:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[ :.E2022-12-12 05:45:15196
2022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178702125 .] .:] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc702171assigning 8 to cpu702163213v100x8, slow pcieE2022-12-12 05:45:15:: 
: ] 
 .178EEremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc702226[[]   
:: 2022-12-12 05:45:152022-12-12 05:45:15v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E[..
::]  2022-12-12 05:45:15702304702306178196[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: : ] ] 2022-12-12 05:45:15
:702347EEv100x8, slow pcieassigning 8 to cpu.213: [  

702388] E2022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: remote time is 8.68421 .::[2022-12-12 05:45:15E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc7024561962122022-12-12 05:45:15. :: [] ] .702503/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E2022-12-12 05:45:15assigning 8 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8702523: :]  .

: E196cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc702565E ] [
::  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu2022-12-12 05:45:15196E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:
.]  :2022-12-12 05:45:15196702660assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212.] : 
:] 702687assigning 8 to cpuE214[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
 ] 2022-12-12 05:45:15
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588.[ :
[702756[2022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-12 05:45:15: 2022-12-12 05:45:15.:] .E.702801212remote time is 8.68421702814 702819: ] 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E:[E 
 2122022-12-12 05:45:15 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8702931:2022-12-12 05:45:15212213
: 212.] ] E] 702983build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[remote time is 8.68421 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
2022-12-12 05:45:15
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E.:[[ 7030512142022-12-12 05:45:15[2022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] .2022-12-12 05:45:15.703100:Ecpu time is 97.0588703097.: 213 
: 703111E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:  remote time is 8.68421: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[214remote time is 8.68421213:2022-12-12 05:45:15] 
] 213.cpu time is 97.0588remote time is 8.68421] [703289

remote time is 8.684212022-12-12 05:45:15: 
[.E2022-12-12 05:45:15703348[ .: 2022-12-12 05:45:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc703382E.::  703395214E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ]  :Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214 
:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214cpu time is 97.0588:] 
214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 05:46:33. 87103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:46:33.127229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:46:33.127305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 20000000
[2022-12-12 05:46:33.247573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:46:33.247690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:46:33.247738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:46:33.247780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:46:33.248336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.249455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.250506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.263212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 05:46:33.263284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 05:46:33.263375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 05:46:33.263436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 05:46:33.263539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 05:46:33.263595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 05:46:33.263610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 05:46:33.263665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-12 05:46:33worker 0 thread 4 initing device 4.
263681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 05:46:332022-12-12 05:46:33..263823[263815: 2022-12-12 05:46:33: [E.E2022-12-12 05:46:33 263832 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc263847:E:: 1980 202E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc]  eager alloc mem 381.47 MB:3 solved[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
202
2022-12-12 05:46:33:] .202[2 solved263990] 2022-12-12 05:46:33
[: 1 solved.2022-12-12 05:46:33E[
264032. 2022-12-12 05:46:33: 264052[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.E: 2022-12-12 05:46:33:264076 E.1980: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 264105] E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: eager alloc mem 381.47 MB 205:E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 1980 :worker 0 thread 3 initing device 3] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205
eager alloc mem 381.47 MB:] 
205worker 0 thread 2 initing device 2] 
worker 0 thread 1 initing device 1
[2022-12-12 05:46:33.264617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 05:46:332022-12-12 05:46:33..264659264659: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 05:46:33.280350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.280401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.280472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.287992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.288063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.288131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.288190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.292566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:46:33.333747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:46:33.334119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:46:33.339204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:46:33.339276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:46:33.339319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:46:33.340108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.340668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.341788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.341940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.342712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:46:33.342796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:46:33.374634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:46:33.374938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:46:33.377541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:46:33.377831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:46:33.379823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:46:33.379884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:46:33.379930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:46:33.380767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.381804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.382848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.382934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.383239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:46:33.383299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:46:33.383337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:46:33.383607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:46:33.383645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:46:33.384172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.384635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[[[2022-12-12 05:46:332022-12-12 05:46:332022-12-12 05:46:332022-12-12 05:46:33....384704384704384704384704: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[2022-12-12 05:46:33.384960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:46:33.[385078[2022-12-12 05:46:33[: 2022-12-12 05:46:33.2022-12-12 05:46:33E.385084. 385087: 385088/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E: :E E1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes:1980:
1980] 1980] eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes

[2022-12-12 05:46:33.398155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.399138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.399222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.399351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 05:46:332022-12-12 05:46:33..399415399431: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 1024

[2022-12-12 05:46:33.[3994982022-12-12 05:46:33: .E399502 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E:2022-12-12 05:46:33 638./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 399504:eager release cuda mem 400000000: 638
E]  eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-12 05:46:33[.2022-12-12 05:46:33[399584.2022-12-12 05:46:33: 399576.E: 399596 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638:eager release cuda mem 400000000] 638
eager release cuda mem 1024] 
eager release cuda mem 2
[[[2022-12-12 05:46:332022-12-12 05:46:332022-12-12 05:46:33...399663399680399684: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::638638638] ] ] eager release cuda mem 1024eager release cuda mem 2eager release cuda mem 400000000


[[2022-12-12 05:46:332022-12-12 05:46:33.[.3997712022-12-12 05:46:33399772: .: E399776E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] :638] ] eager release cuda mem 2eager release cuda mem 25855eager release cuda mem 400000000


[[2022-12-12 05:46:332022-12-12 05:46:33..399925399925: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 400000000eager alloc mem 9.54 GB

[2022-12-12 05:46:33.400424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.412239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.413111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.414892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.415403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:46:33.416230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.416771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.416947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.417063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.417091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:33.417243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.417325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.417797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 05:46:332022-12-12 05:46:33..417876417877: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 25.25 KBeager release cuda mem 25855

[2022-12-12 05:46:33.417952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:46:33.417980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.418058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.418100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.418127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:33.418179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.418205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:46:33.418577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:46:33.418614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:46:33.418723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:46:33.418760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:46:33.418843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:46:33.418871[: 2022-12-12 05:46:33E. 418879/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 25855:
1980] eager alloc mem 9.54 GB
[2022-12-12 05:46:33.418932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 05:46:352022-12-12 05:46:352022-12-12 05:46:352022-12-12 05:46:352022-12-12 05:46:352022-12-12 05:46:352022-12-12 05:46:352022-12-12 05:46:35........426407426407426407426407426407426410426410426427: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[2022-12-12 05:46:352022-12-12 05:46:35.[.427539[2022-12-12 05:46:35427541: 2022-12-12 05:46:35.[[: E.427549[2022-12-12 05:46:35[2022-12-12 05:46:35E 427555: 2022-12-12 05:46:35.2022-12-12 05:46:35. /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E.427563.427571/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E 427571: 427574: :638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E: E638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E E ] eager release cuda mem 625663:638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663
638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
] eager release cuda mem 625663638eager release cuda mem 625663:638eager release cuda mem 625663
] 
638] [
eager release cuda mem 625663] eager release cuda mem 6256632022-12-12 05:46:35
[eager release cuda mem 625663
.2022-12-12 05:46:35
427819.: 427849[[E: [2022-12-12 05:46:352022-12-12 05:46:35 E2022-12-12 05:46:35[../hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ .2022-12-12 05:46:35427878[427883:2022-12-12 05:46:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu427897.: 2022-12-12 05:46:35: 1980.:: 427913E.E] 4279231980E:  427934 eager alloc mem 611.00 KB: ]  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
Eeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :E: 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :] 1980eager alloc mem 611.00 KB:eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB] 
1980
] 
eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-12 05:46:35.428804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 05:46:35] .eager release cuda mem 625663428822
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.428884: [[[E2022-12-12 05:46:35[2022-12-12 05:46:352022-12-12 05:46:35 .[[2022-12-12 05:46:35..[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu4288872022-12-12 05:46:352022-12-12 05:46:35.4288964288882022-12-12 05:46:35:: ..428899: : .1980E428908428908: EE428919]  : : E  : eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE
:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ::1980] ] :eager release cuda mem 625663638638] eager release cuda mem 625663eager release cuda mem 625663638
] ] eager alloc mem 611.00 KB

] eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663


[2022-12-12 05:46:35.429221: E[ [2022-12-12 05:46:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:46:35.:[.4292321980[2022-12-12 05:46:35429233[: ] 2022-12-12 05:46:35.: 2022-12-12 05:46:35Eeager alloc mem 611.00 KB.429255E. 
429261:  429267/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E :E1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:1980eager alloc mem 611.00 KB:
1980] 
1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-12 05:46:35.429802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.429869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.429931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.429997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.430052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.430120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.430142: E[ 2022-12-12 05:46:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:430152638[: [] 2022-12-12 05:46:35E2022-12-12 05:46:35eager release cuda mem 625663. .[
430176/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc4301782022-12-12 05:46:35: :: .E638E430207 ]  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:2022-12-12 05:46:35
: 638.638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 430283] :eager release cuda mem 625663: eager release cuda mem 625663638
E
[]  2022-12-12 05:46:35eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.
:4303681980: [] E2022-12-12 05:46:35[eager alloc mem 611.00 KB .2022-12-12 05:46:35
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu430418.[:: 4304252022-12-12 05:46:351980E: .]  E430456eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980: ] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] :
eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.430616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.430680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.430742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.430811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.430868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.430936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.431181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 05:46:352022-12-12 05:46:35..431249431250: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[2022-12-12 05:46:35[:6382022-12-12 05:46:35.2022-12-12 05:46:351980] .431295.] eager release cuda mem 625663431302: 431309eager alloc mem 611.00 KB
: E: 
E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:[638:2022-12-12 05:46:356382022-12-12 05:46:35] 638.] .eager release cuda mem 625663] 431428eager release cuda mem 625663431434
eager release cuda mem 625663: 
: 
EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980[] ] [2022-12-12 05:46:35[[eager release cuda mem 625663eager alloc mem 611.00 KB2022-12-12 05:46:35.2022-12-12 05:46:352022-12-12 05:46:35

.431543..431559: 431553431555: E: : E [EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:46:35  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:1980431640:2022-12-12 05:46:35:1980] : 1980.638] eager alloc mem 611.00 KBE] 431684] eager alloc mem 611.00 KB
 eager alloc mem 611.00 KB: eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E
: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 05:46:35.431812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.431836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.432129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.432204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.432328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.432395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.432469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] [2022-12-12 05:46:35eager release cuda mem 6256632022-12-12 05:46:35.
.[4324874324932022-12-12 05:46:35: : .EE432512  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:: 638[638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[] 2022-12-12 05:46:35] :2022-12-12 05:46:35eager release cuda mem 625663.eager release cuda mem 625663638[.
432556
] 2022-12-12 05:46:35432560: eager release cuda mem 625663.: E
432586E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-12 05:46:35 2022-12-12 05:46:35:638.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.1980] 4326512022-12-12 05:46:35:432656] eager release cuda mem 625663: .638: eager alloc mem 611.00 KB
E432681] E
 : eager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980: ] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB[] :
2022-12-12 05:46:35eager alloc mem 611.00 KB1980.
] 432786eager alloc mem 611.00 KB: 
[E2022-12-12 05:46:35 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu432817:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.432951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.433016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.433141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.433208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:46:35.433513[: 2022-12-12 05:46:35E[.[ 2022-12-12 05:46:354335282022-12-12 05:46:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: .:433538E[433542638[:  2022-12-12 05:46:35: ] 2022-12-12 05:46:35E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.Eeager release cuda mem 625663. :433575 
433583/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :] E:E638eager release cuda mem 625663 638 ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:eager release cuda mem 625663:
638
638] [] eager release cuda mem 6256632022-12-12 05:46:35eager release cuda mem 625663[
.
[2022-12-12 05:46:35[4337052022-12-12 05:46:35[.2022-12-12 05:46:35[: .2022-12-12 05:46:35.[433716.2022-12-12 05:46:35E4337284337442022-12-12 05:46:35: 433742. : : .E: 433751/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE433757 E: :  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] :: 638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB1980638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 1980:
] ] :eager release cuda mem 80400000] 638eager alloc mem 611.00 KBeager release cuda mem 80400000638
eager alloc mem 611.00 KB] 
[
] 
eager release cuda mem 804000002022-12-12 05:46:35eager release cuda mem 625663
.
433949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 05:46:35eager release cuda mem 625663.
434011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 05:46:35] .eager release cuda mem 80400000434043
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:46:35.434661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:46:35.[4346892022-12-12 05:46:35[: .2022-12-12 05:46:35E434697. : 434701/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: : E638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638:
] 638eager release cuda mem 80400000] 
[eager release cuda mem 6256632022-12-12 05:46:35
.434817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 05:46:35] .eager release cuda mem 80400000434842
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:46:35.434892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17091 secs 
[2022-12-12 05:46:35.435170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17151 secs 
[2022-12-12 05:46:35.435334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17152 secs 
[2022-12-12 05:46:35.435502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17146 secs 
[2022-12-12 05:46:35.436185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17154 secs 
[2022-12-12 05:46:35.436715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17207 secs 
[2022-12-12 05:46:35.437212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.18889 secs 
[2022-12-12 05:46:35.437392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.17279 secs 
[2022-12-12 05:46:35.439603: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.86 GB
[2022-12-12 05:46:36.754939: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.12 GB
[2022-12-12 05:46:36.756032: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.12 GB
[2022-12-12 05:46:36.756565: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.12 GB
[2022-12-12 05:46:38.287102: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.38 GB
[2022-12-12 05:46:38.288379: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.38 GB
[2022-12-12 05:46:38.289710: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.38 GB
[2022-12-12 05:46:39.374275: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.60 GB
[2022-12-12 05:46:39.374466: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.60 GB
[2022-12-12 05:46:39.374900: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.60 GB
[2022-12-12 05:46:40.443717: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.81 GB
[2022-12-12 05:46:40.444127: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.81 GB
[2022-12-12 05:46:40.444739: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.81 GB
[2022-12-12 05:46:41.595048: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.27 GB
[2022-12-12 05:46:41.596099: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.27 GB
[2022-12-12 05:46:41.597063: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.27 GB
[2022-12-12 05:46:43. 34102: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.47 GB
[2022-12-12 05:46:43. 34955: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.47 GB
[HCTR][05:46:44.479][ERROR][RK0][tid #140255260808960]: replica 3 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][tid #140254791046912]: replica 4 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][tid #140254304532224]: replica 5 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][tid #140254304532224]: replica 2 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][tid #140255260808960]: replica 7 calling init per replica done, doing barrier
[HCTR][05:46:44.479][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][tid #140255260808960]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][tid #140254304532224]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][tid #140254791046912]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][tid #140255260808960]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][tid #140254304532224]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:46:44.479][ERROR][RK0][main]: init per replica done
[HCTR][05:46:44.479][ERROR][RK0][tid #140255260808960]: init per replica done
[HCTR][05:46:44.479][ERROR][RK0][tid #140254304532224]: init per replica done
[HCTR][05:46:44.479][ERROR][RK0][tid #140254791046912]: init per replica done
[HCTR][05:46:44.479][ERROR][RK0][tid #140255260808960]: init per replica done
[HCTR][05:46:44.479][ERROR][RK0][main]: init per replica done
[HCTR][05:46:44.479][ERROR][RK0][tid #140254304532224]: init per replica done
[HCTR][05:46:44.482][ERROR][RK0][main]: init per replica done
[HCTR][05:46:44.517][ERROR][RK0][tid #140255260808960]: 7 allocated 3276800 at 0x7f7178238400
[HCTR][05:46:44.517][ERROR][RK0][tid #140255260808960]: 7 allocated 6553600 at 0x7f7178558400
[HCTR][05:46:44.517][ERROR][RK0][tid #140255260808960]: 7 allocated 3276800 at 0x7f7178b98400
[HCTR][05:46:44.517][ERROR][RK0][tid #140255260808960]: 7 allocated 6553600 at 0x7f7178eb8400
[HCTR][05:46:44.518][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f71a8238400
[HCTR][05:46:44.518][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f71a8558400
[HCTR][05:46:44.518][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f71a8b98400
[HCTR][05:46:44.518][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f71a8eb8400
[HCTR][05:46:44.518][ERROR][RK0][tid #140255260808960]: 3 allocated 3276800 at 0x7f70c0238400
[HCTR][05:46:44.518][ERROR][RK0][tid #140255260808960]: 3 allocated 6553600 at 0x7f70c0558400
[HCTR][05:46:44.518][ERROR][RK0][tid #140255260808960]: 3 allocated 3276800 at 0x7f70c0b98400
[HCTR][05:46:44.518][ERROR][RK0][tid #140255260808960]: 3 allocated 6553600 at 0x7f70c0eb8400
[HCTR][05:46:44.518][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f70bc238400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 1 allocated 3276800 at 0x7f71de238400
[HCTR][05:46:44.518][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f70bc558400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 1 allocated 6553600 at 0x7f71de558400
[HCTR][05:46:44.518][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f70bcb98400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 1 allocated 3276800 at 0x7f71deb98400
[HCTR][05:46:44.518][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f70bceb8400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 1 allocated 6553600 at 0x7f71deeb8400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254304532224]: 2 allocated 3276800 at 0x7f713c238400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254304532224]: 2 allocated 6553600 at 0x7f713c558400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254304532224]: 2 allocated 3276800 at 0x7f713cb98400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254304532224]: 2 allocated 6553600 at 0x7f713ceb8400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 6 allocated 3276800 at 0x7f7138238400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 6 allocated 6553600 at 0x7f7138558400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 6 allocated 3276800 at 0x7f7138b98400
[HCTR][05:46:44.518][ERROR][RK0][tid #140254925264640]: 6 allocated 6553600 at 0x7f7138eb8400
[HCTR][05:46:44.521][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f71ac320000
[HCTR][05:46:44.521][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f71ac640000
[HCTR][05:46:44.521][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f71acc80000
[HCTR][05:46:44.521][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f71acfa0000
