2022-12-12 03:56:10.693359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.703228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.708664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.716610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.722551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.728004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.731818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.738011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.793530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.799981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.806165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.810178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.818276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.822170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.825931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.829323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.840971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.856617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.861243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.865684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.869632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.872641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.884911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.896173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.901860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.902563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.910402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.911016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.914785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.914960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.916573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.916654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.918314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.918407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.920117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.920198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.921799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.921878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.923444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.923524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.929686: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:10.931853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.933037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.934328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.935384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.936409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.937470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.938013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.938987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.939501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.940493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.940882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.948055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.949678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.951274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.953009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.954808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.955200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.956668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.957011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.958752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.959188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.959740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.968700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.969456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.970110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.970697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.972424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.973469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.973807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:10.987348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.010829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.011965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.011986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.012561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.012888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.014961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.015960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.016846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.017570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.017622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.017716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.018032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.019268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.020968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.022024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.022449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.022602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.023536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.024139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.025579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.027246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.027347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.027365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.027934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.028293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.029448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.031543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.031610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.031768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.032419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.032735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.033925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.036076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.036110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.036991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.037601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.038051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.039529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.039559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.040836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.041224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.041432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.042786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.042874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.044357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.044947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.045033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.046005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.046189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.048250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.048477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.049386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.049433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.051284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.052346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.052410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.053894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.054794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.054985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.056259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.057170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.057412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.058560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.059386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.059717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.061556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.061793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.061854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.063864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.064616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.065169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.066066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.066697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.067577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.069106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.069589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.069750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.070193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.071601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.072362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.072678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.073347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.074433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.075551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.075771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.076303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.077233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.078473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.078512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.078869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.080199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.081191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.081377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.081439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.081945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.084461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.084496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.084573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.084835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.087213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.087215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.087496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.087901: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.089596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.089726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.089749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.091680: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.092181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.092416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.093445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.094324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.094576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.095971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.096168: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.096561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.096691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.096803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.096998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.099435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.100149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.100477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.100862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.100963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.101152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.103600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.104109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.104515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.104554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.104856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.105207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.105427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.108214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.108325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.108778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.108907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.109749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.109998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.112418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.112646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.113512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.114102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.114343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.147106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.147674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.148175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.148515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.151975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.152705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.153104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.154219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.156740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.157429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.157765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.158883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.162382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.162742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.163071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.164311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.167243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.167523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.169159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.170609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.172713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.172848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.174093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.175555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.177711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.177808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.179152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.179750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.181739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.182039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.183215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.184292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.192477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.192691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.195052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.197324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.208193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.209250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.210356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.221230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.221512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.221953: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.222993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.230223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.255374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.256715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.287473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.287473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.290773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.291910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.321217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.321217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.325251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.326226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.331666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.332338: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.333216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.337728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.338353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.340799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.343042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.343803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.347105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.348677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.349617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.352154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.354966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.358034: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.362619: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:56:11.366419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.370644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.372385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.376097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.384584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:11.389242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.223276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.224222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.224996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.225458: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.225519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:56:12.245000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.245858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.246375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.246959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.247646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.248323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:56:12.296023: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.296223: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.339836: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:56:12.496248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.497115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.501215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.502975: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.503038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:56:12.504055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.505301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.506324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.507153: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.507229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:56:12.522281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.522973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.523517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.524111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.524635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.525158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:56:12.525275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.525917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.526443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.527252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.528323: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.528383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:56:12.528682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.529557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.530141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.531109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.531591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:56:12.548260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.550579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.552840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.553951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.554950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.555915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:56:12.580961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.582184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.583710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.584688: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.584754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:56:12.589704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.589888: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.590931: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:56:12.593381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.594733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.595989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.597083: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.597150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:56:12.603267: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.603438: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.603631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.604466: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 03:56:12.605133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.608486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.609523: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.609657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.609676: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.610292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.610675: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:56:12.610776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:56:12.616355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.617555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.618805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.619905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.620939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.621427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:56:12.632363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.632997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.633516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.634020: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.634067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.634084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:56:12.634857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.635409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.635876: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:56:12.635925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:56:12.652395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.653076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.653579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.654165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.654520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.654718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.655703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.655740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:56:12.656230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.656813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.657179: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.657329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:56:12.657326: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.657797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:56:12.658216: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 03:56:12.666888: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.667050: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.667884: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 03:56:12.700367: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.700547: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.702246: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 03:56:12.703657: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.703832: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:56:12.704942: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][03:56:13.980][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.981][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:56:13.983][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 93it [00:01, 76.43it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 188it [00:01, 168.32it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 101it [00:01, 84.44it/s]warmup run: 283it [00:01, 270.25it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 93it [00:01, 79.41it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 97it [00:01, 83.70it/s]warmup run: 202it [00:01, 183.35it/s]warmup run: 376it [00:01, 373.99it/s]warmup run: 100it [00:01, 86.89it/s]warmup run: 191it [00:01, 177.48it/s]warmup run: 95it [00:01, 80.00it/s]warmup run: 190it [00:01, 176.68it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 298it [00:01, 286.18it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 468it [00:02, 473.87it/s]warmup run: 194it [00:01, 181.17it/s]warmup run: 291it [00:01, 288.05it/s]warmup run: 190it [00:01, 173.62it/s]warmup run: 284it [00:01, 279.93it/s]warmup run: 98it [00:01, 84.96it/s]warmup run: 398it [00:01, 399.61it/s]warmup run: 92it [00:01, 79.57it/s]warmup run: 563it [00:02, 572.51it/s]warmup run: 292it [00:01, 289.83it/s]warmup run: 390it [00:01, 401.09it/s]warmup run: 287it [00:01, 279.69it/s]warmup run: 383it [00:01, 394.92it/s]warmup run: 195it [00:01, 182.72it/s]warmup run: 496it [00:02, 506.23it/s]warmup run: 185it [00:01, 173.21it/s]warmup run: 659it [00:02, 660.21it/s]warmup run: 392it [00:01, 405.46it/s]warmup run: 480it [00:02, 494.36it/s]warmup run: 385it [00:01, 391.03it/s]warmup run: 484it [00:02, 510.31it/s]warmup run: 288it [00:01, 284.17it/s]warmup run: 594it [00:02, 605.11it/s]warmup run: 284it [00:01, 284.28it/s]warmup run: 756it [00:02, 734.59it/s]warmup run: 493it [00:01, 519.01it/s]warmup run: 571it [00:02, 581.37it/s]warmup run: 483it [00:02, 500.19it/s]warmup run: 587it [00:02, 619.72it/s]warmup run: 383it [00:01, 392.24it/s]warmup run: 695it [00:02, 698.10it/s]warmup run: 382it [00:01, 397.33it/s]warmup run: 850it [00:02, 786.73it/s]warmup run: 594it [00:02, 622.24it/s]warmup run: 662it [00:02, 656.12it/s]warmup run: 583it [00:02, 603.83it/s]warmup run: 691it [00:02, 715.67it/s]warmup run: 476it [00:02, 493.12it/s]warmup run: 795it [00:02, 772.29it/s]warmup run: 484it [00:02, 513.93it/s]warmup run: 944it [00:02, 825.21it/s]warmup run: 695it [00:02, 712.10it/s]warmup run: 759it [00:02, 733.49it/s]warmup run: 682it [00:02, 691.36it/s]warmup run: 795it [00:02, 795.68it/s]warmup run: 569it [00:02, 584.83it/s]warmup run: 895it [00:02, 831.53it/s]warmup run: 585it [00:02, 618.67it/s]warmup run: 1038it [00:02, 856.23it/s]warmup run: 795it [00:02, 782.89it/s]warmup run: 853it [00:02, 786.17it/s]warmup run: 781it [00:02, 763.44it/s]warmup run: 899it [00:02, 858.99it/s]warmup run: 661it [00:02, 661.89it/s]warmup run: 995it [00:02, 875.29it/s]warmup run: 688it [00:02, 713.36it/s]warmup run: 1133it [00:02, 882.23it/s]warmup run: 893it [00:02, 828.63it/s]warmup run: 946it [00:02, 824.14it/s]warmup run: 880it [00:02, 821.63it/s]warmup run: 1001it [00:02, 902.35it/s]warmup run: 753it [00:02, 725.23it/s]warmup run: 1096it [00:02, 910.44it/s]warmup run: 790it [00:02, 787.98it/s]warmup run: 1227it [00:02, 891.77it/s]warmup run: 991it [00:02, 869.12it/s]warmup run: 978it [00:02, 863.92it/s]warmup run: 1039it [00:02, 845.77it/s]warmup run: 1103it [00:02, 933.32it/s]warmup run: 844it [00:02, 773.38it/s]warmup run: 1195it [00:02, 931.70it/s]warmup run: 891it [00:02, 845.55it/s]warmup run: 1329it [00:02, 927.70it/s]warmup run: 1090it [00:02, 900.59it/s]warmup run: 1076it [00:02, 895.29it/s]warmup run: 1131it [00:02, 866.74it/s]warmup run: 1205it [00:02, 954.06it/s]warmup run: 935it [00:02, 806.54it/s]warmup run: 1294it [00:02, 947.35it/s]warmup run: 992it [00:02, 888.30it/s]warmup run: 1430it [00:03, 951.21it/s]warmup run: 1190it [00:02, 926.07it/s]warmup run: 1224it [00:02, 883.66it/s]warmup run: 1175it [00:02, 920.43it/s]warmup run: 1306it [00:02, 964.48it/s]warmup run: 1026it [00:02, 831.47it/s]warmup run: 1393it [00:02, 959.77it/s]warmup run: 1092it [00:02, 913.98it/s]warmup run: 1531it [00:03, 968.31it/s]warmup run: 1289it [00:02, 942.07it/s]warmup run: 1317it [00:02, 896.89it/s]warmup run: 1274it [00:02, 938.08it/s]warmup run: 1407it [00:02, 976.36it/s]warmup run: 1118it [00:02, 854.09it/s]warmup run: 1493it [00:03, 971.57it/s]warmup run: 1192it [00:02, 933.93it/s]warmup run: 1633it [00:03, 983.20it/s]warmup run: 1388it [00:02, 955.34it/s]warmup run: 1411it [00:03, 906.99it/s]warmup run: 1373it [00:02, 951.00it/s]warmup run: 1508it [00:03, 983.76it/s]warmup run: 1212it [00:02, 876.18it/s]warmup run: 1594it [00:03, 982.56it/s]warmup run: 1736it [00:03, 995.45it/s]warmup run: 1291it [00:02, 940.40it/s]warmup run: 1487it [00:03, 962.04it/s]warmup run: 1471it [00:03, 954.43it/s]warmup run: 1504it [00:03, 906.24it/s]warmup run: 1609it [00:03, 982.97it/s]warmup run: 1305it [00:02, 889.86it/s]warmup run: 1696it [00:03, 991.42it/s]warmup run: 1838it [00:03, 1001.33it/s]warmup run: 1389it [00:02, 942.56it/s]warmup run: 1586it [00:03, 968.77it/s]warmup run: 1570it [00:03, 962.66it/s]warmup run: 1597it [00:03, 911.41it/s]warmup run: 1709it [00:03, 983.05it/s]warmup run: 1401it [00:03, 908.79it/s]warmup run: 1799it [00:03, 1000.99it/s]warmup run: 1941it [00:03, 1007.15it/s]warmup run: 1486it [00:03, 943.53it/s]warmup run: 1685it [00:03, 971.16it/s]warmup run: 1671it [00:03, 974.22it/s]warmup run: 1691it [00:03, 917.57it/s]warmup run: 1809it [00:03, 985.00it/s]warmup run: 1502it [00:03, 937.46it/s]warmup run: 1900it [00:03, 1002.35it/s]warmup run: 2051it [00:03, 1033.24it/s]warmup run: 1584it [00:03, 952.24it/s]warmup run: 1784it [00:03, 973.25it/s]warmup run: 1773it [00:03, 986.36it/s]warmup run: 1784it [00:03, 917.17it/s]warmup run: 1909it [00:03, 982.14it/s]warmup run: 1603it [00:03, 957.76it/s]warmup run: 2001it [00:03, 992.75it/s] warmup run: 2173it [00:03, 1087.36it/s]warmup run: 1684it [00:03, 965.39it/s]warmup run: 1882it [00:03, 973.55it/s]warmup run: 1876it [00:03, 997.60it/s]warmup run: 1878it [00:03, 923.45it/s]warmup run: 2012it [00:03, 993.92it/s]warmup run: 1705it [00:03, 975.57it/s]warmup run: 2116it [00:03, 1039.18it/s]warmup run: 2295it [00:03, 1126.27it/s]warmup run: 1784it [00:03, 973.50it/s]warmup run: 1983it [00:03, 982.48it/s]warmup run: 1972it [00:03, 926.69it/s]warmup run: 1977it [00:03, 997.04it/s]warmup run: 2133it [00:03, 1057.51it/s]warmup run: 1809it [00:03, 993.15it/s]warmup run: 2232it [00:03, 1072.93it/s]warmup run: 2417it [00:04, 1152.78it/s]warmup run: 1884it [00:03, 980.14it/s]warmup run: 2099it [00:03, 1034.28it/s]warmup run: 2084it [00:03, 983.79it/s]warmup run: 2094it [00:03, 1047.92it/s]warmup run: 2254it [00:03, 1102.38it/s]warmup run: 1910it [00:03, 995.56it/s]warmup run: 2348it [00:03, 1098.56it/s]warmup run: 2538it [00:04, 1169.86it/s]warmup run: 1987it [00:03, 994.45it/s]warmup run: 2218it [00:03, 1080.48it/s]warmup run: 2216it [00:03, 1099.15it/s]warmup run: 2205it [00:03, 1049.65it/s]warmup run: 2376it [00:03, 1134.57it/s]warmup run: 2013it [00:03, 1004.60it/s]warmup run: 2465it [00:03, 1119.22it/s]warmup run: 2659it [00:04, 1179.13it/s]warmup run: 2104it [00:03, 1046.27it/s]warmup run: 2338it [00:03, 1113.73it/s]warmup run: 2339it [00:03, 1136.05it/s]warmup run: 2326it [00:03, 1095.22it/s]warmup run: 2497it [00:03, 1155.79it/s]warmup run: 2133it [00:03, 1060.90it/s]warmup run: 2583it [00:04, 1134.49it/s]warmup run: 2781it [00:04, 1189.72it/s]warmup run: 2225it [00:03, 1093.21it/s]warmup run: 2458it [00:03, 1137.04it/s]warmup run: 2461it [00:03, 1160.80it/s]warmup run: 2446it [00:04, 1125.57it/s]warmup run: 2618it [00:04, 1171.40it/s]warmup run: 2253it [00:03, 1100.35it/s]warmup run: 2699it [00:04, 1141.11it/s]warmup run: 2903it [00:04, 1197.78it/s]warmup run: 2346it [00:03, 1126.69it/s]warmup run: 2578it [00:04, 1154.42it/s]warmup run: 2583it [00:04, 1177.60it/s]warmup run: 2566it [00:04, 1145.35it/s]warmup run: 2739it [00:04, 1182.48it/s]warmup run: 2374it [00:03, 1130.67it/s]warmup run: 2819it [00:04, 1156.68it/s]warmup run: 3000it [00:04, 667.65it/s] warmup run: 2467it [00:03, 1149.06it/s]warmup run: 2696it [00:04, 1161.45it/s]warmup run: 2704it [00:04, 1186.38it/s]warmup run: 2684it [00:04, 1153.40it/s]warmup run: 2858it [00:04, 1174.70it/s]warmup run: 2494it [00:04, 1149.63it/s]warmup run: 2939it [00:04, 1169.30it/s]warmup run: 2588it [00:04, 1166.81it/s]warmup run: 2815it [00:04, 1168.16it/s]warmup run: 3000it [00:04, 676.96it/s] warmup run: 2826it [00:04, 1195.96it/s]warmup run: 2803it [00:04, 1162.02it/s]warmup run: 2976it [00:04, 1172.13it/s]warmup run: 2614it [00:04, 1163.89it/s]warmup run: 3000it [00:04, 687.79it/s] warmup run: 2707it [00:04, 1172.05it/s]warmup run: 2934it [00:04, 1172.71it/s]warmup run: 2948it [00:04, 1201.45it/s]warmup run: 2920it [00:04, 1120.68it/s]warmup run: 3000it [00:04, 685.51it/s] warmup run: 2734it [00:04, 1174.13it/s]warmup run: 3000it [00:04, 679.46it/s] warmup run: 2828it [00:04, 1183.30it/s]warmup run: 3000it [00:04, 661.86it/s] warmup run: 2852it [00:04, 1170.01it/s]warmup run: 2949it [00:04, 1191.20it/s]warmup run: 3000it [00:04, 685.73it/s] warmup run: 2970it [00:04, 1168.19it/s]warmup run: 3000it [00:04, 673.66it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.57it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1639.48it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.12it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.29it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1622.96it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.73it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1600.97it/s]warmup should be done:   4%|         | 109/3000 [00:00<00:02, 1084.53it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1622.96it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.67it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1619.30it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.30it/s]warmup should be done:   9%|         | 271/3000 [00:00<00:01, 1397.61it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1609.07it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1627.37it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1622.81it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1617.16it/s]warmup should be done:  16%|        | 484/3000 [00:00<00:01, 1608.85it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1618.84it/s]warmup should be done:  14%|        | 434/3000 [00:00<00:01, 1500.35it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1621.34it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1631.29it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1633.59it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1621.21it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1613.43it/s]warmup should be done:  20%|        | 595/3000 [00:00<00:01, 1540.97it/s]warmup should be done:  22%|       | 645/3000 [00:00<00:01, 1603.69it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1613.33it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1619.64it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1617.92it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1628.48it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1622.84it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1614.73it/s]warmup should be done:  25%|       | 758/3000 [00:00<00:01, 1569.73it/s]warmup should be done:  27%|       | 807/3000 [00:00<00:01, 1606.16it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1611.64it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1615.47it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1615.67it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1623.16it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1610.72it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1614.00it/s]warmup should be done:  31%|       | 920/3000 [00:00<00:01, 1584.78it/s]warmup should be done:  32%|      | 968/3000 [00:00<00:01, 1603.10it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1606.23it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1612.91it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1601.17it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1614.39it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1599.18it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1615.32it/s]warmup should be done:  36%|      | 1081/3000 [00:00<00:01, 1589.88it/s]warmup should be done:  38%|      | 1130/3000 [00:00<00:01, 1605.41it/s]warmup should be done:  38%|      | 1136/3000 [00:00<00:01, 1607.55it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1608.13it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1613.00it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1595.33it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1592.42it/s]warmup should be done:  43%|     | 1298/3000 [00:00<00:01, 1617.96it/s]warmup should be done:  41%|     | 1242/3000 [00:00<00:01, 1595.33it/s]warmup should be done:  43%|     | 1292/3000 [00:00<00:01, 1608.10it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1611.54it/s]warmup should be done:  43%|     | 1300/3000 [00:00<00:01, 1607.05it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1621.01it/s]warmup should be done:  43%|     | 1297/3000 [00:00<00:01, 1591.42it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1591.82it/s]warmup should be done:  49%|     | 1461/3000 [00:00<00:00, 1620.31it/s]warmup should be done:  47%|     | 1403/3000 [00:00<00:00, 1597.50it/s]warmup should be done:  48%|     | 1454/3000 [00:00<00:00, 1608.94it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1617.49it/s]warmup should be done:  49%|     | 1461/3000 [00:00<00:00, 1605.95it/s]warmup should be done:  49%|     | 1473/3000 [00:00<00:00, 1620.62it/s]warmup should be done:  49%|     | 1457/3000 [00:00<00:00, 1588.89it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1598.98it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1621.36it/s]warmup should be done:  52%|    | 1565/3000 [00:01<00:00, 1602.62it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1610.88it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1624.77it/s]warmup should be done:  54%|    | 1622/3000 [00:01<00:00, 1605.06it/s]warmup should be done:  55%|    | 1636/3000 [00:01<00:00, 1616.31it/s]warmup should be done:  54%|    | 1616/3000 [00:01<00:00, 1587.85it/s]warmup should be done:  54%|    | 1629/3000 [00:01<00:00, 1611.74it/s]warmup should be done:  60%|    | 1787/3000 [00:01<00:00, 1622.07it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1614.51it/s]warmup should be done:  59%|    | 1778/3000 [00:01<00:00, 1613.33it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1630.33it/s]warmup should be done:  59%|    | 1783/3000 [00:01<00:00, 1604.99it/s]warmup should be done:  60%|    | 1798/3000 [00:01<00:00, 1615.88it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1586.05it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1621.27it/s]warmup should be done:  65%|   | 1950/3000 [00:01<00:00, 1622.04it/s]warmup should be done:  63%|   | 1895/3000 [00:01<00:00, 1622.95it/s]warmup should be done:  65%|   | 1941/3000 [00:01<00:00, 1616.57it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1633.47it/s]warmup should be done:  65%|   | 1944/3000 [00:01<00:00, 1604.35it/s]warmup should be done:  65%|   | 1960/3000 [00:01<00:00, 1614.23it/s]warmup should be done:  64%|   | 1934/3000 [00:01<00:00, 1584.91it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1627.07it/s]warmup should be done:  70%|   | 2113/3000 [00:01<00:00, 1622.23it/s]warmup should be done:  70%|   | 2105/3000 [00:01<00:00, 1622.42it/s]warmup should be done:  69%|   | 2060/3000 [00:01<00:00, 1628.75it/s]warmup should be done:  71%|   | 2122/3000 [00:01<00:00, 1635.10it/s]warmup should be done:  70%|   | 2105/3000 [00:01<00:00, 1604.13it/s]warmup should be done:  71%|   | 2122/3000 [00:01<00:00, 1614.04it/s]warmup should be done:  70%|   | 2093/3000 [00:01<00:00, 1584.57it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1632.63it/s]warmup should be done:  76%|  | 2276/3000 [00:01<00:00, 1619.21it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1623.47it/s]warmup should be done:  74%|  | 2225/3000 [00:01<00:00, 1633.82it/s]warmup should be done:  76%|  | 2286/3000 [00:01<00:00, 1633.66it/s]warmup should be done:  76%|  | 2266/3000 [00:01<00:00, 1603.45it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1611.08it/s]warmup should be done:  75%|  | 2252/3000 [00:01<00:00, 1581.51it/s]warmup should be done:  76%|  | 2288/3000 [00:01<00:00, 1633.39it/s]warmup should be done:  81%|  | 2432/3000 [00:01<00:00, 1627.60it/s]warmup should be done:  81%| | 2439/3000 [00:01<00:00, 1621.01it/s]warmup should be done:  80%|  | 2389/3000 [00:01<00:00, 1634.48it/s]warmup should be done:  82%| | 2451/3000 [00:01<00:00, 1635.70it/s]warmup should be done:  81%|  | 2427/3000 [00:01<00:00, 1602.02it/s]warmup should be done:  82%| | 2446/3000 [00:01<00:00, 1611.82it/s]warmup should be done:  80%|  | 2411/3000 [00:01<00:00, 1582.83it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1636.85it/s]warmup should be done:  87%| | 2596/3000 [00:01<00:00, 1629.88it/s]warmup should be done:  85%| | 2554/3000 [00:01<00:00, 1638.02it/s]warmup should be done:  87%| | 2602/3000 [00:01<00:00, 1622.16it/s]warmup should be done:  87%| | 2616/3000 [00:01<00:00, 1638.04it/s]warmup should be done:  86%| | 2588/3000 [00:01<00:00, 1603.10it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1611.48it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1583.31it/s]warmup should be done:  87%| | 2617/3000 [00:01<00:00, 1633.98it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1623.81it/s]warmup should be done:  92%|| 2760/3000 [00:01<00:00, 1632.03it/s]warmup should be done:  91%| | 2719/3000 [00:01<00:00, 1640.34it/s]warmup should be done:  93%|| 2781/3000 [00:01<00:00, 1638.63it/s]warmup should be done:  92%|| 2749/3000 [00:01<00:00, 1603.19it/s]warmup should be done:  92%|| 2770/3000 [00:01<00:00, 1611.64it/s]warmup should be done:  91%| | 2731/3000 [00:01<00:00, 1588.55it/s]warmup should be done:  93%|| 2781/3000 [00:01<00:00, 1630.20it/s]warmup should be done:  98%|| 2929/3000 [00:01<00:00, 1627.28it/s]warmup should be done:  96%|| 2885/3000 [00:01<00:00, 1645.01it/s]warmup should be done:  98%|| 2925/3000 [00:01<00:00, 1635.67it/s]warmup should be done:  98%|| 2947/3000 [00:01<00:00, 1644.31it/s]warmup should be done:  97%|| 2911/3000 [00:01<00:00, 1607.98it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1617.81it/s]warmup should be done:  96%|| 2894/3000 [00:01<00:00, 1600.58it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1632.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.85it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1619.07it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1608.21it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1598.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1596.23it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.77it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.80it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.55it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.07it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1643.94it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.76it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.71it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1691.71it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1652.20it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1676.89it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1644.80it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.91it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1683.92it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1669.79it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.44it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1687.76it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1681.99it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1654.54it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1648.88it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1684.83it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1654.79it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1651.81it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1670.71it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1686.83it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1684.84it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1655.98it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1658.67it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1656.54it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1689.70it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1668.55it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1409.81it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1428.50it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1656.14it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1683.59it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1660.59it/s]warmup should be done:  28%|       | 831/3000 [00:00<00:01, 1659.33it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1692.44it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1629.15it/s]warmup should be done:  27%|       | 807/3000 [00:00<00:01, 1404.34it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1423.18it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1656.25it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1682.75it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1665.85it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1659.18it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1692.29it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1608.44it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1489.01it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1495.45it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1657.91it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1682.37it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1668.22it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1658.31it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1690.23it/s]warmup should be done:  39%|      | 1163/3000 [00:00<00:01, 1595.73it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1544.90it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1546.78it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1663.73it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1684.00it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:00, 1668.43it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1656.33it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1691.73it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1586.04it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1581.24it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1580.42it/s]warmup should be done:  50%|     | 1499/3000 [00:00<00:00, 1670.09it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1668.57it/s]warmup should be done:  51%|     | 1519/3000 [00:00<00:00, 1684.40it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1657.07it/s]warmup should be done:  51%|     | 1529/3000 [00:00<00:00, 1691.59it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1581.90it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1609.44it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1605.93it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1674.05it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1685.19it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1670.64it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1659.13it/s]warmup should be done:  57%|    | 1699/3000 [00:01<00:00, 1692.58it/s]warmup should be done:  55%|    | 1641/3000 [00:01<00:00, 1578.60it/s]warmup should be done:  55%|    | 1645/3000 [00:01<00:00, 1628.00it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1621.11it/s]warmup should be done:  61%|   | 1839/3000 [00:01<00:00, 1678.61it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1684.95it/s]warmup should be done:  61%|    | 1831/3000 [00:01<00:00, 1661.57it/s]warmup should be done:  61%|    | 1836/3000 [00:01<00:00, 1669.10it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1695.06it/s]warmup should be done:  60%|    | 1799/3000 [00:01<00:00, 1574.33it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1639.52it/s]warmup should be done:  61%|    | 1820/3000 [00:01<00:00, 1626.51it/s]warmup should be done:  67%|   | 2009/3000 [00:01<00:00, 1683.83it/s]warmup should be done:  68%|   | 2026/3000 [00:01<00:00, 1682.75it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1661.78it/s]warmup should be done:  67%|   | 2003/3000 [00:01<00:00, 1664.40it/s]warmup should be done:  68%|   | 2041/3000 [00:01<00:00, 1697.02it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1569.53it/s]warmup should be done:  66%|   | 1977/3000 [00:01<00:00, 1641.55it/s]warmup should be done:  66%|   | 1984/3000 [00:01<00:00, 1627.24it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1687.61it/s]warmup should be done:  73%|  | 2195/3000 [00:01<00:00, 1680.73it/s]warmup should be done:  72%|  | 2165/3000 [00:01<00:00, 1661.00it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1661.66it/s]warmup should be done:  74%|  | 2211/3000 [00:01<00:00, 1695.98it/s]warmup should be done:  70%|   | 2114/3000 [00:01<00:00, 1568.29it/s]warmup should be done:  71%|  | 2142/3000 [00:01<00:00, 1643.22it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1629.01it/s]warmup should be done:  78%|  | 2348/3000 [00:01<00:00, 1685.02it/s]warmup should be done:  78%|  | 2332/3000 [00:01<00:00, 1661.83it/s]warmup should be done:  78%|  | 2337/3000 [00:01<00:00, 1661.43it/s]warmup should be done:  79%|  | 2364/3000 [00:01<00:00, 1674.79it/s]warmup should be done:  79%|  | 2381/3000 [00:01<00:00, 1694.91it/s]warmup should be done:  76%|  | 2271/3000 [00:01<00:00, 1568.40it/s]warmup should be done:  77%|  | 2308/3000 [00:01<00:00, 1646.29it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1632.26it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1682.51it/s]warmup should be done:  83%| | 2499/3000 [00:01<00:00, 1658.62it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1694.68it/s]warmup should be done:  83%| | 2504/3000 [00:01<00:00, 1660.58it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1672.19it/s]warmup should be done:  81%|  | 2428/3000 [00:01<00:00, 1566.05it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1645.73it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1633.65it/s]warmup should be done:  91%| | 2721/3000 [00:01<00:00, 1695.30it/s]warmup should be done:  89%| | 2665/3000 [00:01<00:00, 1653.82it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1657.93it/s]warmup should be done:  90%| | 2700/3000 [00:01<00:00, 1668.92it/s]warmup should be done:  90%| | 2686/3000 [00:01<00:00, 1658.69it/s]warmup should be done:  86%| | 2585/3000 [00:01<00:00, 1566.65it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1650.50it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1633.97it/s]warmup should be done:  96%|| 2891/3000 [00:01<00:00, 1693.64it/s]warmup should be done:  94%|| 2831/3000 [00:01<00:00, 1650.41it/s]warmup should be done:  95%|| 2837/3000 [00:01<00:00, 1655.96it/s]warmup should be done:  96%|| 2867/3000 [00:01<00:00, 1666.20it/s]warmup should be done:  95%|| 2853/3000 [00:01<00:00, 1661.77it/s]warmup should be done:  91%|| 2743/3000 [00:01<00:00, 1568.26it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1661.84it/s]warmup should be done:  94%|| 2805/3000 [00:01<00:00, 1635.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1692.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.24it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.21it/s]warmup should be done: 100%|| 2997/3000 [00:01<00:00, 1649.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.21it/s]warmup should be done:  97%|| 2900/3000 [00:01<00:00, 1568.12it/s]warmup should be done:  99%|| 2976/3000 [00:01<00:00, 1663.52it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1638.37it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1606.46it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1600.40it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1588.48it/s]2022-12-12 03:57:49.986357: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f24ec02df00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:49.986418: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.042216: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2530029d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.042278: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.048519: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f25b802fd60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.048561: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.074412: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f430f796090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.074480: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.514946: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4323830830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.515012: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.551996: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f25e802d1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.552071: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.570379: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f430782b410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.570452: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:50.595581: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2550030200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:57:50.595666: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:57:52.274047: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.280936: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.370263: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.399838: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.830721: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.861916: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.894603: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:52.988662: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:57:55.105599: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.171748: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.278072: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.294694: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.692967: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.801400: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.805207: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:57:55.860504: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:58:21.831][ERROR][RK0][tid #139926309943040]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:58:21.831][ERROR][RK0][tid #139926309943040]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.837][ERROR][RK0][tid #139926309943040]: coll ps creation done
[HCTR][03:58:21.837][ERROR][RK0][tid #139926309943040]: replica 7 waits for coll ps creation barrier
[HCTR][03:58:21.875][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:58:21.875][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.881][ERROR][RK0][main]: coll ps creation done
[HCTR][03:58:21.881][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][03:58:21.888][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:58:21.888][ERROR][RK0][tid #139926318335744]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:58:21.888][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.888][ERROR][RK0][tid #139926318335744]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.894][ERROR][RK0][main]: coll ps creation done
[HCTR][03:58:21.894][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][03:58:21.897][ERROR][RK0][tid #139926318335744]: coll ps creation done
[HCTR][03:58:21.897][ERROR][RK0][tid #139926318335744]: replica 3 waits for coll ps creation barrier
[HCTR][03:58:21.903][ERROR][RK0][tid #139926855206656]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:58:21.903][ERROR][RK0][tid #139926855206656]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.905][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:58:21.905][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.908][ERROR][RK0][tid #139926855206656]: coll ps creation done
[HCTR][03:58:21.908][ERROR][RK0][tid #139926855206656]: replica 4 waits for coll ps creation barrier
[HCTR][03:58:21.910][ERROR][RK0][main]: coll ps creation done
[HCTR][03:58:21.910][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][03:58:21.927][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:58:21.927][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.928][ERROR][RK0][tid #139926528055040]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:58:21.928][ERROR][RK0][tid #139926528055040]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:58:21.931][ERROR][RK0][main]: coll ps creation done
[HCTR][03:58:21.932][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][03:58:21.933][ERROR][RK0][tid #139926528055040]: coll ps creation done
[HCTR][03:58:21.933][ERROR][RK0][tid #139926528055040]: replica 0 waits for coll ps creation barrier
[HCTR][03:58:21.933][ERROR][RK0][tid #139926528055040]: replica 0 preparing frequency
[HCTR][03:58:22.809][ERROR][RK0][tid #139926528055040]: replica 0 preparing frequency done
[HCTR][03:58:22.856][ERROR][RK0][tid #139926528055040]: replica 0 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][tid #139926309943040]: replica 7 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][tid #139926318335744]: replica 3 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][tid #139926855206656]: replica 4 calling init per replica
[HCTR][03:58:22.856][ERROR][RK0][tid #139926528055040]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][tid #139926309943040]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][main]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][main]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][tid #139926318335744]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][main]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][main]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][tid #139926855206656]: Calling build_v2
[HCTR][03:58:22.856][ERROR][RK0][tid #139926528055040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][tid #139926309943040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][tid #139926318335744]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:58:22.856][ERROR][RK0][tid #139926855206656]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 03:58:22[2022-12-12 03:58:222022-12-12 03:58:222022-12-12 03:58:222022-12-12 03:58:22[.2022-12-12 03:58:222022-12-12 03:58:22....856610..856610856614856610856624: 2022-12-12 03:58:22856624856626: : : : E.: : EEEE 856645E: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:::136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] 136136136] 136:136using concurrent impl MPSPhase] ] ] using concurrent impl MPSPhase] 136] 
using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase] using concurrent impl MPSPhase



using concurrent impl MPSPhase

[2022-12-12 03:58:22.861201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:58:22.861239: [E2022-12-12 03:58:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc861242:: 196E]  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:178] v100x8, slow pcie
[2022-12-12 03:58:22[.2022-12-12 03:58:22861294.: 861290E:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:58:22 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196861316:] : 178assigning 8 to cpuE] 
 v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[
[2022-12-12 03:58:222022-12-12 03:58:22.[.8613682022-12-12 03:58:22[861357: .2022-12-12 03:58:22: E861381.[E : 8613902022-12-12 03:58:22 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E861401196:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : ] 178:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEassigning 8 to cpu] 2122022-12-12 03:58:22: [
v100x8, slow pcie] .213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:58:22
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8861461] :[.
: remote time is 8.68421178[2022-12-12 03:58:22861510E[
] [2022-12-12 03:58:22.:  2022-12-12 03:58:22v100x8, slow pcie2022-12-12 03:58:22[.861551E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
.2022-12-12 03:58:22861556:  :861573861579.[: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178: : 8616062022-12-12 03:58:22E :] EE: . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178v100x8, slow pcie  E861661/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : :178v100x8, slow pcie::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[196] 
212213: 2022-12-12 03:58:22] v100x8, slow pcie] ] 214[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 8 to cpu
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421] 2022-12-12 03:58:22:861794


[cpu time is 97.0588.196: 2022-12-12 03:58:22[861840
[] E.2022-12-12 03:58:22: 2022-12-12 03:58:22assigning 8 to cpu 861889.E[.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 861919 2022-12-12 03:58:22861917:E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.: 196 E:2022-12-12 03:58:22861968E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 196.:  assigning 8 to cpu:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 862053E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196:assigning 8 to cpu:  :] 213
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214assigning 8 to cpu]  [:] 
remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:58:22212cpu time is 97.0588
:[.] 
2122022-12-12 03:58:22[862180build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] [.2022-12-12 03:58:22: 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 03:58:22862222.E
.: 862243 [862258[E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:58:22: 2022-12-12 03:58:22 E:.E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 212862300 862321:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E:E] 214
 212 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[cpu time is 97.0588:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-12 03:58:22
213[
213.] 2022-12-12 03:58:22] 862439[remote time is 8.68421.remote time is 8.68421: 2022-12-12 03:58:22
862469
E.[: [ 8625002022-12-12 03:58:22E2022-12-12 03:58:22/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: . .:E862554/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc862550213 : :: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE213Eremote time is 8.68421: ]  
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :
[:remote time is 8.684212142022-12-12 03:58:22[214
] .2022-12-12 03:58:22] cpu time is 97.0588[862686.cpu time is 97.0588
2022-12-12 03:58:22: 862706
.E: 862739 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214:cpu time is 97.0588] 214
cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 03:59:42.419700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:59:42.468701: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:59:42.468872: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:59:42.470367: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 03:59:42.546606: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 03:59:42.931572: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 03:59:42.931697: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 03:59:50.260023: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 03:59:50.262702: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 03:59:52.635214: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 03:59:52.635328: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 03:59:52.639337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 03:59:52.639447: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 03:59:52.967023: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 03:59:53. 36271: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 03:59:53. 38885: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 03:59:53. 74304: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 03:59:54.107193: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 03:59:54.111751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 03:59:54.117691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 03:59:54.123650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 03:59:54.129559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 03:59:54.135577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 03:59:54.141412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 03:59:54.147535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 03:59:54.153459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 04:03:16.142188: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 04:03:16.151695: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 04:03:16.157481: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 04:03:16.202860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:03:16.202959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:03:16.202995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:03:16.203028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:03:16.203573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:03:16.203634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.204566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.205252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.218534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 04:03:16.218628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-12 04:03:162022-12-12 04:03:16..218718218730: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved6 solved

[[2022-12-12 04:03:162022-12-12 04:03:16..218807218809: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 6 initing device 6

[[2022-12-12 04:03:162022-12-12 04:03:16..218956218964: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved5 solved

[[2022-12-12 04:03:162022-12-12 04:03:16..219038219039: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 5 initing device 5

[2022-12-12 04:03:16.219093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:03:16.219157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.219272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:03:16.219303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:03:161815.] 219323Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.219363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.219454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[[2022-12-12 04:03:16[2022-12-12 04:03:16.2022-12-12 04:03:16.219497.219499: 219507: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:1815:1815] [2052022-12-12 04:03:16] Building Coll Cache with ... num gpu device is 8] .Building Coll Cache with ... num gpu device is 8
worker 0 thread 3 initing device 3219534

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 04:03:16202[.] 2022-12-12 04:03:162196214 solved.: 
219630E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ :2022-12-12 04:03:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980.:] 2196741980eager alloc mem 381.47 MB: ] 
Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 04:03:16.220004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:03:16.220052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.220204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:03:16.220262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.223382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.223654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.223806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.223864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.224355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.224404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.224476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.227712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.227954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.228005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.228054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.228105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.228582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.228650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:03:16.282314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:03:16.287574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:03:16.287691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:03:16.288530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.289164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.290221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:16.290271: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 56.94 MB
[2022-12-12 04:03:16.308602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 04:03:162022-12-12 04:03:16..309378309378: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980[[] ] [[2022-12-12 04:03:162022-12-12 04:03:16eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes2022-12-12 04:03:162022-12-12 04:03:16..

..309446309447309450309451: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 04:03:16.313512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:03:16.313595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:03:16.315382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.315929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.315994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:03:16.[3160702022-12-12 04:03:16: .E316096 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1024638
] eager release cuda mem 400000000
[2022-12-12 04:03:16.316150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10242022-12-12 04:03:16
.316187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[2022-12-12 04:03:162022-12-12 04:03:16..316224316240: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[[2022-12-12 04:03:162022-12-12 04:03:16..316311316327: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 04:03:16.316388[: 2022-12-12 04:03:16E. 316413/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 1024638
] eager release cuda mem 400000000
[2022-12-12 04:03:16.316487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:03:16.316969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:16.317018: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.00 MB
[2022-12-12 04:03:16.317127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.317841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.318641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.319369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.319942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.320556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:03:16.321386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.321540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.321758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.321854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.321897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.321946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:16.322414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:16.322461: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 56.94 MB
[2022-12-12 04:03:16.322580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:16.322627: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.07 MB
[2022-12-12 04:03:16.322798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:16.322845: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 56.93 MB
[2022-12-12 04:03:16.322897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:16.322933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 04:03:16] .eager release cuda mem 625663322953
: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.04 MB
[[2022-12-12 04:03:162022-12-12 04:03:16..322983322986: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 625663WORKER[0] alloc host memory 56.92 MB

[2022-12-12 04:03:16.323052: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.08 MB
[2022-12-12 04:03:16.329428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.330066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.330113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.12 GB
[2022-12-12 04:03:16.353502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.354108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.354152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.13 GB
[2022-12-12 04:03:16.361004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.361181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.361304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.361649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.361693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.12 GB
[2022-12-12 04:03:16.361807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.361849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.14 GB
[2022-12-12 04:03:16.361924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.361969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.13 GB
[2022-12-12 04:03:16.362008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.362118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.362444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:03:16.362625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.362668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.12 GB
[2022-12-12 04:03:16.362729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.362772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.14 GB
[2022-12-12 04:03:16.363064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:03:16.363110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.12 GB
[[[[[[[[2022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:18.2022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:18..853613.....853612853612: 853612853612853612853618853612: : E: : : : : EE EEEEE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:::::19261926] 19261926192619261926] ] Device 0 init p2p of link 3] ] ] ] ] Device 6 init p2p of link 0Device 4 init p2p of link 5
Device 2 init p2p of link 1Device 5 init p2p of link 6Device 7 init p2p of link 4Device 1 init p2p of link 7Device 3 init p2p of link 2






[[[2022-12-12 04:03:18[[2022-12-12 04:03:18[[[2022-12-12 04:03:18.2022-12-12 04:03:182022-12-12 04:03:18.2022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:18.854183..854183...854186: 854187854187: 854186854190854188: E: : E: : : E EE EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::1980:::1980] 19801980] 198019801980] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB] ] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB





[2022-12-12 04:03:18.855371[: [2022-12-12 04:03:18E[[[[2022-12-12 04:03:18.[ 2022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:182022-12-12 04:03:18.8553782022-12-12 04:03:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc....855380: .:855388855387855388855392: E855398638: : : : E : ] EEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager release cuda mem 625663    /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::::638] :638638638638] eager release cuda mem 625663638] ] ] ] eager release cuda mem 625663
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663




[2022-12-12 04:03:18.871785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:03:18.871952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.872625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:03:18.872778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.872840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.873283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 04:03:18.873431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 04:03:182022-12-12 04:03:18..873596873599: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 7 init p2p of link 1Device 5 init p2p of link 4
[
2022-12-12 04:03:18.873660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.873798[: 2022-12-12 04:03:18E. 873805/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.873892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:03:18.873942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:03:18[.2022-12-12 04:03:18874027.: 874047E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1980Device 3 init p2p of link 0] 
eager alloc mem 611.00 KB
[2022-12-12 04:03:18.874099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.874211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.874283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.874653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.874685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.874915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.874954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.875078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.887442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:03:18.887559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.888427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.889235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 04:03:18Device 0 init p2p of link 1.
889258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:03:18.889373: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:03:18:.1980889388] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.889551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:03:18.889670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.890262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 04:03:18
.890282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.890390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 04:03:18.890443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 04:03:18.890505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.890528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 04:03:18
[.2022-12-12 04:03:18890540.: 890562E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1980Device 5 init p2p of link 7] 
eager alloc mem 611.00 KB
[2022-12-12 04:03:18.890731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.891351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.891476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.891596: [E2022-12-12 04:03:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc891597:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 3 init p2p of link 5
[2022-12-12 04:03:18.891734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.892574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.897960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:03:18.898070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.898910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.911852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 04:03:18.911968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.912080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 04:03:18.912129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:03:18.912195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.912247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.912800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.913049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.913093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.913223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:03:18.913291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:03:18.913341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.913408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.913912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:03:18.914032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.914183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.914226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.914413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:03:18.914533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:03:18.914875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.915358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:03:18.915560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.920467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14952248 / 100000000 nodes ( 14.95 %~15.00 %) | remote 35068996 / 100000000 nodes ( 35.07 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.13 GB | 2.70084 secs 
[2022-12-12 04:03:18.932672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.932963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.933077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.933399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.933790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.934095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.934230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:03:18.962782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14920487 / 100000000 nodes ( 14.92 %~15.00 %) | remote 35100757 / 100000000 nodes ( 35.10 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.12 GB | 2.74274 secs 
[2022-12-12 04:03:18.962919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14964325 / 100000000 nodes ( 14.96 %~15.00 %) | remote 35056919 / 100000000 nodes ( 35.06 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.14 GB | 2.74378 secs 
[2022-12-12 04:03:18.963078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14941412 / 100000000 nodes ( 14.94 %~15.00 %) | remote 35079832 / 100000000 nodes ( 35.08 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.13 GB | 2.74283 secs 
[2022-12-12 04:03:18.963215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14961808 / 100000000 nodes ( 14.96 %~15.00 %) | remote 35059436 / 100000000 nodes ( 35.06 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.14 GB | 2.74391 secs 
[2022-12-12 04:03:18.963369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14927625 / 100000000 nodes ( 14.93 %~15.00 %) | remote 35093619 / 100000000 nodes ( 35.09 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.12 GB | 2.75975 secs 
[2022-12-12 04:03:18.963508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14924483 / 100000000 nodes ( 14.92 %~15.00 %) | remote 35096761 / 100000000 nodes ( 35.10 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.12 GB | 2.7439 secs 
[2022-12-12 04:03:18.963713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14926476 / 100000000 nodes ( 14.93 %~15.00 %) | remote 35094768 / 100000000 nodes ( 35.09 %) | cpu 49978756 / 100000000 nodes ( 49.98 %) | 7.12 GB | 2.74436 secs 
[2022-12-12 04:03:18.963823: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 13.76 GB
[2022-12-12 04:03:20.324069: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.02 GB
[2022-12-12 04:03:20.329778: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.02 GB
[2022-12-12 04:03:20.330515: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 14.02 GB
[2022-12-12 04:03:21.607319: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.29 GB
[2022-12-12 04:03:21.608179: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.29 GB
[2022-12-12 04:03:21.612133: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 14.29 GB
[2022-12-12 04:03:23. 16580: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.50 GB
[2022-12-12 04:03:23. 16774: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.50 GB
[2022-12-12 04:03:23. 17088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 14.50 GB
[2022-12-12 04:03:24.335454: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.72 GB
[2022-12-12 04:03:24.335606: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.72 GB
[2022-12-12 04:03:24.336719: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 14.72 GB
[2022-12-12 04:03:24.336891: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 14.72 GB
[2022-12-12 04:03:24.337234: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 14.72 GB
[2022-12-12 04:03:25.639939: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.91 GB
[2022-12-12 04:03:25.640118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.91 GB
[HCTR][04:03:25.921][ERROR][RK0][tid #139926309943040]: replica 7 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][tid #139926318335744]: replica 3 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][tid #139926528055040]: replica 0 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][tid #139926855206656]: replica 4 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:03:25.921][ERROR][RK0][tid #139926528055040]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][tid #139926309943040]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][tid #139926855206656]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][tid #139926318335744]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:03:25.921][ERROR][RK0][tid #139926309943040]: init per replica done
[HCTR][04:03:25.921][ERROR][RK0][main]: init per replica done
[HCTR][04:03:25.921][ERROR][RK0][tid #139926855206656]: init per replica done
[HCTR][04:03:25.921][ERROR][RK0][main]: init per replica done
[HCTR][04:03:25.921][ERROR][RK0][main]: init per replica done
[HCTR][04:03:25.921][ERROR][RK0][tid #139926318335744]: init per replica done
[HCTR][04:03:25.921][ERROR][RK0][main]: init per replica done
[HCTR][04:03:25.924][ERROR][RK0][tid #139926528055040]: init per replica done
[HCTR][04:03:25.928][ERROR][RK0][tid #139926855206656]: 4 allocated 3276800 at 0x7f44fe320000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926855206656]: 4 allocated 6553600 at 0x7f44fe800000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926855206656]: 4 allocated 3276800 at 0x7f44fee40000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926855206656]: 4 allocated 6553600 at 0x7f44ff160000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926846813952]: 2 allocated 3276800 at 0x7f44fe320000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926846813952]: 2 allocated 6553600 at 0x7f44fe800000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926846813952]: 2 allocated 3276800 at 0x7f44fee40000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926846813952]: 2 allocated 6553600 at 0x7f44ff160000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926309943040]: 7 allocated 3276800 at 0x7f44fe320000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926309943040]: 7 allocated 6553600 at 0x7f44fe800000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926309943040]: 7 allocated 3276800 at 0x7f44fee40000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926309943040]: 7 allocated 6553600 at 0x7f44ff160000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926318335744]: 3 allocated 3276800 at 0x7f44f0320000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926318335744]: 3 allocated 6553600 at 0x7f44f0800000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926318335744]: 3 allocated 3276800 at 0x7f44f0e40000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926318335744]: 3 allocated 6553600 at 0x7f44f1160000
[HCTR][04:03:25.928][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f44fe320000
[HCTR][04:03:25.928][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f44fe800000
[HCTR][04:03:25.928][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f44fee40000
[HCTR][04:03:25.928][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f44ff160000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926393837312]: 5 allocated 3276800 at 0x7f4500320000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926393837312]: 5 allocated 6553600 at 0x7f4500800000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926393837312]: 5 allocated 3276800 at 0x7f4500e40000
[HCTR][04:03:25.928][ERROR][RK0][tid #139926393837312]: 5 allocated 6553600 at 0x7f4501160000
[HCTR][04:03:25.928][ERROR][RK0][tid #139927190750976]: 1 allocated 3276800 at 0x7f44fe320000
[HCTR][04:03:25.928][ERROR][RK0][tid #139927190750976]: 1 allocated 6553600 at 0x7f44fe800000
[HCTR][04:03:25.928][ERROR][RK0][tid #139927190750976]: 1 allocated 3276800 at 0x7f44fee40000
[HCTR][04:03:25.928][ERROR][RK0][tid #139927190750976]: 1 allocated 6553600 at 0x7f44ff160000
[HCTR][04:03:25.930][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f4502920000
[HCTR][04:03:25.930][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f4502e00000
[HCTR][04:03:25.930][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f298a50e800
[HCTR][04:03:25.930][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f298a82e800








