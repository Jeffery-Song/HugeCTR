2022-12-12 03:04:01.425783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.433092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.439228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.443069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.455314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.463310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.466671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.478549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.532459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.533421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.534330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.535318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.536705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.538382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.539059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.539659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.540756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.541293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.542440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.542927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.544291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.544405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.545945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.545951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.547605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.547630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.549387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.549399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.550976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.551047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.552765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.553760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.555553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.556547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.557472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.558406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.559409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.560339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.561270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.562204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.567813: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.573319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.573948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.575437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.576046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.577894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.578009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.578339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.578512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.580246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.580411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.580825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.580996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.583222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.583332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.583779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.583882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.586529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.586707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.587377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.588242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.590118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.590200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.590988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.592191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.592784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.593784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.593824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.594680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.595833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.596414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.598104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.599160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.599576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.600560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.600742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.601654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.601829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.602263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.603808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.604663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.605008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.605340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.606653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.607294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.607753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.608168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.609567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.609955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.610609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.611057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.612306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.612535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.613712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.623945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.624902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.626744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.626792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.628281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.628361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.642688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.642688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.647754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.658082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.663602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.665304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.665348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.665386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.665477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.666809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.669150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.669190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.669280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.669380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.670234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.670269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.673376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.673571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.673603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.674776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.675641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.675811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.677975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.678346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.678442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.678724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.679676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.679945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.682697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.682947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.683077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.683368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.684247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.684437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.687584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.687779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.687869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.688105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.689046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.689185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.691683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.691718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.691908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.692188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.693172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.693308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.695786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.695899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.696038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.696273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.697382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.698058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.699848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.699953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.700060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.700335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.701641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.702267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.704469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.704509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.704608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.704803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.706836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.707519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.708415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.708852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.709092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.709145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.709348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.711115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.712020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.713640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.713954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.714193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.714315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.714520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.715800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.716983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.717972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.718544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.718588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.718842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.719126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.720432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.721537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.722729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.723154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.723180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.723462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.723712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.724951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.726269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.727253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.727669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.727699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.727941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.728227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.729761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.731103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.732330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.733728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.733740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.734194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.735221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.735523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.737310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.738417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.738532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.738648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.739303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.739326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.741154: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.741339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.742368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.742414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.742449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.743487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.744129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.744993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.746259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.746392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.746484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.747454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.748115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.748813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.750231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.750247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.750393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.751963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.752109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.753040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.754207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.756065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.756144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.756143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.757273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.757508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.758509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.758978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.760600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.760768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.761516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.761957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.762067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.762914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.763647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.765009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.765199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.766002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.766433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.767651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.768043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.771398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.771596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.773313: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.773342: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.773536: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.774135: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.775362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.775563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.779977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.781939: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.782139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.784307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.784308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.784345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.784495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.786665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.788569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.788612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.788658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.788687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.790791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.792319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.792392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.792485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.792529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.792625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.795056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.796995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.827934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.830752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.833763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.840129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.880168: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:04:01.890626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.896580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:01.901845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.912189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.913180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.913716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.914178: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:02.914233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:04:02.931516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.932163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.932737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.933538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.934258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:02.934730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:04:02.979010: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:02.979223: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.023800: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:04:03.163627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.164276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.164819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.165358: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.165414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:04:03.184125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.184805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.185587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.186433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.186987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.187482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:04:03.212055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.212667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.213406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.213978: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.214038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:04:03.231626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.232268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.232779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.233341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.233855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.234547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:04:03.235701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.236631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.237163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.237622: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.237682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:04:03.240142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.240721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.241236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.241700: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.241743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:04:03.243225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.243844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.244362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.244832: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.244881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:04:03.255683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.256149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.256282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.257148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.257245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.258048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.258117: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.258168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:04:03.258550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.259050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.260251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.260498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.261210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.261484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:04:03.261971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.262486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.262964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:04:03.263241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.263891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.264607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.265189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.265711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.266183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:04:03.273675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.273716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.273885: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.274285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.274822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.275064: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:04:03.275297: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:04:03.275353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:04:03.277669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.278284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.278803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.279412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.279942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.280420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:04:03.293394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.294075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.294580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.295189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.295771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:04:03.296266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:04:03.308677: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.308898: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.310836: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 03:04:03.311002: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.311159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.312486: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.312685: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.313188: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 03:04:03.313694: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 03:04:03.316849: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.316995: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.318031: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:04:03.326884: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.327024: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.327887: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 03:04:03.341625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.341842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:04:03.343796: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][03:04:04.597][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.599][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.599][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.601][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.601][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.602][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:04:04.646][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 100it [00:01, 83.14it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 201it [00:01, 181.69it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 97it [00:01, 82.81it/s]warmup run: 100it [00:01, 84.72it/s]warmup run: 98it [00:01, 80.91it/s]warmup run: 99it [00:01, 81.72it/s]warmup run: 302it [00:01, 290.63it/s]warmup run: 102it [00:01, 89.73it/s]warmup run: 197it [00:01, 182.69it/s]warmup run: 195it [00:01, 178.26it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 197it [00:01, 176.99it/s]warmup run: 198it [00:01, 177.69it/s]warmup run: 399it [00:01, 398.21it/s]warmup run: 203it [00:01, 192.72it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 297it [00:01, 292.71it/s]warmup run: 287it [00:01, 277.54it/s]warmup run: 98it [00:01, 85.40it/s]warmup run: 296it [00:01, 283.62it/s]warmup run: 297it [00:01, 284.23it/s]warmup run: 499it [00:02, 508.58it/s]warmup run: 304it [00:01, 305.20it/s]warmup run: 100it [00:01, 87.82it/s]warmup run: 397it [00:01, 406.77it/s]warmup run: 381it [00:01, 383.84it/s]warmup run: 195it [00:01, 183.59it/s]warmup run: 393it [00:01, 391.57it/s]warmup run: 398it [00:01, 398.40it/s]warmup run: 601it [00:02, 614.98it/s]warmup run: 405it [00:01, 420.49it/s]warmup run: 200it [00:01, 189.77it/s]warmup run: 497it [00:02, 517.77it/s]warmup run: 482it [00:02, 500.29it/s]warmup run: 294it [00:01, 293.58it/s]warmup run: 491it [00:02, 499.05it/s]warmup run: 500it [00:02, 512.57it/s]warmup run: 704it [00:02, 709.57it/s]warmup run: 506it [00:01, 532.20it/s]warmup run: 300it [00:01, 301.20it/s]warmup run: 600it [00:02, 624.57it/s]warmup run: 584it [00:02, 609.33it/s]warmup run: 393it [00:01, 406.80it/s]warmup run: 592it [00:02, 604.93it/s]warmup run: 603it [00:02, 619.95it/s]warmup run: 806it [00:02, 785.58it/s]warmup run: 608it [00:02, 636.22it/s]warmup run: 399it [00:01, 414.23it/s]warmup run: 702it [00:02, 714.80it/s]warmup run: 685it [00:02, 701.00it/s]warmup run: 491it [00:01, 514.90it/s]warmup run: 691it [00:02, 691.28it/s]warmup run: 706it [00:02, 712.60it/s]warmup run: 908it [00:02, 845.34it/s]warmup run: 710it [00:02, 725.48it/s]warmup run: 495it [00:01, 516.81it/s]warmup run: 803it [00:02, 787.45it/s]warmup run: 785it [00:02, 775.08it/s]warmup run: 591it [00:02, 617.75it/s]warmup run: 791it [00:02, 765.46it/s]warmup run: 809it [00:02, 789.10it/s]warmup run: 1011it [00:02, 893.31it/s]warmup run: 813it [00:02, 800.38it/s]warmup run: 595it [00:02, 619.39it/s]warmup run: 904it [00:02, 845.09it/s]warmup run: 886it [00:02, 836.00it/s]warmup run: 691it [00:02, 705.91it/s]warmup run: 893it [00:02, 830.62it/s]warmup run: 910it [00:02, 845.74it/s]warmup run: 1115it [00:02, 932.81it/s]warmup run: 915it [00:02, 857.42it/s]warmup run: 697it [00:02, 711.70it/s]warmup run: 1005it [00:02, 889.80it/s]warmup run: 986it [00:02, 879.21it/s]warmup run: 791it [00:02, 779.05it/s]warmup run: 995it [00:02, 880.74it/s]warmup run: 1011it [00:02, 886.03it/s]warmup run: 1217it [00:02, 952.08it/s]warmup run: 1018it [00:02, 903.47it/s]warmup run: 800it [00:02, 790.12it/s]warmup run: 1107it [00:02, 924.47it/s]warmup run: 1088it [00:02, 916.33it/s]warmup run: 891it [00:02, 835.79it/s]warmup run: 1098it [00:02, 920.38it/s]warmup run: 1111it [00:02, 915.81it/s]warmup run: 1122it [00:02, 939.42it/s]warmup run: 903it [00:02, 851.05it/s]warmup run: 1319it [00:02, 946.81it/s]warmup run: 1208it [00:02, 947.42it/s]warmup run: 1189it [00:02, 942.99it/s]warmup run: 990it [00:02, 876.83it/s]warmup run: 1201it [00:02, 951.23it/s]warmup run: 1211it [00:02, 933.53it/s]warmup run: 1226it [00:02, 966.96it/s]warmup run: 1005it [00:02, 896.77it/s]warmup run: 1418it [00:02, 951.78it/s]warmup run: 1309it [00:02, 960.96it/s]warmup run: 1289it [00:02, 956.38it/s]warmup run: 1089it [00:02, 908.08it/s]warmup run: 1303it [00:02, 969.08it/s]warmup run: 1329it [00:02, 983.66it/s]warmup run: 1311it [00:02, 938.15it/s]warmup run: 1108it [00:02, 932.64it/s]warmup run: 1518it [00:03, 965.65it/s]warmup run: 1414it [00:02, 985.49it/s]warmup run: 1389it [00:02, 967.90it/s]warmup run: 1189it [00:02, 932.63it/s]warmup run: 1407it [00:02, 988.14it/s]warmup run: 1433it [00:02, 997.46it/s]warmup run: 1409it [00:02, 945.16it/s]warmup run: 1211it [00:02, 958.21it/s]warmup run: 1620it [00:03, 979.21it/s]warmup run: 1516it [00:03, 988.51it/s]warmup run: 1491it [00:03, 982.02it/s]warmup run: 1288it [00:02, 949.05it/s]warmup run: 1510it [00:03, 998.19it/s]warmup run: 1537it [00:02, 1008.99it/s]warmup run: 1507it [00:03, 950.19it/s]warmup run: 1313it [00:02, 972.88it/s]warmup run: 1721it [00:03, 987.79it/s]warmup run: 1593it [00:03, 992.12it/s]warmup run: 1617it [00:03, 987.24it/s]warmup run: 1388it [00:02, 962.29it/s]warmup run: 1612it [00:03, 1001.60it/s]warmup run: 1641it [00:03, 1016.20it/s]warmup run: 1605it [00:03, 955.26it/s]warmup run: 1415it [00:02, 985.70it/s]warmup run: 1822it [00:03, 993.44it/s]warmup run: 1694it [00:03, 997.00it/s]warmup run: 1718it [00:03, 984.61it/s]warmup run: 1487it [00:02, 969.99it/s]warmup run: 1714it [00:03, 997.71it/s] warmup run: 1745it [00:03, 1022.37it/s]warmup run: 1517it [00:02, 988.20it/s]warmup run: 1924it [00:03, 999.70it/s]warmup run: 1702it [00:03, 935.72it/s]warmup run: 1795it [00:03, 991.69it/s]warmup run: 1818it [00:03, 986.40it/s]warmup run: 1587it [00:03, 978.09it/s]warmup run: 1816it [00:03, 1004.00it/s]warmup run: 1849it [00:03, 1024.41it/s]warmup run: 1618it [00:03, 993.90it/s]warmup run: 2030it [00:03, 1017.21it/s]warmup run: 1797it [00:03, 920.38it/s]warmup run: 1895it [00:03, 987.11it/s]warmup run: 1918it [00:03, 984.64it/s]warmup run: 1687it [00:03, 983.86it/s]warmup run: 1918it [00:03, 1006.87it/s]warmup run: 1953it [00:03, 1026.91it/s]warmup run: 1719it [00:03, 998.38it/s]warmup run: 2149it [00:03, 1068.38it/s]warmup run: 1890it [00:03, 915.35it/s]warmup run: 2024it [00:03, 1004.48it/s]warmup run: 1995it [00:03, 979.32it/s]warmup run: 1788it [00:03, 989.75it/s]warmup run: 2026it [00:03, 1027.19it/s]warmup run: 2063it [00:03, 1046.65it/s]warmup run: 1820it [00:03, 998.06it/s]warmup run: 2270it [00:03, 1109.21it/s]warmup run: 1983it [00:03, 917.21it/s]warmup run: 2147it [00:03, 1070.37it/s]warmup run: 2114it [00:03, 1039.81it/s]warmup run: 1892it [00:03, 1003.09it/s]warmup run: 2149it [00:03, 1087.34it/s]warmup run: 2181it [00:03, 1084.28it/s]warmup run: 1921it [00:03, 1000.27it/s]warmup run: 2391it [00:03, 1137.81it/s]warmup run: 2091it [00:03, 964.74it/s]warmup run: 2270it [00:03, 1116.40it/s]warmup run: 2234it [00:03, 1085.74it/s]warmup run: 1995it [00:03, 1009.48it/s]warmup run: 2272it [00:03, 1129.41it/s]warmup run: 2301it [00:03, 1116.12it/s]warmup run: 2025it [00:03, 1010.11it/s]warmup run: 2512it [00:03, 1157.03it/s]warmup run: 2203it [00:03, 1009.95it/s]warmup run: 2392it [00:03, 1146.78it/s]warmup run: 2356it [00:03, 1123.65it/s]warmup run: 2115it [00:03, 1064.46it/s]warmup run: 2396it [00:03, 1159.66it/s]warmup run: 2422it [00:03, 1141.59it/s]warmup run: 2144it [00:03, 1062.94it/s]warmup run: 2633it [00:04, 1169.90it/s]warmup run: 2316it [00:03, 1043.53it/s]warmup run: 2515it [00:03, 1169.84it/s]warmup run: 2478it [00:03, 1151.78it/s]warmup run: 2237it [00:03, 1110.66it/s]warmup run: 2519it [00:03, 1180.28it/s]warmup run: 2544it [00:03, 1162.47it/s]warmup run: 2264it [00:03, 1103.45it/s]warmup run: 2753it [00:04, 1177.76it/s]warmup run: 2429it [00:04, 1067.37it/s]warmup run: 2638it [00:04, 1187.39it/s]warmup run: 2600it [00:04, 1170.71it/s]warmup run: 2360it [00:03, 1144.37it/s]warmup run: 2643it [00:04, 1196.30it/s]warmup run: 2666it [00:03, 1178.17it/s]warmup run: 2384it [00:03, 1131.64it/s]warmup run: 2872it [00:04, 1180.36it/s]warmup run: 2542it [00:04, 1083.50it/s]warmup run: 2760it [00:04, 1195.86it/s]warmup run: 2722it [00:04, 1184.69it/s]warmup run: 2483it [00:03, 1168.41it/s]warmup run: 2765it [00:04, 1201.12it/s]warmup run: 2786it [00:04, 1184.36it/s]warmup run: 2504it [00:03, 1151.50it/s]warmup run: 2993it [00:04, 1186.41it/s]warmup run: 3000it [00:04, 680.66it/s] warmup run: 2655it [00:04, 1095.09it/s]warmup run: 2883it [00:04, 1203.50it/s]warmup run: 2843it [00:04, 1190.77it/s]warmup run: 2605it [00:04, 1183.68it/s]warmup run: 2886it [00:04, 1203.01it/s]warmup run: 2908it [00:04, 1192.17it/s]warmup run: 2626it [00:03, 1170.14it/s]warmup run: 2767it [00:04, 1101.30it/s]warmup run: 3000it [00:04, 689.94it/s] warmup run: 2965it [00:04, 1199.48it/s]warmup run: 2728it [00:04, 1197.50it/s]warmup run: 3000it [00:04, 681.89it/s] warmup run: 3000it [00:04, 683.05it/s] warmup run: 3000it [00:04, 700.33it/s] warmup run: 2747it [00:04, 1181.84it/s]warmup run: 2882it [00:04, 1113.19it/s]warmup run: 2850it [00:04, 1201.66it/s]warmup run: 2866it [00:04, 1183.94it/s]warmup run: 3000it [00:04, 661.89it/s] warmup run: 2973it [00:04, 1209.13it/s]warmup run: 3000it [00:04, 692.62it/s] warmup run: 2987it [00:04, 1189.71it/s]warmup run: 3000it [00:04, 694.95it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.20it/s]warmup should be done:   5%|▌         | 151/3000 [00:00<00:01, 1509.84it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.13it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1638.31it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1623.85it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.32it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1592.78it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.74it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1640.60it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.92it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1649.28it/s]warmup should be done:  10%|█         | 307/3000 [00:00<00:01, 1535.99it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.60it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1612.11it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.02it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.86it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1653.84it/s]warmup should be done:  15%|█▌        | 462/3000 [00:00<00:01, 1540.16it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1639.02it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1664.00it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1629.23it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1652.63it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1633.19it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1601.89it/s]warmup should be done:  21%|██        | 617/3000 [00:00<00:01, 1542.87it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1666.32it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1636.29it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1647.78it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1631.66it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1651.28it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1618.82it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1623.92it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1667.25it/s]warmup should be done:  26%|██▌       | 776/3000 [00:00<00:01, 1556.88it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1634.86it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1632.01it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1642.15it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1640.07it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1649.24it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1623.53it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1667.12it/s]warmup should be done:  31%|███       | 933/3000 [00:00<00:01, 1560.59it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1643.84it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1650.18it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1630.80it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1638.17it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1645.60it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1605.40it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1663.03it/s]warmup should be done:  36%|███▋      | 1090/3000 [00:00<00:01, 1559.21it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1643.17it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1650.04it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1622.23it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1641.93it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1630.26it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1592.05it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1664.55it/s]warmup should be done:  42%|████▏     | 1247/3000 [00:00<00:01, 1560.95it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1646.02it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1649.37it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1647.31it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1623.64it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1608.53it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1602.16it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1664.38it/s]warmup should be done:  47%|████▋     | 1404/3000 [00:00<00:01, 1562.79it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1647.99it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1652.38it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1654.73it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1619.37it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1603.84it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1613.61it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1665.85it/s]warmup should be done:  52%|█████▏    | 1561/3000 [00:01<00:00, 1561.80it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1643.57it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1654.64it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1658.77it/s]warmup should be done:  55%|█████▍    | 1645/3000 [00:01<00:00, 1617.05it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1608.18it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1620.79it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1666.38it/s]warmup should be done:  57%|█████▋    | 1718/3000 [00:01<00:00, 1552.26it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1663.14it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1635.80it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1643.18it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1616.05it/s]warmup should be done:  60%|█████▉    | 1798/3000 [00:01<00:00, 1614.90it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1627.56it/s]warmup should be done:  67%|██████▋   | 2003/3000 [00:01<00:00, 1661.72it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1560.02it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1639.29it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1662.46it/s]warmup should be done:  66%|██████▌   | 1981/3000 [00:01<00:00, 1643.79it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1614.05it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1618.79it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1630.57it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1660.46it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1577.27it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1661.99it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1643.32it/s]warmup should be done:  71%|███████   | 2123/3000 [00:01<00:00, 1617.74it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1621.23it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1619.22it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1631.42it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1658.59it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1588.29it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1660.52it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1639.75it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1628.66it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1620.06it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1630.08it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1631.45it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1655.72it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1591.32it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1657.40it/s]warmup should be done:  82%|████████▎ | 2475/3000 [00:01<00:00, 1636.72it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1633.87it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1636.22it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1618.26it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1629.09it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1593.81it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1644.87it/s]warmup should be done:  89%|████████▉ | 2665/3000 [00:01<00:00, 1657.63it/s]warmup should be done:  88%|████████▊ | 2639/3000 [00:01<00:00, 1637.07it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1642.54it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1620.58it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1643.51it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1630.43it/s]warmup should be done:  89%|████████▉ | 2682/3000 [00:01<00:00, 1595.93it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1658.51it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1636.74it/s]warmup should be done:  94%|█████████▍| 2834/3000 [00:01<00:00, 1635.96it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1648.75it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1648.22it/s]warmup should be done:  92%|█████████▎| 2775/3000 [00:01<00:00, 1622.04it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1631.91it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1598.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.48it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1647.71it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1635.43it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1657.12it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1629.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.85it/s]warmup should be done:  99%|█████████▉| 2966/3000 [00:01<00:00, 1654.70it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1637.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1625.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1574.05it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1696.89it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.97it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.70it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.97it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.89it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.43it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.14it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.54it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1702.50it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1677.64it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1702.29it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.17it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1671.77it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1700.54it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1654.94it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1677.55it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1706.02it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1684.17it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1686.07it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1706.20it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1702.28it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1656.28it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1687.88it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1689.39it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1707.25it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1689.85it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1704.51it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1696.57it/s]warmup should be done:  23%|██▎       | 687/3000 [00:00<00:01, 1712.59it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1659.63it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1694.04it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1700.44it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1705.72it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1694.09it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1700.22it/s]warmup should be done:  29%|██▊       | 856/3000 [00:00<00:01, 1707.05it/s]warmup should be done:  29%|██▊       | 860/3000 [00:00<00:01, 1716.18it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1660.78it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1696.44it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1706.70it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1706.60it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1694.71it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1706.35it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1704.06it/s]warmup should be done:  34%|███▍      | 1033/3000 [00:00<00:01, 1717.56it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1696.92it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1709.07it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1658.72it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1707.68it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1692.98it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1696.16it/s]warmup should be done:  40%|████      | 1205/3000 [00:00<00:01, 1715.16it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1708.49it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1700.18it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1657.41it/s]warmup should be done:  40%|███▉      | 1198/3000 [00:00<00:01, 1693.38it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1708.84it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1696.39it/s]warmup should be done:  46%|████▌     | 1378/3000 [00:00<00:00, 1719.08it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1698.75it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1657.71it/s]warmup should be done:  46%|████▌     | 1371/3000 [00:00<00:00, 1712.73it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1695.20it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:00, 1686.53it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1697.14it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1707.25it/s]warmup should be done:  52%|█████▏    | 1551/3000 [00:00<00:00, 1721.18it/s]warmup should be done:  51%|█████▏    | 1543/3000 [00:00<00:00, 1714.69it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1699.38it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1658.61it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1694.54it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1678.87it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1697.72it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1707.57it/s]warmup should be done:  57%|█████▋    | 1715/3000 [00:01<00:00, 1715.08it/s]warmup should be done:  57%|█████▋    | 1724/3000 [00:01<00:00, 1720.87it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1700.33it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1660.75it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1687.90it/s]warmup should be done:  57%|█████▋    | 1705/3000 [00:01<00:00, 1675.68it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1698.06it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1707.82it/s]warmup should be done:  63%|██████▎   | 1887/3000 [00:01<00:00, 1715.45it/s]warmup should be done:  63%|██████▎   | 1897/3000 [00:01<00:00, 1721.20it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1661.70it/s]warmup should be done:  63%|██████▎   | 1876/3000 [00:01<00:00, 1695.73it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1685.57it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1673.64it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1698.11it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1705.71it/s]warmup should be done:  69%|██████▊   | 2059/3000 [00:01<00:00, 1715.70it/s]warmup should be done:  69%|██████▉   | 2070/3000 [00:01<00:00, 1721.34it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1658.48it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1690.04it/s]warmup should be done:  68%|██████▊   | 2042/3000 [00:01<00:00, 1684.86it/s]warmup should be done:  68%|██████▊   | 2041/3000 [00:01<00:00, 1674.29it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1696.28it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1701.52it/s]warmup should be done:  74%|███████▍  | 2231/3000 [00:01<00:00, 1711.70it/s]warmup should be done:  75%|███████▍  | 2243/3000 [00:01<00:00, 1719.90it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1656.03it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1680.59it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1684.29it/s]warmup should be done:  74%|███████▎  | 2209/3000 [00:01<00:00, 1675.43it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1696.54it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1701.34it/s]warmup should be done:  80%|████████  | 2415/3000 [00:01<00:00, 1718.93it/s]warmup should be done:  80%|████████  | 2403/3000 [00:01<00:00, 1710.53it/s]warmup should be done:  78%|███████▊  | 2333/3000 [00:01<00:00, 1655.15it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1682.28it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1676.70it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1681.09it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1689.55it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1702.29it/s]warmup should be done:  86%|████████▌ | 2575/3000 [00:01<00:00, 1710.94it/s]warmup should be done:  83%|████████▎ | 2499/3000 [00:01<00:00, 1652.29it/s]warmup should be done:  86%|████████▌ | 2587/3000 [00:01<00:00, 1705.94it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1685.53it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1683.81it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1680.17it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1691.39it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1702.93it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1711.80it/s]warmup should be done:  89%|████████▉ | 2665/3000 [00:01<00:00, 1650.16it/s]warmup should be done:  92%|█████████▏| 2758/3000 [00:01<00:00, 1704.71it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1687.95it/s]warmup should be done:  91%|█████████ | 2719/3000 [00:01<00:00, 1688.84it/s]warmup should be done:  91%|█████████ | 2723/3000 [00:01<00:00, 1679.78it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1691.08it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1702.61it/s]warmup should be done:  97%|█████████▋| 2919/3000 [00:01<00:00, 1711.03it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1650.74it/s]warmup should be done:  98%|█████████▊| 2931/3000 [00:01<00:00, 1710.34it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1691.70it/s]warmup should be done:  96%|█████████▋| 2891/3000 [00:01<00:00, 1676.93it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1664.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1713.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1708.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1704.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.52it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1679.34it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1655.33it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.32it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dc8c10d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dc8bf1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dc8c1100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dcc04730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dcc06d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dcc03b80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dc8cf2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f65dc8c1100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 03:05:33.816885: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f60ff031140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:33.816943: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:33.826381: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:33.907543: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6112830ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:33.907614: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:33.917036: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:34.661575: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6112830fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:34.661639: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:34.671182: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:34.709539: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f61168317c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:34.709601: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:34.711607: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6113029b60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:34.711654: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:34.715358: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f611a82cac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:34.715409: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:34.718771: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:34.718976: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:34.724201: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:34.792316: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6112f92780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:34.792380: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:34.800469: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:34.822648: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6107031870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:05:34.822714: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:05:34.832033: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:05:41.139626: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.257374: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.604851: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.617132: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.633469: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.737160: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.805854: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:05:41.816545: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:06:34.846][ERROR][RK0][tid #140055242856192]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:06:34.847][ERROR][RK0][tid #140055242856192]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:34.853][ERROR][RK0][tid #140055242856192]: coll ps creation done
[HCTR][03:06:34.853][ERROR][RK0][tid #140055242856192]: replica 5 waits for coll ps creation barrier
[HCTR][03:06:35.213][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:06:35.213][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.221][ERROR][RK0][main]: coll ps creation done
[HCTR][03:06:35.221][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][03:06:35.241][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:06:35.241][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.250][ERROR][RK0][main]: coll ps creation done
[HCTR][03:06:35.250][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][03:06:35.275][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:06:35.276][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.278][ERROR][RK0][tid #140055301572352]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:06:35.279][ERROR][RK0][tid #140055301572352]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.281][ERROR][RK0][main]: coll ps creation done
[HCTR][03:06:35.281][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][03:06:35.283][ERROR][RK0][tid #140055301572352]: coll ps creation done
[HCTR][03:06:35.283][ERROR][RK0][tid #140055301572352]: replica 7 waits for coll ps creation barrier
[HCTR][03:06:35.337][ERROR][RK0][tid #140055704225536]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:06:35.337][ERROR][RK0][tid #140055704225536]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.346][ERROR][RK0][tid #140055704225536]: coll ps creation done
[HCTR][03:06:35.346][ERROR][RK0][tid #140055704225536]: replica 1 waits for coll ps creation barrier
[HCTR][03:06:35.386][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:06:35.386][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.391][ERROR][RK0][main]: coll ps creation done
[HCTR][03:06:35.391][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][03:06:35.419][ERROR][RK0][tid #140055838443264]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:06:35.419][ERROR][RK0][tid #140055838443264]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][03:06:35.427][ERROR][RK0][tid #140055838443264]: coll ps creation done
[HCTR][03:06:35.427][ERROR][RK0][tid #140055838443264]: replica 2 waits for coll ps creation barrier
[HCTR][03:06:35.427][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][03:06:36.257][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][03:06:36.301][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][tid #140055704225536]: replica 1 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][tid #140055242856192]: replica 5 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][tid #140055838443264]: replica 2 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][main]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][tid #140055838443264]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][tid #140055301572352]: replica 7 calling init per replica
[HCTR][03:06:36.301][ERROR][RK0][main]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][tid #140055704225536]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][main]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][tid #140055242856192]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][tid #140055838443264]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][main]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][tid #140055704225536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][tid #140055301572352]: Calling build_v2
[HCTR][03:06:36.301][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][tid #140055242856192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:06:36.301][ERROR][RK0][tid #140055301572352]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 03:06:362022-12-12 03:06:362022-12-12 03:06:36[.2022-12-12 03:06:36..2022-12-12 03:06:362022-12-12 03:06:36301428.3014233014302022-12-12 03:06:36..: 301428: : [.301439301431E: EE301454: :  E  : EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 2022-12-12 03:06:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc.:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:301496136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::] 136: ] ] :136136using concurrent impl MPS] 
Eusing concurrent impl MPSusing concurrent impl MPS136] ] using concurrent impl MPS 

] using concurrent impl MPSusing concurrent impl MPS
/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccusing concurrent impl MPS

:
136] using concurrent impl MPS
[2022-12-12 03:06:36.305704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:06:36.305741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 03:06:36.305797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 03:06:36212.] 305804build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:06:36.[[3058482022-12-12 03:06:362022-12-12 03:06:36: ..E305851305847 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE:  213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :[:remote time is 8.684211962022-12-12 03:06:36178
] .] assigning 8 to cpu305893v100x8, slow pcie[
: 
2022-12-12 03:06:36E. 305924[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: 2022-12-12 03:06:36:2022-12-12 03:06:36E.178.[ 305942] 3059382022-12-12 03:06:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pcie: .:E
E305970[214  : [2022-12-12 03:06:36] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 03:06:36.cpu time is 97.0588:: .305988
196178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc306009: ] [] 2022-12-12 03:06:36:: Eassigning 8 to cpuv100x8, slow pcie.212E 

306042]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 03:06:36E
2022-12-12 03:06:36:178.[ .196] 306087[2022-12-12 03:06:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc306097] v100x8, slow pcie: 2022-12-12 03:06:36.:: assigning 8 to cpu
E.306127178E
 306145[: ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 03:06:36Ev100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E. 
:178 306204/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:2022-12-12 03:06:36] v100x8, slow pcie:E2022-12-12 03:06:36212.assigning 8 to cpu
213 .] 306278306265
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : remote time is 8.68421:2022-12-12 03:06:36
EE
196.  [] [[306335/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:06:36assigning 8 to cpu2022-12-12 03:06:362022-12-12 03:06:36: ::.
..E196212306380306387306389 ] ] : : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8EEE:[

   1962022-12-12 03:06:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] .:::2022-12-12 03:06:36assigning 8 to cpu306487212213214.
: ] ] ] 306520[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421cpu time is 97.0588: 2022-12-12 03:06:36 


E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [306570[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:06:36: 2022-12-12 03:06:36212[:..E] 2022-12-12 03:06:36213306612306610 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
306621remote time is 8.68421EE:: 
  [212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:06:36[]  ::.2022-12-12 03:06:36build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213214306702.
:] ] : 306722212[remote time is 8.68421cpu time is 97.0588E: ] 
2022-12-12 03:06:36
 Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
3068002022-12-12 03:06:36:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .213:E306831] [214 : remote time is 8.684212022-12-12 03:06:36] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
.cpu time is 97.0588: 213306873[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 2022-12-12 03:06:36:remote time is 8.68421E.214
 306924] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588:[E
2132022-12-12 03:06:36 ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421306989:
: 214E] [ cpu time is 97.05882022-12-12 03:06:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.:307061214: ] Ecpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 03:07:53.448690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:07:53.504343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 03:07:53.504424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 10000000
[2022-12-12 03:07:53.622517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:07:53.622621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:07:53.622664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:07:53.622708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:07:53.623314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.624598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.625597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.637784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 03:07:53.637844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 03:07:53.638120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 03:07:53.638176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 03:07:53.638252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.638305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 03:07:53.638358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 03:07:53.638391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 03:07:53.638446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 03:07:53.638573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.638763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.638850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.640937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.641210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.641268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.641759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.643490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.643589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.643647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.643693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.645035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 03:07:53.645110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 03:07:53.645684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.647190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 03:07:532022-12-12 03:07:53..647660647660: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 5 solved4 solved

[[2022-12-12 03:07:532022-12-12 03:07:53..647736647736: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 5 initing device 5worker 0 thread 4 initing device 4

[[2022-12-12 03:07:532022-12-12 03:07:53..648267648267: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 03:07:53.648369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.650630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.650689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.651999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:07:53.652050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[[[2022-12-12 03:07:532022-12-12 03:07:532022-12-12 03:07:532022-12-12 03:07:53....724695724695724686724695: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[[2022-12-12 03:07:53[2022-12-12 03:07:53[.2022-12-12 03:07:53.2022-12-12 03:07:53725122.725125.: 725127: 725128E: E:  E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:1980:] 1980] 1980eager alloc mem 1024.00 Bytes] eager alloc mem 1024.00 Bytes] 
eager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Bytes

[2022-12-12 03:07:53.733377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 03:07:53.733855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 03:07:532022-12-12 03:07:53..735563735567: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 03:07:53.[7358832022-12-12 03:07:53: .E735888 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 1024.00 Bytes1980
] eager alloc mem 1024.00 Bytes
[2022-12-12 03:07:53.749508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:07:53.749581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:07:53.749598: E[ 2022-12-12 03:07:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:749627638: ] Eeager release cuda mem 1024 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:07:53.749678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 03:07:53] .eager release cuda mem 2749679
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:07:53.749736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 03:07:53eager release cuda mem 400000000.
749757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:07:53[.2022-12-12 03:07:53749811.: 749800E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[2022-12-12 03:07:53.749892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:07:53.749936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:07:53.755143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.755536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:07:53[.2022-12-12 03:07:53755603.: 755596E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 1024
[2022-12-12 03:07:53.[7556742022-12-12 03:07:53: .E755680 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[] :2022-12-12 03:07:53eager release cuda mem 400000000638.
] [755692eager release cuda mem 22022-12-12 03:07:53: 
.E755721 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 1024[1980
2022-12-12 03:07:53] .eager alloc mem 38.53 MB755771
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 03:07:53.755807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:07:53.755862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:07:53.756231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.756739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.756977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 03:07:53.757249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 03:07:53.758429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.758997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.759546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.764458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.764563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.765084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.765127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.765476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:53.765560: E[ 2022-12-12 03:07:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7655681980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:53.765665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:07:53.765796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 03:07:53eager alloc mem 611.00 KB.
765819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.765875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.766057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:53.766124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:07:53638.] 766140eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:07:53.766216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:07:53.766241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:07:53.766282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:07:53.766330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:07:53.766369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:07:53.766785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:53[.2022-12-12 03:07:53766822.: 766827E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 25855[
[2022-12-12 03:07:532022-12-12 03:07:53..766870766870[: : [2022-12-12 03:07:53EE2022-12-12 03:07:53.  .766887/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc766892: ::: E[1980638E 2022-12-12 03:07:53] ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager alloc mem 25.25 KBeager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:766925

:638: 1980] E] eager release cuda mem 25855 eager alloc mem 4.77 GB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-12 03:07:53.767002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB[
2022-12-12 03:07:53.767022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:07:53.767313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:07:53.767375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 03:07:53.767416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:07:53.767615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:07:53.767644: E[ 2022-12-12 03:07:53/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:767657638: ] Eeager release cuda mem 25855 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[2022-12-12 03:07:532022-12-12 03:07:53..767695767698: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 4.77 GB

[2022-12-12 03:07:53.767769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:07:53.806435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:07:53.808050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:53.809045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:53.809127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:07:53.809798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:07:53.809837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 03:07:542022-12-12 03:07:542022-12-12 03:07:542022-12-12 03:07:542022-12-12 03:07:542022-12-12 03:07:542022-12-12 03:07:542022-12-12 03:07:54........738512738512738520738514738512738513738516738512: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[[[[2022-12-12 03:07:542022-12-12 03:07:54[[2022-12-12 03:07:54[[2022-12-12 03:07:54..2022-12-12 03:07:542022-12-12 03:07:54.2022-12-12 03:07:542022-12-12 03:07:54.739577739580..739578..739579: : 739582739583: 739582739587: EE: : E: : E  EE EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638638::638::638] ] 638638] 638638] eager release cuda mem 625663eager release cuda mem 625663] ] eager release cuda mem 625663] ] eager release cuda mem 625663

eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663




[2022-12-12 03:07:54[[.2022-12-12 03:07:542022-12-12 03:07:54739824.[[.[: 739828[[2022-12-12 03:07:542022-12-12 03:07:547398312022-12-12 03:07:54E: 2022-12-12 03:07:542022-12-12 03:07:54..: . E..739837739839E739841/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 739844739844: :  : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE1980:EE  : ] 1980  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::] :
eager alloc mem 611.00 KB::19801980eager alloc mem 611.00 KB1980
19801980] ] 
] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 03:07:54.740724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:07:54638.] [740739eager release cuda mem 6256632022-12-12 03:07:54: 
.E[[740748 2022-12-12 03:07:54[2022-12-12 03:07:54[[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.2022-12-12 03:07:54.2022-12-12 03:07:542022-12-12 03:07:54E:740764.740769.. 638: 740770: [740772740772/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] E: E2022-12-12 03:07:54: : :eager release cuda mem 625663 E .EE638
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc740811  ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663638:638E::
] 638]  638638eager release cuda mem 625663] [eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] 
eager release cuda mem 6256632022-12-12 03:07:54
:eager release cuda mem 625663eager release cuda mem 625663
.1980

740911] [: eager alloc mem 611.00 KB2022-12-12 03:07:54E
. 740952/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :[E19802022-12-12 03:07:54 [] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-12 03:07:54eager alloc mem 611.00 KB740977[:[2022-12-12 03:07:54.
: 2022-12-12 03:07:5419802022-12-12 03:07:54.740985E.] .740991:  740997eager alloc mem 611.00 KB740998: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 
: E :EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980eager alloc mem 611.00 KB::1980] 
19801980] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 03:07:54.741700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.741744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.741771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 03:07:54eager alloc mem 611.00 KB.
741787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 03:07:54eager release cuda mem 625663.
741813: E[ 2022-12-12 03:07:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7418301980: [] E[2022-12-12 03:07:54[eager alloc mem 611.00 KB 2022-12-12 03:07:54[.2022-12-12 03:07:54
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.2022-12-12 03:07:54[741847.:741850.2022-12-12 03:07:54: 741852638: 741859.E: ] E: 741873 Eeager release cuda mem 625663 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638:638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 638] 638:eager release cuda mem 625663] eager release cuda mem 625663] 1980
eager release cuda mem 625663
eager release cuda mem 625663[] 

2022-12-12 03:07:54eager alloc mem 611.00 KB.
741980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 03:07:54eager alloc mem 611.00 KB.
742028[: 2022-12-12 03:07:54E. [[742041/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:07:542022-12-12 03:07:54: :..E1980742051742052 ] : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBEE:
  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ::eager alloc mem 611.00 KB19801980
] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 03:07:54.742529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.742597: [E2022-12-12 03:07:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu742605:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 03:07:54.742687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:54.742746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 03:07:54.742768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.742818: E[ 2022-12-12 03:07:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:[74282819802022-12-12 03:07:54: ] .E[eager alloc mem 611.00 KB742838 2022-12-12 03:07:54[
: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.2022-12-12 03:07:54E2022-12-12 03:07:54:742851. .638: 742860/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu742863] E: :: eager release cuda mem 625663 E1980E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ]  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:
:] 638638eager release cuda mem 625663] ] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 03:07:54.742970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:54.[7430062022-12-12 03:07:54[: .2022-12-12 03:07:54E743011. : 743014/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: : E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980:
] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 03:07:54.743364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.743434: E[ 2022-12-12 03:07:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7434431980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.743525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:54.743595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.743663: [E2022-12-12 03:07:54 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu743670: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 03:07:54.743721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.743759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:54.[7437902022-12-12 03:07:54: .[E7437952022-12-12 03:07:54 : [./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE2022-12-12 03:07:54743802: .: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc743813E] ::  eager alloc mem 611.00 KB638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
]  :eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638
:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 03:07:54.743926: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:07:54[:.2022-12-12 03:07:541980743938.] : 743942eager alloc mem 611.00 KBE: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 03:07:54.744196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.744263: E[ 2022-12-12 03:07:54/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7442721980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.744352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:54.744433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.744501[: 2022-12-12 03:07:54E. 744507/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 03:07:54.744590[: 2022-12-12 03:07:54E. 744596/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 03:07:54.744681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 03:07:54
.744702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:07:54[638.2022-12-12 03:07:54] 744722.eager release cuda mem 625663: 744727
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-12 03:07:54.744795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:07:54[.2022-12-12 03:07:54744824.: 744826E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 03:07:54.745025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.745063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:07:54.745095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.745132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:07:54.745271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.745309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:07:54.745353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.745391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:07:54.745444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.745490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:07:54.745543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:07:54.745580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 03:07:54[eager release cuda mem 40400000.2022-12-12 03:07:54
745595.: 745600E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[[2022-12-12 03:07:542022-12-12 03:07:54[..2022-12-12 03:07:54745655745664.: E: 745668 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 1793:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638:Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.1069 secs ] 638
eager release cuda mem 40400000] 
eager release cuda mem 40400000
[2022-12-12 03:07:54.745831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.09757 secs 
[2022-12-12 03:07:54.746460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.0982 secs 
[2022-12-12 03:07:54.746698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10785 secs 
[2022-12-12 03:07:54.747405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10173 secs 
[2022-12-12 03:07:54.747871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10963 secs 
[2022-12-12 03:07:54.748005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.10944 secs 
[2022-12-12 03:07:54.748188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 10000000 / 100000000 nodes ( 10.00 %~10.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 90000000 / 100000000 nodes ( 90.00 %) | 4.77 GB | 1.12488 secs 
[2022-12-12 03:07:54.750117: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.09 GB
[2022-12-12 03:07:56.229326: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.35 GB
[2022-12-12 03:07:56.229565: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.35 GB
[2022-12-12 03:07:56.230613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.35 GB
[2022-12-12 03:07:57.608830: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.61 GB
[2022-12-12 03:07:57.609227: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.61 GB
[2022-12-12 03:07:57.609863: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.61 GB
[2022-12-12 03:07:58.934002: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 13.83 GB
[2022-12-12 03:07:58.934791: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 13.83 GB
[2022-12-12 03:07:58.935162: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.83 GB
[2022-12-12 03:07:59.979223: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.04 GB
[2022-12-12 03:07:59.980683: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.04 GB
[2022-12-12 03:07:59.983191: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.04 GB
[2022-12-12 03:08:01.632244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.50 GB
[2022-12-12 03:08:01.632887: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.50 GB
[2022-12-12 03:08:01.633540: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.50 GB
[2022-12-12 03:08:03.135427: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.70 GB
[2022-12-12 03:08:03.136153: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.70 GB
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][tid #140055704225536]: replica 1 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][tid #140055301572352]: replica 7 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][tid #140055242856192]: replica 5 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][tid #140055838443264]: replica 2 calling init per replica done, doing barrier
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055838443264]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055301572352]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055242856192]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055704225536]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:08:04.101][ERROR][RK0][main]: init per replica done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055838443264]: init per replica done
[HCTR][03:08:04.101][ERROR][RK0][main]: init per replica done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055301572352]: init per replica done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055242856192]: init per replica done
[HCTR][03:08:04.101][ERROR][RK0][main]: init per replica done
[HCTR][03:08:04.101][ERROR][RK0][tid #140055704225536]: init per replica done
[HCTR][03:08:04.104][ERROR][RK0][main]: init per replica done
[HCTR][03:08:04.139][ERROR][RK0][tid #140055242856192]: 5 allocated 3276800 at 0x7f4464238400
[HCTR][03:08:04.139][ERROR][RK0][tid #140055242856192]: 5 allocated 6553600 at 0x7f4464558400
[HCTR][03:08:04.139][ERROR][RK0][tid #140055242856192]: 5 allocated 3276800 at 0x7f4464b98400
[HCTR][03:08:04.139][ERROR][RK0][tid #140055242856192]: 5 allocated 6553600 at 0x7f4464eb8400
[HCTR][03:08:04.140][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f4420238400
[HCTR][03:08:04.140][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f4420558400
[HCTR][03:08:04.140][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f4420b98400
[HCTR][03:08:04.140][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f4420eb8400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055704225536]: 1 allocated 3276800 at 0x7f43ac238400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055704225536]: 1 allocated 6553600 at 0x7f43ac558400
[HCTR][03:08:04.140][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f44b6238400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055704225536]: 1 allocated 3276800 at 0x7f43acb98400
[HCTR][03:08:04.140][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f44b6558400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055704225536]: 1 allocated 6553600 at 0x7f43aceb8400
[HCTR][03:08:04.140][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f44b6b98400
[HCTR][03:08:04.140][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f44b6eb8400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055377073920]: 4 allocated 3276800 at 0x7f445c238400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055377073920]: 4 allocated 6553600 at 0x7f445c558400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055377073920]: 4 allocated 3276800 at 0x7f445cb98400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055377073920]: 4 allocated 6553600 at 0x7f445ceb8400
[HCTR][03:08:04.140][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f43c4238400
[HCTR][03:08:04.140][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f43c4558400
[HCTR][03:08:04.140][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f43c4b98400
[HCTR][03:08:04.140][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f43c4eb8400
[HCTR][03:08:04.140][ERROR][RK0][tid #140055838443264]: 2 allocated 3276800 at 0x7f4394238400
[HCTR][03:08:04.141][ERROR][RK0][tid #140055838443264]: 2 allocated 6553600 at 0x7f4394558400
[HCTR][03:08:04.141][ERROR][RK0][tid #140055838443264]: 2 allocated 3276800 at 0x7f4394b98400
[HCTR][03:08:04.141][ERROR][RK0][tid #140055838443264]: 2 allocated 6553600 at 0x7f4394eb8400
[HCTR][03:08:04.142][ERROR][RK0][tid #140055368681216]: 0 allocated 3276800 at 0x7f4482320000
[HCTR][03:08:04.142][ERROR][RK0][tid #140055368681216]: 0 allocated 6553600 at 0x7f4482640000
[HCTR][03:08:04.142][ERROR][RK0][tid #140055368681216]: 0 allocated 3276800 at 0x7f4482c80000
[HCTR][03:08:04.142][ERROR][RK0][tid #140055368681216]: 0 allocated 6553600 at 0x7f4482fa0000
