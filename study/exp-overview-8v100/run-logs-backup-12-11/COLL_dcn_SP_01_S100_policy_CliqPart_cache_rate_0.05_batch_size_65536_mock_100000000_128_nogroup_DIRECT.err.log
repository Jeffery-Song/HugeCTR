2022-12-12 01:33:27.234170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.240722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.246921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.258101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.266040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.270037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.282287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.290182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.343535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.345531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.346186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.347116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.347785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.348904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.349289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.350682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.350781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.352739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.352847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.354834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.354928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.356418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.356469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.357932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.359028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.360198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.361607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.362637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.363517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.364463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.365358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.366256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.368062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.369454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.370515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.371623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.372545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.373589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.374704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.376170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.381790: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.381893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.383550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.385654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.387073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.387426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.389009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.389467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.391097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.391407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.391819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.391875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.394327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.394802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.395486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.395585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.397692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.398580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.399234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.399419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.399993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.401769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.402491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.403092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.403291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.404996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.405946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.406576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.406913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.408922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.410172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.410403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.411156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.413771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.414039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.414373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.415008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.417248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.417346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.417751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.418287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.420406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.420789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.421814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.422144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.423081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.423892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.424652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.425395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.425874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.426952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.428227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.428251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.428803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.430285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.430368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.434464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.436944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.437298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.438017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.439384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.440041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.447304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.457554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.466650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.471240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.475659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.476783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.478689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.478697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.478901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.478923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.479849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.481051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.483400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.483561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.483632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.483658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.484257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.485207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.488819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.488901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.488972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.489088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.489663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.490122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.492677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.492761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.492833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.492974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.493992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.494389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.496914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.497000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.497117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.497235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.498437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.499489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.502033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.502165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.502353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.502374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.503652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.504254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.506743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.506784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.507200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.507285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.508713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.509374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.511498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.511581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.511731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.511751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.513008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.514640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.517270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.517461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.517547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.517569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.519041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.519631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.521851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.522080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.522149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.522267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.523704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.524208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.526268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.526313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.526522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.526543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.528182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.528604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.530757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.530911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.531007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.531075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.531202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.532898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.535642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.535822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.535865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.535980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.535999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.537457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.537920: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.540240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.540318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.540366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.540545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.540567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.541865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.544960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.545021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.545086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.545236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.545280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.546757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.547026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.550162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.551243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.551551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.551652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.551867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.551928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.552107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.555062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.556231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.556890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.557211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.557476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.557624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.557884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.560443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.562142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.562870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.563156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.563234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.563354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.566233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.567459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.568306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.568731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.568810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.568824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.571520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.573105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.573967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.574355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.574684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.574870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.577263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.578683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.579626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.579820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.580050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.580180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.582788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.584305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.585127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.585416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.585591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.585647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.588472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.589744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.590557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.591160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.591352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.591404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.593629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.596982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.597801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.599194: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.599694: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.599796: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.599855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.599928: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.600695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.604031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.606643: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.607021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.608421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.609044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.609252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.609831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.615196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.618253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.618305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.618332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.618379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.618450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.620362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.623195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.623202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.623390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.623390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.623443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.625148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.658074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.663431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.697350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.703837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.710121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.723794: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 01:33:27.733087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.740715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:27.745338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.758461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.760141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.762157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.763656: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:28.763717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:33:28.780993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.781642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.782146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.782780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.783335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.783810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 01:33:28.830514: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:28.830724: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:28.868524: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 01:33:28.991346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.994609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.995882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:28.996585: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:28.996642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:33:29.013796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.014513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.015022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.015633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.016474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.017089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 01:33:29.043182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.044955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.046153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.047050: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:29.047107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:33:29.057303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.058687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.059844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.060901: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:29.060959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:33:29.061154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.062514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.065144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.065918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.066301: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:29.066348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:33:29.067370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.068351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.069416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.070511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.071509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 01:33:29.078006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.079267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.080541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.081215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.082147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.082557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.082801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.083345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.084317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.084971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.085202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.085948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.086896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 01:33:29.087514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.087595: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:29.087654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:33:29.088155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.089282: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:29.089327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:33:29.089865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.091196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.092757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 01:33:29.097663: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.097862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.099761: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 01:33:29.106047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.106501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.106790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.107558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.107989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.108471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.109196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.109663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.110102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.110554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.110962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 01:33:29.111237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 01:33:29.117025: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.117187: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.119118: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 01:33:29.131760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.131943: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.133695: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 01:33:29.134783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.135458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.135984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.136464: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 01:33:29.136515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:33:29.153003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.153673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.154174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.154755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.155286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 01:33:29.155750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 01:33:29.156181: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.156352: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.156758: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.156950: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.158121: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 01:33:29.158797: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 01:33:29.190104: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.190311: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.192088: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 01:33:29.200947: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.201152: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 01:33:29.203101: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][01:33:30.464][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.465][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.466][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][01:33:30.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 101it [00:01, 85.17it/s]warmup run: 101it [00:01, 85.13it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 97it [00:01, 81.20it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 202it [00:01, 184.69it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 198it [00:01, 180.45it/s]warmup run: 97it [00:01, 84.39it/s]warmup run: 193it [00:01, 175.18it/s]warmup run: 95it [00:01, 81.33it/s]warmup run: 302it [00:01, 293.16it/s]warmup run: 96it [00:01, 82.97it/s]warmup run: 95it [00:01, 80.65it/s]warmup run: 298it [00:01, 289.57it/s]warmup run: 193it [00:01, 181.46it/s]warmup run: 291it [00:01, 281.67it/s]warmup run: 191it [00:01, 177.18it/s]warmup run: 1it [00:01,  1.46s/it]warmup run: 403it [00:01, 407.33it/s]warmup run: 193it [00:01, 180.67it/s]warmup run: 191it [00:01, 175.99it/s]warmup run: 399it [00:01, 404.50it/s]warmup run: 289it [00:01, 287.57it/s]warmup run: 391it [00:01, 395.84it/s]warmup run: 287it [00:01, 282.55it/s]warmup run: 102it [00:01, 90.33it/s]warmup run: 503it [00:02, 517.22it/s]warmup run: 291it [00:01, 289.15it/s]warmup run: 288it [00:01, 282.22it/s]warmup run: 499it [00:02, 515.07it/s]warmup run: 386it [00:01, 398.58it/s]warmup run: 491it [00:02, 507.33it/s]warmup run: 386it [00:01, 396.45it/s]warmup run: 203it [00:01, 193.81it/s]warmup run: 604it [00:02, 619.89it/s]warmup run: 389it [00:01, 400.84it/s]warmup run: 386it [00:01, 393.72it/s]warmup run: 602it [00:02, 622.14it/s]warmup run: 486it [00:01, 512.20it/s]warmup run: 592it [00:02, 612.15it/s]warmup run: 488it [00:02, 512.36it/s]warmup run: 305it [00:01, 308.01it/s]warmup run: 706it [00:02, 710.96it/s]warmup run: 486it [00:02, 507.48it/s]warmup run: 485it [00:02, 504.57it/s]warmup run: 706it [00:02, 717.23it/s]warmup run: 587it [00:02, 617.88it/s]warmup run: 693it [00:02, 703.34it/s]warmup run: 589it [00:02, 616.87it/s]warmup run: 407it [00:01, 425.16it/s]warmup run: 807it [00:02, 784.86it/s]warmup run: 584it [00:02, 606.70it/s]warmup run: 584it [00:02, 605.49it/s]warmup run: 810it [00:02, 795.59it/s]warmup run: 689it [00:02, 711.22it/s]warmup run: 794it [00:02, 777.74it/s]warmup run: 690it [00:02, 706.66it/s]warmup run: 509it [00:01, 537.83it/s]warmup run: 908it [00:02, 841.92it/s]warmup run: 684it [00:02, 697.70it/s]warmup run: 685it [00:02, 698.61it/s]warmup run: 912it [00:02, 853.99it/s]warmup run: 790it [00:02, 785.43it/s]warmup run: 894it [00:02, 835.58it/s]warmup run: 793it [00:02, 785.38it/s]warmup run: 612it [00:02, 642.86it/s]warmup run: 1008it [00:02, 880.40it/s]warmup run: 783it [00:02, 770.39it/s]warmup run: 784it [00:02, 769.79it/s]warmup run: 1015it [00:02, 900.97it/s]warmup run: 892it [00:02, 844.78it/s]warmup run: 995it [00:02, 880.65it/s]warmup run: 894it [00:02, 843.96it/s]warmup run: 716it [00:02, 734.74it/s]warmup run: 1110it [00:02, 918.98it/s]warmup run: 883it [00:02, 828.62it/s]warmup run: 885it [00:02, 830.61it/s]warmup run: 1119it [00:02, 939.73it/s]warmup run: 994it [00:02, 890.05it/s]warmup run: 1097it [00:02, 917.39it/s]warmup run: 995it [00:02, 887.28it/s]warmup run: 820it [00:02, 809.98it/s]warmup run: 1212it [00:02, 947.69it/s]warmup run: 982it [00:02, 872.40it/s]warmup run: 986it [00:02, 876.72it/s]warmup run: 1222it [00:02, 959.93it/s]warmup run: 1097it [00:02, 928.72it/s]warmup run: 1198it [00:02, 943.68it/s]warmup run: 1096it [00:02, 920.31it/s]warmup run: 922it [00:02, 863.30it/s]warmup run: 1316it [00:02, 973.22it/s]warmup run: 1082it [00:02, 907.55it/s]warmup run: 1087it [00:02, 913.17it/s]warmup run: 1324it [00:02, 963.76it/s]warmup run: 1200it [00:02, 957.39it/s]warmup run: 1299it [00:02, 957.92it/s]warmup run: 1198it [00:02, 947.07it/s]warmup run: 1023it [00:02, 888.37it/s]warmup run: 1420it [00:02, 992.01it/s]warmup run: 1182it [00:02, 933.22it/s]warmup run: 1188it [00:02, 940.55it/s]warmup run: 1425it [00:02, 962.45it/s]warmup run: 1302it [00:02, 971.61it/s]warmup run: 1399it [00:02, 968.92it/s]warmup run: 1299it [00:02, 964.16it/s]warmup run: 1123it [00:02, 910.60it/s]warmup run: 1525it [00:03, 1008.06it/s]warmup run: 1282it [00:02, 951.41it/s]warmup run: 1288it [00:02, 955.66it/s]warmup run: 1525it [00:03, 971.77it/s]warmup run: 1404it [00:02, 981.49it/s]warmup run: 1501it [00:03, 982.14it/s]warmup run: 1400it [00:02, 975.78it/s]warmup run: 1222it [00:02, 928.99it/s]warmup run: 1629it [00:03, 1014.58it/s]warmup run: 1382it [00:02, 961.37it/s]warmup run: 1389it [00:02, 969.64it/s]warmup run: 1626it [00:03, 981.78it/s]warmup run: 1505it [00:03, 989.14it/s]warmup run: 1604it [00:03, 993.45it/s]warmup run: 1501it [00:03, 979.84it/s]warmup run: 1321it [00:02, 941.61it/s]warmup run: 1733it [00:03, 1021.43it/s]warmup run: 1481it [00:03, 967.66it/s]warmup run: 1489it [00:03, 974.68it/s]warmup run: 1726it [00:03, 985.74it/s]warmup run: 1607it [00:03, 995.39it/s]warmup run: 1706it [00:03, 1001.28it/s]warmup run: 1601it [00:03, 982.13it/s]warmup run: 1422it [00:02, 960.41it/s]warmup run: 1837it [00:03, 1025.71it/s]warmup run: 1580it [00:03, 972.37it/s]warmup run: 1589it [00:03, 972.43it/s]warmup run: 1828it [00:03, 993.38it/s]warmup run: 1708it [00:03, 990.21it/s]warmup run: 1808it [00:03, 1005.95it/s]warmup run: 1703it [00:03, 991.64it/s]warmup run: 1522it [00:02, 971.78it/s]warmup run: 1942it [00:03, 1030.72it/s]warmup run: 1679it [00:03, 977.24it/s]warmup run: 1689it [00:03, 979.38it/s]warmup run: 1931it [00:03, 1004.11it/s]warmup run: 1808it [00:03, 982.54it/s]warmup run: 1911it [00:03, 1010.26it/s]warmup run: 1806it [00:03, 1000.36it/s]warmup run: 1622it [00:03, 979.95it/s]warmup run: 2053it [00:03, 1053.53it/s]warmup run: 1778it [00:03, 979.80it/s]warmup run: 1788it [00:03, 982.45it/s]warmup run: 2038it [00:03, 1022.75it/s]warmup run: 1907it [00:03, 973.84it/s]warmup run: 2015it [00:03, 1017.43it/s]warmup run: 1908it [00:03, 1005.92it/s]warmup run: 1723it [00:03, 986.89it/s]warmup run: 2175it [00:03, 1102.40it/s]warmup run: 1878it [00:03, 983.78it/s]warmup run: 1891it [00:03, 994.78it/s]warmup run: 2158it [00:03, 1073.02it/s]warmup run: 2005it [00:03, 972.56it/s]warmup run: 2135it [00:03, 1070.94it/s]warmup run: 2010it [00:03, 1007.76it/s]warmup run: 1824it [00:03, 991.19it/s]warmup run: 2296it [00:03, 1133.86it/s]warmup run: 1977it [00:03, 984.58it/s]warmup run: 1992it [00:03, 997.49it/s]warmup run: 2278it [00:03, 1109.55it/s]warmup run: 2251it [00:03, 1096.91it/s]warmup run: 2127it [00:03, 1044.52it/s]warmup run: 2131it [00:03, 1067.35it/s]warmup run: 1925it [00:03, 994.02it/s]warmup run: 2416it [00:03, 1151.27it/s]warmup run: 2093it [00:03, 1034.63it/s]warmup run: 2106it [00:03, 1038.45it/s]warmup run: 2397it [00:03, 1131.57it/s]warmup run: 2249it [00:03, 1095.89it/s]warmup run: 2361it [00:03, 1084.73it/s]warmup run: 2253it [00:03, 1110.52it/s]warmup run: 2030it [00:03, 1008.63it/s]warmup run: 2535it [00:03, 1162.77it/s]warmup run: 2214it [00:03, 1086.48it/s]warmup run: 2224it [00:03, 1079.75it/s]warmup run: 2516it [00:03, 1146.21it/s]warmup run: 2372it [00:03, 1133.10it/s]warmup run: 2473it [00:03, 1092.35it/s]warmup run: 2374it [00:03, 1139.23it/s]warmup run: 2151it [00:03, 1066.35it/s]warmup run: 2655it [00:04, 1172.17it/s]warmup run: 2335it [00:03, 1121.12it/s]warmup run: 2343it [00:03, 1110.27it/s]warmup run: 2636it [00:04, 1160.57it/s]warmup run: 2494it [00:03, 1157.08it/s]warmup run: 2584it [00:04, 1096.18it/s]warmup run: 2495it [00:03, 1159.78it/s]warmup run: 2272it [00:03, 1106.77it/s]warmup run: 2774it [00:04, 1175.16it/s]warmup run: 2456it [00:03, 1146.34it/s]warmup run: 2461it [00:03, 1128.97it/s]warmup run: 2756it [00:04, 1165.88it/s]warmup run: 2614it [00:04, 1169.29it/s]warmup run: 2696it [00:04, 1100.44it/s]warmup run: 2617it [00:04, 1175.64it/s]warmup run: 2393it [00:03, 1135.43it/s]warmup run: 2894it [00:04, 1181.48it/s]warmup run: 2577it [00:04, 1164.78it/s]warmup run: 2579it [00:04, 1142.15it/s]warmup run: 2876it [00:04, 1174.12it/s]warmup run: 2734it [00:04, 1177.48it/s]warmup run: 2807it [00:04, 1100.05it/s]warmup run: 2739it [00:04, 1185.96it/s]warmup run: 3000it [00:04, 689.08it/s] warmup run: 2514it [00:03, 1157.06it/s]warmup run: 2698it [00:04, 1176.75it/s]warmup run: 2696it [00:04, 1150.26it/s]warmup run: 2996it [00:04, 1179.20it/s]warmup run: 3000it [00:04, 684.65it/s] warmup run: 2852it [00:04, 1178.17it/s]warmup run: 2918it [00:04, 1100.22it/s]warmup run: 2858it [00:04, 1183.07it/s]warmup run: 2637it [00:03, 1178.00it/s]warmup run: 2819it [00:04, 1184.97it/s]warmup run: 2814it [00:04, 1157.71it/s]warmup run: 3000it [00:04, 672.92it/s] warmup run: 2973it [00:04, 1185.12it/s]warmup run: 2978it [00:04, 1186.13it/s]warmup run: 3000it [00:04, 690.35it/s] warmup run: 2757it [00:04, 1184.11it/s]warmup run: 3000it [00:04, 687.45it/s] warmup run: 2942it [00:04, 1195.96it/s]warmup run: 2932it [00:04, 1163.28it/s]warmup run: 3000it [00:04, 686.24it/s] warmup run: 3000it [00:04, 679.43it/s] warmup run: 2877it [00:04, 1187.39it/s]warmup run: 2997it [00:04, 1190.60it/s]warmup run: 3000it [00:04, 697.80it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.41it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1596.60it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1597.70it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.96it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.20it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.18it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.89it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1610.48it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1615.62it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1631.95it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.88it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1617.52it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1643.94it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.62it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1637.80it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1622.46it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1617.39it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1632.81it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1648.95it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1624.60it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1634.58it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1626.40it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1638.21it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1618.63it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1617.47it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1635.59it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1630.83it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1649.38it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1626.94it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1626.25it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1616.66it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1635.09it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1616.30it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1627.98it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1648.75it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1635.43it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1626.07it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1627.00it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1612.23it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1631.88it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1615.28it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1647.67it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1626.42it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1634.97it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1625.29it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1625.56it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1609.53it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1629.45it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1610.25it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1643.16it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1622.35it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1631.52it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1621.15it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1621.73it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1608.80it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1622.95it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1623.63it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1643.04it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1631.89it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1607.07it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1621.71it/s]warmup should be done:  43%|████▎     | 1300/3000 [00:00<00:01, 1616.11it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1620.65it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1621.66it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1625.57it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1643.90it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1631.77it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1606.63it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1621.96it/s]warmup should be done:  49%|████▉     | 1464/3000 [00:00<00:00, 1621.36it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1620.48it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1621.43it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1626.06it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1606.92it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1630.88it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1638.25it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1621.26it/s]warmup should be done:  54%|█████▍    | 1628/3000 [00:01<00:00, 1624.91it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1620.38it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1620.63it/s]warmup should be done:  60%|█████▉    | 1794/3000 [00:01<00:00, 1627.38it/s]warmup should be done:  59%|█████▉    | 1778/3000 [00:01<00:00, 1606.14it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1630.56it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1621.22it/s]warmup should be done:  60%|█████▉    | 1792/3000 [00:01<00:00, 1627.84it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1618.66it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1626.14it/s]warmup should be done:  60%|██████    | 1802/3000 [00:01<00:00, 1620.23it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1627.31it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1605.15it/s]warmup should be done:  65%|██████▌   | 1955/3000 [00:01<00:00, 1628.30it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1629.13it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1621.56it/s]warmup should be done:  65%|██████▌   | 1959/3000 [00:01<00:00, 1617.60it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1628.45it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1620.72it/s]warmup should be done:  71%|███████   | 2120/3000 [00:01<00:00, 1627.24it/s]warmup should be done:  70%|███████   | 2101/3000 [00:01<00:00, 1606.71it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1621.82it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1625.58it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1618.93it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1635.04it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1624.95it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1609.70it/s]warmup should be done:  76%|███████▌  | 2283/3000 [00:01<00:00, 1627.14it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1608.32it/s]warmup should be done:  76%|███████▌  | 2284/3000 [00:01<00:00, 1622.10it/s]warmup should be done:  76%|███████▌  | 2284/3000 [00:01<00:00, 1618.86it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1619.53it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1639.80it/s]warmup should be done:  76%|███████▋  | 2294/3000 [00:01<00:00, 1630.48it/s]warmup should be done:  76%|███████▌  | 2280/3000 [00:01<00:00, 1602.82it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1624.99it/s]warmup should be done:  81%|████████  | 2424/3000 [00:01<00:00, 1606.83it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1619.75it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1615.94it/s]warmup should be done:  82%|████████▏ | 2459/3000 [00:01<00:00, 1635.32it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1641.12it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1610.47it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1609.50it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1624.97it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1608.37it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1619.03it/s]warmup should be done:  87%|████████▋ | 2608/3000 [00:01<00:00, 1616.81it/s]warmup should be done:  88%|████████▊ | 2625/3000 [00:01<00:00, 1642.60it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1644.27it/s]warmup should be done:  87%|████████▋ | 2619/3000 [00:01<00:00, 1604.63it/s]warmup should be done:  87%|████████▋ | 2607/3000 [00:01<00:00, 1616.72it/s]warmup should be done:  92%|█████████▏| 2772/3000 [00:01<00:00, 1625.39it/s]warmup should be done:  92%|█████████▏| 2748/3000 [00:01<00:00, 1610.02it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1618.73it/s]warmup should be done:  92%|█████████▏| 2770/3000 [00:01<00:00, 1616.41it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1647.15it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1647.00it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1622.12it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1599.39it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1629.93it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1612.60it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1623.07it/s]warmup should be done:  98%|█████████▊| 2934/3000 [00:01<00:00, 1620.59it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1651.65it/s]warmup should be done:  98%|█████████▊| 2937/3000 [00:01<00:00, 1633.03it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1630.16it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1602.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.08it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1625.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1621.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.01it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.10it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.96it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.52it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.03it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.62it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.52it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.93it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1700.05it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.36it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1668.18it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.94it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1674.26it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1663.21it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1682.20it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1650.94it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1700.09it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1675.13it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1676.28it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1674.19it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1664.72it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1697.08it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1683.49it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1655.30it/s]warmup should be done:  17%|█▋        | 513/3000 [00:00<00:01, 1699.37it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1680.67it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1678.58it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1700.93it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1666.89it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1676.43it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1686.93it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1659.45it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1701.07it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1679.29it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1680.06it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1703.07it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1687.58it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1679.03it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1668.67it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1661.01it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1702.71it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1676.69it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1677.43it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1678.41it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1684.50it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1661.16it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1696.94it/s]warmup should be done:  34%|███▍      | 1026/3000 [00:00<00:01, 1699.83it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1675.61it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1677.24it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1675.19it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1680.03it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1696.29it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1659.73it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1654.79it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1688.75it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1677.58it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1676.25it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1676.29it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1682.94it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1699.21it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1688.36it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:01, 1654.60it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1647.35it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1677.53it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1675.97it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1674.96it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1698.47it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1677.32it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1685.59it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1648.08it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1640.14it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1679.36it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1678.09it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1676.80it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1700.04it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1677.79it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1680.88it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1647.40it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1636.04it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1680.41it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1679.14it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1677.92it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1701.40it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1679.04it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1679.46it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1644.10it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1632.85it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1679.17it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1679.56it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1676.87it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1701.59it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1680.52it/s]warmup should be done:  68%|██████▊   | 2039/3000 [00:01<00:00, 1680.57it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1640.10it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1629.56it/s]warmup should be done:  73%|███████▎  | 2189/3000 [00:01<00:00, 1677.39it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1675.43it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1672.94it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1699.80it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1678.48it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1680.02it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1636.59it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1626.55it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1677.14it/s]warmup should be done:  79%|███████▊  | 2356/3000 [00:01<00:00, 1676.22it/s]warmup should be done:  79%|███████▊  | 2356/3000 [00:01<00:00, 1668.02it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1687.58it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1637.02it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1669.39it/s]warmup should be done:  77%|███████▋  | 2317/3000 [00:01<00:00, 1626.90it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1646.39it/s]warmup should be done:  84%|████████▍ | 2526/3000 [00:01<00:00, 1678.89it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1676.70it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1691.68it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1655.21it/s]warmup should be done:  83%|████████▎ | 2491/3000 [00:01<00:00, 1636.89it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1672.36it/s]warmup should be done:  83%|████████▎ | 2480/3000 [00:01<00:00, 1625.97it/s]warmup should be done:  84%|████████▍ | 2530/3000 [00:01<00:00, 1639.72it/s]warmup should be done:  90%|████████▉ | 2694/3000 [00:01<00:00, 1677.68it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1675.90it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1695.48it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1655.67it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1673.76it/s]warmup should be done:  88%|████████▊ | 2655/3000 [00:01<00:00, 1633.43it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1618.61it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1638.18it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1677.34it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1675.46it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1697.01it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1654.60it/s]warmup should be done:  94%|█████████▍| 2819/3000 [00:01<00:00, 1633.16it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1672.54it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1620.00it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1647.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1682.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1677.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.24it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1637.42it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1623.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.82it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f5c4100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f5d22b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f917730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f91ee80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f916e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f5c3190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f919d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f768f5d30d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 01:35:00.488647: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71bf02eb50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:00.488709: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:00.498203: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:00.764210: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71ca8312f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:00.764273: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:00.774575: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:00.986870: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71be82cf30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:00.986935: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:00.997251: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:01.431291: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71c682cf40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:01.431359: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:01.440677: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71c6834cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:01.440737: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:01.440883: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:01.443333: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71ca831130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:01.443385: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:01.450817: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:01.451341: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:01.463597: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71c6834c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:01.463656: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:01.464293: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71bef929c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 01:35:01.464338: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 01:35:01.471156: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:01.473856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 01:35:07.892344: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:07.896899: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:07.949946: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:08.215057: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:08.288613: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:08.344618: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:08.544438: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 01:35:08.545168: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][01:36:05.188][ERROR][RK0][tid #140126797690624]: replica 7 reaches 1000, calling init pre replica
[HCTR][01:36:05.188][ERROR][RK0][tid #140126797690624]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.194][ERROR][RK0][tid #140126797690624]: coll ps creation done
[HCTR][01:36:05.194][ERROR][RK0][tid #140126797690624]: replica 7 waits for coll ps creation barrier
[HCTR][01:36:05.396][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][01:36:05.396][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.401][ERROR][RK0][main]: coll ps creation done
[HCTR][01:36:05.401][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][01:36:05.407][ERROR][RK0][tid #140126789297920]: replica 1 reaches 1000, calling init pre replica
[HCTR][01:36:05.407][ERROR][RK0][tid #140126789297920]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.412][ERROR][RK0][tid #140126789297920]: coll ps creation done
[HCTR][01:36:05.412][ERROR][RK0][tid #140126789297920]: replica 1 waits for coll ps creation barrier
[HCTR][01:36:05.439][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][01:36:05.439][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.448][ERROR][RK0][main]: coll ps creation done
[HCTR][01:36:05.448][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][01:36:05.472][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][01:36:05.472][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.480][ERROR][RK0][main]: coll ps creation done
[HCTR][01:36:05.480][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][01:36:05.505][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][01:36:05.505][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.513][ERROR][RK0][main]: coll ps creation done
[HCTR][01:36:05.513][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][01:36:05.767][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][01:36:05.767][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.771][ERROR][RK0][main]: coll ps creation done
[HCTR][01:36:05.771][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][01:36:05.774][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][01:36:05.774][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][01:36:05.782][ERROR][RK0][main]: coll ps creation done
[HCTR][01:36:05.782][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][01:36:05.782][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][01:36:06.660][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][01:36:06.694][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][tid #140126797690624]: replica 7 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][tid #140126789297920]: replica 1 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][01:36:06.694][ERROR][RK0][main]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][main]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][main]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][main]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][tid #140126797690624]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][tid #140126789297920]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][main]: Calling build_v2
[HCTR][01:36:06.694][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][tid #140126797690624]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][tid #140126789297920]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][01:36:06.694][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 01:36:06.698811: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie2022-12-12 01:36:06
.698851: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:36:06:.178[698896] : v100x8, slow pcieE
 2022-12-12 01:36:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[6988961962022-12-12 01:36:06: ] .Eassigning 0 to cpu698933 
[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :v100x8, slow pcie196
] assigning 0 to cpu
[[2022-12-12 01:36:062022-12-12 01:36:06[..2022-12-12 01:36:066989946989442022-12-12 01:36:06.: : .699001EE698985:   : E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E 2022-12-12 01:36:06:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:36:06:699042] ] :.212: assigning 0 to cpuv100x8, slow pcie178699034] E

] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [v100x8, slow pcieE
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 2022-12-12 01:36:06[:2022-12-12 01:36:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[[[212.:2022-12-12 01:36:066991192022-12-12 01:36:062022-12-12 01:36:062022-12-12 01:36:06] 699086178.: ...build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] 699138E699160699165699165
Ev100x8, slow pcie:  : : :  
E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 01:36:06:[   :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.1962022-12-12 01:36:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:699275] .:::] 178: assigning 0 to cpu699299196212213v100x8, slow pcie] E
: ] ] ] 
v100x8, slow pcie Eassigning 0 to cpu[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
2022-12-12 01:36:06

:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.[213:[2022-12-12 01:36:06[6994322022-12-12 01:36:06] 1962022-12-12 01:36:06.[2022-12-12 01:36:06: .remote time is 8.68421] .6994582022-12-12 01:36:06.E699465
assigning 0 to cpu699476: .699478 : 
: [E699496: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE2022-12-12 01:36:06 : E:  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 196[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc699562: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 01:36:06::: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:assigning 0 to cpu.212214E] :213
699626] ]  assigning 0 to cpu212] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] remote time is 8.68421E

:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
 [214
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[2022-12-12 01:36:06] :2022-12-12 01:36:062022-12-12 01:36:06[.cpu time is 97.0588212[..2022-12-12 01:36:06699759
] 2022-12-12 01:36:06699769699773.: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: : 699790E
699795EE:  :   [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 01:36:06 : ::./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213214699886:] :] ] : 213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212remote time is 8.68421cpu time is 97.0588E] 
] 

 remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
[
:2022-12-12 01:36:062022-12-12 01:36:06[213..2022-12-12 01:36:06[] 699997700001.2022-12-12 01:36:06remote time is 8.68421: : 700018.
EE: 700031 [ E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 01:36:06/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E214.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 700097213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588: ] 214:
Eremote time is 8.68421] 213 
cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[remote time is 8.68421:2022-12-12 01:36:06
214.] 700236[cpu time is 97.0588: 2022-12-12 01:36:06
E. 700268/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E214 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 01:37:25.868293: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 01:37:25.908016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 01:37:26. 39391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 01:37:26. 39453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 01:37:26. 39482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 01:37:26. 39509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 01:37:26. 39986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:37:26. 40027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 40887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 41541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 54690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 01:37:26. 54751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-12 01:37:262022-12-12 01:37:26. 55006.:  55011E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022022-12-12 01:37:26:] .2026 solved 55054] 
: 5 solvedE
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 01:37:26[:[.2022022-12-12 01:37:262022-12-12 01:37:26 55105] .1 solved.:  55099
 55113E: :  E[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 2022-12-12 01:37:26 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205: 55165:[] 202: 2052022-12-12 01:37:26worker 0 thread 6 initing device 6] E] .
3 solved worker 0 thread 5 initing device 5 55190
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
: :E[205 2022-12-12 01:37:26] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.worker 0 thread 1 initing device 1: 55238
1815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 01:37:26. 55296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 55315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [7 solved2022-12-12 01:37:26
. 55346: E[ 2022-12-12 01:37:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.: 55381202: ] E2 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 7 initing device 72022-12-12 01:37:26
. 55415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 01:37:26. 55607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[18152022-12-12 01:37:26] .Building Coll Cache with ... num gpu device is 8 55623
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 8[2022-12-12 01:37:26
2022-12-12 01:37:26..[ 55651 556572022-12-12 01:37:26: : .[EE 556662022-12-12 01:37:26  : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 55680:: : 18151980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] ] : Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

] :Building Coll Cache with ... num gpu device is 81980
] eager alloc mem 381.47 MB[
2022-12-12 01:37:26. 55758: E[ 2022-12-12 01:37:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: 557711980: ] Eeager alloc mem 381.47 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 01:37:26] .eager alloc mem 381.47 MB[ 55809
2022-12-12 01:37:26: .E 55819 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 01:37:26.[ 558802022-12-12 01:37:26: .E 55886 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 59490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 59768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 59822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 59877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 59938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 60436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 60489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 63768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 63937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 63990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 64042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 64096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 64148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26. 64631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 01:37:26.122134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:37:26.127851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:37:26.127982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:37:26.128848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.129589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.130590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:26.130636: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:37:26.135165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.135918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.135962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[2022-12-12 01:37:262022-12-12 01:37:262022-12-12 01:37:262022-12-12 01:37:262022-12-12 01:37:262022-12-12 01:37:26..149378....149378: 149378149378149378149378: E: : : : E EEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::::1980] 1980198019801980] eager alloc mem 5.00 Bytes] ] ] ] eager alloc mem 5.00 Bytes
eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes




[2022-12-12 01:37:26.150113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 01:37:26.161626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 01:37:262022-12-12 01:37:26..161704161723: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 01:37:26.161812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 01:37:26.161821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:37:26.161904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 01:37:26] .eager release cuda mem 400000000161906
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:37:26.[1619932022-12-12 01:37:26: .E161986 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 5
[2022-12-12 01:37:26.162066: [E2022-12-12 01:37:26 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc162090:: 638E]  eager release cuda mem 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[2022-12-12 01:37:26.162158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 01:37:26638.] 162160eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 01:37:26.162248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 01:37:26.162809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.163546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.164339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.165076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.165643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.166205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.166770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 01:37:26.167419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.167724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.167966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.168208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.168251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.168297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.168352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:26.168390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:26.168433: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:37:26.168702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:26.168746: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:37:26.168936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:26.168978: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:37:26.169182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:26.169226: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 01:37:26] .WORKER[0] alloc host memory 19.07 MB169238
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 01:37:26eager release cuda mem 625663.
169264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:26.169297: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 01:37:26WORKER[0] alloc host memory 19.07 MB.
169314[: 2022-12-12 01:37:26W. 169324/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: :E43 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccWORKER[0] alloc host memory 19.07 MB:
638] eager release cuda mem 625663
[2022-12-12 01:37:26.169387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 01:37:26.173673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.174404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.174448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:37:26.181311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.181914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.181954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:37:26.182184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.182227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.182789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.182818: E[ 2022-12-12 01:37:26/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:182830638: ] Eeager release cuda mem 25855 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:37:26[.2022-12-12 01:37:26182872.: 182863E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 2.39 GB] 
eager alloc mem 25.25 KB
[2022-12-12 01:37:26.183014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.183138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 01:37:26.183488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.183537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:37:26.183627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.183670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 01:37:26.183737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 01:37:26.183778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[[[[[[[[2022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:27........ 86117 86117 86119 86118 86118 86117 86118 86117: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] Device 0 init p2p of link 3] ] ] ] Device 4 init p2p of link 5Device 6 init p2p of link 0Device 7 init p2p of link 4
Device 1 init p2p of link 7Device 3 init p2p of link 2Device 5 init p2p of link 6Device 2 init p2p of link 1






[[[[2022-12-12 01:37:27[[[2022-12-12 01:37:272022-12-12 01:37:272022-12-12 01:37:27.2022-12-12 01:37:272022-12-12 01:37:27[2022-12-12 01:37:27... 86555..2022-12-12 01:37:27. 86555 86556 86555:  86559 86560. 86561: : : E: :  86574: EEE EE: E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:198019801980] 19801980:1980] ] ] eager alloc mem 611.00 KB] ] 1980] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB




eager alloc mem 611.00 KB

[2022-12-12 01:37:27. 87518: E[ 2022-12-12 01:37:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 87530638[: [] 2022-12-12 01:37:27E[2022-12-12 01:37:27eager release cuda mem 625663.[[ 2022-12-12 01:37:27.
 875442022-12-12 01:37:27[2022-12-12 01:37:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc. 87546: .2022-12-12 01:37:27.: 87553: E 87567. 87560638: E :  87572: ] E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: Eeager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663638:638] eager release cuda mem 625663
] 638] eager release cuda mem 625663
eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-12 01:37:27. 99847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 01:37:27.100017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.100076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 01:37:27.100238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.100835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.101075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.102413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 01:37:27.102568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.102648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 01:37:27.102803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.102878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 01:37:27.103032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.103056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 01:37:27.103236[: 2022-12-12 01:37:27E. 103233/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1926] Device 0 init p2p of link 6
[2022-12-12 01:37:27.103339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.103383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.103410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 01:37:27.103564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.103611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.103842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.104070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.104152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.104375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.111445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 01:37:27.111563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.112288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 01:37:27.112368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.112409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.113215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.114030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 01:37:27.114150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.114976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.115778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 01:37:27.115889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.116722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.117120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 01:37:27.117229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.117298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 01:37:27.117409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.117532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 01:37:27.117645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.118032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.118114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 01:37:27.118172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.118227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.118417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.119033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.129283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 01:37:27.129400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.129647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 01:37:27.129758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.129981: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 01:37:27:.1926130000] : Device 4 init p2p of link 6E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 01:37:27.130112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 01:37:27] .eager alloc mem 611.00 KB130128
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.130220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.130568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.130934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 01:37:27eager release cuda mem 625663.
130953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.131812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 01:37:27.131927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.132756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.133000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 01:37:27.133113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.133494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 01:37:27.133605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.133924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.134372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.134448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 01:37:27.134558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 01:37:27.135322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 01:37:27.145832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.146167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.146433[: 2022-12-12 01:37:27E. 146448/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 20400000:
1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09057 secs 
[2022-12-12 01:37:27.146984: [E2022-12-12 01:37:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu146995:: 1955E]  Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09131 secs /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.147177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09153 secs 
[2022-12-12 01:37:27.147417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09165 secs 
[2022-12-12 01:37:27.147779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.148107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09282 secs 
[2022-12-12 01:37:27.148445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.148793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09304 secs 
[2022-12-12 01:37:27.149122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.149443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.09357 secs 
[2022-12-12 01:37:27.149735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 01:37:27.150382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 5000000 / 100000000 nodes ( 5.00 %~5.00 %) | remote 15000000 / 100000000 nodes ( 15.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 2.39 GB | 1.11036 secs 
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][tid #140126797690624]: replica 7 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][tid #140126789297920]: replica 1 calling init per replica done, doing barrier
[HCTR][01:37:27.150][ERROR][RK0][tid #140126789297920]: replica 1 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][tid #140126797690624]: replica 7 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][01:37:27.150][ERROR][RK0][tid #140126789297920]: init per replica done
[HCTR][01:37:27.150][ERROR][RK0][tid #140126797690624]: init per replica done
[HCTR][01:37:27.150][ERROR][RK0][main]: init per replica done
[HCTR][01:37:27.150][ERROR][RK0][main]: init per replica done
[HCTR][01:37:27.150][ERROR][RK0][main]: init per replica done
[HCTR][01:37:27.150][ERROR][RK0][main]: init per replica done
[HCTR][01:37:27.150][ERROR][RK0][main]: init per replica done
[HCTR][01:37:27.153][ERROR][RK0][main]: init per replica done
