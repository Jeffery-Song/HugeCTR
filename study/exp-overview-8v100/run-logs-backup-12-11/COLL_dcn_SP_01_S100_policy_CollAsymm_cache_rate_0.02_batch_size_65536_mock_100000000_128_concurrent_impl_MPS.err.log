2022-12-11 22:24:00.307710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.314384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.320287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.324903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.330960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.343728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.351861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.357736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.386132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.391654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.394471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.403690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.406966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.415145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.416947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.426636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.427721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.431364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.431566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.433104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.433208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.434942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.435900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.436896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.437733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.438624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.439676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.440756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.441812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.442887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.444366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.445430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.447195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.448417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.449430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.450462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.451519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.452570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.453511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.454538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.459554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.460063: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.460754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.461725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.462774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.463785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.465155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.467034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.467634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.468984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.469321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.469696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.469816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.471909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.472475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.472647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.474296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.475295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.475446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.476048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.477851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.477989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.478838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.479708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.480792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.480904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.482490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.483453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.484805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.485099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.486814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.487440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.488070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.488840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.489115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.490806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.491095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.492054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.493260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.494599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.494647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.495951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.497578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.497740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.497982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.499029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.500478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.500590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.500926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.501957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.503038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.503484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.504458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.504963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.505469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.506949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.507169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.509005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.510019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.510090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.511065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.512173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.516600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.537634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.541168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.547377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.547675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.548630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.550224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.550261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.550608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.551377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.551503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.552457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.554569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.554601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.554891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.555440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.556193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.557126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.558964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.559926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.560161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.560589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.561503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.561810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.563410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.563867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.564089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.564522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.565537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.566625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.567596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.568081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.568267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.568640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.569660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.570834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.572023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.572530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.572641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.573010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.574062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.575182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.576083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.576594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.576712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.577126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.578150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.579753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.580352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.580818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.581021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.581518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.582389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.583821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.584307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.584731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.584866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.585434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.586559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.588005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.588344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.588935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.589115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.589747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.590677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.592193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.592348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.592967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.593102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.593638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.594832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.596351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.596702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.597258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.597429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.597876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.598785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.599914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.600639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.601456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.601684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.602026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.603002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.603923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.604699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.604807: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.605299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.605341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.605846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.606503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.607939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.608510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.609197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.609209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.609557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.610820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.611999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.613517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.614006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.614084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.614299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.614525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.614752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.615770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.617724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.618341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.618401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.618683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.618802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.619532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.620998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.623070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.623712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.623862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.624097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.624886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.625685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.625704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.627764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.628450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.628918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.629551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.630674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.630708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.632547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.633046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.633685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.634407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.636273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.636325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.638120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.638637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.639022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.639837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.640737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.640957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.642961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.643572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.644499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.644636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.645594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.645736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.647343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.647964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.649577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.649647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.650505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.650558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.654432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.654448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.656511: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.656514: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.656943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.656964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.657086: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.657176: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.659938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.662068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.663125: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.664523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.665755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.665908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.666008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.666387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.668872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.669777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.669865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.669874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.670166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.671986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.672725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.673688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.673813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.673944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.674261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.704598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.705236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.708854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.737842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.742790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.748337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.753014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.763324: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:24:00.772067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.776246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:00.780251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.763172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.764004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.764540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.765738: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:01.765796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:24:01.783863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.784502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.785001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.785693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.786426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.787310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:24:01.833501: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:01.833729: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:01.859782: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:24:01.960183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.961099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.961986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.962456: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:01.962510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:24:01.980383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.981022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.981928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.982522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.983049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:01.983537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:24:02.043314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.044053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.044598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.045064: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:02.045122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:24:02.059357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.059689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.060218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.060724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.061230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.061646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.061664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.062520: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:02.062584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:24:02.062931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.063577: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:02.063637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:24:02.063657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.064533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.064711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.065529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.065681: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:02.065742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:24:02.066399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.067167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.067643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:24:02.069328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.069506: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.070601: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:24:02.071286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.071863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.072392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.072849: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:02.072897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:24:02.080911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.081084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.082244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.082410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.083036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.083363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.084092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.084277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.084911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.085043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.085216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.086285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:24:02.086609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:24:02.086648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.086763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.087662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.087828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.088699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.088822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.089698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.089766: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:24:02.089815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:24:02.090259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.090560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:24:02.090992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.091528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.092102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.092614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.093084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:24:02.108014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.108674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.109194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.109839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.110400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:24:02.110872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:24:02.132179: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.132399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.132814: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.133006: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.133379: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:24:02.134051: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:24:02.136343: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.136475: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.138234: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:24:02.138828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.138986: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.139976: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 22:24:02.140426: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.140561: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.141601: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:24:02.157423: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.157605: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:24:02.158913: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][22:24:03.380][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.390][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.414][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.414][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.419][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.419][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.419][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:24:03.419][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.60s/it]warmup run: 97it [00:01, 79.76it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 100it [00:01, 85.88it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 98it [00:01, 79.96it/s]warmup run: 195it [00:01, 174.49it/s]warmup run: 100it [00:01, 84.32it/s]warmup run: 201it [00:01, 186.97it/s]warmup run: 100it [00:01, 86.19it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 95it [00:01, 83.03it/s]warmup run: 199it [00:01, 177.14it/s]warmup run: 295it [00:01, 282.30it/s]warmup run: 202it [00:01, 185.05it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 302it [00:01, 298.01it/s]warmup run: 200it [00:01, 186.55it/s]warmup run: 93it [00:01, 80.59it/s]warmup run: 191it [00:01, 180.63it/s]warmup run: 299it [00:01, 283.84it/s]warmup run: 395it [00:01, 394.71it/s]warmup run: 96it [00:01, 84.50it/s]warmup run: 299it [00:01, 289.40it/s]warmup run: 402it [00:01, 411.51it/s]warmup run: 300it [00:01, 296.90it/s]warmup run: 186it [00:01, 174.32it/s]warmup run: 289it [00:01, 290.30it/s]warmup run: 399it [00:02, 395.38it/s]warmup run: 495it [00:02, 504.97it/s]warmup run: 193it [00:01, 183.58it/s]warmup run: 396it [00:01, 398.12it/s]warmup run: 503it [00:02, 523.12it/s]warmup run: 400it [00:01, 410.75it/s]warmup run: 277it [00:01, 274.47it/s]warmup run: 387it [00:01, 403.14it/s]warmup run: 498it [00:02, 503.77it/s]warmup run: 597it [00:02, 611.26it/s]warmup run: 292it [00:01, 294.51it/s]warmup run: 606it [00:02, 630.07it/s]warmup run: 493it [00:02, 503.40it/s]warmup run: 501it [00:02, 522.86it/s]warmup run: 370it [00:01, 380.81it/s]warmup run: 485it [00:01, 511.96it/s]warmup run: 599it [00:02, 607.70it/s]warmup run: 699it [00:02, 703.60it/s]warmup run: 392it [00:01, 410.37it/s]warmup run: 708it [00:02, 720.08it/s]warmup run: 591it [00:02, 602.05it/s]warmup run: 601it [00:02, 623.32it/s]warmup run: 464it [00:02, 485.94it/s]warmup run: 584it [00:02, 613.25it/s]warmup run: 698it [00:02, 694.04it/s]warmup run: 800it [00:02, 777.27it/s]warmup run: 493it [00:01, 524.21it/s]warmup run: 807it [00:02, 786.70it/s]warmup run: 692it [00:02, 695.12it/s]warmup run: 703it [00:02, 714.83it/s]warmup run: 561it [00:02, 588.45it/s]warmup run: 683it [00:02, 700.57it/s]warmup run: 797it [00:02, 765.16it/s]warmup run: 899it [00:02, 831.22it/s]warmup run: 597it [00:02, 633.02it/s]warmup run: 907it [00:02, 840.51it/s]warmup run: 794it [00:02, 774.49it/s]warmup run: 803it [00:02, 785.74it/s]warmup run: 660it [00:02, 680.62it/s]warmup run: 782it [00:02, 771.71it/s]warmup run: 897it [00:02, 824.82it/s]warmup run: 998it [00:02, 871.12it/s]warmup run: 700it [00:02, 724.60it/s]warmup run: 1008it [00:02, 886.24it/s]warmup run: 895it [00:02, 834.60it/s]warmup run: 904it [00:02, 843.47it/s]warmup run: 761it [00:02, 761.33it/s]warmup run: 881it [00:02, 827.35it/s]warmup run: 997it [00:02, 871.71it/s]warmup run: 1098it [00:02, 906.12it/s]warmup run: 1110it [00:02, 921.87it/s]warmup run: 802it [00:02, 796.72it/s]warmup run: 994it [00:02, 876.64it/s]warmup run: 1005it [00:02, 887.95it/s]warmup run: 858it [00:02, 815.42it/s]warmup run: 978it [00:02, 865.87it/s]warmup run: 1097it [00:02, 904.84it/s]warmup run: 1197it [00:02, 922.95it/s]warmup run: 902it [00:02, 849.54it/s]warmup run: 1212it [00:02, 948.87it/s]warmup run: 1093it [00:02, 891.19it/s]warmup run: 1109it [00:02, 928.18it/s]warmup run: 954it [00:02, 853.21it/s]warmup run: 1075it [00:02, 892.04it/s]warmup run: 1197it [00:02, 930.13it/s]warmup run: 1296it [00:02, 940.05it/s]warmup run: 1315it [00:02, 970.27it/s]warmup run: 1003it [00:02, 891.02it/s]warmup run: 1194it [00:02, 924.23it/s]warmup run: 1213it [00:02, 958.87it/s]warmup run: 1050it [00:02, 875.99it/s]warmup run: 1172it [00:02, 913.42it/s]warmup run: 1296it [00:02, 945.12it/s]warmup run: 1397it [00:03, 958.58it/s]warmup run: 1106it [00:02, 928.75it/s]warmup run: 1418it [00:02, 985.92it/s]warmup run: 1295it [00:02, 948.10it/s]warmup run: 1316it [00:02, 977.57it/s]warmup run: 1145it [00:02, 894.50it/s]warmup run: 1271it [00:02, 934.80it/s]warmup run: 1396it [00:03, 958.50it/s]warmup run: 1498it [00:03, 971.54it/s]warmup run: 1521it [00:03, 996.25it/s]warmup run: 1207it [00:02, 949.22it/s]warmup run: 1397it [00:02, 966.46it/s]warmup run: 1419it [00:02, 991.37it/s]warmup run: 1240it [00:02, 905.52it/s]warmup run: 1371it [00:02, 951.40it/s]warmup run: 1496it [00:03, 967.99it/s]warmup run: 1599it [00:03, 981.76it/s]warmup run: 1623it [00:03, 1001.87it/s]warmup run: 1308it [00:02, 960.69it/s]warmup run: 1498it [00:03, 977.26it/s]warmup run: 1522it [00:03, 1002.31it/s]warmup run: 1335it [00:02, 913.67it/s]warmup run: 1471it [00:02, 965.12it/s]warmup run: 1597it [00:03, 978.38it/s]warmup run: 1699it [00:03, 973.58it/s]warmup run: 1725it [00:03, 1005.89it/s]warmup run: 1408it [00:02, 963.89it/s]warmup run: 1601it [00:03, 990.53it/s]warmup run: 1625it [00:03, 1009.28it/s]warmup run: 1430it [00:03, 924.23it/s]warmup run: 1570it [00:03, 970.88it/s]warmup run: 1697it [00:03, 981.70it/s]warmup run: 1798it [00:03, 970.27it/s]warmup run: 1827it [00:03, 1007.98it/s]warmup run: 1704it [00:03, 1001.35it/s]warmup run: 1508it [00:02, 966.44it/s]warmup run: 1728it [00:03, 1004.50it/s]warmup run: 1525it [00:03, 927.96it/s]warmup run: 1669it [00:03, 975.58it/s]warmup run: 1797it [00:03, 983.68it/s]warmup run: 1896it [00:03, 971.54it/s]warmup run: 1929it [00:03, 997.26it/s] warmup run: 1807it [00:03, 1009.49it/s]warmup run: 1608it [00:03, 973.56it/s]warmup run: 1830it [00:03, 993.22it/s] warmup run: 1620it [00:03, 932.25it/s]warmup run: 1769it [00:03, 981.01it/s]warmup run: 1897it [00:03, 985.44it/s]warmup run: 1994it [00:03, 972.70it/s]warmup run: 2030it [00:03, 1000.56it/s]warmup run: 1911it [00:03, 1016.41it/s]warmup run: 1707it [00:03, 977.88it/s]warmup run: 1931it [00:03, 995.45it/s]warmup run: 1715it [00:03, 934.60it/s]warmup run: 1869it [00:03, 985.61it/s]warmup run: 1997it [00:03, 986.18it/s]warmup run: 2111it [00:03, 1028.87it/s]warmup run: 2150it [00:03, 1058.64it/s]warmup run: 2017it [00:03, 1028.63it/s]warmup run: 1806it [00:03, 979.82it/s]warmup run: 2038it [00:03, 1015.42it/s]warmup run: 1810it [00:03, 933.27it/s]warmup run: 1970it [00:03, 990.36it/s]warmup run: 2114it [00:03, 1038.63it/s]warmup run: 2230it [00:03, 1075.31it/s]warmup run: 2271it [00:03, 1102.82it/s]warmup run: 2140it [00:03, 1086.21it/s]warmup run: 1905it [00:03, 982.48it/s]warmup run: 2157it [00:03, 1066.04it/s]warmup run: 1907it [00:03, 942.22it/s]warmup run: 2085it [00:03, 1037.94it/s]warmup run: 2232it [00:03, 1080.10it/s]warmup run: 2349it [00:03, 1108.37it/s]warmup run: 2392it [00:03, 1133.81it/s]warmup run: 2005it [00:03, 987.38it/s]warmup run: 2262it [00:03, 1123.47it/s]warmup run: 2276it [00:03, 1102.40it/s]warmup run: 2003it [00:03, 947.28it/s]warmup run: 2207it [00:03, 1092.08it/s]warmup run: 2350it [00:03, 1109.60it/s]warmup run: 2468it [00:04, 1131.23it/s]warmup run: 2513it [00:03, 1155.60it/s]warmup run: 2124it [00:03, 1047.60it/s]warmup run: 2384it [00:03, 1149.80it/s]warmup run: 2395it [00:03, 1127.31it/s]warmup run: 2120it [00:03, 1013.11it/s]warmup run: 2330it [00:03, 1130.67it/s]warmup run: 2469it [00:04, 1131.58it/s]warmup run: 2587it [00:04, 1147.05it/s]warmup run: 2635it [00:04, 1172.43it/s]warmup run: 2244it [00:03, 1090.81it/s]warmup run: 2506it [00:03, 1168.71it/s]warmup run: 2514it [00:03, 1144.80it/s]warmup run: 2237it [00:03, 1058.90it/s]warmup run: 2453it [00:03, 1158.03it/s]warmup run: 2588it [00:04, 1146.53it/s]warmup run: 2707it [00:04, 1161.03it/s]warmup run: 2755it [00:04, 1179.13it/s]warmup run: 2364it [00:03, 1121.07it/s]warmup run: 2629it [00:04, 1185.12it/s]warmup run: 2633it [00:04, 1156.91it/s]warmup run: 2354it [00:03, 1090.33it/s]warmup run: 2576it [00:03, 1176.92it/s]warmup run: 2707it [00:04, 1157.84it/s]warmup run: 2826it [00:04, 1166.84it/s]warmup run: 2876it [00:04, 1187.25it/s]warmup run: 2484it [00:03, 1142.13it/s]warmup run: 2752it [00:04, 1197.19it/s]warmup run: 2752it [00:04, 1163.88it/s]warmup run: 2471it [00:04, 1111.76it/s]warmup run: 2699it [00:04, 1191.10it/s]warmup run: 2825it [00:04, 1161.49it/s]warmup run: 2946it [00:04, 1175.00it/s]warmup run: 2997it [00:04, 1192.48it/s]warmup run: 2603it [00:03, 1154.78it/s]warmup run: 2872it [00:04, 1197.84it/s]warmup run: 3000it [00:04, 690.87it/s] warmup run: 2869it [00:04, 1163.02it/s]warmup run: 3000it [00:04, 671.57it/s] warmup run: 2587it [00:04, 1125.63it/s]warmup run: 2820it [00:04, 1195.20it/s]warmup run: 2943it [00:04, 1165.68it/s]warmup run: 2724it [00:04, 1169.25it/s]warmup run: 2993it [00:04, 1199.55it/s]warmup run: 3000it [00:04, 684.96it/s] warmup run: 3000it [00:04, 670.17it/s] warmup run: 2986it [00:04, 1160.34it/s]warmup run: 3000it [00:04, 689.62it/s] warmup run: 2705it [00:04, 1140.18it/s]warmup run: 2942it [00:04, 1200.61it/s]warmup run: 2842it [00:04, 1171.78it/s]warmup run: 3000it [00:04, 689.19it/s] warmup run: 2821it [00:04, 1144.75it/s]warmup run: 2962it [00:04, 1178.07it/s]warmup run: 3000it [00:04, 692.24it/s] warmup run: 2939it [00:04, 1153.67it/s]warmup run: 3000it [00:04, 669.04it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1619.54it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1586.16it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1636.62it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1595.51it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1607.15it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1595.82it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1572.58it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1650.88it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1617.79it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1641.57it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1614.10it/s]warmup should be done:  11%|         | 319/3000 [00:00<00:01, 1590.20it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1608.03it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1608.43it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1660.24it/s]warmup should be done:  11%|         | 316/3000 [00:00<00:01, 1569.67it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1619.97it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1608.81it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1606.64it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1660.80it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1638.55it/s]warmup should be done:  16%|        | 475/3000 [00:00<00:01, 1577.74it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1609.11it/s]warmup should be done:  16%|        | 479/3000 [00:00<00:01, 1586.49it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1620.94it/s]warmup should be done:  21%|       | 644/3000 [00:00<00:01, 1605.39it/s]warmup should be done:  21%|       | 644/3000 [00:00<00:01, 1602.64it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1635.07it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1657.73it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1603.71it/s]warmup should be done:  21%|        | 633/3000 [00:00<00:01, 1571.33it/s]warmup should be done:  21%|       | 638/3000 [00:00<00:01, 1581.62it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1622.70it/s]warmup should be done:  27%|       | 805/3000 [00:00<00:01, 1602.65it/s]warmup should be done:  27%|       | 805/3000 [00:00<00:01, 1601.97it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1631.77it/s]warmup should be done:  27%|       | 808/3000 [00:00<00:01, 1600.50it/s]warmup should be done:  27%|       | 797/3000 [00:00<00:01, 1579.53it/s]warmup should be done:  26%|       | 791/3000 [00:00<00:01, 1568.81it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1650.69it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1621.91it/s]warmup should be done:  32%|      | 966/3000 [00:00<00:01, 1597.73it/s]warmup should be done:  32%|      | 966/3000 [00:00<00:01, 1598.50it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1628.57it/s]warmup should be done:  32%|      | 952/3000 [00:00<00:01, 1580.76it/s]warmup should be done:  32%|      | 955/3000 [00:00<00:01, 1576.17it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1648.49it/s]warmup should be done:  32%|      | 969/3000 [00:00<00:01, 1596.70it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1617.39it/s]warmup should be done:  38%|      | 1126/3000 [00:00<00:01, 1592.87it/s]warmup should be done:  38%|      | 1126/3000 [00:00<00:01, 1593.56it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1624.45it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1647.27it/s]warmup should be done:  37%|      | 1113/3000 [00:00<00:01, 1570.86it/s]warmup should be done:  38%|      | 1129/3000 [00:00<00:01, 1593.01it/s]warmup should be done:  37%|      | 1111/3000 [00:00<00:01, 1573.92it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1624.02it/s]warmup should be done:  43%|     | 1286/3000 [00:00<00:01, 1593.84it/s]warmup should be done:  43%|     | 1286/3000 [00:00<00:01, 1592.92it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1624.82it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1648.72it/s]warmup should be done:  43%|     | 1289/3000 [00:00<00:01, 1594.45it/s]warmup should be done:  42%|     | 1271/3000 [00:00<00:01, 1571.21it/s]warmup should be done:  42%|     | 1271/3000 [00:00<00:01, 1579.31it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1629.04it/s]warmup should be done:  48%|     | 1446/3000 [00:00<00:00, 1594.14it/s]warmup should be done:  48%|     | 1446/3000 [00:00<00:00, 1593.15it/s]warmup should be done:  49%|     | 1475/3000 [00:00<00:00, 1625.16it/s]warmup should be done:  50%|     | 1496/3000 [00:00<00:00, 1649.36it/s]warmup should be done:  48%|     | 1451/3000 [00:00<00:00, 1600.66it/s]warmup should be done:  48%|     | 1429/3000 [00:00<00:00, 1571.00it/s]warmup should be done:  48%|     | 1429/3000 [00:00<00:00, 1575.13it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1631.94it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1625.65it/s]warmup should be done:  54%|    | 1606/3000 [00:01<00:00, 1592.95it/s]warmup should be done:  54%|    | 1606/3000 [00:01<00:00, 1592.82it/s]warmup should be done:  55%|    | 1662/3000 [00:01<00:00, 1650.96it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1605.51it/s]warmup should be done:  53%|    | 1587/3000 [00:01<00:00, 1571.11it/s]warmup should be done:  53%|    | 1589/3000 [00:01<00:00, 1581.53it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1632.96it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1625.41it/s]warmup should be done:  59%|    | 1766/3000 [00:01<00:00, 1594.30it/s]warmup should be done:  59%|    | 1775/3000 [00:01<00:00, 1608.33it/s]warmup should be done:  61%|    | 1828/3000 [00:01<00:00, 1649.69it/s]warmup should be done:  58%|    | 1745/3000 [00:01<00:00, 1570.76it/s]warmup should be done:  59%|    | 1766/3000 [00:01<00:00, 1559.35it/s]warmup should be done:  58%|    | 1748/3000 [00:01<00:00, 1554.81it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1632.06it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1625.58it/s]warmup should be done:  64%|   | 1926/3000 [00:01<00:00, 1594.34it/s]warmup should be done:  65%|   | 1937/3000 [00:01<00:00, 1609.86it/s]warmup should be done:  63%|   | 1903/3000 [00:01<00:00, 1570.85it/s]warmup should be done:  66%|   | 1993/3000 [00:01<00:00, 1642.19it/s]warmup should be done:  63%|   | 1904/3000 [00:01<00:00, 1553.68it/s]warmup should be done:  64%|   | 1923/3000 [00:01<00:00, 1516.92it/s]warmup should be done:  71%|   | 2123/3000 [00:01<00:00, 1629.56it/s]warmup should be done:  70%|   | 2086/3000 [00:01<00:00, 1593.85it/s]warmup should be done:  71%|   | 2127/3000 [00:01<00:00, 1623.63it/s]warmup should be done:  70%|   | 2098/3000 [00:01<00:00, 1609.52it/s]warmup should be done:  69%|   | 2061/3000 [00:01<00:00, 1569.68it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1638.37it/s]warmup should be done:  69%|   | 2062/3000 [00:01<00:00, 1558.76it/s]warmup should be done:  69%|   | 2081/3000 [00:01<00:00, 1533.89it/s]warmup should be done:  76%|  | 2286/3000 [00:01<00:00, 1626.91it/s]warmup should be done:  75%|  | 2246/3000 [00:01<00:00, 1593.85it/s]warmup should be done:  75%|  | 2259/3000 [00:01<00:00, 1607.48it/s]warmup should be done:  76%|  | 2290/3000 [00:01<00:00, 1618.44it/s]warmup should be done:  74%|  | 2218/3000 [00:01<00:00, 1568.75it/s]warmup should be done:  77%|  | 2322/3000 [00:01<00:00, 1628.61it/s]warmup should be done:  74%|  | 2218/3000 [00:01<00:00, 1557.67it/s]warmup should be done:  75%|  | 2242/3000 [00:01<00:00, 1555.69it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1621.89it/s]warmup should be done:  80%|  | 2406/3000 [00:01<00:00, 1590.02it/s]warmup should be done:  81%|  | 2420/3000 [00:01<00:00, 1604.84it/s]warmup should be done:  82%| | 2452/3000 [00:01<00:00, 1615.01it/s]warmup should be done:  79%|  | 2375/3000 [00:01<00:00, 1565.43it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1620.35it/s]warmup should be done:  79%|  | 2375/3000 [00:01<00:00, 1559.96it/s]warmup should be done:  80%|  | 2402/3000 [00:01<00:00, 1568.20it/s]warmup should be done:  86%| | 2566/3000 [00:01<00:00, 1592.10it/s]warmup should be done:  86%| | 2581/3000 [00:01<00:00, 1605.52it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1618.36it/s]warmup should be done:  87%| | 2612/3000 [00:01<00:00, 1613.16it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1566.59it/s]warmup should be done:  84%| | 2532/3000 [00:01<00:00, 1555.27it/s]warmup should be done:  88%| | 2648/3000 [00:01<00:00, 1588.25it/s]warmup should be done:  85%| | 2563/3000 [00:01<00:00, 1579.88it/s]warmup should be done:  91%| | 2726/3000 [00:01<00:00, 1592.93it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1605.03it/s]warmup should be done:  93%|| 2777/3000 [00:01<00:00, 1617.77it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1566.80it/s]warmup should be done:  92%|| 2774/3000 [00:01<00:00, 1604.40it/s]warmup should be done:  90%| | 2688/3000 [00:01<00:00, 1555.04it/s]warmup should be done:  94%|| 2807/3000 [00:01<00:00, 1587.76it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1589.90it/s]warmup should be done:  96%|| 2887/3000 [00:01<00:00, 1597.38it/s]warmup should be done:  97%|| 2904/3000 [00:01<00:00, 1608.02it/s]warmup should be done:  98%|| 2941/3000 [00:01<00:00, 1623.43it/s]warmup should be done:  95%|| 2847/3000 [00:01<00:00, 1569.80it/s]warmup should be done:  98%|| 2938/3000 [00:01<00:00, 1613.87it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1570.45it/s]warmup should be done:  99%|| 2968/3000 [00:01<00:00, 1591.76it/s]warmup should be done:  96%|| 2888/3000 [00:01<00:00, 1601.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1624.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1596.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1584.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1572.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1566.19it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1659.30it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.33it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1606.83it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.74it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.15it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1572.80it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.61it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1633.20it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1614.32it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1663.42it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1642.06it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1585.42it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1642.87it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1650.94it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1656.98it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1595.31it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1616.60it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1653.58it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1662.24it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1665.44it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1649.94it/s]warmup should be done:  16%|        | 477/3000 [00:00<00:01, 1580.56it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1653.26it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1627.17it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1614.94it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1664.46it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1661.42it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1653.69it/s]warmup should be done:  21%|        | 637/3000 [00:00<00:01, 1585.26it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1661.48it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1646.66it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1647.69it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1615.23it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1665.05it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1666.87it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1655.52it/s]warmup should be done:  27%|       | 798/3000 [00:00<00:01, 1593.11it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1665.04it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1654.93it/s]warmup should be done:  28%|       | 831/3000 [00:00<00:01, 1658.17it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1617.33it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1668.07it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1662.27it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1652.38it/s]warmup should be done:  32%|      | 958/3000 [00:00<00:01, 1591.71it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1662.10it/s]warmup should be done:  33%|      | 997/3000 [00:00<00:01, 1655.92it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1648.17it/s]warmup should be done:  38%|      | 1134/3000 [00:00<00:01, 1617.38it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1666.27it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1659.94it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1651.94it/s]warmup should be done:  37%|      | 1120/3000 [00:00<00:01, 1599.42it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1665.17it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1658.52it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1600.20it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:00, 1666.79it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1612.29it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1649.28it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1652.00it/s]warmup should be done:  45%|     | 1338/3000 [00:00<00:00, 1667.87it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1656.18it/s]warmup should be done:  43%|     | 1280/3000 [00:00<00:01, 1581.64it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1590.53it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1666.52it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1645.31it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1670.48it/s]warmup should be done:  50%|     | 1500/3000 [00:00<00:00, 1645.15it/s]warmup should be done:  49%|     | 1458/3000 [00:00<00:00, 1593.20it/s]warmup should be done:  50%|     | 1496/3000 [00:00<00:00, 1655.28it/s]warmup should be done:  48%|     | 1439/3000 [00:00<00:00, 1571.56it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1606.72it/s]warmup should be done:  56%|    | 1669/3000 [00:01<00:00, 1669.23it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1647.24it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1673.36it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1647.41it/s]warmup should be done:  55%|    | 1664/3000 [00:01<00:00, 1660.89it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1573.06it/s]warmup should be done:  53%|    | 1597/3000 [00:01<00:00, 1554.92it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1618.92it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1671.52it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1644.82it/s]warmup should be done:  61%|   | 1843/3000 [00:01<00:00, 1676.13it/s]warmup should be done:  61%|    | 1833/3000 [00:01<00:00, 1651.32it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1664.35it/s]warmup should be done:  59%|    | 1776/3000 [00:01<00:00, 1559.89it/s]warmup should be done:  58%|    | 1753/3000 [00:01<00:00, 1541.12it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1627.18it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1670.29it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1645.40it/s]warmup should be done:  67%|   | 2011/3000 [00:01<00:00, 1674.08it/s]warmup should be done:  67%|   | 1999/3000 [00:01<00:00, 1650.63it/s]warmup should be done:  67%|   | 1999/3000 [00:01<00:00, 1664.93it/s]warmup should be done:  65%|   | 1936/3000 [00:01<00:00, 1570.69it/s]warmup should be done:  64%|   | 1910/3000 [00:01<00:00, 1547.03it/s]warmup should be done:  66%|   | 1988/3000 [00:01<00:00, 1630.22it/s]warmup should be done:  72%|  | 2173/3000 [00:01<00:00, 1668.38it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1645.10it/s]warmup should be done:  72%|  | 2166/3000 [00:01<00:00, 1653.83it/s]warmup should be done:  72%|  | 2166/3000 [00:01<00:00, 1660.77it/s]warmup should be done:  73%|  | 2179/3000 [00:01<00:00, 1642.54it/s]warmup should be done:  70%|   | 2098/3000 [00:01<00:00, 1584.38it/s]warmup should be done:  69%|   | 2069/3000 [00:01<00:00, 1557.97it/s]warmup should be done:  72%|  | 2152/3000 [00:01<00:00, 1629.82it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1670.47it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1647.05it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1659.70it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1665.00it/s]warmup should be done:  75%|  | 2260/3000 [00:01<00:00, 1594.76it/s]warmup should be done:  78%|  | 2344/3000 [00:01<00:00, 1631.87it/s]warmup should be done:  74%|  | 2229/3000 [00:01<00:00, 1568.63it/s]warmup should be done:  77%|  | 2316/3000 [00:01<00:00, 1632.80it/s]warmup should be done:  83%| | 2486/3000 [00:01<00:00, 1649.34it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1663.76it/s]warmup should be done:  83%| | 2502/3000 [00:01<00:00, 1667.91it/s]warmup should be done:  84%| | 2509/3000 [00:01<00:00, 1643.66it/s]warmup should be done:  81%|  | 2422/3000 [00:01<00:00, 1600.58it/s]warmup should be done:  84%| | 2508/3000 [00:01<00:00, 1621.87it/s]warmup should be done:  80%|  | 2386/3000 [00:01<00:00, 1564.26it/s]warmup should be done:  83%| | 2482/3000 [00:01<00:00, 1638.26it/s]warmup should be done:  88%| | 2651/3000 [00:01<00:00, 1648.74it/s]warmup should be done:  89%| | 2669/3000 [00:01<00:00, 1667.21it/s]warmup should be done:  89%| | 2669/3000 [00:01<00:00, 1658.16it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1648.84it/s]warmup should be done:  86%| | 2584/3000 [00:01<00:00, 1603.71it/s]warmup should be done:  85%| | 2546/3000 [00:01<00:00, 1573.87it/s]warmup should be done:  89%| | 2671/3000 [00:01<00:00, 1614.39it/s]warmup should be done:  88%| | 2647/3000 [00:01<00:00, 1638.90it/s]warmup should be done:  94%|| 2816/3000 [00:01<00:00, 1642.23it/s]warmup should be done:  95%|| 2836/3000 [00:01<00:00, 1667.29it/s]warmup should be done:  95%|| 2836/3000 [00:01<00:00, 1659.83it/s]warmup should be done:  95%|| 2842/3000 [00:01<00:00, 1652.11it/s]warmup should be done:  92%|| 2747/3000 [00:01<00:00, 1608.99it/s]warmup should be done:  90%| | 2712/3000 [00:01<00:00, 1598.77it/s]warmup should be done:  94%|| 2833/3000 [00:01<00:00, 1613.94it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1648.40it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1660.48it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.03it/s]warmup should be done:  99%|| 2981/3000 [00:01<00:00, 1640.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.08it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1608.82it/s]warmup should be done:  96%|| 2880/3000 [00:01<00:00, 1622.42it/s]warmup should be done: 100%|| 2998/3000 [00:01<00:00, 1624.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.79it/s]warmup should be done:  99%|| 2983/3000 [00:01<00:00, 1656.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.00it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1600.44it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1586.34it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f5f4280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f5e71f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f8f9730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f5e50d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f8fbd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f8f8e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f5f41c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f380f5e5190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 22:25:33.010135: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f334302e1d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:33.010206: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:33.019437: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f334e82b0d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:33.019484: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:33.020437: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:33.027042: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:33.946120: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f334a830870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:33.946183: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:33.955665: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:34.049864: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f33468309b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:34.049937: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:34.054014: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3347028e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:34.054076: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:34.060244: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:34.062680: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:34.113792: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f334e8342f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:34.113860: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:34.123028: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:34.141837: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f334e795bb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:34.141906: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:34.149471: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:34.167852: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3332837ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:25:34.167920: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:25:34.176593: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:25:40.296787: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:40.384474: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:40.832792: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:40.857336: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:41.006435: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:41.006760: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:41.232722: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:25:41.260681: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:26:42.458][ERROR][RK0][tid #139858672600832]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:26:42.458][ERROR][RK0][tid #139858672600832]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.467][ERROR][RK0][tid #139858672600832]: coll ps creation done
[HCTR][22:26:42.467][ERROR][RK0][tid #139858672600832]: replica 5 waits for coll ps creation barrier
[HCTR][22:26:42.566][ERROR][RK0][tid #139858731316992]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:26:42.566][ERROR][RK0][tid #139858731316992]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.571][ERROR][RK0][tid #139858731316992]: coll ps creation done
[HCTR][22:26:42.571][ERROR][RK0][tid #139858731316992]: replica 1 waits for coll ps creation barrier
[HCTR][22:26:42.657][ERROR][RK0][tid #139858664208128]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:26:42.657][ERROR][RK0][tid #139858664208128]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.662][ERROR][RK0][tid #139858664208128]: coll ps creation done
[HCTR][22:26:42.662][ERROR][RK0][tid #139858664208128]: replica 3 waits for coll ps creation barrier
[HCTR][22:26:42.669][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:26:42.669][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.679][ERROR][RK0][main]: coll ps creation done
[HCTR][22:26:42.679][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][22:26:42.684][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:26:42.684][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.685][ERROR][RK0][tid #139858806818560]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:26:42.685][ERROR][RK0][tid #139858806818560]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.691][ERROR][RK0][tid #139858806818560]: coll ps creation done
[HCTR][22:26:42.691][ERROR][RK0][tid #139858806818560]: replica 7 waits for coll ps creation barrier
[HCTR][22:26:42.691][ERROR][RK0][main]: coll ps creation done
[HCTR][22:26:42.691][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][22:26:42.711][ERROR][RK0][tid #139858731316992]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:26:42.711][ERROR][RK0][tid #139858731316992]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.715][ERROR][RK0][tid #139858731316992]: coll ps creation done
[HCTR][22:26:42.715][ERROR][RK0][tid #139858731316992]: replica 6 waits for coll ps creation barrier
[HCTR][22:26:42.756][ERROR][RK0][tid #139859402405632]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:26:42.756][ERROR][RK0][tid #139859402405632]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][22:26:42.761][ERROR][RK0][tid #139859402405632]: coll ps creation done
[HCTR][22:26:42.761][ERROR][RK0][tid #139859402405632]: replica 0 waits for coll ps creation barrier
[HCTR][22:26:42.761][ERROR][RK0][tid #139859402405632]: replica 0 preparing frequency
[HCTR][22:26:43.602][ERROR][RK0][tid #139859402405632]: replica 0 preparing frequency done
[HCTR][22:26:43.649][ERROR][RK0][tid #139859402405632]: replica 0 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][tid #139858664208128]: replica 3 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][tid #139858731316992]: replica 6 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][tid #139858731316992]: replica 1 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][tid #139858672600832]: replica 5 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][tid #139858806818560]: replica 7 calling init per replica
[HCTR][22:26:43.649][ERROR][RK0][tid #139859402405632]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][tid #139858664208128]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][main]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][tid #139858731316992]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][main]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][tid #139858731316992]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][tid #139858672600832]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][tid #139859402405632]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][tid #139858806818560]: Calling build_v2
[HCTR][22:26:43.649][ERROR][RK0][tid #139858664208128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][tid #139858731316992]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][tid #139858731316992]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][tid #139858672600832]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:26:43.649][ERROR][RK0][tid #139858806818560]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 22:26:432022-12-11 22:26:432022-12-11 22:26:432022-12-11 22:26:432022-12-11 22:26:432022-12-11 22:26:432022-12-11 22:26:43.......2022-12-11 22:26:43649277649277649285649285649277649285649285.: : : : : : : 649296EEEEEEE:        E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :::::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136136136136136:] ] ] ] ] ] ] 136using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] 






using concurrent impl MPS
[2022-12-11 22:26:43.654225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:26:43.654264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-11 22:26:43] .assigning 8 to cpu654273
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:26:43.654328: [E[2022-12-11 22:26:43 2022-12-11 22:26:43./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.654328:654340: 196: [E] E2022-12-11 22:26:43 assigning 8 to cpu ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc654378::: [178212E2022-12-11 22:26:43] ]  .v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc654429[

[[:: 2022-12-11 22:26:432022-12-11 22:26:432022-12-11 22:26:43178E[...[]  2022-12-11 22:26:436544716544746545002022-12-11 22:26:43v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: : : .
[:654515EEE6545212022-12-11 22:26:43[178:    : .2022-12-11 22:26:43] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE654571.v100x8, slow pcie ::: : 654603
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212178196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: :] [] ] : E213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 22:26:43v100x8, slow pcieassigning 8 to cpu178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 
.

] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421654710v100x8, slow pcie[178[:
: 
2022-12-11 22:26:43[] [2022-12-11 22:26:43196E.[2022-12-11 22:26:43v100x8, slow pcie2022-12-11 22:26:43.]  6547882022-12-11 22:26:43.
.654790assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .654804654826[: 
:E654822: : 2022-12-11 22:26:43E196 : EE. ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE  654895/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :2022-12-11 22:26:43
196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::E213.] :212196 ] [654969assigning 8 to cpu214] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-11 22:26:43: 
] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 8 to cpu:
.Ecpu time is 97.0588

196655045 [
[] [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:26:432022-12-11 22:26:43assigning 8 to cpu2022-12-11 22:26:43E:.[.
. 2126551452022-12-11 22:26:43655167655160/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : .: : :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E655190EE212
 :  [ ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 22:26:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :2022-12-11 22:26:43.:
214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213.655270212] :] 655289[: ] cpu time is 97.0588212remote time is 8.68421: 2022-12-11 22:26:43Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] 
E.[ 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 6553592022-12-11 22:26:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [.:[:E2022-12-11 22:26:436554352122022-12-11 22:26:43213 .: ] .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc655457Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8655481remote time is 8.68421::  
: 
213E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] [ [: remote time is 8.684212022-12-11 22:26:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:26:43214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.:.] :655591[213655595cpu time is 97.0588213: 2022-12-11 22:26:43] : 
] E.remote time is 8.68421Eremote time is 8.68421 655647
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2142022-12-11 22:26:43:[E] .2132022-12-11 22:26:43 cpu time is 97.0588655738] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: remote time is 8.68421655759:E
: 214 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ cpu time is 97.0588:2022-12-11 22:26:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214.:] 655846214cpu time is 97.0588: ] 
Ecpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 22:28:00.159196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:28:00.199319: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 22:28:00.199422: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 22:28:00.200511: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 22:28:00.279259: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 22:28:00.670007: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 22:28:00.670099: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 22:28:08.306681: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-11 22:28:08.306781: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 22:28:10. 70970: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 22:28:10. 71112: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-11 22:28:10. 74511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 22:28:10. 74603: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-11 22:28:10.552777: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 22:28:10.594815: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 22:28:10.596883: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 22:28:10.634228: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 22:28:11.193176: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 22:28:30.111444: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 22:28:30.119178: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 22:28:30.122725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 22:28:30.169871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:28:30.169970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:28:30.170002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:28:30.170029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:28:30.170657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:28:30.170710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.171943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.172616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.185349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 22:28:30.185424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-11 22:28:30.2022-12-11 22:28:30185654.: 185664E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 202] 6 solved1 solved

[[2022-12-11 22:28:302022-12-11 22:28:30..185740185742: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 6 initing device 6worker 0 thread 1 initing device 1

[2022-12-11 22:28:30.185851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:28:30.185899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.185966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 22:28:30.186040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 22:28:30.186106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 22:28:30.186167: E[[ 2022-12-11 22:28:302022-12-11 22:28:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc..:186175186177205: : ] EEworker 0 thread 4 initing device 4  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 22:28:302022-12-11 22:28:30..186296186297: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 22:28:30.186482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:28:30.186531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.186670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:28:30.186715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.186955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 22:28:30.187006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 22:28:30.187071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 22:28:30.187124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 22:28:30.187460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:28:30.187507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.187564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:28:30.187607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.190037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.190375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.190430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.191081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.191145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.191667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.192203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.194433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.194667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.194771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.195357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.195410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.195464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.195968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:28:30.248253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 22:28:30.253532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 22:28:30.253649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:28:30.254480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.255050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.256038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.257717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:28:30.258456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:28:30.258501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.04 MB
[[[[[[[2022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:30.......275451275451275451275451275451275459275451: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes






[2022-12-11 22:28:30.282210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 22:28:30.282295: [E2022-12-11 22:28:30 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc282292:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1023
[2022-12-11 22:28:30.282363[: 2022-12-11 22:28:30E. 282385/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1023:
638] eager release cuda mem 400000000
[2022-12-11 22:28:30.282451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:28:30.282466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 22:28:30.282548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:28:30.282571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 22:28:30.282627: [E2022-12-11 22:28:30 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc282653:: 638E]  eager release cuda mem 1023/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 400000000
[[2022-12-11 22:28:302022-12-11 22:28:30..282719282707: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1023

[2022-12-11 22:28:30.282808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:28:30.283390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.284055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.284841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.285622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.286361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.286923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.287532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:28:30.288104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.288270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.288537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.288640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.288812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.288854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.288901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.289066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.289222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.289511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.289595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.289767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.289815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.289859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.295168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:28:30.295379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:28:30.295513: E[ 2022-12-11 22:28:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2955341980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:28:30.295625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:28:30.295751: E[ 2022-12-11 22:28:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2957741980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:28:30.295850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.59 MB
[2022-12-11 22:28:30.295869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:28:30.296028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:28:30.296070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 980.91 MB
[2022-12-11 22:28:30.296245: [E2022-12-11 22:28:30 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc296263:: 638E]  eager release cuda mem 25855/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 25855
[2022-12-11 22:28:30.296314: [E2022-12-11 22:28:30 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu296322:[: 19802022-12-11 22:28:30E] . eager alloc mem 981.32 MB296333/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 981.12 MB:
638] eager release cuda mem 25855
[2022-12-11 22:28:30.296441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 979.85 MB
[2022-12-11 22:28:30.296492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:28:30.296536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.01 MB
[2022-12-11 22:28:30.296578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:28:30.296631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 979.54 MB
[[[[[[[[2022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:302022-12-11 22:28:30........589935589936589936589935589935589936589935589934: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 4 init p2p of link 5Device 0 init p2p of link 3Device 7 init p2p of link 4Device 3 init p2p of link 2Device 1 init p2p of link 7Device 6 init p2p of link 0Device 2 init p2p of link 1Device 5 init p2p of link 6







[[2022-12-11 22:28:30[2022-12-11 22:28:30[[.[2022-12-11 22:28:30[.[2022-12-11 22:28:302022-12-11 22:28:305904352022-12-11 22:28:30.2022-12-11 22:28:305904352022-12-11 22:28:30..: .590439.: .590439590439E590447: 590451E590451: :  : E:  : EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::] :1980:] :19801980eager alloc mem 611.00 KB1980] 1980eager alloc mem 611.00 KB1980] ] 
] eager alloc mem 611.00 KB] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-11 22:28:30[.2022-12-11 22:28:30[591506.2022-12-11 22:28:30[: 591513[.2022-12-11 22:28:30E: [2022-12-11 22:28:30591517.[[ E2022-12-11 22:28:30.: 5915342022-12-11 22:28:302022-12-11 22:28:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc .591534E: ..:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc591548:  E591554591557638:: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : : ] 638E :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEEeager release cuda mem 625663]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:  
eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638eager release cuda mem 625663] ::638] 
eager release cuda mem 625663638638] eager release cuda mem 625663
] ] eager release cuda mem 625663
eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 22:28:30.604640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 22:28:30.604772: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:28:30:.1926604799] : Device 6 init p2p of link 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:28:30] .eager alloc mem 611.00 KB604832
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 22:28:30.604939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.604986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.605158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 22:28:30.605222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 22:28:30.605310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.605359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.605626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.605664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 22:28:30.605735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.605794: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:28:30:.638605809] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.605868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 22:28:30.606021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.606071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 22:28:30.606110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.606146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.606237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.606625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.606801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.607025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.618130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 22:28:30.618259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.618498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 22:28:30.618619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.618752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 22:28:30.618871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.619012: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:28:30:.1926619037] : Device 1 init p2p of link 3E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.619169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:28:30] .eager alloc mem 611.00 KB619178
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 22:28:30.619255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 22:28:30.619307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.619370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.619405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 22:28:30
.619419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 22:28:30.619539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.619617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1[
2022-12-11 22:28:30.619651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.619742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.619977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.620093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.620164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.620321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.620528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.633413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 22:28:30.633528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.634314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.634488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 22:28:30.634543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 22:28:30.634602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.634658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.635122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 22:28:30.635254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.635339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 22:28:30.635390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:28:302022-12-11 22:28:30..635450635452: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 22:28:30.635889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 22:28:30.636002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.636052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.636160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 22:28:30.636260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 22:28:30
.636280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.636381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 22:28:30.636516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:28:30.636787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.637059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.637301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:28:30.649951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.650366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.650476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.651144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.651201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.651515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.651842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.652247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1996740 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5991293 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 979.85 MB | 0.464644 secs 
[2022-12-11 22:28:30.652288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:28:30.652327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1990056 / 100000000 nodes ( 1.99 %~2.00 %) | remote 5997977 / 100000000 nodes ( 6.00 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 976.59 MB | 0.465621 secs 
[2022-12-11 22:28:30.652474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999743 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5988290 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 981.32 MB | 0.466185 secs 
[2022-12-11 22:28:30.652641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999115 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5988918 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 981.01 MB | 0.466352 secs 
[2022-12-11 22:28:30.652726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1999339 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5988694 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 981.12 MB | 0.466841 secs 
[2022-12-11 22:28:30.652965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1996090 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5991943 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 979.54 MB | 0.465465 secs 
[2022-12-11 22:28:30.653162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1998895 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5989138 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 980.91 MB | 0.466643 secs 
[2022-12-11 22:28:30.653619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1997115 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5990918 / 100000000 nodes ( 5.99 %) | cpu 92011967 / 100000000 nodes ( 92.01 %) | 980.04 MB | 0.482923 secs 
[2022-12-11 22:28:30.654847: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.28 GB
[2022-12-11 22:28:31.976958: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.54 GB
[2022-12-11 22:28:31.977298: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.54 GB
[2022-12-11 22:28:31.977602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.54 GB
[2022-12-11 22:28:33.333697: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.80 GB
[2022-12-11 22:28:33.345615: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.80 GB
[2022-12-11 22:28:33.346217: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.80 GB
[2022-12-11 22:28:34.733286: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.02 GB
[2022-12-11 22:28:34.733484: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.02 GB
[2022-12-11 22:28:34.733825: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.02 GB
[2022-12-11 22:28:36.210345: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.23 GB
[2022-12-11 22:28:36.210536: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.23 GB
[2022-12-11 22:28:36.210892: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.23 GB
[2022-12-11 22:28:37.741839: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.69 GB
[2022-12-11 22:28:37.741981: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.69 GB
[2022-12-11 22:28:37.742522: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.69 GB
[2022-12-11 22:28:39. 93303: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.89 GB
[2022-12-11 22:28:39. 93653: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.89 GB
[HCTR][22:28:39.365][ERROR][RK0][tid #139858731316992]: replica 6 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][tid #139858731316992]: replica 1 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][tid #139858672600832]: replica 5 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][tid #139859402405632]: replica 0 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][tid #139858806818560]: replica 7 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][tid #139858664208128]: replica 3 calling init per replica done, doing barrier
[HCTR][22:28:39.365][ERROR][RK0][tid #139858731316992]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858672600832]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858806818560]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858672600832]: init per replica done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858664208128]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][tid #139859402405632]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858731316992]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858731316992]: init per replica done
[HCTR][22:28:39.365][ERROR][RK0][main]: init per replica done
[HCTR][22:28:39.365][ERROR][RK0][main]: init per replica done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858806818560]: init per replica done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858664208128]: init per replica done
[HCTR][22:28:39.365][ERROR][RK0][tid #139858731316992]: init per replica done
[HCTR][22:28:39.368][ERROR][RK0][tid #139859402405632]: init per replica done
[HCTR][22:28:39.404][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f1684238400
[HCTR][22:28:39.404][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f1684558400
[HCTR][22:28:39.404][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f1684b98400
[HCTR][22:28:39.404][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f1684eb8400
[HCTR][22:28:39.404][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f1758238400
[HCTR][22:28:39.404][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f1758558400
[HCTR][22:28:39.404][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f1758b98400
[HCTR][22:28:39.404][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f1758eb8400
[HCTR][22:28:39.404][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f1794238400
[HCTR][22:28:39.404][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f1794558400
[HCTR][22:28:39.404][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f1794b98400
[HCTR][22:28:39.404][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f1794eb8400
[HCTR][22:28:39.404][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f1784238400
[HCTR][22:28:39.404][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f1784558400
[HCTR][22:28:39.404][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f1784b98400
[HCTR][22:28:39.404][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f1784eb8400
[HCTR][22:28:39.404][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f17bc238400
[HCTR][22:28:39.404][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f17bc558400
[HCTR][22:28:39.404][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f17bcb98400
[HCTR][22:28:39.404][ERROR][RK0][tid #139858731316992]: 6 allocated 3276800 at 0x7f17dc238400
[HCTR][22:28:39.404][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f17bceb8400
[HCTR][22:28:39.404][ERROR][RK0][tid #139858731316992]: 6 allocated 6553600 at 0x7f17dc558400
[HCTR][22:28:39.404][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f16e8238400
[HCTR][22:28:39.404][ERROR][RK0][tid #139858731316992]: 6 allocated 3276800 at 0x7f17dcb98400
[HCTR][22:28:39.404][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f16e8558400
[HCTR][22:28:39.404][ERROR][RK0][tid #139858731316992]: 6 allocated 6553600 at 0x7f17dceb8400
[HCTR][22:28:39.404][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f16e8b98400
[HCTR][22:28:39.404][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f16e8eb8400
[HCTR][22:28:39.407][ERROR][RK0][tid #139859402405632]: 0 allocated 3276800 at 0x7f1788320000
[HCTR][22:28:39.407][ERROR][RK0][tid #139859402405632]: 0 allocated 6553600 at 0x7f1788640000
[HCTR][22:28:39.407][ERROR][RK0][tid #139859402405632]: 0 allocated 3276800 at 0x7f1788c80000
[HCTR][22:28:39.407][ERROR][RK0][tid #139859402405632]: 0 allocated 6553600 at 0x7f1788fa0000
