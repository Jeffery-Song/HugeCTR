2022-12-12 06:47:24.784236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.791254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.796003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.803233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.808174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.820405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.826677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.832264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.888730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.897293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.897716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.900871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.902024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.903010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.903741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.904521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.905597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.906014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.907338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.907477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.909130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.909212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.910824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.910941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.912487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.912566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.914153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.914989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.915883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.916817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.917839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.918872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.920614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.921695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.922811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.924069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.925812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.926424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.927040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.928134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.928636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.929762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.930120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.931364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.932444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.933523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.934603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.935694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.935904: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:24.940592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.941280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.942191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.942949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.943743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.944448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.945335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.945737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.945905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.947392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.947653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.947804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.949369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.949632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.949769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.951355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.951746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.953309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.953744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.959251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.961190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.961529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.963334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.964107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.965574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.966525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.966761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.974598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.976909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.977497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.977891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.978028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.979319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.980151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.980595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.980786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:24.980888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.000105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.000105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.016765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.017748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.018109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.018462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.018756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.020534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.020576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.021714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.022547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.022768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.023848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.024656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.025730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.025814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.028051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.028563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.028795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.029176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.031096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.031481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.031724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.033702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.034179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.034270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.035685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.035729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.035897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.038234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.038845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.039374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.040208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.040249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.040501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.042783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.043500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.044560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.044599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.045262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.046328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.047100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.048206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.048287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.050120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.050648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.051450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.051535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.053317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.053716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.054431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.054511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.057000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.057380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.057625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.057660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.060236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.060620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.060679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.060780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.063287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.063724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.063732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.063832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.066242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.066738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.066796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.066834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.069172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.069711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.069769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.069807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.072100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.072885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.072991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.073043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.076040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.076074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.076921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.077491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.077806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.078512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.078627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.080493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.081099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.081300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.082377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.083715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.084188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.084530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.085070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.086353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.086897: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.087148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.087271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.087845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.087883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.089722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.090640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.090644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.091119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.091353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.092651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.092926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.094123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.094343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.094959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.095066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.096591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.097156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.097696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.098091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.098394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.098953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.099155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.100852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.101755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.102059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.102666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.102929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.103493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.105123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.105931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.106942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.107454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.107777: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.108442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.109806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.109940: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.110106: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.110177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.111074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.112563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.113246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.113951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.115667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.116083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.116771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.117578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.118636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.119010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.119780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.119841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.120137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.121526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.123101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.123445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.124319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.124414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.124687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.126029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.127741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.128159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.129060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.129100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.129235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.132093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.132730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.133808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.166279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.167020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.167250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.170732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.172094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.172111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.175409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.187463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.187585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.193389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.193822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.193932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.198400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.198791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.200725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.231862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.232178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.232963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.237624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.238007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.238876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.242824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.242952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.243559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.246978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.251192: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.260787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.262928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.290644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.292907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.292964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.297422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.298298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.298414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.331118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.333928: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.342597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.393845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.396337: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:47:25.398730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.405897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.411192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:25.416466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.253309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.253929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.254470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.255184: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.255247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:47:26.274435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.275179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.275695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.276290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.276799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.277272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:47:26.322431: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.322620: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.363739: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:47:26.492986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.493844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.494384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.495219: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.495281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:47:26.514280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.514903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.515814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.516515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.517076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.517725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:47:26.522099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.522700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.523260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.523726: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.523783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:47:26.535545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.536520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.537181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.537647: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.537703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:47:26.541451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.542058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.542563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.542818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.543847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.544026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.544899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.545468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.545958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:47:26.546183: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.546231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:47:26.555177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.556101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.556990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.557598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.558122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.558586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:47:26.564344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.565161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.565774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.566348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.566853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.567334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:47:26.590652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.591324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.591843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.592309: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.592363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:47:26.592891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.592981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.594089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.594114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.595147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.595164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.596131: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.596190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:47:26.596181: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:47:26.596251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:47:26.598218: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.598369: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.599478: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:47:26.603966: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.604140: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.605928: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:47:26.611532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.612208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.612715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.613166: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.613300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.613311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.613827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.614303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:47:26.615012: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:47:26.615411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.616066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.616299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.616998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.617268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.618063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.618187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.619088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.619308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.619977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:47:26.620221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:47:26.620688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:47:26.633064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.633248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.635181: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:47:26.660395: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.660585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.661406: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 06:47:26.665913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.666084: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.666711: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.666841: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:47:26.667108: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:47:26.667863: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][06:47:27.940][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.940][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.940][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.941][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.941][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.941][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.941][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:47:27.941][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 97it [00:01, 81.44it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 195it [00:01, 177.92it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 102it [00:01, 86.01it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 292it [00:01, 283.24it/s]warmup run: 83it [00:01, 71.50it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 99it [00:01, 85.11it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 94it [00:01, 80.40it/s]warmup run: 203it [00:01, 185.47it/s]warmup run: 96it [00:01, 82.96it/s]warmup run: 390it [00:01, 393.85it/s]warmup run: 168it [00:01, 157.03it/s]warmup run: 101it [00:01, 88.81it/s]warmup run: 199it [00:01, 185.38it/s]warmup run: 99it [00:01, 85.32it/s]warmup run: 188it [00:01, 174.22it/s]warmup run: 191it [00:01, 178.49it/s]warmup run: 304it [00:01, 295.10it/s]warmup run: 488it [00:02, 502.09it/s]warmup run: 269it [00:01, 272.66it/s]warmup run: 201it [00:01, 190.59it/s]warmup run: 198it [00:01, 184.71it/s]warmup run: 299it [00:01, 295.47it/s]warmup run: 284it [00:01, 279.97it/s]warmup run: 286it [00:01, 283.39it/s]warmup run: 405it [00:01, 408.97it/s]warmup run: 589it [00:02, 607.15it/s]warmup run: 368it [00:01, 389.03it/s]warmup run: 298it [00:01, 297.83it/s]warmup run: 399it [00:01, 409.44it/s]warmup run: 293it [00:01, 288.34it/s]warmup run: 378it [00:01, 386.28it/s]warmup run: 383it [00:01, 394.53it/s]warmup run: 506it [00:02, 519.97it/s]warmup run: 691it [00:02, 700.53it/s]warmup run: 470it [00:02, 507.55it/s]warmup run: 393it [00:01, 404.44it/s]warmup run: 500it [00:02, 521.34it/s]warmup run: 392it [00:01, 401.57it/s]warmup run: 473it [00:02, 491.86it/s]warmup run: 481it [00:02, 503.98it/s]warmup run: 609it [00:02, 626.26it/s]warmup run: 793it [00:02, 778.67it/s]warmup run: 574it [00:02, 619.59it/s]warmup run: 489it [00:01, 509.59it/s]warmup run: 490it [00:02, 510.22it/s]warmup run: 601it [00:02, 623.68it/s]warmup run: 570it [00:02, 591.55it/s]warmup run: 581it [00:02, 608.83it/s]warmup run: 712it [00:02, 718.11it/s]warmup run: 892it [00:02, 833.57it/s]warmup run: 678it [00:02, 715.61it/s]warmup run: 589it [00:02, 613.65it/s]warmup run: 591it [00:02, 614.79it/s]warmup run: 698it [00:02, 700.93it/s]warmup run: 668it [00:02, 680.20it/s]warmup run: 684it [00:02, 706.22it/s]warmup run: 815it [00:02, 793.95it/s]warmup run: 781it [00:02, 792.44it/s]warmup run: 991it [00:02, 823.97it/s]warmup run: 688it [00:02, 700.11it/s]warmup run: 691it [00:02, 703.64it/s]warmup run: 800it [00:02, 778.60it/s]warmup run: 765it [00:02, 750.06it/s]warmup run: 787it [00:02, 785.18it/s]warmup run: 915it [00:02, 845.98it/s]warmup run: 882it [00:02, 848.33it/s]warmup run: 1084it [00:02, 811.85it/s]warmup run: 784it [00:02, 762.77it/s]warmup run: 790it [00:02, 773.51it/s]warmup run: 901it [00:02, 838.15it/s]warmup run: 862it [00:02, 806.08it/s]warmup run: 890it [00:02, 848.28it/s]warmup run: 1015it [00:02, 882.78it/s]warmup run: 982it [00:02, 887.13it/s]warmup run: 1181it [00:02, 852.70it/s]warmup run: 880it [00:02, 814.19it/s]warmup run: 887it [00:02, 821.89it/s]warmup run: 1000it [00:02, 878.82it/s]warmup run: 959it [00:02, 848.35it/s]warmup run: 991it [00:02, 891.87it/s]warmup run: 1116it [00:02, 916.50it/s]warmup run: 1082it [00:02, 915.19it/s]warmup run: 1278it [00:02, 883.57it/s]warmup run: 979it [00:02, 861.14it/s]warmup run: 987it [00:02, 868.18it/s]warmup run: 1103it [00:02, 919.20it/s]warmup run: 1057it [00:02, 884.61it/s]warmup run: 1093it [00:02, 925.48it/s]warmup run: 1219it [00:02, 948.08it/s]warmup run: 1182it [00:02, 938.45it/s]warmup run: 1376it [00:03, 909.83it/s]warmup run: 1079it [00:02, 898.88it/s]warmup run: 1207it [00:02, 952.53it/s]warmup run: 1087it [00:02, 903.50it/s]warmup run: 1155it [00:02, 911.16it/s]warmup run: 1195it [00:02, 951.50it/s]warmup run: 1320it [00:02, 963.26it/s]warmup run: 1282it [00:02, 948.70it/s]warmup run: 1476it [00:03, 935.41it/s]warmup run: 1180it [00:02, 928.37it/s]warmup run: 1310it [00:02, 972.75it/s]warmup run: 1187it [00:02, 928.74it/s]warmup run: 1254it [00:02, 931.69it/s]warmup run: 1297it [00:02, 970.69it/s]warmup run: 1423it [00:02, 980.54it/s]warmup run: 1381it [00:02, 952.71it/s]warmup run: 1578it [00:03, 957.20it/s]warmup run: 1279it [00:02, 945.60it/s]warmup run: 1414it [00:02, 992.00it/s]warmup run: 1287it [00:02, 948.89it/s]warmup run: 1358it [00:02, 960.66it/s]warmup run: 1400it [00:02, 986.89it/s]warmup run: 1527it [00:03, 997.92it/s]warmup run: 1479it [00:03, 959.04it/s]warmup run: 1680it [00:03, 975.17it/s]warmup run: 1378it [00:02, 957.54it/s]warmup run: 1518it [00:03, 1006.02it/s]warmup run: 1389it [00:02, 969.50it/s]warmup run: 1461it [00:03, 980.08it/s]warmup run: 1503it [00:03, 996.80it/s]warmup run: 1631it [00:03, 1009.03it/s]warmup run: 1580it [00:03, 973.14it/s]warmup run: 1783it [00:03, 991.09it/s]warmup run: 1479it [00:02, 970.27it/s]warmup run: 1622it [00:03, 1015.08it/s]warmup run: 1491it [00:03, 983.40it/s]warmup run: 1563it [00:03, 989.25it/s]warmup run: 1605it [00:03, 1000.21it/s]warmup run: 1734it [00:03, 1014.04it/s]warmup run: 1681it [00:03, 981.75it/s]warmup run: 1886it [00:03, 1000.84it/s]warmup run: 1580it [00:03, 981.01it/s]warmup run: 1593it [00:03, 992.02it/s]warmup run: 1727it [00:03, 1022.53it/s]warmup run: 1665it [00:03, 996.26it/s]warmup run: 1707it [00:03, 1001.18it/s]warmup run: 1837it [00:03, 1018.09it/s]warmup run: 1782it [00:03, 986.86it/s]warmup run: 1989it [00:03, 1006.88it/s]warmup run: 1681it [00:03, 989.40it/s]warmup run: 1696it [00:03, 1000.77it/s]warmup run: 1831it [00:03, 1022.63it/s]warmup run: 1767it [00:03, 1000.49it/s]warmup run: 1941it [00:03, 1023.88it/s]warmup run: 1809it [00:03, 999.19it/s] warmup run: 1886it [00:03, 1001.03it/s]warmup run: 2108it [00:03, 1059.46it/s]warmup run: 1782it [00:03, 992.90it/s]warmup run: 1798it [00:03, 1006.44it/s]warmup run: 1934it [00:03, 1022.23it/s]warmup run: 1868it [00:03, 1001.96it/s]warmup run: 2053it [00:03, 1052.18it/s]warmup run: 1910it [00:03, 991.92it/s]warmup run: 1989it [00:03, 1008.36it/s]warmup run: 2231it [00:03, 1107.95it/s]warmup run: 1886it [00:03, 1004.18it/s]warmup run: 1901it [00:03, 1010.47it/s]warmup run: 2042it [00:03, 1037.92it/s]warmup run: 1970it [00:03, 1004.72it/s]warmup run: 2173it [00:03, 1094.25it/s]warmup run: 2010it [00:03, 990.10it/s]warmup run: 2109it [00:03, 1064.58it/s]warmup run: 2355it [00:03, 1145.56it/s]warmup run: 1990it [00:03, 1013.65it/s]warmup run: 2003it [00:03, 1011.60it/s]warmup run: 2163it [00:03, 1087.23it/s]warmup run: 2083it [00:03, 1041.93it/s]warmup run: 2295it [00:03, 1129.20it/s]warmup run: 2131it [00:03, 1053.19it/s]warmup run: 2232it [00:03, 1113.15it/s]warmup run: 2479it [00:04, 1172.81it/s]warmup run: 2111it [00:03, 1070.98it/s]warmup run: 2127it [00:03, 1078.14it/s]warmup run: 2284it [00:03, 1121.59it/s]warmup run: 2203it [00:03, 1087.34it/s]warmup run: 2417it [00:03, 1154.86it/s]warmup run: 2252it [00:03, 1098.22it/s]warmup run: 2356it [00:03, 1148.42it/s]warmup run: 2603it [00:04, 1192.40it/s]warmup run: 2235it [00:03, 1119.01it/s]warmup run: 2251it [00:03, 1124.83it/s]warmup run: 2404it [00:03, 1144.94it/s]warmup run: 2323it [00:03, 1118.84it/s]warmup run: 2539it [00:03, 1173.29it/s]warmup run: 2373it [00:03, 1130.44it/s]warmup run: 2480it [00:03, 1173.06it/s]warmup run: 2727it [00:04, 1206.39it/s]warmup run: 2358it [00:03, 1151.87it/s]warmup run: 2375it [00:03, 1157.99it/s]warmup run: 2524it [00:03, 1160.60it/s]warmup run: 2443it [00:03, 1141.13it/s]warmup run: 2661it [00:04, 1186.17it/s]warmup run: 2494it [00:03, 1153.04it/s]warmup run: 2604it [00:04, 1191.07it/s]warmup run: 2850it [00:04, 1212.02it/s]warmup run: 2482it [00:03, 1175.80it/s]warmup run: 2498it [00:03, 1178.44it/s]warmup run: 2644it [00:04, 1172.31it/s]warmup run: 2563it [00:04, 1157.74it/s]warmup run: 2782it [00:04, 1191.73it/s]warmup run: 2615it [00:04, 1169.62it/s]warmup run: 2728it [00:04, 1204.30it/s]warmup run: 2975it [00:04, 1221.20it/s]warmup run: 2605it [00:03, 1190.72it/s]warmup run: 2618it [00:04, 1181.97it/s]warmup run: 2763it [00:04, 1175.04it/s]warmup run: 3000it [00:04, 676.49it/s] warmup run: 2682it [00:04, 1166.82it/s]warmup run: 2905it [00:04, 1202.12it/s]warmup run: 2736it [00:04, 1180.53it/s]warmup run: 2850it [00:04, 1208.96it/s]warmup run: 2728it [00:04, 1199.81it/s]warmup run: 2740it [00:04, 1193.23it/s]warmup run: 2881it [00:04, 1174.94it/s]warmup run: 2803it [00:04, 1178.11it/s]warmup run: 3000it [00:04, 690.56it/s] warmup run: 2856it [00:04, 1184.49it/s]warmup run: 2974it [00:04, 1216.13it/s]warmup run: 2850it [00:04, 1203.72it/s]warmup run: 2862it [00:04, 1200.25it/s]warmup run: 3000it [00:04, 692.19it/s] warmup run: 3000it [00:04, 688.57it/s] warmup run: 2924it [00:04, 1187.62it/s]warmup run: 2977it [00:04, 1190.24it/s]warmup run: 3000it [00:04, 689.47it/s] warmup run: 3000it [00:04, 680.86it/s] warmup run: 2974it [00:04, 1212.58it/s]warmup run: 2986it [00:04, 1209.79it/s]warmup run: 3000it [00:04, 690.75it/s] warmup run: 3000it [00:04, 693.99it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1657.59it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1639.27it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.28it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.14it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1637.11it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1644.36it/s]warmup should be done:   5%|         | 154/3000 [00:00<00:01, 1531.59it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.10it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.39it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1641.84it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1660.36it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.70it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.50it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1611.31it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1685.17it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1654.19it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1632.63it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1662.52it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1642.62it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1644.97it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1641.28it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1684.11it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1657.45it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1643.77it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1668.38it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1640.12it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1642.11it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1643.61it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1683.21it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1656.99it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1639.63it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1655.59it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1668.11it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1642.89it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1641.97it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1638.10it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1642.14it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1654.88it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1679.96it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1661.66it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1667.22it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1643.72it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1635.88it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1639.94it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1652.13it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1677.21it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1637.38it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1659.69it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1665.78it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1641.30it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1637.41it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1631.65it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1673.44it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1647.82it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1628.00it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1652.42it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:00, 1666.73it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1641.85it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1637.06it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1631.85it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1648.42it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1672.91it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1627.67it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:01, 1648.85it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1666.13it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1637.87it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1640.26it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1648.07it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1632.13it/s]warmup should be done:  51%|     | 1517/3000 [00:00<00:00, 1672.01it/s]warmup should be done:  49%|     | 1480/3000 [00:00<00:00, 1629.66it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1646.03it/s]warmup should be done:  56%|    | 1670/3000 [00:01<00:00, 1667.38it/s]warmup should be done:  55%|    | 1647/3000 [00:01<00:00, 1639.27it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1631.85it/s]warmup should be done:  56%|    | 1685/3000 [00:01<00:00, 1673.01it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1634.94it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1629.69it/s]warmup should be done:  55%|    | 1659/3000 [00:01<00:00, 1629.91it/s]warmup should be done:  56%|    | 1667/3000 [00:01<00:00, 1646.21it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1667.94it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1639.94it/s]warmup should be done:  62%|   | 1853/3000 [00:01<00:00, 1672.62it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1630.31it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1630.55it/s]warmup should be done:  60%|    | 1808/3000 [00:01<00:00, 1630.16it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1645.69it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1619.68it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1668.91it/s]warmup should be done:  66%|   | 1978/3000 [00:01<00:00, 1643.67it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1671.90it/s]warmup should be done:  66%|   | 1971/3000 [00:01<00:00, 1628.87it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1629.77it/s]warmup should be done:  66%|   | 1971/3000 [00:01<00:00, 1628.21it/s]warmup should be done:  67%|   | 1997/3000 [00:01<00:00, 1646.43it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1619.78it/s]warmup should be done:  72%|  | 2173/3000 [00:01<00:00, 1669.84it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1651.04it/s]warmup should be done:  71%|   | 2134/3000 [00:01<00:00, 1627.43it/s]warmup should be done:  73%|  | 2189/3000 [00:01<00:00, 1667.76it/s]warmup should be done:  71%|   | 2134/3000 [00:01<00:00, 1626.96it/s]warmup should be done:  71%|   | 2136/3000 [00:01<00:00, 1630.07it/s]warmup should be done:  72%|  | 2162/3000 [00:01<00:00, 1647.48it/s]warmup should be done:  72%|  | 2149/3000 [00:01<00:00, 1620.02it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1671.00it/s]warmup should be done:  77%|  | 2297/3000 [00:01<00:00, 1627.40it/s]warmup should be done:  77%|  | 2299/3000 [00:01<00:00, 1631.56it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1630.10it/s]warmup should be done:  78%|  | 2327/3000 [00:01<00:00, 1648.15it/s]warmup should be done:  79%|  | 2356/3000 [00:01<00:00, 1650.03it/s]warmup should be done:  77%|  | 2312/3000 [00:01<00:00, 1614.99it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1539.49it/s]warmup should be done:  84%| | 2509/3000 [00:01<00:00, 1667.74it/s]warmup should be done:  82%| | 2460/3000 [00:01<00:00, 1625.61it/s]warmup should be done:  82%| | 2465/3000 [00:01<00:00, 1638.59it/s]warmup should be done:  82%| | 2464/3000 [00:01<00:00, 1626.86it/s]warmup should be done:  83%| | 2492/3000 [00:01<00:00, 1646.19it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1609.84it/s]warmup should be done:  84%| | 2522/3000 [00:01<00:00, 1618.68it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1547.73it/s]warmup should be done:  89%| | 2676/3000 [00:01<00:00, 1667.69it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1627.71it/s]warmup should be done:  88%| | 2631/3000 [00:01<00:00, 1644.21it/s]warmup should be done:  89%| | 2657/3000 [00:01<00:00, 1646.50it/s]warmup should be done:  88%| | 2627/3000 [00:01<00:00, 1623.51it/s]warmup should be done:  88%| | 2635/3000 [00:01<00:00, 1603.24it/s]warmup should be done:  90%| | 2685/3000 [00:01<00:00, 1597.60it/s]warmup should be done:  88%| | 2626/3000 [00:01<00:00, 1556.14it/s]warmup should be done:  95%|| 2844/3000 [00:01<00:00, 1668.79it/s]warmup should be done:  93%|| 2787/3000 [00:01<00:00, 1627.88it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1648.98it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1648.82it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1621.49it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1609.09it/s]warmup should be done:  95%|| 2845/3000 [00:01<00:00, 1587.58it/s]warmup should be done:  93%|| 2784/3000 [00:01<00:00, 1562.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.57it/s]warmup should be done:  98%|| 2953/3000 [00:01<00:00, 1634.86it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1658.65it/s]warmup should be done: 100%|| 2989/3000 [00:01<00:00, 1650.88it/s]warmup should be done:  98%|| 2954/3000 [00:01<00:00, 1626.48it/s]warmup should be done:  99%|| 2962/3000 [00:01<00:00, 1617.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1649.88it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1647.30it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.28it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1574.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1631.99it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1630.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1607.79it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1659.67it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.11it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1636.54it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1696.29it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.31it/s]warmup should be done:   6%|         | 172/3000 [00:00<00:01, 1715.50it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.48it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1704.77it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1702.51it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1696.24it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1705.89it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1667.40it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1684.02it/s]warmup should be done:  11%|        | 343/3000 [00:00<00:01, 1709.39it/s]warmup should be done:  12%|        | 345/3000 [00:00<00:01, 1718.63it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1631.57it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1710.82it/s]warmup should be done:  17%|        | 517/3000 [00:00<00:01, 1718.83it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1704.55it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1698.41it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1683.07it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1633.39it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1663.57it/s]warmup should be done:  17%|        | 516/3000 [00:00<00:01, 1714.44it/s]warmup should be done:  23%|       | 682/3000 [00:00<00:01, 1702.44it/s]warmup should be done:  23%|       | 686/3000 [00:00<00:01, 1716.09it/s]warmup should be done:  23%|       | 690/3000 [00:00<00:01, 1721.44it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1705.25it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1685.69it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1669.96it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1636.75it/s]warmup should be done:  23%|       | 689/3000 [00:00<00:01, 1717.59it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1706.86it/s]warmup should be done:  29%|       | 859/3000 [00:00<00:01, 1719.89it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1668.04it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1705.10it/s]warmup should be done:  29%|       | 864/3000 [00:00<00:01, 1724.63it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1687.70it/s]warmup should be done:  29%|       | 861/3000 [00:00<00:01, 1717.03it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1637.97it/s]warmup should be done:  34%|      | 1032/3000 [00:00<00:01, 1721.90it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1704.19it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1666.29it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1704.63it/s]warmup should be done:  35%|      | 1037/3000 [00:00<00:01, 1723.96it/s]warmup should be done:  34%|      | 1033/3000 [00:00<00:01, 1717.02it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1634.85it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1669.96it/s]warmup should be done:  40%|      | 1206/3000 [00:00<00:01, 1718.51it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1702.49it/s]warmup should be done:  40%|      | 1205/3000 [00:00<00:01, 1717.87it/s]warmup should be done:  40%|      | 1210/3000 [00:00<00:01, 1722.05it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1636.06it/s]warmup should be done:  39%|      | 1170/3000 [00:00<00:01, 1661.17it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1692.44it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1668.10it/s]warmup should be done:  46%|     | 1379/3000 [00:00<00:00, 1721.57it/s]warmup should be done:  46%|     | 1378/3000 [00:00<00:00, 1721.08it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1706.34it/s]warmup should be done:  46%|     | 1384/3000 [00:00<00:00, 1725.52it/s]warmup should be done:  45%|     | 1338/3000 [00:00<00:00, 1665.26it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1633.11it/s]warmup should be done:  46%|     | 1367/3000 [00:00<00:00, 1690.43it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1672.45it/s]warmup should be done:  52%|    | 1552/3000 [00:00<00:00, 1721.56it/s]warmup should be done:  52%|    | 1557/3000 [00:00<00:00, 1725.67it/s]warmup should be done:  52%|    | 1551/3000 [00:00<00:00, 1720.68it/s]warmup should be done:  51%|    | 1539/3000 [00:00<00:00, 1704.43it/s]warmup should be done:  50%|     | 1506/3000 [00:00<00:00, 1667.84it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1636.07it/s]warmup should be done:  51%|     | 1537/3000 [00:00<00:00, 1686.46it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1683.94it/s]warmup should be done:  58%|    | 1730/3000 [00:01<00:00, 1725.56it/s]warmup should be done:  57%|    | 1725/3000 [00:01<00:00, 1721.07it/s]warmup should be done:  57%|    | 1710/3000 [00:01<00:00, 1704.21it/s]warmup should be done:  57%|    | 1724/3000 [00:01<00:00, 1720.52it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1638.62it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1684.83it/s]warmup should be done:  56%|    | 1695/3000 [00:01<00:00, 1692.24it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1640.23it/s]warmup should be done:  63%|   | 1903/3000 [00:01<00:00, 1725.98it/s]warmup should be done:  63%|   | 1881/3000 [00:01<00:00, 1705.46it/s]warmup should be done:  63%|   | 1897/3000 [00:01<00:00, 1722.41it/s]warmup should be done:  63%|   | 1898/3000 [00:01<00:00, 1720.92it/s]warmup should be done:  60%|    | 1808/3000 [00:01<00:00, 1638.63it/s]warmup should be done:  62%|   | 1875/3000 [00:01<00:00, 1685.94it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1695.15it/s]warmup should be done:  61%|   | 1838/3000 [00:01<00:00, 1638.34it/s]warmup should be done:  68%|   | 2052/3000 [00:01<00:00, 1706.15it/s]warmup should be done:  69%|   | 2076/3000 [00:01<00:00, 1723.66it/s]warmup should be done:  69%|   | 2071/3000 [00:01<00:00, 1721.04it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1636.19it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1715.49it/s]warmup should be done:  68%|   | 2044/3000 [00:01<00:00, 1683.20it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1688.83it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1645.44it/s]warmup should be done:  74%|  | 2223/3000 [00:01<00:00, 1704.44it/s]warmup should be done:  71%|   | 2136/3000 [00:01<00:00, 1635.58it/s]warmup should be done:  75%|  | 2244/3000 [00:01<00:00, 1719.25it/s]warmup should be done:  75%|  | 2249/3000 [00:01<00:00, 1711.77it/s]warmup should be done:  75%|  | 2242/3000 [00:01<00:00, 1707.37it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1681.95it/s]warmup should be done:  74%|  | 2205/3000 [00:01<00:00, 1688.13it/s]warmup should be done:  72%|  | 2170/3000 [00:01<00:00, 1632.09it/s]warmup should be done:  80%|  | 2394/3000 [00:01<00:00, 1704.14it/s]warmup should be done:  81%|  | 2416/3000 [00:01<00:00, 1718.12it/s]warmup should be done:  77%|  | 2301/3000 [00:01<00:00, 1638.22it/s]warmup should be done:  81%|  | 2421/3000 [00:01<00:00, 1705.21it/s]warmup should be done:  79%|  | 2382/3000 [00:01<00:00, 1680.60it/s]warmup should be done:  79%|  | 2374/3000 [00:01<00:00, 1687.70it/s]warmup should be done:  80%|  | 2413/3000 [00:01<00:00, 1697.15it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1633.52it/s]warmup should be done:  86%| | 2566/3000 [00:01<00:00, 1707.76it/s]warmup should be done:  82%| | 2465/3000 [00:01<00:00, 1638.12it/s]warmup should be done:  86%| | 2589/3000 [00:01<00:00, 1719.64it/s]warmup should be done:  86%| | 2592/3000 [00:01<00:00, 1704.85it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1681.82it/s]warmup should be done:  85%| | 2546/3000 [00:01<00:00, 1695.28it/s]warmup should be done:  86%| | 2583/3000 [00:01<00:00, 1691.66it/s]warmup should be done:  83%| | 2499/3000 [00:01<00:00, 1636.32it/s]warmup should be done:  91%|| 2738/3000 [00:01<00:00, 1708.98it/s]warmup should be done:  88%| | 2629/3000 [00:01<00:00, 1637.67it/s]warmup should be done:  92%|| 2762/3000 [00:01<00:00, 1720.86it/s]warmup should be done:  92%|| 2763/3000 [00:01<00:00, 1705.38it/s]warmup should be done:  91%| | 2716/3000 [00:01<00:00, 1694.10it/s]warmup should be done:  91%| | 2720/3000 [00:01<00:00, 1673.09it/s]warmup should be done:  92%|| 2753/3000 [00:01<00:00, 1688.57it/s]warmup should be done:  89%| | 2663/3000 [00:01<00:00, 1636.08it/s]warmup should be done:  97%|| 2909/3000 [00:01<00:00, 1707.65it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1638.68it/s]warmup should be done:  98%|| 2935/3000 [00:01<00:00, 1720.75it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1705.15it/s]warmup should be done:  96%|| 2888/3000 [00:01<00:00, 1700.95it/s]warmup should be done:  96%|| 2888/3000 [00:01<00:00, 1674.57it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1682.15it/s]warmup should be done:  94%|| 2827/3000 [00:01<00:00, 1634.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1718.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1715.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1704.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1704.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1689.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.85it/s]warmup should be done:  99%|| 2959/3000 [00:01<00:00, 1639.76it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1646.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1649.23it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.73it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49f2ea0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff4a060fd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff4a0614e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff4a060d730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49f2e91f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff4a060cb80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49f2f62b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff49f2e7190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 06:48:57.716164: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefd3029690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:57.716226: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:57.724408: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:57.753044: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefd302e930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:57.753115: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:57.761337: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:57.916097: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefd302ee80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:57.916155: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:57.926387: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:57.954997: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc2f931a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:57.955059: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:57.963772: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:58.338845: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefd2835040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:58.338909: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:58.348512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:58.442263: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefcf02f020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:58.442334: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:58.442940: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefcb02a520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:58.443005: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:58.452684: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:58.453136: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:48:58.475210: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fefc6838290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:48:58.475284: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:48:58.482783: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:49:04.821742: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:04.855161: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:04.961481: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:05.026702: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:05.285269: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:05.314231: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:05.418175: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:49:05.426746: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:49:54.872][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:49:54.872][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:54.878][ERROR][RK0][main]: coll ps creation done
[HCTR][06:49:54.878][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][06:49:54.885][ERROR][RK0][tid #140668676589312]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:49:54.886][ERROR][RK0][tid #140668676589312]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:54.893][ERROR][RK0][tid #140668676589312]: coll ps creation done
[HCTR][06:49:54.893][ERROR][RK0][tid #140668676589312]: replica 5 waits for coll ps creation barrier
[HCTR][06:49:55.191][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:49:55.191][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:55.196][ERROR][RK0][main]: coll ps creation done
[HCTR][06:49:55.196][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][06:49:55.198][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:49:55.198][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:55.202][ERROR][RK0][main]: coll ps creation done
[HCTR][06:49:55.202][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][06:49:55.249][ERROR][RK0][tid #140668282328832]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:49:55.249][ERROR][RK0][tid #140668282328832]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:55.258][ERROR][RK0][tid #140668282328832]: coll ps creation done
[HCTR][06:49:55.258][ERROR][RK0][tid #140668282328832]: replica 4 waits for coll ps creation barrier
[HCTR][06:49:55.260][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:49:55.260][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:55.267][ERROR][RK0][main]: coll ps creation done
[HCTR][06:49:55.267][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][06:49:55.410][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:49:55.410][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:55.415][ERROR][RK0][main]: coll ps creation done
[HCTR][06:49:55.415][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][06:49:55.475][ERROR][RK0][tid #140668416546560]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:49:55.475][ERROR][RK0][tid #140668416546560]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][06:49:55.479][ERROR][RK0][tid #140668416546560]: coll ps creation done
[HCTR][06:49:55.479][ERROR][RK0][tid #140668416546560]: replica 2 waits for coll ps creation barrier
[HCTR][06:49:55.479][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][06:49:56.346][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][06:49:56.391][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][tid #140668282328832]: replica 4 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][tid #140668416546560]: replica 2 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][tid #140668416546560]: Calling build_v2
[HCTR][06:49:56.391][ERROR][RK0][tid #140668676589312]: replica 5 calling init per replica
[HCTR][06:49:56.391][ERROR][RK0][main]: Calling build_v2
[HCTR][06:49:56.391][ERROR][RK0][main]: Calling build_v2
[HCTR][06:49:56.391][ERROR][RK0][main]: Calling build_v2
[HCTR][06:49:56.391][ERROR][RK0][main]: Calling build_v2
[HCTR][06:49:56.391][ERROR][RK0][tid #140668282328832]: Calling build_v2
[HCTR][06:49:56.392][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][tid #140668416546560]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][main]: Calling build_v2
[HCTR][06:49:56.392][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][tid #140668676589312]: Calling build_v2
[HCTR][06:49:56.392][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][tid #140668282328832]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:49:56.392][ERROR][RK0][tid #140668676589312]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 06:49:562022-12-12 06:49:56[2022-12-12 06:49:56.2022-12-12 06:49:56.2022-12-12 06:49:562022-12-12 06:49:56.392085.2022-12-12 06:49:56392076[..392084: 392091.: 392085392095: E: 392095E: 2022-12-12 06:49:56: E E:  E.E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 392131 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:E:136] 136:] 136 136] using concurrent impl MPS] 136using concurrent impl MPS] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] using concurrent impl MPS
using concurrent impl MPS] 
using concurrent impl MPS:using concurrent impl MPS

using concurrent impl MPS
136

] using concurrent impl MPS
[2022-12-12 06:49:56.396529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:49:56.396566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 06:49:56] .assigning 8 to cpu396575
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:49:56.396618: E[ 2022-12-12 06:49:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:396622196[: ] 2022-12-12 06:49:56Eassigning 8 to cpu. 
396643/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:
212[] 2022-12-12 06:49:56[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-12 06:49:56
396673.: 396687E: [ E2022-12-12 06:49:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:49:56396708178:[.: ] 1962022-12-12 06:49:56396718Ev100x8, slow pcie] .:  
assigning 8 to cpu396724E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[
:  :2022-12-12 06:49:562022-12-12 06:49:56E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212.. :] 396775[396785[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 2022-12-12 06:49:56: 2022-12-12 06:49:56:] 
E[.E.178remote time is 8.68421 2022-12-12 06:49:56396827[ 396841] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 2022-12-12 06:49:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: v100x8, slow pcie:[396870E.:E
1782022-12-12 06:49:56:  396906196 ] .[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie3969502022-12-12 06:49:56 :Eassigning 8 to cpu:
: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178 
212E396996[:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  : 2022-12-12 06:49:56178v100x8, slow pcie[:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.] 
2022-12-12 06:49:56213
: 397056v100x8, slow pcie.] [214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [
397086remote time is 8.684212022-12-12 06:49:56] :E2022-12-12 06:49:56: 
[.cpu time is 97.0588196 .E2022-12-12 06:49:56397133[
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc397144 .: 2022-12-12 06:49:56assigning 8 to cpu:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc397182E.
196E::  397207]  212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  :E
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[196 213
:2022-12-12 06:49:56] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 196.assigning 8 to cpu[:[remote time is 8.68421] 397331
2022-12-12 06:49:562142022-12-12 06:49:56
assigning 8 to cpu: .] .
E[397381cpu time is 97.0588397373[ 2022-12-12 06:49:56: 
: 2022-12-12 06:49:56/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.EE.:397419  397427[212: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 06:49:56] E::E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 212213 397476
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421:E[214

212 2022-12-12 06:49:56] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.[cpu time is 97.0588build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-12 06:49:563975602022-12-12 06:49:56

212.: .] 397591E[397592build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  2022-12-12 06:49:56: 
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E :[397654 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-12 06:49:56: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] .E:214remote time is 8.68421397705 213] 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [cpu time is 97.0588E:remote time is 8.684212022-12-12 06:49:56
 213
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 397777:remote time is 8.68421[: 213
2022-12-12 06:49:56E] . [remote time is 8.68421397822/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:49:56
: :.E214[397857 ] 2022-12-12 06:49:56: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588.E:
397891 214: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E:cpu time is 97.0588 214
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 06:51:13.380301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:51:13.420563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 06:51:13.524894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:51:13.524956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:51:13.524990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:51:13.525020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:51:13.525523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:51:13.525573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.526475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.527152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.540280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 06:51:13.540343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 06:51:13.540769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:51:13.540821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.541017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved[
2022-12-12 06:51:13.541056[: 2022-12-12 06:51:13E. 541085/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc5 solved:
205[] [2022-12-12 06:51:13worker 0 thread 3 initing device 32022-12-12 06:51:13.
[.5410992022-12-12 06:51:13541131: .: E541144E :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :202/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205] :] 6 solved202worker 0 thread 5 initing device 5
] 
4 solved[
2022-12-12 06:51:13.541296: [E2022-12-12 06:51:13 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc541309:: 205E]  worker 0 thread 6 initing device 6/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 4 initing device 4
[2022-12-12 06:51:13.541579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:51:13.541629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.541684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 06:51:13.541735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[[19802022-12-12 06:51:132022-12-12 06:51:13] ..eager alloc mem 381.47 MB541759541761
: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-12 06:51:132022-12-12 06:51:13..541864541864: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 06:51:13.[542047: 2022-12-12 06:51:13E. 542060/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1 solved:
202] 7 solved[
2022-12-12 06:51:13.542124: [E2022-12-12 06:51:13 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc542136:: 205E]  worker 0 thread 1 initing device 1/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 7 initing device 7
[2022-12-12 06:51:13.542552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 06:51:13.542574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 06:51:13.542601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 06:51:13.542625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.545375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.545837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.546492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.546554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.546609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.547158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.547272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.549935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.550328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.550938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.551034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.551083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.551153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.551692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:51:13.604539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:51:13.617825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:51:13.617927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:51:13.618862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.619580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.620664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13.620710: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:51:13.633356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:51:13.636885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:51:13.637046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 06:51:13.638412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:51:13.638498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:51:13.639299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.639937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.641008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13.641053: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[[2022-12-12 06:51:132022-12-12 06:51:13..641834641830: : EE  [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:51:132022-12-12 06:51:13::..19801980641893641893] ] : : eager alloc mem 5.00 Byteseager alloc mem 5.00 BytesEE

  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 06:51:13.642183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:51:13.642255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:51:13.642328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:51:13.642401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:51:13.643069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.643811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.644376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.644466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.645441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13.645487: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:51:13.645529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13.645576: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:51:13.647691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 06:51:13[.2022-12-12 06:51:13647783.: 647773E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-12 06:51:13.647859: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:51:13:.638647891] : eager release cuda mem 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:51:13[.2022-12-12 06:51:13647950.: 647939E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 5
[2022-12-12 06:51:13.648043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:51:13.648876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.649511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.650265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.650919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:51:13.651728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.651881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.652001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.652053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:13.652812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13.652859: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:51:13.652959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13.653006: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:51:13.653073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:13[.2022-12-12 06:51:13653122.: 653123W:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43:] 638WORKER[0] alloc host memory 95.37 MB] 
eager release cuda mem 625663
[2022-12-12 06:51:13.653193: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 95.37 MB
[2022-12-12 06:51:13.685239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.685888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.685932: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.702480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.703095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.703149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.708485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.709113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.709156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.709521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.710168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.710210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.715451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.716073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.716114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.717193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.717704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.717806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.717849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.717973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:51:13.718304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.718346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[2022-12-12 06:51:13.718575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:51:13.718618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.93 GB
[[[[[[[[2022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:18........ 77409 77410 77409 77409 77409 77409 77409 77410: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 5 init p2p of link 6Device 3 init p2p of link 2Device 1 init p2p of link 7Device 4 init p2p of link 5Device 0 init p2p of link 3Device 7 init p2p of link 4Device 6 init p2p of link 0







[[2022-12-12 06:51:18[[[[2022-12-12 06:51:18[[.2022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:182022-12-12 06:51:18.2022-12-12 06:51:182022-12-12 06:51:18 77957... 77957. 77963..:  77957 77957:  77957:  77958 77972E: : E: E: :  EE E EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::1980:1980::] 19801980] 1980] 19801980eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB




[[2022-12-12 06:51:182022-12-12 06:51:18.. 79050 79052: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 06:51:18. 79183: E[ 2022-12-12 06:51:18[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.2022-12-12 06:51:18[[: 79197.[2022-12-12 06:51:182022-12-12 06:51:18638:  792062022-12-12 06:51:18..] E: . 79216 79210eager release cuda mem 625663 E 79218: : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : EE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE  638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::eager release cuda mem 625663] :638638
eager release cuda mem 625663638] ] 
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 06:51:18. 92762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 06:51:18. 92915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18. 92992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 06:51:18. 93172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18. 93860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18. 94156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.102147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 06:51:18.102299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.102481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 06:51:18.102564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 06:51:18.102629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.102737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.102791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 06:51:18.102959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 06:51:18.102968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 06:51:18.103057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 06:51:18.103150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.103223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.103251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.103597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.103674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.103740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.103927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.104188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.106929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 06:51:18.107057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.107162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 06:51:18.107280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.107987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.108210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.116761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 06:51:18.116878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.117269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 06:51:18.117399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.117842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.118372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.124738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 06:51:18.124861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.124966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 06:51:18.125093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.125328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 06:51:18.125454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.125652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.126020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.126231: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.126541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 06:51:18.126662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.127589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.133332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 06:51:18.133448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.133993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 06:51:18.134116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.134400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.135067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.136411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 06:51:18.136537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.136759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 06:51:18.136872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.137487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.137815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.139982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 06:51:18.140099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.140298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 06:51:18.140412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.141051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.141358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.151692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 06:51:18.151814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.152493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 06:51:18.152589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 06:51:18eager release cuda mem 625663.
152607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:51:18.153392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:51:18.156115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:51:18.156896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61517 secs 
[2022-12-12 06:51:18.157239[: 2022-12-12 06:51:18E. 157249/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 100400000:
638] eager release cuda mem 100400000
[[2022-12-12 06:51:182022-12-12 06:51:18..157786157782: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1955638] ] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61698 secs eager release cuda mem 100400000

[2022-12-12 06:51:18.157968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61635 secs 
[2022-12-12 06:51:18.158938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.61709 secs 
[2022-12-12 06:51:18.162652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:51:18.163160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:51:18.164659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62207 secs 
[2022-12-12 06:51:18.164837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62299 secs 
[2022-12-12 06:51:18.164926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:51:18.165311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:51:18.167190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.62457 secs 
[2022-12-12 06:51:18.167413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 75000000 / 100000000 nodes ( 75.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 11.93 GB | 4.64185 secs 
[2022-12-12 06:51:18.168232: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.24 GB
[2022-12-12 06:51:19.583416: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.51 GB
[2022-12-12 06:51:19.610381: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.51 GB
[2022-12-12 06:51:19.611900: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.51 GB
[2022-12-12 06:51:20.876836: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.77 GB
[2022-12-12 06:51:20.882044: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.77 GB
[2022-12-12 06:51:20.882737: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.77 GB
[2022-12-12 06:51:22.100780: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.99 GB
[2022-12-12 06:51:22.100952: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.99 GB
[2022-12-12 06:51:22.101321: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 20.99 GB
[2022-12-12 06:51:23.446325: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.20 GB
[2022-12-12 06:51:23.446545: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.20 GB
[2022-12-12 06:51:23.447694: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.20 GB
[2022-12-12 06:51:24.725632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.66 GB
[2022-12-12 06:51:24.726103: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.66 GB
[2022-12-12 06:51:24.726585: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 21.66 GB
[2022-12-12 06:51:25.882664: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 21.86 GB
[2022-12-12 06:51:25.883490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 21.86 GB
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][tid #140668282328832]: replica 4 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][tid #140668416546560]: replica 2 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][tid #140668676589312]: replica 5 calling init per replica done, doing barrier
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][tid #140668282328832]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][tid #140668676589312]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][tid #140668416546560]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:51:26.795][ERROR][RK0][main]: init per replica done
[HCTR][06:51:26.795][ERROR][RK0][tid #140668282328832]: init per replica done
[HCTR][06:51:26.795][ERROR][RK0][tid #140668676589312]: init per replica done
[HCTR][06:51:26.795][ERROR][RK0][main]: init per replica done
[HCTR][06:51:26.795][ERROR][RK0][main]: init per replica done
[HCTR][06:51:26.795][ERROR][RK0][main]: init per replica done
[HCTR][06:51:26.795][ERROR][RK0][tid #140668416546560]: init per replica done
[HCTR][06:51:26.797][ERROR][RK0][main]: init per replica done
[HCTR][06:51:26.833][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fd148238400
[HCTR][06:51:26.833][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fd148558400
[HCTR][06:51:26.833][ERROR][RK0][main]: 7 allocated 3276800 at 0x7fd148b98400
[HCTR][06:51:26.833][ERROR][RK0][main]: 7 allocated 6553600 at 0x7fd148eb8400
[HCTR][06:51:26.833][ERROR][RK0][tid #140668416546560]: 2 allocated 3276800 at 0x7fd0d0238400
[HCTR][06:51:26.833][ERROR][RK0][tid #140668416546560]: 2 allocated 6553600 at 0x7fd0d0558400
[HCTR][06:51:26.833][ERROR][RK0][tid #140668416546560]: 2 allocated 3276800 at 0x7fd0d0b98400
[HCTR][06:51:26.833][ERROR][RK0][tid #140668416546560]: 2 allocated 6553600 at 0x7fd0d0eb8400
[HCTR][06:51:26.834][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fd19a238400
[HCTR][06:51:26.834][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fd19a558400
[HCTR][06:51:26.834][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fd19ab98400
[HCTR][06:51:26.834][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fd19aeb8400
[HCTR][06:51:26.834][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fd0bc238400
[HCTR][06:51:26.834][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fd184238400
[HCTR][06:51:26.834][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fd0bc558400
[HCTR][06:51:26.834][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fd184558400
[HCTR][06:51:26.834][ERROR][RK0][main]: 3 allocated 3276800 at 0x7fd0bcb98400
[HCTR][06:51:26.834][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fd184b98400
[HCTR][06:51:26.834][ERROR][RK0][main]: 3 allocated 6553600 at 0x7fd0bceb8400
[HCTR][06:51:26.834][ERROR][RK0][tid #140668282328832]: 4 allocated 3276800 at 0x7fd110238400
[HCTR][06:51:26.834][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fd184eb8400
[HCTR][06:51:26.834][ERROR][RK0][tid #140668282328832]: 4 allocated 6553600 at 0x7fd110558400
[HCTR][06:51:26.834][ERROR][RK0][tid #140668282328832]: 4 allocated 3276800 at 0x7fd110b98400
[HCTR][06:51:26.834][ERROR][RK0][tid #140668282328832]: 4 allocated 6553600 at 0x7fd110eb8400
[HCTR][06:51:26.834][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fd18c238400
[HCTR][06:51:26.834][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fd18c558400
[HCTR][06:51:26.834][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fd18cb98400
[HCTR][06:51:26.834][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fd18ceb8400
[HCTR][06:51:26.836][ERROR][RK0][tid #140668693374720]: 0 allocated 3276800 at 0x7fd16c320000
[HCTR][06:51:26.836][ERROR][RK0][tid #140668693374720]: 0 allocated 6553600 at 0x7fd16c640000
[HCTR][06:51:26.836][ERROR][RK0][tid #140668693374720]: 0 allocated 3276800 at 0x7fd16cc80000
[HCTR][06:51:26.836][ERROR][RK0][tid #140668693374720]: 0 allocated 6553600 at 0x7fd16cfa0000
