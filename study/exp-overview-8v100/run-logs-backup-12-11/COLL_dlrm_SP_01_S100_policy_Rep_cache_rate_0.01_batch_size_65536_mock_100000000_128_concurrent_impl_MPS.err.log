2022-12-11 20:27:32.548679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.553617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.562833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.567835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.574057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.584578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.594208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.604373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.654852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.661757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.666151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.667091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.667983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.668874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.669838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.670762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.671883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.673126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.674764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.675751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.675939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.677454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.677469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.678935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.679154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.680394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.680787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.682064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.682372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.684002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.684444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.685345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.686005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.687000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.687511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.688960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.689908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.690862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.691796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.692722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.696345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.697908: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:32.698068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.699050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.699398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.700667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.701037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.702253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.702746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.703924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.704359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.705564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.705878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.707114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.707460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.708507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.708705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.710277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.710480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.711671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.720642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.722751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.724487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.724739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.726297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.726541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.728234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.728560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.732840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.735545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.736241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.736963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.737043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.738857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.739459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.740429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.740558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.741295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.755208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.755673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.774527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.775315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.776010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.776366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.777569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.778681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.778734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.780007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.780159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.781259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.782718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.782921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.782969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.785152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.785341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.786363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.788160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.788277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.788382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.789708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.791227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.791952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.791985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.792435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.793016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.795251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.795689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.795721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.796396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.796857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.799139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.799351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.799384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.800343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.800589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.802767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.802962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.803607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.804280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.805605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.805649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.806510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.808287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.808549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.809124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.810966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.811228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.811799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.813376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.813546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.814354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.816046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.816075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.816763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.818559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.818727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.819935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.820698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.821033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.822487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.822809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.823036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.823987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.825454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.826194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.826985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.827949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.828065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.828616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.829803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.831034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.831095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.831362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.832739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.833641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.834191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.834309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.834594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.836172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.837390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.837707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.837828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.838065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.840240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.841104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.841463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.841680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.841724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.843844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.844783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.845556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.845617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.846546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.847314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.847916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.848708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.849525: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:32.849753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.850078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.850476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.851161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.851945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.853377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.853543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.854963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.855718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.856569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.857017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.858061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.858061: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:32.858546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.859726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.859939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.860392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.860820: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:32.861038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.861794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.863214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.863972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.864478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.864986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.865775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.867051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.867349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.867608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.868608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.868620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.869187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.870810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.871015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.871537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.873027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.873060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.873676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.875443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.875702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.876395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.877738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.877928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.879953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.880231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.880375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.880923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.882436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.884861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.884976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.885741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.887234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.918612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.918613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.919175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.920963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.924628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.924634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.925205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.927175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.929568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.929575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.930143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.931741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.935562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.935577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.936085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.938368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.941934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.942021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.942526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.944084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.946014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.946150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.947785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.948871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.950990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.952416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.954320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.954597: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:32.956181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.956800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.957701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.959941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.960133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.962108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.964778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.995148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.995191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.996729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.999041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:32.999093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.000103: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:33.001170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.010547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.033049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.033053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.062402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.063937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.063937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.069834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.070218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.073923: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:33.076921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.081514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.082746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.087749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.089682: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:27:33.091717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.100097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.105101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.111308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.922367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.923817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.924594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.925068: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:33.925125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:27:33.942082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.942752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.943282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.943871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.945191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:33.945693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:27:33.991825: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:33.992030: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.039831: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:27:34.190177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.190799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.191412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.191954: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.192008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:27:34.208891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.209521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.210019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.210743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.211297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.211905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:27:34.212901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.213497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.214015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.214517: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.214529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.214567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:27:34.215276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.215804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.216260: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.216300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:27:34.231806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.232285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.233154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.233308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.234032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.234233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.235024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.235211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.235956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.236089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.236836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:27:34.236950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:27:34.281169: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.281384: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.282375: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:27:34.290577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.291191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.291735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.292201: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.292251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:27:34.303579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.303666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.303668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.305122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.305285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.305333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.306316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.306761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.306845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.307704: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.307764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:27:34.308077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.308122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:27:34.308249: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:27:34.308312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:27:34.309029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.309647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.310156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.310737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.311275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.311828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:27:34.317286: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.317510: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.318623: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:27:34.325064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.325075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.325280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.331858: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.332009: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.332846: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:27:34.332866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.332890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.332959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.334452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.334484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.334525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.336105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.336115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.336125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.337579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.337622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.337675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:27:34.338927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:27:34.338968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:27:34.339051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:27:34.357712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.357925: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.359818: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:27:34.384621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.384832: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.385073: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.385224: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.385586: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.385711: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:27:34.386535: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:27:34.386883: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:27:34.387398: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:27:35.657][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 100it [00:01, 85.17it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 200it [00:01, 184.57it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 97it [00:01, 83.04it/s]warmup run: 96it [00:01, 82.00it/s]warmup run: 300it [00:01, 294.16it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 83it [00:01, 70.82it/s]warmup run: 95it [00:01, 82.28it/s]warmup run: 98it [00:01, 84.69it/s]warmup run: 100it [00:01, 86.00it/s]warmup run: 195it [00:01, 180.98it/s]warmup run: 195it [00:01, 180.93it/s]warmup run: 399it [00:01, 405.58it/s]warmup run: 103it [00:01, 90.40it/s]warmup run: 175it [00:01, 163.21it/s]warmup run: 192it [00:01, 180.15it/s]warmup run: 195it [00:01, 182.24it/s]warmup run: 198it [00:01, 184.02it/s]warmup run: 294it [00:01, 289.79it/s]warmup run: 297it [00:01, 293.58it/s]warmup run: 497it [00:02, 512.29it/s]warmup run: 206it [00:01, 195.26it/s]warmup run: 266it [00:01, 263.46it/s]warmup run: 288it [00:01, 286.31it/s]warmup run: 294it [00:01, 291.75it/s]warmup run: 298it [00:01, 294.53it/s]warmup run: 393it [00:01, 402.64it/s]warmup run: 398it [00:01, 408.98it/s]warmup run: 596it [00:02, 612.64it/s]warmup run: 309it [00:01, 309.87it/s]warmup run: 357it [00:01, 367.27it/s]warmup run: 387it [00:01, 400.22it/s]warmup run: 393it [00:01, 404.65it/s]warmup run: 398it [00:01, 408.85it/s]warmup run: 492it [00:02, 512.39it/s]warmup run: 499it [00:02, 521.36it/s]warmup run: 695it [00:02, 699.26it/s]warmup run: 411it [00:01, 425.93it/s]warmup run: 455it [00:02, 480.94it/s]warmup run: 485it [00:02, 509.25it/s]warmup run: 491it [00:02, 512.10it/s]warmup run: 498it [00:02, 519.77it/s]warmup run: 591it [00:02, 613.16it/s]warmup run: 596it [00:02, 614.78it/s]warmup run: 794it [00:02, 770.08it/s]warmup run: 512it [00:01, 536.18it/s]warmup run: 551it [00:02, 581.65it/s]warmup run: 582it [00:02, 606.19it/s]warmup run: 591it [00:02, 614.97it/s]warmup run: 599it [00:02, 623.42it/s]warmup run: 691it [00:02, 701.64it/s]warmup run: 692it [00:02, 694.46it/s]warmup run: 894it [00:02, 829.99it/s]warmup run: 614it [00:02, 638.87it/s]warmup run: 648it [00:02, 670.90it/s]warmup run: 682it [00:02, 696.25it/s]warmup run: 687it [00:02, 694.97it/s]warmup run: 700it [00:02, 711.79it/s]warmup run: 791it [00:02, 774.24it/s]warmup run: 789it [00:02, 761.21it/s]warmup run: 994it [00:02, 876.18it/s]warmup run: 718it [00:02, 730.72it/s]warmup run: 744it [00:02, 742.29it/s]warmup run: 778it [00:02, 760.18it/s]warmup run: 786it [00:02, 768.28it/s]warmup run: 800it [00:02, 782.82it/s]warmup run: 890it [00:02, 829.47it/s]warmup run: 885it [00:02, 813.03it/s]warmup run: 1096it [00:02, 913.92it/s]warmup run: 822it [00:02, 806.61it/s]warmup run: 841it [00:02, 801.24it/s]warmup run: 877it [00:02, 818.59it/s]warmup run: 884it [00:02, 821.74it/s]warmup run: 900it [00:02, 837.74it/s]warmup run: 989it [00:02, 872.71it/s]warmup run: 981it [00:02, 852.46it/s]warmup run: 1197it [00:02, 940.18it/s]warmup run: 926it [00:02, 866.71it/s]warmup run: 937it [00:02, 842.77it/s]warmup run: 974it [00:02, 853.65it/s]warmup run: 981it [00:02, 853.42it/s]warmup run: 1000it [00:02, 880.57it/s]warmup run: 1089it [00:02, 906.55it/s]warmup run: 1078it [00:02, 884.36it/s]warmup run: 1299it [00:02, 961.91it/s]warmup run: 1029it [00:02, 910.49it/s]warmup run: 1035it [00:02, 878.98it/s]warmup run: 1076it [00:02, 899.12it/s]warmup run: 1077it [00:02, 878.22it/s]warmup run: 1100it [00:02, 913.86it/s]warmup run: 1188it [00:02, 927.91it/s]warmup run: 1177it [00:02, 913.98it/s]warmup run: 1401it [00:02, 976.79it/s]warmup run: 1133it [00:02, 945.67it/s]warmup run: 1132it [00:02, 904.76it/s]warmup run: 1177it [00:02, 930.20it/s]warmup run: 1173it [00:02, 896.47it/s]warmup run: 1201it [00:02, 939.24it/s]warmup run: 1287it [00:02, 944.82it/s]warmup run: 1277it [00:02, 936.91it/s]warmup run: 1506it [00:03, 996.82it/s]warmup run: 1237it [00:02, 972.55it/s]warmup run: 1229it [00:02, 922.10it/s]warmup run: 1280it [00:02, 956.43it/s]warmup run: 1268it [00:02, 889.31it/s]warmup run: 1301it [00:02, 955.33it/s]warmup run: 1386it [00:02, 956.91it/s]warmup run: 1376it [00:02, 951.84it/s]warmup run: 1610it [00:03, 1009.02it/s]warmup run: 1342it [00:02, 994.80it/s]warmup run: 1326it [00:02, 926.68it/s]warmup run: 1381it [00:02, 971.32it/s]warmup run: 1361it [00:02, 899.31it/s]warmup run: 1402it [00:02, 970.36it/s]warmup run: 1485it [00:03, 963.50it/s]warmup run: 1474it [00:03, 958.46it/s]warmup run: 1446it [00:02, 1002.40it/s]warmup run: 1713it [00:03, 1003.63it/s]warmup run: 1422it [00:03, 929.19it/s]warmup run: 1481it [00:03, 978.16it/s]warmup run: 1454it [00:03, 905.83it/s]warmup run: 1502it [00:03, 976.78it/s]warmup run: 1588it [00:03, 981.38it/s]warmup run: 1574it [00:03, 967.97it/s]warmup run: 1816it [00:03, 1009.80it/s]warmup run: 1550it [00:02, 979.55it/s] warmup run: 1519it [00:03, 939.68it/s]warmup run: 1581it [00:03, 983.49it/s]warmup run: 1549it [00:03, 918.21it/s]warmup run: 1603it [00:03, 985.99it/s]warmup run: 1693it [00:03, 999.75it/s]warmup run: 1673it [00:03, 972.96it/s]warmup run: 1919it [00:03, 1015.21it/s]warmup run: 1651it [00:03, 967.47it/s]warmup run: 1618it [00:03, 952.20it/s]warmup run: 1682it [00:03, 990.85it/s]warmup run: 1646it [00:03, 933.00it/s]warmup run: 1706it [00:03, 996.29it/s]warmup run: 1798it [00:03, 1013.47it/s]warmup run: 1773it [00:03, 979.35it/s]warmup run: 2025it [00:03, 1026.27it/s]warmup run: 1716it [00:03, 959.44it/s]warmup run: 1750it [00:03, 961.55it/s]warmup run: 1783it [00:03, 996.02it/s]warmup run: 1744it [00:03, 946.04it/s]warmup run: 1810it [00:03, 1008.87it/s]warmup run: 1903it [00:03, 1021.49it/s]warmup run: 1873it [00:03, 983.98it/s]warmup run: 2144it [00:03, 1074.95it/s]warmup run: 1814it [00:03, 964.24it/s]warmup run: 1849it [00:03, 968.17it/s]warmup run: 1884it [00:03, 994.32it/s]warmup run: 1842it [00:03, 955.54it/s]warmup run: 1912it [00:03, 1008.84it/s]warmup run: 2008it [00:03, 1027.99it/s]warmup run: 1973it [00:03, 986.62it/s]warmup run: 2264it [00:03, 1110.71it/s]warmup run: 1911it [00:03, 964.09it/s]warmup run: 1949it [00:03, 975.78it/s]warmup run: 1985it [00:03, 996.62it/s]warmup run: 1942it [00:03, 966.41it/s]warmup run: 2016it [00:03, 1015.66it/s]warmup run: 2131it [00:03, 1085.88it/s]warmup run: 2087it [00:03, 1030.24it/s]warmup run: 2384it [00:03, 1137.21it/s]warmup run: 2010it [00:03, 970.23it/s]warmup run: 2058it [00:03, 1009.13it/s]warmup run: 2103it [00:03, 1049.49it/s]warmup run: 2047it [00:03, 990.99it/s]warmup run: 2136it [00:03, 1069.90it/s]warmup run: 2254it [00:03, 1127.80it/s]warmup run: 2207it [00:03, 1079.60it/s]warmup run: 2504it [00:03, 1154.94it/s]warmup run: 2131it [00:03, 1041.44it/s]warmup run: 2179it [00:03, 1066.73it/s]warmup run: 2224it [00:03, 1095.94it/s]warmup run: 2164it [00:03, 1042.56it/s]warmup run: 2255it [00:03, 1104.46it/s]warmup run: 2377it [00:03, 1157.16it/s]warmup run: 2327it [00:03, 1112.95it/s]warmup run: 2624it [00:04, 1163.24it/s]warmup run: 2253it [00:03, 1093.03it/s]warmup run: 2300it [00:03, 1107.91it/s]warmup run: 2345it [00:03, 1129.10it/s]warmup run: 2279it [00:03, 1072.96it/s]warmup run: 2374it [00:03, 1127.65it/s]warmup run: 2500it [00:03, 1177.59it/s]warmup run: 2447it [00:03, 1137.77it/s]warmup run: 2744it [00:04, 1172.70it/s]warmup run: 2374it [00:03, 1127.11it/s]warmup run: 2421it [00:03, 1137.37it/s]warmup run: 2466it [00:03, 1152.69it/s]warmup run: 2394it [00:03, 1095.58it/s]warmup run: 2493it [00:03, 1143.67it/s]warmup run: 2623it [00:04, 1191.22it/s]warmup run: 2566it [00:04, 1153.13it/s]warmup run: 2864it [00:04, 1179.77it/s]warmup run: 2496it [00:04, 1152.67it/s]warmup run: 2542it [00:03, 1157.94it/s]warmup run: 2587it [00:04, 1169.21it/s]warmup run: 2511it [00:04, 1116.32it/s]warmup run: 2612it [00:04, 1155.18it/s]warmup run: 2745it [00:04, 1197.06it/s]warmup run: 2685it [00:04, 1162.31it/s]warmup run: 2984it [00:04, 1184.43it/s]warmup run: 2618it [00:04, 1169.96it/s]warmup run: 2661it [00:04, 1166.55it/s]warmup run: 3000it [00:04, 687.11it/s] warmup run: 2706it [00:04, 1175.14it/s]warmup run: 2626it [00:04, 1123.58it/s]warmup run: 2730it [00:04, 1162.26it/s]warmup run: 2866it [00:04, 1198.30it/s]warmup run: 2805it [00:04, 1170.70it/s]warmup run: 2738it [00:04, 1177.40it/s]warmup run: 2778it [00:04, 1164.26it/s]warmup run: 2827it [00:04, 1184.37it/s]warmup run: 2741it [00:04, 1128.77it/s]warmup run: 2847it [00:04, 1164.07it/s]warmup run: 2986it [00:04, 1197.81it/s]warmup run: 2926it [00:04, 1179.63it/s]warmup run: 3000it [00:04, 689.00it/s] warmup run: 2859it [00:04, 1185.87it/s]warmup run: 2895it [00:04, 1163.25it/s]warmup run: 2949it [00:04, 1194.03it/s]warmup run: 2858it [00:04, 1138.73it/s]warmup run: 2965it [00:04, 1167.19it/s]warmup run: 3000it [00:04, 680.74it/s] warmup run: 3000it [00:04, 687.14it/s] warmup run: 3000it [00:04, 687.72it/s] warmup run: 2978it [00:04, 1183.96it/s]warmup run: 3000it [00:04, 695.41it/s] warmup run: 3000it [00:04, 671.03it/s] warmup run: 2975it [00:04, 1145.32it/s]warmup run: 3000it [00:04, 672.48it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.98it/s]warmup should be done:   5%|         | 152/3000 [00:00<00:01, 1515.22it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.53it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.46it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1636.10it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.96it/s]warmup should be done:   5%|         | 157/3000 [00:00<00:01, 1563.30it/s]warmup should be done:   5%|         | 156/3000 [00:00<00:01, 1553.13it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1639.86it/s]warmup should be done:  10%|         | 305/3000 [00:00<00:01, 1520.85it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1643.40it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1606.65it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.45it/s]warmup should be done:  10%|         | 315/3000 [00:00<00:01, 1573.07it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1628.25it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1611.35it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1639.03it/s]warmup should be done:  15%|        | 458/3000 [00:00<00:01, 1523.96it/s]warmup should be done:  16%|        | 475/3000 [00:00<00:01, 1584.00it/s]warmup should be done:  16%|        | 484/3000 [00:00<00:01, 1614.95it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1626.64it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1641.58it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1641.99it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1605.62it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1638.42it/s]warmup should be done:  21%|        | 635/3000 [00:00<00:01, 1590.04it/s]warmup should be done:  22%|       | 646/3000 [00:00<00:01, 1616.48it/s]warmup should be done:  20%|        | 613/3000 [00:00<00:01, 1530.42it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1642.78it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1623.47it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1641.15it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1601.00it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1638.56it/s]warmup should be done:  27%|       | 808/3000 [00:00<00:01, 1617.27it/s]warmup should be done:  26%|       | 795/3000 [00:00<00:01, 1592.26it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1643.92it/s]warmup should be done:  26%|       | 767/3000 [00:00<00:01, 1531.05it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1641.47it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1619.91it/s]warmup should be done:  27%|       | 808/3000 [00:00<00:01, 1595.07it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1617.76it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1641.04it/s]warmup should be done:  32%|      | 955/3000 [00:00<00:01, 1585.57it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1623.63it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1637.44it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1614.51it/s]warmup should be done:  31%|       | 921/3000 [00:00<00:01, 1523.32it/s]warmup should be done:  32%|      | 968/3000 [00:00<00:01, 1591.40it/s]warmup should be done:  38%|      | 1132/3000 [00:00<00:01, 1612.90it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1642.85it/s]warmup should be done:  37%|      | 1114/3000 [00:00<00:01, 1584.06it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1629.37it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1639.28it/s]warmup should be done:  36%|      | 1074/3000 [00:00<00:01, 1523.67it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1613.18it/s]warmup should be done:  38%|      | 1128/3000 [00:00<00:01, 1592.31it/s]warmup should be done:  43%|     | 1295/3000 [00:00<00:01, 1616.09it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1644.59it/s]warmup should be done:  42%|     | 1274/3000 [00:00<00:01, 1586.21it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1633.44it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1640.23it/s]warmup should be done:  41%|      | 1227/3000 [00:00<00:01, 1518.37it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1605.04it/s]warmup should be done:  43%|     | 1288/3000 [00:00<00:01, 1593.52it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1624.49it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1645.08it/s]warmup should be done:  48%|     | 1433/3000 [00:00<00:00, 1584.34it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1636.40it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1640.36it/s]warmup should be done:  46%|     | 1381/3000 [00:00<00:01, 1523.41it/s]warmup should be done:  48%|     | 1448/3000 [00:00<00:00, 1594.31it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1601.25it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1630.03it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1639.65it/s]warmup should be done:  55%|    | 1644/3000 [00:01<00:00, 1639.26it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1640.47it/s]warmup should be done:  54%|    | 1608/3000 [00:01<00:00, 1594.26it/s]warmup should be done:  53%|    | 1592/3000 [00:01<00:00, 1569.81it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1598.25it/s]warmup should be done:  51%|     | 1534/3000 [00:01<00:00, 1513.12it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1633.80it/s]warmup should be done:  60%|    | 1809/3000 [00:01<00:00, 1639.85it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1641.24it/s]warmup should be done:  59%|    | 1768/3000 [00:01<00:00, 1595.28it/s]warmup should be done:  60%|    | 1813/3000 [00:01<00:00, 1626.05it/s]warmup should be done:  58%|    | 1751/3000 [00:01<00:00, 1574.54it/s]warmup should be done:  59%|    | 1784/3000 [00:01<00:00, 1596.65it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1523.59it/s]warmup should be done:  65%|   | 1955/3000 [00:01<00:00, 1635.67it/s]warmup should be done:  66%|   | 1974/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1641.21it/s]warmup should be done:  64%|   | 1928/3000 [00:01<00:00, 1593.30it/s]warmup should be done:  64%|   | 1915/3000 [00:01<00:00, 1591.97it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1618.27it/s]warmup should be done:  65%|   | 1944/3000 [00:01<00:00, 1593.15it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1529.35it/s]warmup should be done:  71%|   | 2119/3000 [00:01<00:00, 1627.97it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1640.53it/s]warmup should be done:  70%|   | 2088/3000 [00:01<00:00, 1593.63it/s]warmup should be done:  71%|  | 2139/3000 [00:01<00:00, 1629.77it/s]warmup should be done:  69%|   | 2078/3000 [00:01<00:00, 1601.38it/s]warmup should be done:  71%|  | 2141/3000 [00:01<00:00, 1626.83it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1591.22it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1517.25it/s]warmup should be done:  76%|  | 2282/3000 [00:01<00:00, 1626.49it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1637.28it/s]warmup should be done:  77%|  | 2303/3000 [00:01<00:00, 1632.22it/s]warmup should be done:  75%|  | 2239/3000 [00:01<00:00, 1603.63it/s]warmup should be done:  77%|  | 2305/3000 [00:01<00:00, 1629.75it/s]warmup should be done:  75%|  | 2248/3000 [00:01<00:00, 1589.39it/s]warmup should be done:  75%|  | 2264/3000 [00:01<00:00, 1588.73it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1543.36it/s]warmup should be done:  82%| | 2445/3000 [00:01<00:00, 1623.58it/s]warmup should be done:  82%| | 2475/3000 [00:01<00:00, 1638.25it/s]warmup should be done:  80%|  | 2401/3000 [00:01<00:00, 1608.24it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1635.16it/s]warmup should be done:  82%| | 2470/3000 [00:01<00:00, 1635.44it/s]warmup should be done:  80%|  | 2407/3000 [00:01<00:00, 1588.40it/s]warmup should be done:  81%|  | 2424/3000 [00:01<00:00, 1589.96it/s]warmup should be done:  77%|  | 2316/3000 [00:01<00:00, 1550.11it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1622.91it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1639.48it/s]warmup should be done:  85%| | 2564/3000 [00:01<00:00, 1611.78it/s]warmup should be done:  88%| | 2633/3000 [00:01<00:00, 1636.69it/s]warmup should be done:  86%| | 2567/3000 [00:01<00:00, 1589.45it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1629.68it/s]warmup should be done:  86%| | 2583/3000 [00:01<00:00, 1588.94it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1558.64it/s]warmup should be done:  92%|| 2771/3000 [00:01<00:00, 1623.11it/s]warmup should be done:  94%|| 2805/3000 [00:01<00:00, 1640.62it/s]warmup should be done:  93%|| 2797/3000 [00:01<00:00, 1636.40it/s]warmup should be done:  91%| | 2727/3000 [00:01<00:00, 1614.89it/s]warmup should be done:  91%| | 2727/3000 [00:01<00:00, 1590.51it/s]warmup should be done:  93%|| 2797/3000 [00:01<00:00, 1625.27it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1589.14it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1558.45it/s]warmup should be done:  98%|| 2936/3000 [00:01<00:00, 1628.25it/s]warmup should be done:  99%|| 2971/3000 [00:01<00:00, 1644.14it/s]warmup should be done:  96%|| 2891/3000 [00:01<00:00, 1620.74it/s]warmup should be done:  96%|| 2888/3000 [00:01<00:00, 1594.53it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1624.75it/s]warmup should be done:  97%|| 2903/3000 [00:01<00:00, 1593.58it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1542.24it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1598.42it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.77it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.93it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.69it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1622.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1601.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1597.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1594.75it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1553.61it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1536.82it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1586.63it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.67it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.97it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.17it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.05it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1622.29it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.42it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.96it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1687.37it/s]warmup should be done:  11%|         | 319/3000 [00:00<00:01, 1590.65it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1630.30it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.09it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1662.83it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1668.92it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1627.27it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1588.40it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1692.65it/s]warmup should be done:  16%|        | 481/3000 [00:00<00:01, 1601.96it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1636.45it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1676.54it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1662.87it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1634.70it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1639.53it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1615.55it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1691.67it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1637.01it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1636.44it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1647.48it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1683.48it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1666.98it/s]warmup should be done:  21%|       | 642/3000 [00:00<00:01, 1595.73it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1627.11it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1634.97it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1690.51it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1686.96it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1669.06it/s]warmup should be done:  27%|       | 807/3000 [00:00<00:01, 1612.01it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1616.06it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1631.56it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1604.79it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1637.72it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1691.46it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1669.75it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1614.96it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1673.20it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1638.38it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1607.30it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1608.32it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1648.01it/s]warmup should be done:  39%|      | 1172/3000 [00:00<00:01, 1669.75it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1690.07it/s]warmup should be done:  38%|      | 1132/3000 [00:00<00:01, 1608.91it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1642.75it/s]warmup should be done:  38%|      | 1147/3000 [00:00<00:01, 1597.25it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1565.73it/s]warmup should be done:  38%|      | 1143/3000 [00:00<00:01, 1492.34it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1651.12it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1671.69it/s]warmup should be done:  45%|     | 1358/3000 [00:00<00:00, 1691.47it/s]warmup should be done:  43%|     | 1293/3000 [00:00<00:01, 1602.29it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1642.06it/s]warmup should be done:  44%|     | 1307/3000 [00:00<00:01, 1586.04it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:01, 1557.21it/s]warmup should be done:  43%|     | 1294/3000 [00:00<00:01, 1484.28it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1656.87it/s]warmup should be done:  50%|     | 1508/3000 [00:00<00:00, 1671.28it/s]warmup should be done:  51%|     | 1528/3000 [00:00<00:00, 1690.23it/s]warmup should be done:  48%|     | 1454/3000 [00:00<00:00, 1601.41it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1643.44it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1599.03it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1599.43it/s]warmup should be done:  49%|     | 1461/3000 [00:00<00:01, 1537.78it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1662.25it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1673.01it/s]warmup should be done:  57%|    | 1698/3000 [00:01<00:00, 1687.67it/s]warmup should be done:  54%|    | 1615/3000 [00:01<00:00, 1601.38it/s]warmup should be done:  55%|    | 1652/3000 [00:01<00:00, 1647.45it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1618.70it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1629.42it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1576.40it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1665.20it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1673.50it/s]warmup should be done:  62%|   | 1867/3000 [00:01<00:00, 1688.13it/s]warmup should be done:  59%|    | 1777/3000 [00:01<00:00, 1605.73it/s]warmup should be done:  61%|    | 1818/3000 [00:01<00:00, 1648.70it/s]warmup should be done:  60%|    | 1803/3000 [00:01<00:00, 1631.21it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1651.13it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1600.48it/s]warmup should be done:  66%|   | 1988/3000 [00:01<00:00, 1662.40it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1671.49it/s]warmup should be done:  68%|   | 2037/3000 [00:01<00:00, 1690.12it/s]warmup should be done:  65%|   | 1938/3000 [00:01<00:00, 1599.69it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1645.82it/s]warmup should be done:  66%|   | 1969/3000 [00:01<00:00, 1637.14it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1667.18it/s]warmup should be done:  65%|   | 1960/3000 [00:01<00:00, 1616.77it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1659.09it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1668.03it/s]warmup should be done:  74%|  | 2207/3000 [00:01<00:00, 1686.27it/s]warmup should be done:  70%|   | 2102/3000 [00:01<00:00, 1611.51it/s]warmup should be done:  71%|   | 2134/3000 [00:01<00:00, 1640.11it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1605.35it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1676.71it/s]warmup should be done:  71%|   | 2123/3000 [00:01<00:00, 1617.12it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1666.71it/s]warmup should be done:  79%|  | 2377/3000 [00:01<00:00, 1688.32it/s]warmup should be done:  76%|  | 2265/3000 [00:01<00:00, 1616.14it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1645.74it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1618.46it/s]warmup should be done:  76%|  | 2290/3000 [00:01<00:00, 1630.99it/s]warmup should be done:  77%|  | 2321/3000 [00:01<00:00, 1510.18it/s]warmup should be done:  79%|  | 2359/3000 [00:01<00:00, 1523.19it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1662.42it/s]warmup should be done:  85%| | 2547/3000 [00:01<00:00, 1689.65it/s]warmup should be done:  82%| | 2467/3000 [00:01<00:00, 1650.26it/s]warmup should be done:  81%|  | 2427/3000 [00:01<00:00, 1585.58it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1623.51it/s]warmup should be done:  82%| | 2457/3000 [00:01<00:00, 1640.25it/s]warmup should be done:  83%| | 2487/3000 [00:01<00:00, 1551.45it/s]warmup should be done:  84%| | 2527/3000 [00:01<00:00, 1564.83it/s]warmup should be done:  91%| | 2717/3000 [00:01<00:00, 1690.75it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1654.87it/s]warmup should be done:  88%| | 2633/3000 [00:01<00:00, 1651.83it/s]warmup should be done:  86%| | 2587/3000 [00:01<00:00, 1587.59it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1625.95it/s]warmup should be done:  87%| | 2622/3000 [00:01<00:00, 1642.86it/s]warmup should be done:  88%| | 2652/3000 [00:01<00:00, 1577.41it/s]warmup should be done:  90%| | 2695/3000 [00:01<00:00, 1595.50it/s]warmup should be done:  96%|| 2887/3000 [00:01<00:00, 1687.26it/s]warmup should be done:  95%|| 2847/3000 [00:01<00:00, 1650.98it/s]warmup should be done:  93%|| 2799/3000 [00:01<00:00, 1652.75it/s]warmup should be done:  92%|| 2749/3000 [00:01<00:00, 1596.40it/s]warmup should be done:  93%|| 2804/3000 [00:01<00:00, 1626.91it/s]warmup should be done:  93%|| 2787/3000 [00:01<00:00, 1643.63it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1596.23it/s]warmup should be done:  95%|| 2862/3000 [00:01<00:00, 1615.69it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.51it/s]warmup should be done:  99%|| 2965/3000 [00:01<00:00, 1652.98it/s]warmup should be done:  97%|| 2909/3000 [00:01<00:00, 1595.15it/s]warmup should be done:  99%|| 2968/3000 [00:01<00:00, 1629.75it/s]warmup should be done:  98%|| 2954/3000 [00:01<00:00, 1650.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.14it/s]warmup should be done:  99%|| 2978/3000 [00:01<00:00, 1599.85it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1630.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1623.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1619.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1607.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1598.78it/s]2022-12-11 20:29:09.109224: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6d4b794d00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:09.109286: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:10.981819: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6d47833510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:10.981889: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:10.982443: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f51b802d230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:10.982502: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:11.091447: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6d3f82ca40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:11.091527: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:11.118754: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6d4782c8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:11.118819: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:11.230954: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6d3f830bf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:11.231017: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:11.339539: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6d3f830be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:11.339610: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:11.382639: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f51b402faf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:29:11.382720: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:29:11.384778: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.211508: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.313984: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.442448: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.446629: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.468451: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.640658: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:13.704653: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:29:14.281144: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.049262: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.252233: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.319202: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.378596: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.381083: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.525246: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:29:16.542486: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:29:59.528][ERROR][RK0][tid #140107763918592]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:29:59.529][ERROR][RK0][tid #140107763918592]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.534][ERROR][RK0][tid #140107763918592]: coll ps creation done
[HCTR][20:29:59.534][ERROR][RK0][tid #140107763918592]: replica 1 waits for coll ps creation barrier
[HCTR][20:29:59.548][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:29:59.549][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.556][ERROR][RK0][main]: coll ps creation done
[HCTR][20:29:59.557][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][20:29:59.703][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:29:59.704][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.711][ERROR][RK0][main]: coll ps creation done
[HCTR][20:29:59.712][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][20:29:59.723][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:29:59.723][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.727][ERROR][RK0][main]: coll ps creation done
[HCTR][20:29:59.727][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][20:29:59.759][ERROR][RK0][tid #140107898136320]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:29:59.759][ERROR][RK0][tid #140107898136320]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.764][ERROR][RK0][tid #140107898136320]: coll ps creation done
[HCTR][20:29:59.764][ERROR][RK0][tid #140107898136320]: replica 4 waits for coll ps creation barrier
[HCTR][20:29:59.773][ERROR][RK0][tid #140107772311296]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:29:59.773][ERROR][RK0][tid #140107772311296]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.781][ERROR][RK0][tid #140107772311296]: coll ps creation done
[HCTR][20:29:59.781][ERROR][RK0][tid #140107772311296]: replica 5 waits for coll ps creation barrier
[HCTR][20:29:59.808][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:29:59.808][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.816][ERROR][RK0][main]: coll ps creation done
[HCTR][20:29:59.816][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][20:29:59.825][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:29:59.825][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][20:29:59.832][ERROR][RK0][main]: coll ps creation done
[HCTR][20:29:59.832][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][20:29:59.832][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][20:30:00.647][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][20:30:00.676][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][20:30:00.676][ERROR][RK0][tid #140107763918592]: replica 1 calling init per replica
[HCTR][20:30:00.677][ERROR][RK0][main]: Calling build_v2
[HCTR][20:30:00.676][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][20:30:00.676][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][20:30:00.676][ERROR][RK0][tid #140107898136320]: replica 4 calling init per replica
[HCTR][20:30:00.676][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][20:30:00.676][ERROR][RK0][tid #140107772311296]: replica 5 calling init per replica
[HCTR][20:30:00.676][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][20:30:00.677][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][tid #140107763918592]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][main]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][main]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][tid #140107898136320]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][main]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][tid #140107772311296]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][main]: Calling build_v2
[HCTR][20:30:00.677][ERROR][RK0][tid #140107763918592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][tid #140107898136320]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][tid #140107772311296]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:30:00.677][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-11 20:30:00[[[2022-12-11 20:30:00.2022-12-11 20:30:002022-12-11 20:30:002022-12-11 20:30:002022-12-11 20:30:00.6771742022-12-11 20:30:00..2022-12-11 20:30:00..677174: .677174677174.677174677175: E677185: : 677189: : E : EE: EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE  E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136] :136136:136136] using concurrent impl MPS136] ] 136] ] using concurrent impl MPS
] using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS

using concurrent impl MPS



[2022-12-11 20:30:00.681549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:30:00.681590: [E2022-12-11 20:30:00 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc681594:: 196E]  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:178] v100x8, slow pcie
[[2022-12-11 20:30:002022-12-11 20:30:00..681640681645: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[:1782022-12-11 20:30:00196] .] v100x8, slow pcie681673[assigning 8 to cpu
: 2022-12-11 20:30:00
E.[ 6816852022-12-11 20:30:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .:E681707212 : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 2022-12-11 20:30:00
178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :681730v100x8, slow pcie[196: 
2022-12-11 20:30:00] [E.assigning 8 to cpu2022-12-11 20:30:00[ 681760
.[2022-12-11 20:30:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 6817682022-12-11 20:30:00.:E: .681778178 E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[681780: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:30:00:2022-12-11 20:30:00: Ev100x8, slow pcie:.213.E [
212681823] 681828 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:30:00] [: remote time is 8.68421: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:30:00E
E:196681890
.  178[] : 681915/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 2022-12-11 20:30:00assigning 8 to cpuE: ::2022-12-11 20:30:00v100x8, slow pcie.
 E178212.
681975/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [] ] 681995: [:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:30:00v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E2022-12-11 20:30:00178:.

E .] 196682075 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[682073[v100x8, slow pcie] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 20:30:00: 2022-12-11 20:30:00
assigning 8 to cpuE:[214.E.
 2132022-12-11 20:30:00] 682132 682139/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .cpu time is 97.0588: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :remote time is 8.68421682197
E:[E196
:  2122022-12-11 20:30:00 ] E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu 2022-12-11 20:30:00:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8682261:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.196
: 213:682302] E] 196: [assigning 8 to cpu remote time is 8.68421] E2022-12-11 20:30:00[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
assigning 8 to cpu .2022-12-11 20:30:00:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[682400.212:2022-12-11 20:30:00: [682419] 214.E2022-12-11 20:30:00: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 682453 .E
cpu time is 97.0588: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc682472 
E2022-12-11 20:30:00:[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 213.2022-12-11 20:30:00E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 682514. 212:remote time is 8.68421: 682539/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214
E: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8]  [E212
cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:30:00 ] 
:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[682628212:
2022-12-11 20:30:00: ] 213[.Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] 2022-12-11 20:30:00682667 
remote time is 8.68421.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
682712E:[: [ 2142022-12-11 20:30:00E2022-12-11 20:30:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] . .:cpu time is 97.0588682752/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc682762: 213
: :E] E213 remote time is 8.68421 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421::[
2142132022-12-11 20:30:00] ] [.cpu time is 97.0588remote time is 8.684212022-12-11 20:30:00682905

.: 682929E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ :2022-12-11 20:30:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214.:] 682978214cpu time is 97.0588: ] 
Ecpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 20:31:18.316775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:31:18.356806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 20:31:18.356869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 999999
[2022-12-11 20:31:18.465297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:31:18.465386: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:31:18.465419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:31:18.465449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:31:18.465886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.466771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.467513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.480712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 20:31:18.480784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 20:31:18.480794: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 20:31:18.480872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-11 20:31:182022-12-11 20:31:18..481011481021: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] [] 3 solved2022-12-11 20:31:18[5 solved
.[2022-12-11 20:31:182022-12-11 20:31:18
481070[..: 2022-12-11 20:31:18481084[481094E.: 2022-12-11 20:31:18:  481120E.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:  481137 :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202 :E:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc4 solved202 202:
] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 2051 solved[2022-12-11 20:31:18:] 6 solved
2022-12-11 20:31:18.205worker 0 thread 3 initing device 3
.[481201] 
4812202022-12-11 20:31:18[: worker 0 thread 5 initing device 5: .2022-12-11 20:31:18E
E481252.[  : 4812662022-12-11 20:31:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: .:: E4812961980205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc : ] ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccEeager alloc mem 381.47 MBworker 0 thread 4 initing device 4205: 

] 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuworker 0 thread 1 initing device 1] :
worker 0 thread 6 initing device 61980
] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.481645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.481668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.481773: E[ 2022-12-11 20:31:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[.:2022-12-11 20:31:184817851980.: ] 481792Eeager alloc mem 381.47 MB:  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-11 20:31:18.485866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.485920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.485975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.486025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.486079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.486130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.486190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.490433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:31:18.543038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 20:31:18.543376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 20:31:18.548491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:31:18.548559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:31:18.548603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:31:18.549401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.549838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.550777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.550865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:31:18.551539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:31:18.551581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:31:18.571984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 20:31:18.572291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 20:31:18.577122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:31:18.577185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:31:18.577226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:31:18.578006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.578399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.579339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.579426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:31:18.580104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:31:18.580146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[[[[[[2022-12-11 20:31:182022-12-11 20:31:182022-12-11 20:31:182022-12-11 20:31:182022-12-11 20:31:182022-12-11 20:31:18....583770..583770583770583770: 583770583770: : : E: : EEE EE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980::198019801980] 19801980] ] eager alloc mem 2.00 Bytes] eager alloc mem 2.00 Bytes] ] eager alloc mem 2.00 Bytes
eager alloc mem 2.00 Bytes
eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes



[2022-12-11 20:31:18[.2022-12-11 20:31:18584239[.[[: [2022-12-11 20:31:185842432022-12-11 20:31:182022-12-11 20:31:18E2022-12-11 20:31:18.: .. .584249E584250584250/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu584254:  : : :: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE1980E :  ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ::
:1980eager alloc mem 1024.00 Bytes198019801980] 
] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-11 20:31:18.590720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-11 20:31:182022-12-11 20:31:18..590775590798: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 2

[[2022-12-11 20:31:18[2022-12-11 20:31:18.2022-12-11 20:31:18.590871.590872: 590862: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:638] 638] eager release cuda mem 2] eager release cuda mem 400000000
eager release cuda mem 1024

[2022-12-11 20:31:18.590937: [E2022-12-11 20:31:18 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc5909632022-12-11 20:31:18:: .638E590970]  eager release cuda mem 1024: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
E: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 2
[2022-12-11 20:31:18.591024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 2[2022-12-11 20:31:18
2022-12-11 20:31:18..591046591033: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-11 20:31:18:638.638] 591080] [eager release cuda mem 400000000: eager release cuda mem 10242022-12-11 20:31:18
E
. 591092/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000[:
2022-12-11 20:31:18638.] 591156eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 20:31:18.591200: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:31:18:.638591214] : eager release cuda mem 2E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:31:18.591257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:31:18.591980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.592586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.593572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.594098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.594600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.595164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:31:18.595810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.595912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.596233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.596277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.596317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.596353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.596758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.596840: [E2022-12-11 20:31:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu596852:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 20:31:18.596951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:31:18.597176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.597212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.597247: E[ 2022-12-11 20:31:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:597261638: ] Eeager release cuda mem 625663 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:31:18:.[19805972842022-12-11 20:31:18] : .eager alloc mem 25.25 KBE597295
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] [eager alloc mem 25.25 KB2022-12-11 20:31:18
.597346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:31:18.597400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:31:18.597531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:31:18.597571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:31:18.597623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:31:18.597663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:31:18.597972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:31:18.598003: [E2022-12-11 20:31:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc598013:: [638E2022-12-11 20:31:18]  .eager release cuda mem 25855/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu598027
:: 1980E]  eager alloc mem 488.28 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:[638[2022-12-11 20:31:18] 2022-12-11 20:31:18.eager release cuda mem 25855.598068
598070: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[:19802022-12-11 20:31:18638] .] eager alloc mem 488.28 MB598108eager release cuda mem 25855
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 20:31:18.598155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[[[[[2022-12-11 20:31:18[[2022-12-11 20:31:18[2022-12-11 20:31:182022-12-11 20:31:182022-12-11 20:31:18.2022-12-11 20:31:182022-12-11 20:31:18.2022-12-11 20:31:18...691246..691247.691247691248691248: 691259691261: 691267: : : E: : E: EEE EE E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980::1980:198019801980] 19801980] 1980] ] ] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB





[2022-12-11 20:31:18[.2022-12-11 20:31:18[692331.2022-12-11 20:31:18[[: 692336[.2022-12-11 20:31:182022-12-11 20:31:18E: 2022-12-11 20:31:18692343.[. [E.: 6923552022-12-11 20:31:18692353/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:31:18 692354E: .: :./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  E692384E638692390:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :  ] : 638 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638: :
 eager release cuda mem 625663:] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
638eager release cuda mem 625663] :] :] 
eager release cuda mem 625663638eager release cuda mem 625663638[eager release cuda mem 625663
] 
] 2022-12-11 20:31:18
eager release cuda mem 625663eager release cuda mem 625663.[

6925992022-12-11 20:31:18[: .2022-12-11 20:31:18E692621[. [: 2022-12-11 20:31:18[692642/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:31:18E.2022-12-11 20:31:18[: [:. 692662.2022-12-11 20:31:18E2022-12-11 20:31:181980692668/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 692673. .] : :E: 692691/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu692696eager alloc mem 611.00 KBE1980 E: :: 
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E1980E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ]  :
1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] 1980:
:] eager alloc mem 611.00 KB] 19801980eager alloc mem 611.00 KB
eager alloc mem 611.00 KB] ] 

eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:31:18.693587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.693627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.693654[: 2022-12-11 20:31:18E[. 2022-12-11 20:31:18693661[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.: 2022-12-11 20:31:18:[2022-12-11 20:31:18693671E.6382022-12-11 20:31:18.:  693686] [.[693688E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: eager release cuda mem 6256632022-12-11 20:31:186937002022-12-11 20:31:18:  :E
.: .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980 693738E693741 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager alloc mem 611.00 KB:E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:] 
638 2022-12-11 20:31:18: 638eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
eager release cuda mem 625663:693851] :eager release cuda mem 625663
638: eager alloc mem 611.00 KB638
] E
] eager release cuda mem 625663 eager release cuda mem 625663
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
[:2022-12-11 20:31:181980.] [693972eager alloc mem 611.00 KB2022-12-11 20:31:18: 
[.E2022-12-11 20:31:18693992 [.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-11 20:31:18694000E:2022-12-11 20:31:18.:  1980.694018E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 694030:  :eager alloc mem 611.00 KB: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980
E :]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] 
:1980eager alloc mem 611.00 KB1980] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:31:18.694654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18[.2022-12-11 20:31:18694719.: 694725E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[ :2022-12-11 20:31:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638.:] 6947621980eager release cuda mem 625663: ] 
Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.694861[: 2022-12-11 20:31:18E. 694868/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :[E19802022-12-11 20:31:18 [] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:31:18eager alloc mem 611.00 KB694886:.
: 638694894[E] : 2022-12-11 20:31:18 eager release cuda mem 625663E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
 694917:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 1980:[[E] 6382022-12-11 20:31:182022-12-11 20:31:18 eager alloc mem 611.00 KB] ../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
eager release cuda mem 625663694969694970:2022-12-11 20:31:18
: : 638.EE] 695000  eager release cuda mem 625663: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
E::[ 6386382022-12-11 20:31:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] .:eager release cuda mem 625663eager release cuda mem 6256636950761980

: ] Eeager alloc mem 611.00 KB [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:31:18:.1980695117] : eager alloc mem 611.00 KBE
[[ 2022-12-11 20:31:182022-12-11 20:31:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..:6951656951651980: : ] EEeager alloc mem 611.00 KB  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:31:18.695592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.695664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 20:31:181980.] 695676eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.695767: [E2022-12-11 20:31:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu695774:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 20:31:18.695864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.695896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.695933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.695963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.[6959942022-12-11 20:31:18: .E696002 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980[
[] 2022-12-11 20:31:182022-12-11 20:31:18eager alloc mem 611.00 KB..
696062696064: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] [eager release cuda mem 625663eager release cuda mem 6256632022-12-11 20:31:18

.696116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:31:182022-12-11 20:31:18..696184696187: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:31:18.696472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.696544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 20:31:18
.696561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18[.2022-12-11 20:31:18696639.: 696641E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 20:31:18.696729[: 2022-12-11 20:31:18E. 696736/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-11 20:31:18.696818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.696849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 20:31:182022-12-11 20:31:18..696916696918: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[[[2022-12-11 20:31:182022-12-11 20:31:182022-12-11 20:31:18...697033697033697033: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::1980638638] ] ] eager alloc mem 611.00 KBeager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 20:31:18[.2022-12-11 20:31:18697210.: 697213E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 20:31:18.697335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.697405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.697439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.697507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 20:31:18.697526: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18[.2022-12-11 20:31:18697596.: 697598E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 20:31:18.697685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.697750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.697816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18.697916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.697983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:31:18[.2022-12-11 20:31:18698055.: 698060E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[[2022-12-11 20:31:182022-12-11 20:31:18..698144698147: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB[

2022-12-11 20:31:18.698185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.698236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:31:18.698288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.698325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:31:18.698394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.698430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:31:18.698456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.698491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:31:18.698602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.698637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:31:18.698761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:31:18.698798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[[2022-12-11 20:31:182022-12-11 20:31:18..698999699001: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 20:31:18[.2022-12-11 20:31:18699059.: 699061E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 4399996] 
eager release cuda mem 4399996
[2022-12-11 20:31:18.705662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.224467 secs 
[2022-12-11 20:31:18.706621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.224957 secs 
[2022-12-11 20:31:18.707063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.225427 secs 
[2022-12-11 20:31:18.707484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.225705 secs 
[2022-12-11 20:31:18.707884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.226117 secs 
[2022-12-11 20:31:18.708289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.24241 secs 
[2022-12-11 20:31:18.708700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.227416 secs 
[2022-12-11 20:31:18.709105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.227321 secs 
[2022-12-11 20:31:18.709256: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.12 GB
[2022-12-11 20:31:20.270489: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.38 GB
[2022-12-11 20:31:20.270846: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.38 GB
[2022-12-11 20:31:20.271746: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.38 GB
[2022-12-11 20:31:21.684804: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.64 GB
[2022-12-11 20:31:21.685454: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.64 GB
[2022-12-11 20:31:21.686048: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.64 GB
[2022-12-11 20:31:22.842001: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 7.86 GB
[2022-12-11 20:31:22.842134: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 7.86 GB
[2022-12-11 20:31:22.843282: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 7.86 GB
[2022-12-11 20:31:23.791527: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.07 GB
[2022-12-11 20:31:23.792367: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.07 GB
[2022-12-11 20:31:23.793363: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.07 GB
[2022-12-11 20:31:25.267229: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.53 GB
[2022-12-11 20:31:25.267829: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.53 GB
[2022-12-11 20:31:25.268430: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.53 GB
[2022-12-11 20:31:26.687707: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.73 GB
[2022-12-11 20:31:26.688766: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.73 GB
[HCTR][20:31:28.192][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][tid #140107898136320]: replica 4 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][tid #140107772311296]: replica 5 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][tid #140107763918592]: replica 1 calling init per replica done, doing barrier
[HCTR][20:31:28.192][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][20:31:28.193][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][tid #140107763918592]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][tid #140107772311296]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][tid #140107898136320]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:31:28.193][ERROR][RK0][main]: init per replica done
[HCTR][20:31:28.193][ERROR][RK0][tid #140107772311296]: init per replica done
[HCTR][20:31:28.193][ERROR][RK0][tid #140107763918592]: init per replica done
[HCTR][20:31:28.193][ERROR][RK0][main]: init per replica done
[HCTR][20:31:28.193][ERROR][RK0][main]: init per replica done
[HCTR][20:31:28.193][ERROR][RK0][tid #140107898136320]: init per replica done
[HCTR][20:31:28.193][ERROR][RK0][main]: init per replica done
[HCTR][20:31:28.196][ERROR][RK0][main]: init per replica done
[HCTR][20:31:28.199][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.199][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.199][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.199][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.199][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.199][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.199][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.199][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.199][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.199][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.199][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.199][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.199][ERROR][RK0][tid #140108040746752]: 3 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.199][ERROR][RK0][tid #140108040746752]: 3 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.199][ERROR][RK0][tid #140108040746752]: 3 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.199][ERROR][RK0][tid #140108040746752]: 3 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.199][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.199][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.199][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.199][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.199][ERROR][RK0][tid #140108099462912]: 7 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.200][ERROR][RK0][tid #140108099462912]: 7 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.200][ERROR][RK0][tid #140108099462912]: 7 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.200][ERROR][RK0][tid #140108099462912]: 7 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.200][ERROR][RK0][tid #140107772311296]: 6 allocated 3276800 at 0x7f6f34b20000
[HCTR][20:31:28.200][ERROR][RK0][tid #140107772311296]: 6 allocated 6553600 at 0x7f6f35000000
[HCTR][20:31:28.200][ERROR][RK0][tid #140107772311296]: 6 allocated 3276800 at 0x7f6f35640000
[HCTR][20:31:28.200][ERROR][RK0][tid #140107772311296]: 6 allocated 6553600 at 0x7f6f35960000
[HCTR][20:31:28.202][ERROR][RK0][tid #140108300789504]: 0 allocated 3276800 at 0x7f6f37120000
[HCTR][20:31:28.202][ERROR][RK0][tid #140108300789504]: 0 allocated 6553600 at 0x7f6f37600000
[HCTR][20:31:28.202][ERROR][RK0][tid #140108300789504]: 0 allocated 3276800 at 0x7f6f3830e800
[HCTR][20:31:28.202][ERROR][RK0][tid #140108300789504]: 0 allocated 6553600 at 0x7f6f3862e800








