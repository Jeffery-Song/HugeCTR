2022-12-11 20:57:56.156634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.162718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.170254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.174640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.181438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.191514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.199660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.211932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.264149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.268343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.271308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.272567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.274296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.275116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.275642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.282989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.283203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.284749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.284771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.286424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.286478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.287969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.288249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.289445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.290033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.291871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.292368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.293413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.293997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.294782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.296008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.296979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.298758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.299777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.300735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.301736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.302655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.303607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.304557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.305584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.311300: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.313572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.315280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.317192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.318510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.318989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.320401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.320899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.321845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.322328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.323394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.324645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.325155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.325428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.326339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.327417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.328197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.328439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.328913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.331233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.331501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.334149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.334389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.335780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.336769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.336985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.338980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.340164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.341197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.342069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.342986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.343102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.344160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.345483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.346668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.346704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.347065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.347367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.348899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.350035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.350526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.350531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.351851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.352808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.353220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.353394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.354535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.355581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.355838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.356007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.357041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.360631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.360839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.361098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.363165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.363201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.363446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.365265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.365439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.365804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.377433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.385090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.394945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.401976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.402995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.403025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.403236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.403324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.405449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.406776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.406859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.407068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.408127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.409326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.410360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.411279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.411424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.412434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.413192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.414030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.414499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.414737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.415858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.416379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.417911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.418242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.418510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.419785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.420569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.422132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.422316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.422358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.423932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.424316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.425779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.425816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.425949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.427149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.428346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.428879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.429015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.429059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.430436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.431718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.432242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.432340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.432385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.433818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.435953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.436235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.436791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.436832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.438349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.440047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.440260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.440302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.440487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.441743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.443894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.443909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.444053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.444252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.445351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.447401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.447504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.447510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.447642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.447794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.448794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.451415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.451534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.451624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.451634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.451737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.452175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.452809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.456012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.456055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.456208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.456209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.456244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.457061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.457447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.462053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.462779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.462861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.462912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.462919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.463147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.465953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.466491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.466529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.466581: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.466608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.466717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.467017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.469765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.470497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.470680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.470782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.470809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.472225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.473746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.474447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.474509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.474704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.474730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.476130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.476219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.477902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.479657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.479712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.479904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.480022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.482747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.482757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.484719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.485647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.485723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.485899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.486012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.487776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.487953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.489455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.490689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.490708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.491038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.491529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.492587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.493944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.494870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.495117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.495277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.495920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.497268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.498514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.499475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.501349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.502731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.503743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.503801: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.504296: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.504296: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.505305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.506073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.506784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.509022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.509892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.511735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.512167: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.512290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.513568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.514348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.514416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.514834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.515449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.518079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.519667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.519752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.520142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.521396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.521715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.522144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.524114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.524137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.524758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.525874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.526212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.529492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.537385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.537844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.568938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.570537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.573847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.575986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.608125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.610840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.614653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.616450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.623702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.623938: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.631026: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:57:56.633401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.637053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.640574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.644761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.651230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:56.656238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.644832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.645908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.646886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.648065: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:57.648127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:57:57.667846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.669089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.669607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.670181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.671104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.672308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:57:57.719371: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:57.719585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:57.767832: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:57:57.895875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.897105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.898446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.899954: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:57.900015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:57:57.919548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.921660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.922944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.923678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.924571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.925281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:57:57.952711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.953587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.953595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.955029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.955058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.956009: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:57.956036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.956069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:57:57.956678: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:57.956725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:57:57.959787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.960376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.961220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.961724: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:57.961786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:57:57.966129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.966749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.967611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.968108: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:57.968160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:57:57.974051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.974547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.975505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.976535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.977227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.978689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.979255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.979670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.980671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.982582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.983186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.984099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.985001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.985478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:57:57.985524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.986215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:57:57.987020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.987221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.988738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.988893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.990135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:57:57.990424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.991399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:57.992749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:57:58.000934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.001122: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.002945: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:57:58.007934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.009223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.010348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.011462: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:58.011518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:57:58.029409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.032032: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.032058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.032192: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.033763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.033986: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:57:58.034718: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.034803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.034922: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.035357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.035849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:57:58.036802: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:57:58.038815: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.038977: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.040896: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:57:58.045793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.047017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.048144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.049077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:57:58.049153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:57:58.064256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.064452: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.066324: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:57:58.067724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.068362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.068876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.069448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.069965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:57:58.070429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:57:58.081119: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.081279: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.083042: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:57:58.114603: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.114800: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:57:58.116491: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:57:59.395][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 103it [00:01, 84.99it/s]warmup run: 206it [00:01, 184.84it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 310it [00:01, 296.75it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 99it [00:01, 84.73it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 414it [00:01, 413.77it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 77it [00:01, 65.14it/s]warmup run: 198it [00:01, 183.50it/s]warmup run: 100it [00:01, 84.29it/s]warmup run: 94it [00:01, 77.95it/s]warmup run: 94it [00:01, 79.24it/s]warmup run: 515it [00:02, 522.90it/s]warmup run: 97it [00:01, 80.65it/s]warmup run: 96it [00:01, 82.73it/s]warmup run: 177it [00:01, 166.17it/s]warmup run: 298it [00:01, 293.45it/s]warmup run: 199it [00:01, 181.84it/s]warmup run: 188it [00:01, 169.39it/s]warmup run: 187it [00:01, 170.83it/s]warmup run: 617it [00:02, 625.96it/s]warmup run: 195it [00:01, 176.27it/s]warmup run: 193it [00:01, 180.23it/s]warmup run: 273it [00:01, 272.36it/s]warmup run: 397it [00:01, 405.67it/s]warmup run: 298it [00:01, 289.47it/s]warmup run: 282it [00:01, 270.80it/s]warmup run: 279it [00:01, 270.58it/s]warmup run: 720it [00:02, 716.93it/s]warmup run: 294it [00:01, 283.29it/s]warmup run: 292it [00:01, 289.78it/s]warmup run: 373it [00:01, 388.99it/s]warmup run: 496it [00:02, 514.30it/s]warmup run: 398it [00:01, 402.91it/s]warmup run: 376it [00:01, 376.64it/s]warmup run: 371it [00:01, 374.41it/s]warmup run: 821it [00:02, 788.98it/s]warmup run: 390it [00:01, 401.59it/s]warmup run: 394it [00:01, 396.39it/s]warmup run: 475it [00:02, 505.91it/s]warmup run: 597it [00:02, 617.74it/s]warmup run: 500it [00:02, 517.54it/s]warmup run: 471it [00:02, 481.93it/s]warmup run: 464it [00:02, 477.24it/s]warmup run: 923it [00:02, 847.85it/s]warmup run: 489it [00:02, 511.90it/s]warmup run: 496it [00:02, 510.36it/s]warmup run: 574it [00:02, 607.18it/s]warmup run: 694it [00:02, 697.92it/s]warmup run: 603it [00:02, 624.56it/s]warmup run: 567it [00:02, 580.62it/s]warmup run: 557it [00:02, 571.54it/s]warmup run: 1025it [00:02, 893.55it/s]warmup run: 590it [00:02, 616.98it/s]warmup run: 598it [00:02, 615.52it/s]warmup run: 673it [00:02, 695.47it/s]warmup run: 795it [00:02, 775.20it/s]warmup run: 707it [00:02, 719.79it/s]warmup run: 663it [00:02, 666.82it/s]warmup run: 651it [00:02, 655.69it/s]warmup run: 1129it [00:02, 932.53it/s]warmup run: 692it [00:02, 709.33it/s]warmup run: 700it [00:02, 706.44it/s]warmup run: 773it [00:02, 770.48it/s]warmup run: 895it [00:02, 833.94it/s]warmup run: 810it [00:02, 796.56it/s]warmup run: 758it [00:02, 735.39it/s]warmup run: 744it [00:02, 722.29it/s]warmup run: 1232it [00:02, 958.84it/s]warmup run: 791it [00:02, 777.08it/s]warmup run: 798it [00:02, 764.76it/s]warmup run: 875it [00:02, 835.43it/s]warmup run: 996it [00:02, 879.59it/s]warmup run: 913it [00:02, 856.54it/s]warmup run: 853it [00:02, 789.36it/s]warmup run: 836it [00:02, 773.66it/s]warmup run: 1334it [00:02, 973.98it/s]warmup run: 892it [00:02, 836.36it/s]warmup run: 897it [00:02, 816.62it/s]warmup run: 976it [00:02, 882.49it/s]warmup run: 1097it [00:02, 913.76it/s]warmup run: 1015it [00:02, 892.34it/s]warmup run: 948it [00:02, 831.21it/s]warmup run: 929it [00:02, 814.80it/s]warmup run: 1436it [00:02, 975.68it/s]warmup run: 994it [00:02, 885.65it/s]warmup run: 996it [00:02, 862.83it/s]warmup run: 1077it [00:02, 917.39it/s]warmup run: 1198it [00:02, 939.02it/s]warmup run: 1116it [00:02, 915.11it/s]warmup run: 1042it [00:02, 861.12it/s]warmup run: 1021it [00:02, 842.68it/s]warmup run: 1537it [00:03, 980.18it/s]warmup run: 1095it [00:02, 918.94it/s]warmup run: 1097it [00:02, 901.85it/s]warmup run: 1298it [00:02, 955.83it/s]warmup run: 1177it [00:02, 926.79it/s]warmup run: 1216it [00:02, 935.34it/s]warmup run: 1136it [00:02, 879.47it/s]warmup run: 1115it [00:02, 867.98it/s]warmup run: 1638it [00:03, 986.50it/s]warmup run: 1198it [00:02, 948.13it/s]warmup run: 1197it [00:02, 928.59it/s]warmup run: 1399it [00:02, 969.52it/s]warmup run: 1277it [00:02, 946.35it/s]warmup run: 1231it [00:02, 899.24it/s]warmup run: 1316it [00:02, 937.41it/s]warmup run: 1208it [00:02, 885.17it/s]warmup run: 1739it [00:03, 989.98it/s]warmup run: 1299it [00:02, 964.23it/s]warmup run: 1296it [00:02, 942.39it/s]warmup run: 1500it [00:03, 980.10it/s]warmup run: 1378it [00:02, 963.97it/s]warmup run: 1325it [00:02, 908.87it/s]warmup run: 1415it [00:02, 950.76it/s]warmup run: 1302it [00:02, 899.39it/s]warmup run: 1840it [00:03, 993.05it/s]warmup run: 1400it [00:02, 973.94it/s]warmup run: 1396it [00:02, 957.32it/s]warmup run: 1604it [00:03, 995.26it/s]warmup run: 1478it [00:03, 972.84it/s]warmup run: 1516it [00:03, 967.72it/s]warmup run: 1425it [00:03, 933.33it/s]warmup run: 1400it [00:03, 922.47it/s]warmup run: 1941it [00:03, 995.16it/s]warmup run: 1501it [00:03, 975.05it/s]warmup run: 1500it [00:03, 980.56it/s]warmup run: 1709it [00:03, 1011.24it/s]warmup run: 1578it [00:03, 976.42it/s]warmup run: 1524it [00:03, 949.39it/s]warmup run: 1618it [00:03, 980.42it/s]warmup run: 1500it [00:03, 943.90it/s]warmup run: 2046it [00:03, 1010.78it/s]warmup run: 1601it [00:03, 977.50it/s]warmup run: 1601it [00:03, 987.73it/s]warmup run: 1812it [00:03, 1013.63it/s]warmup run: 1678it [00:03, 979.80it/s]warmup run: 1720it [00:03, 991.97it/s]warmup run: 1628it [00:03, 973.43it/s]warmup run: 1600it [00:03, 957.83it/s]warmup run: 2165it [00:03, 1063.43it/s]warmup run: 1702it [00:03, 985.05it/s]warmup run: 1702it [00:03, 975.84it/s]warmup run: 1777it [00:03, 981.73it/s]warmup run: 1915it [00:03, 1005.21it/s]warmup run: 1730it [00:03, 986.79it/s]warmup run: 1825it [00:03, 1007.57it/s]warmup run: 1697it [00:03, 949.25it/s]warmup run: 2279it [00:03, 1086.18it/s]warmup run: 1804it [00:03, 994.98it/s]warmup run: 1801it [00:03, 978.72it/s]warmup run: 1876it [00:03, 981.84it/s]warmup run: 2022it [00:03, 1021.97it/s]warmup run: 1930it [00:03, 1019.36it/s]warmup run: 1830it [00:03, 986.67it/s]warmup run: 1795it [00:03, 957.08it/s]warmup run: 2398it [00:03, 1116.53it/s]warmup run: 1906it [00:03, 1002.36it/s]warmup run: 1901it [00:03, 983.36it/s]warmup run: 1975it [00:03, 982.25it/s]warmup run: 2146it [00:03, 1084.44it/s]warmup run: 2041it [00:03, 1045.52it/s]warmup run: 1930it [00:03, 985.93it/s]warmup run: 1896it [00:03, 970.55it/s]warmup run: 2517it [00:04, 1137.49it/s]warmup run: 2009it [00:03, 1009.97it/s]warmup run: 2001it [00:03, 985.98it/s]warmup run: 2084it [00:03, 1012.60it/s]warmup run: 2270it [00:03, 1128.12it/s]warmup run: 2165it [00:03, 1101.35it/s]warmup run: 2034it [00:03, 1001.53it/s]warmup run: 1997it [00:03, 980.44it/s]warmup run: 2636it [00:04, 1151.77it/s]warmup run: 2130it [00:03, 1067.93it/s]warmup run: 2121it [00:03, 1048.19it/s]warmup run: 2203it [00:03, 1063.55it/s]warmup run: 2394it [00:03, 1160.84it/s]warmup run: 2289it [00:03, 1139.95it/s]warmup run: 2155it [00:03, 1061.66it/s]warmup run: 2115it [00:03, 1037.59it/s]warmup run: 2756it [00:04, 1164.16it/s]warmup run: 2251it [00:03, 1109.84it/s]warmup run: 2241it [00:03, 1092.88it/s]warmup run: 2322it [00:03, 1099.10it/s]warmup run: 2518it [00:03, 1183.54it/s]warmup run: 2413it [00:03, 1168.07it/s]warmup run: 2275it [00:03, 1101.52it/s]warmup run: 2234it [00:03, 1081.75it/s]warmup run: 2874it [00:04, 1168.61it/s]warmup run: 2372it [00:03, 1138.89it/s]warmup run: 2362it [00:03, 1126.09it/s]warmup run: 2441it [00:03, 1124.83it/s]warmup run: 2642it [00:04, 1199.58it/s]warmup run: 2537it [00:03, 1186.87it/s]warmup run: 2394it [00:03, 1127.89it/s]warmup run: 2353it [00:03, 1112.32it/s]warmup run: 2994it [00:04, 1177.28it/s]warmup run: 3000it [00:04, 679.76it/s] warmup run: 2493it [00:03, 1159.13it/s]warmup run: 2483it [00:03, 1148.39it/s]warmup run: 2560it [00:04, 1143.34it/s]warmup run: 2765it [00:04, 1207.14it/s]warmup run: 2659it [00:04, 1195.21it/s]warmup run: 2513it [00:04, 1145.79it/s]warmup run: 2471it [00:04, 1132.11it/s]warmup run: 2615it [00:04, 1174.39it/s]warmup run: 2604it [00:04, 1165.11it/s]warmup run: 2680it [00:04, 1159.15it/s]warmup run: 2889it [00:04, 1215.01it/s]warmup run: 2780it [00:04, 1197.02it/s]warmup run: 2633it [00:04, 1161.49it/s]warmup run: 2589it [00:04, 1145.63it/s]warmup run: 2737it [00:04, 1185.76it/s]warmup run: 2725it [00:04, 1177.11it/s]warmup run: 3000it [00:04, 691.75it/s] warmup run: 2798it [00:04, 1163.63it/s]warmup run: 2901it [00:04, 1200.26it/s]warmup run: 2753it [00:04, 1171.85it/s]warmup run: 2708it [00:04, 1157.75it/s]warmup run: 2857it [00:04, 1189.66it/s]warmup run: 2844it [00:04, 1180.59it/s]warmup run: 2918it [00:04, 1173.03it/s]warmup run: 3000it [00:04, 688.29it/s] warmup run: 2873it [00:04, 1179.30it/s]warmup run: 2826it [00:04, 1162.05it/s]warmup run: 2979it [00:04, 1198.12it/s]warmup run: 2965it [00:04, 1187.35it/s]warmup run: 3000it [00:04, 688.52it/s] warmup run: 3000it [00:04, 675.52it/s] warmup run: 3000it [00:04, 675.87it/s] warmup run: 2991it [00:04, 1174.26it/s]warmup run: 3000it [00:04, 665.92it/s] warmup run: 2944it [00:04, 1165.99it/s]warmup run: 3000it [00:04, 663.95it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.93it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1597.03it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.27it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1606.06it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.56it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.39it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1624.71it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.38it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1596.92it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1687.36it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1647.24it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.85it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.41it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1641.02it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1633.34it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1637.60it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1654.37it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1684.75it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1644.87it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1637.42it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1651.96it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1589.31it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1639.80it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1622.14it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1684.16it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1643.59it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1639.31it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1635.77it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1649.78it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1650.79it/s]warmup should be done:  21%|       | 639/3000 [00:00<00:01, 1585.74it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1600.82it/s]warmup should be done:  27%|       | 823/3000 [00:00<00:01, 1639.32it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1681.28it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1642.93it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1634.71it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1653.75it/s]warmup should be done:  27%|       | 798/3000 [00:00<00:01, 1584.69it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1650.03it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1605.31it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1637.92it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1631.91it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1653.00it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1639.99it/s]warmup should be done:  32%|      | 957/3000 [00:00<00:01, 1581.55it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1648.95it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1675.38it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1585.43it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1634.08it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1650.69it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1635.31it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1643.39it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1625.25it/s]warmup should be done:  37%|      | 1116/3000 [00:00<00:01, 1574.84it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1594.90it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1631.68it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1634.07it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1653.25it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1637.69it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1641.99it/s]warmup should be done:  42%|     | 1275/3000 [00:00<00:01, 1577.27it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1619.82it/s]warmup should be done:  43%|     | 1303/3000 [00:00<00:01, 1604.69it/s]warmup should be done:  45%|     | 1346/3000 [00:00<00:01, 1618.74it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1633.56it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1653.58it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1641.10it/s]warmup should be done:  50%|     | 1491/3000 [00:00<00:00, 1641.86it/s]warmup should be done:  48%|     | 1434/3000 [00:00<00:00, 1580.14it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1619.45it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1611.91it/s]warmup should be done:  50%|     | 1509/3000 [00:00<00:00, 1619.67it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1633.39it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1655.81it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1645.31it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1643.82it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1620.26it/s]warmup should be done:  53%|    | 1593/3000 [00:01<00:00, 1578.53it/s]warmup should be done:  54%|    | 1630/3000 [00:01<00:00, 1618.39it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1622.93it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1656.91it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1630.96it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1647.53it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1644.63it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1620.31it/s]warmup should be done:  58%|    | 1751/3000 [00:01<00:00, 1576.80it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1622.80it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1626.39it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1657.23it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1648.42it/s]warmup should be done:  66%|   | 1986/3000 [00:01<00:00, 1644.46it/s]warmup should be done:  66%|   | 1971/3000 [00:01<00:00, 1622.96it/s]warmup should be done:  65%|   | 1963/3000 [00:01<00:00, 1620.27it/s]warmup should be done:  64%|   | 1909/3000 [00:01<00:00, 1575.82it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1624.68it/s]warmup should be done:  67%|   | 2002/3000 [00:01<00:00, 1632.13it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1657.18it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1648.80it/s]warmup should be done:  72%|  | 2151/3000 [00:01<00:00, 1644.33it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1625.95it/s]warmup should be done:  69%|   | 2071/3000 [00:01<00:00, 1587.01it/s]warmup should be done:  71%|   | 2126/3000 [00:01<00:00, 1620.14it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1627.10it/s]warmup should be done:  72%|  | 2166/3000 [00:01<00:00, 1633.76it/s]warmup should be done:  77%|  | 2324/3000 [00:01<00:00, 1657.64it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1650.74it/s]warmup should be done:  77%|  | 2316/3000 [00:01<00:00, 1644.94it/s]warmup should be done:  77%|  | 2299/3000 [00:01<00:00, 1628.47it/s]warmup should be done:  76%|  | 2291/3000 [00:01<00:00, 1628.61it/s]warmup should be done:  74%|  | 2235/3000 [00:01<00:00, 1600.23it/s]warmup should be done:  76%|  | 2285/3000 [00:01<00:00, 1628.52it/s]warmup should be done:  78%|  | 2330/3000 [00:01<00:00, 1633.97it/s]warmup should be done:  83%| | 2490/3000 [00:01<00:00, 1654.58it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1642.35it/s]warmup should be done:  83%| | 2479/3000 [00:01<00:00, 1643.58it/s]warmup should be done:  82%| | 2462/3000 [00:01<00:00, 1627.88it/s]warmup should be done:  82%| | 2456/3000 [00:01<00:00, 1632.49it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1606.59it/s]warmup should be done:  82%| | 2448/3000 [00:01<00:00, 1626.72it/s]warmup should be done:  83%| | 2494/3000 [00:01<00:00, 1634.72it/s]warmup should be done:  89%| | 2656/3000 [00:01<00:00, 1654.49it/s]warmup should be done:  88%| | 2646/3000 [00:01<00:00, 1643.61it/s]warmup should be done:  88%| | 2626/3000 [00:01<00:00, 1630.50it/s]warmup should be done:  88%| | 2644/3000 [00:01<00:00, 1641.46it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1637.33it/s]warmup should be done:  85%| | 2562/3000 [00:01<00:00, 1614.06it/s]warmup should be done:  87%| | 2612/3000 [00:01<00:00, 1628.46it/s]warmup should be done:  89%| | 2660/3000 [00:01<00:00, 1641.93it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1657.15it/s]warmup should be done:  94%|| 2811/3000 [00:01<00:00, 1643.68it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1631.13it/s]warmup should be done:  93%|| 2787/3000 [00:01<00:00, 1642.07it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1639.38it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1618.38it/s]warmup should be done:  93%|| 2776/3000 [00:01<00:00, 1630.39it/s]warmup should be done:  94%|| 2825/3000 [00:01<00:00, 1641.51it/s]warmup should be done: 100%|| 2991/3000 [00:01<00:00, 1661.07it/s]warmup should be done:  99%|| 2977/3000 [00:01<00:00, 1647.81it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1635.22it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1651.39it/s]warmup should be done:  99%|| 2975/3000 [00:01<00:00, 1643.48it/s]warmup should be done:  96%|| 2890/3000 [00:01<00:00, 1625.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1654.66it/s]warmup should be done:  98%|| 2942/3000 [00:01<00:00, 1637.01it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1627.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1642.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1640.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1633.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1598.42it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1709.43it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1689.57it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1707.17it/s]warmup should be done:   5%|         | 156/3000 [00:00<00:01, 1558.82it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1674.44it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1703.81it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.55it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1683.48it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1605.79it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1686.97it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1705.53it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1676.78it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1705.11it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1679.75it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1699.22it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1679.02it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1692.56it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1621.88it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1703.27it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1702.66it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1686.36it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1668.02it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1699.81it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1679.55it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1696.94it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1706.37it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1706.21it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1688.28it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1703.35it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1683.41it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1687.07it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1552.97it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1697.08it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1705.67it/s]warmup should be done:  29%|       | 857/3000 [00:00<00:01, 1708.39it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1686.45it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1697.01it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1685.94it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1698.18it/s]warmup should be done:  27%|       | 804/3000 [00:00<00:01, 1376.74it/s]warmup should be done:  34%|      | 1020/3000 [00:00<00:01, 1698.18it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1687.25it/s]warmup should be done:  34%|      | 1028/3000 [00:00<00:01, 1707.94it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1704.76it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1700.49it/s]warmup should be done:  34%|      | 1015/3000 [00:00<00:01, 1685.19it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1696.24it/s]warmup should be done:  32%|      | 963/3000 [00:00<00:01, 1439.70it/s]warmup should be done:  40%|      | 1190/3000 [00:00<00:01, 1696.58it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1685.95it/s]warmup should be done:  40%|      | 1190/3000 [00:00<00:01, 1700.75it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1701.84it/s]warmup should be done:  40%|      | 1199/3000 [00:00<00:01, 1702.57it/s]warmup should be done:  39%|      | 1184/3000 [00:00<00:01, 1682.08it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1682.10it/s]warmup should be done:  38%|      | 1132/3000 [00:00<00:01, 1514.64it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1698.45it/s]warmup should be done:  45%|     | 1353/3000 [00:00<00:00, 1685.84it/s]warmup should be done:  45%|     | 1362/3000 [00:00<00:00, 1705.62it/s]warmup should be done:  46%|     | 1371/3000 [00:00<00:00, 1706.90it/s]warmup should be done:  46%|     | 1369/3000 [00:00<00:00, 1705.73it/s]warmup should be done:  45%|     | 1354/3000 [00:00<00:00, 1685.11it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1691.34it/s]warmup should be done:  43%|     | 1300/3000 [00:00<00:01, 1564.12it/s]warmup should be done:  51%|     | 1531/3000 [00:00<00:00, 1696.28it/s]warmup should be done:  51%|     | 1522/3000 [00:00<00:00, 1684.77it/s]warmup should be done:  51%|     | 1534/3000 [00:00<00:00, 1707.97it/s]warmup should be done:  51%|    | 1540/3000 [00:00<00:00, 1705.92it/s]warmup should be done:  51%|    | 1542/3000 [00:00<00:00, 1706.62it/s]warmup should be done:  51%|     | 1523/3000 [00:00<00:00, 1685.00it/s]warmup should be done:  51%|    | 1538/3000 [00:00<00:00, 1693.94it/s]warmup should be done:  49%|     | 1470/3000 [00:00<00:00, 1602.81it/s]warmup should be done:  57%|    | 1701/3000 [00:01<00:00, 1695.98it/s]warmup should be done:  56%|    | 1691/3000 [00:01<00:00, 1684.45it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1711.40it/s]warmup should be done:  57%|    | 1711/3000 [00:01<00:00, 1705.39it/s]warmup should be done:  57%|    | 1713/3000 [00:01<00:00, 1705.61it/s]warmup should be done:  56%|    | 1692/3000 [00:01<00:00, 1685.26it/s]warmup should be done:  57%|    | 1709/3000 [00:01<00:00, 1697.78it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1630.40it/s]warmup should be done:  62%|   | 1860/3000 [00:01<00:00, 1685.59it/s]warmup should be done:  62%|   | 1871/3000 [00:01<00:00, 1693.57it/s]warmup should be done:  63%|   | 1882/3000 [00:01<00:00, 1706.14it/s]warmup should be done:  63%|   | 1885/3000 [00:01<00:00, 1707.18it/s]warmup should be done:  62%|   | 1861/3000 [00:01<00:00, 1686.57it/s]warmup should be done:  63%|   | 1881/3000 [00:01<00:00, 1701.55it/s]warmup should be done:  63%|   | 1878/3000 [00:01<00:00, 1691.82it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1648.71it/s]warmup should be done:  68%|   | 2029/3000 [00:01<00:00, 1684.64it/s]warmup should be done:  68%|   | 2041/3000 [00:01<00:00, 1692.16it/s]warmup should be done:  68%|   | 2053/3000 [00:01<00:00, 1705.50it/s]warmup should be done:  69%|   | 2056/3000 [00:01<00:00, 1707.21it/s]warmup should be done:  68%|   | 2030/3000 [00:01<00:00, 1686.18it/s]warmup should be done:  68%|   | 2053/3000 [00:01<00:00, 1705.53it/s]warmup should be done:  68%|   | 2048/3000 [00:01<00:00, 1660.42it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1658.91it/s]warmup should be done:  74%|  | 2211/3000 [00:01<00:00, 1693.67it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1683.21it/s]warmup should be done:  74%|  | 2224/3000 [00:01<00:00, 1702.88it/s]warmup should be done:  74%|  | 2227/3000 [00:01<00:00, 1704.79it/s]warmup should be done:  73%|  | 2199/3000 [00:01<00:00, 1685.18it/s]warmup should be done:  74%|  | 2224/3000 [00:01<00:00, 1705.45it/s]warmup should be done:  74%|  | 2215/3000 [00:01<00:00, 1636.93it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1666.91it/s]warmup should be done:  79%|  | 2381/3000 [00:01<00:00, 1695.50it/s]warmup should be done:  79%|  | 2367/3000 [00:01<00:00, 1683.68it/s]warmup should be done:  79%|  | 2368/3000 [00:01<00:00, 1685.91it/s]warmup should be done:  80%|  | 2395/3000 [00:01<00:00, 1702.44it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1703.91it/s]warmup should be done:  80%|  | 2395/3000 [00:01<00:00, 1705.90it/s]warmup should be done:  79%|  | 2379/3000 [00:01<00:00, 1621.70it/s]warmup should be done:  77%|  | 2318/3000 [00:01<00:00, 1674.82it/s]warmup should be done:  85%| | 2552/3000 [00:01<00:00, 1697.21it/s]warmup should be done:  85%| | 2536/3000 [00:01<00:00, 1683.72it/s]warmup should be done:  85%| | 2537/3000 [00:01<00:00, 1686.83it/s]warmup should be done:  86%| | 2570/3000 [00:01<00:00, 1706.62it/s]warmup should be done:  86%| | 2566/3000 [00:01<00:00, 1695.67it/s]warmup should be done:  86%| | 2567/3000 [00:01<00:00, 1707.14it/s]warmup should be done:  85%| | 2542/3000 [00:01<00:00, 1613.21it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1679.46it/s]warmup should be done:  91%| | 2723/3000 [00:01<00:00, 1698.65it/s]warmup should be done:  90%| | 2705/3000 [00:01<00:00, 1683.79it/s]warmup should be done:  90%| | 2707/3000 [00:01<00:00, 1687.85it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1708.76it/s]warmup should be done:  91%|| 2739/3000 [00:01<00:00, 1709.71it/s]warmup should be done:  91%| | 2736/3000 [00:01<00:00, 1680.66it/s]warmup should be done:  90%| | 2704/3000 [00:01<00:00, 1613.89it/s]warmup should be done:  89%| | 2657/3000 [00:01<00:00, 1679.95it/s]warmup should be done:  96%|| 2893/3000 [00:01<00:00, 1698.86it/s]warmup should be done:  96%|| 2874/3000 [00:01<00:00, 1682.13it/s]warmup should be done:  96%|| 2876/3000 [00:01<00:00, 1687.07it/s]warmup should be done:  97%|| 2913/3000 [00:01<00:00, 1708.68it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1709.61it/s]warmup should be done:  97%|| 2905/3000 [00:01<00:00, 1674.91it/s]warmup should be done:  96%|| 2871/3000 [00:01<00:00, 1629.09it/s]warmup should be done:  94%|| 2826/3000 [00:01<00:00, 1681.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1706.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1700.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1696.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1695.24it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1685.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1683.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.47it/s]warmup should be done: 100%|| 2996/3000 [00:01<00:00, 1684.20it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1614.98it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c2b7250>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c5abe80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c5aed30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c2a9190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c2a9100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c2a90d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c5b3e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7d6c5ada30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 20:59:28.845686: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78a70280e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:28.845746: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:28.853791: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.011789: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f789f034d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.011847: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.021243: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.780263: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78a302c7d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.780319: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.789912: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.863946: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78a215b720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.864013: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.865609: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78a702c5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.865660: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.866075: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78a7028880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.866130: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.874174: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.874538: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.875267: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.940629: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f78aa795090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.940688: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.949478: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:29.976503: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f789a837740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:59:29.976566: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:59:29.984135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:59:36.149876: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.231353: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.508024: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.567505: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.672102: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.769667: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.845902: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:59:36.847795: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:00:41.834][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:00:41.834][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:41.840][ERROR][RK0][main]: coll ps creation done
[HCTR][21:00:41.840][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:00:41.859][ERROR][RK0][tid #140156711458560]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:00:41.859][ERROR][RK0][tid #140156711458560]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:41.868][ERROR][RK0][tid #140156711458560]: coll ps creation done
[HCTR][21:00:41.868][ERROR][RK0][tid #140156711458560]: replica 2 waits for coll ps creation barrier
[HCTR][21:00:41.877][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:00:41.877][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:41.882][ERROR][RK0][main]: coll ps creation done
[HCTR][21:00:41.882][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][21:00:42.063][ERROR][RK0][tid #140156443023104]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:00:42.063][ERROR][RK0][tid #140156443023104]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:42.068][ERROR][RK0][tid #140156443023104]: coll ps creation done
[HCTR][21:00:42.068][ERROR][RK0][tid #140156443023104]: replica 5 waits for coll ps creation barrier
[HCTR][21:00:42.145][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:00:42.145][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:42.150][ERROR][RK0][main]: coll ps creation done
[HCTR][21:00:42.150][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:00:42.227][ERROR][RK0][tid #140157248329472]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:00:42.227][ERROR][RK0][tid #140157248329472]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:42.232][ERROR][RK0][tid #140157248329472]: coll ps creation done
[HCTR][21:00:42.232][ERROR][RK0][tid #140157248329472]: replica 1 waits for coll ps creation barrier
[HCTR][21:00:42.261][ERROR][RK0][tid #140156635956992]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:00:42.262][ERROR][RK0][tid #140156635956992]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:42.267][ERROR][RK0][tid #140156635956992]: coll ps creation done
[HCTR][21:00:42.267][ERROR][RK0][tid #140156635956992]: replica 0 waits for coll ps creation barrier
[HCTR][21:00:42.335][ERROR][RK0][tid #140156434630400]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:00:42.336][ERROR][RK0][tid #140156434630400]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][21:00:42.343][ERROR][RK0][tid #140156434630400]: coll ps creation done
[HCTR][21:00:42.343][ERROR][RK0][tid #140156434630400]: replica 4 waits for coll ps creation barrier
[HCTR][21:00:42.343][ERROR][RK0][tid #140156635956992]: replica 0 preparing frequency
[HCTR][21:00:43.171][ERROR][RK0][tid #140156635956992]: replica 0 preparing frequency done
[HCTR][21:00:43.206][ERROR][RK0][tid #140156635956992]: replica 0 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][tid #140156711458560]: replica 2 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][tid #140157248329472]: replica 1 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][tid #140156443023104]: replica 5 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][tid #140156434630400]: replica 4 calling init per replica
[HCTR][21:00:43.206][ERROR][RK0][tid #140156635956992]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][tid #140156711458560]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][main]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][tid #140157248329472]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][tid #140156443023104]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][main]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][main]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][tid #140156635956992]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][tid #140156711458560]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][tid #140156434630400]: Calling build_v2
[HCTR][21:00:43.206][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][tid #140157248329472]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][tid #140156443023104]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:00:43.206][ERROR][RK0][tid #140156434630400]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 21:00:43.210799: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie2022-12-11 21:00:43
.210839: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:00:43:.178[210884] : v100x8, slow pcieE
 2022-12-11 21:00:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[2108821962022-12-11 21:00:43: ] .Eassigning 0 to cpu210921[ 
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 2022-12-11 21:00:43178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :210926v100x8, slow pcie196: 
] Eassigning 0 to cpu[ 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 21:00:43:2022-12-11 21:00:432022-12-11 21:00:43.178..210990] 210996v100x8, slow pcie[210971: : 
2022-12-11 21:00:43: EE.E  [211018 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:00:43: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::.E:196212211048 178] ] : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] assigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E:v100x8, slow pcie

 2122022-12-11 21:00:43
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[211079196
2022-12-11 21:00:43[: ] .[2022-12-11 21:00:43[Eassigning 0 to cpu211149[.2022-12-11 21:00:43 
: 2022-12-11 21:00:43[211150./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:00:43E.: 211168:. 211176E: 178[2022-12-11 21:00:43211141/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  E] 2022-12-11 21:00:43.: :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc v100x8, slow pcie.211184E213 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
211243:  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196:: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421[:] 212E :
2022-12-11 21:00:43213assigning 0 to cpu]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178.] [
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] 211344remote time is 8.684212022-12-11 21:00:43
:178v100x8, slow pcie: 
.212] 
[E211392] [v100x8, slow pcie2022-12-11 21:00:43[ : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[2022-12-11 21:00:43
.2022-12-11 21:00:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
2022-12-11 21:00:43.211449.[: .211459: 211464[2022-12-11 21:00:43196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc211468: E: 2022-12-11 21:00:43.] :: E E.211502assigning 0 to cpu214E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 211522: 
]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213:E 
:214] 212 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196] remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[] cpu time is 97.0588
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:1962022-12-11 21:00:43assigning 0 to cpu

213] [.
] assigning 0 to cpu2022-12-11 21:00:43[211658remote time is 8.68421
.2022-12-11 21:00:43: 
211694.E: 211716[[2022-12-11 21:00:43 E: 2022-12-11 21:00:43./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E.[211750:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2117482022-12-11 21:00:43: :212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .E214] :E211778 ] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:
remote time is 8.68421: 214[
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 21:00:43] :[cpu time is 97.0588.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82122022-12-11 21:00:43
211880
] .: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8211905E
:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:00:43[ :.2022-12-11 21:00:43/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213211962.:] : 211973214remote time is 8.68421E: ] 
 Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:00:43213:.] 213212062remote time is 8.68421] : 
remote time is 8.68421E
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:00:43[:.2022-12-11 21:00:43214212134.] : 212141cpu time is 97.0588E: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 21:02:01.135330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:02:01.175241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:02:01.175307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 999999
[2022-12-11 21:02:01.294756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:02:01.294845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:02:01.450890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:02:01.450924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:02:01.451448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.452368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.453230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.466049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 21:02:01.466106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 21:02:01.466484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01[.2022-12-11 21:02:01466557.: 466569E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 202] 3 solved7 solved

[[2022-12-11 21:02:012022-12-11 21:02:01..466640466642: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 3 initing device 3worker 0 thread 7 initing device 7

[2022-12-11 21:02:01.466886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-11 21:02:015 solved.
466901: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 21:02:01:.202466964] : 4 solvedE 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 21:02:01.467024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-11 21:02:012022-12-11 21:02:01[[..2022-12-11 21:02:012022-12-11 21:02:01467054467055..: : 467045467047EE: :   EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc19801980::] ] 202202eager alloc mem 381.47 MBeager alloc mem 381.47 MB] ] 

6 solved1 solved

[[2022-12-11 21:02:012022-12-11 21:02:01..467254467255: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 6 initing device 6worker 0 thread 1 initing device 1

[2022-12-11 21:02:01.467408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.467447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 21:02:012022-12-11 21:02:01..467659467660: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 21:02:01.468452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.471949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.472001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.472190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.472247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.472305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.472361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.473373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.476579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.476629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.476738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.476795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.476869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.476920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:02:01.529519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:02:01.529896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:02:01.547656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:02:01.547767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:02:01.547814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:02:01.548775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.554352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.555331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.555422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.556100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:02:01.556142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 21:02:01] .eager alloc mem 488.28 MB556128
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:02:01.556477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:02:01.558698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes[
[2022-12-11 21:02:012022-12-11 21:02:01..558747558755: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-11 21:02:01.559053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-11 21:02:012022-12-11 21:02:01..559099559099: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-11 21:02:01.561532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:02:01.561602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:02:01.561647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:02:01.563048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.563847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.564785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.564868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-11 21:02:012022-12-11 21:02:01..564922564922: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-11 21:02:01:1980.1980] 564988] eager alloc mem 2.00 Bytes: eager alloc mem 2.00 Bytes
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 21:02:01.565073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:02:01.565138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:02:01.565181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:02:01.565232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:02:01.565295: [E[2022-12-11 21:02:01 2022-12-11 21:02:01.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[5653002022-12-11 21:02:01:5653032022-12-11 21:02:01: .638: .E565317] E565316 : eager release cuda mem 2 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
E: : 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :[] :eager alloc mem 1024.00 Bytes19802022-12-11 21:02:01eager alloc mem 1024.00 Bytes638
] .
] eager alloc mem 1024.00 Bytes565458eager release cuda mem 1024
: 
[E2022-12-11 21:02:01 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc565536:: 638E]  eager release cuda mem 400000000[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
2022-12-11 21:02:01:.638565564] : eager release cuda mem 25855E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:02:01.565608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:02:01eager alloc mem 488.28 MB.
565625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:02:01.565937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.567456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.567961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.568830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.569760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.569840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.569891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.569928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.570508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:02:01.570549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:02:01.570824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.570874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.570908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.570957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.571239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:02:01.571307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:02:01.571349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:02:01.571408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:02:01.571474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 22022-12-11 21:02:01
.571480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024[
2022-12-11 21:02:01.571525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:02:01.571552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 21:02:01.571582: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:02:01:.638571597] : eager release cuda mem 25855E
[ 2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[:5716332022-12-11 21:02:01638: .] E571657eager release cuda mem 400000000 : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 258551980
] eager alloc mem 488.28 MB
[2022-12-11 21:02:01.571746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:02:01.572185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.574060: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.574900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:02:01.575499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.575658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.575775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.576430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.576514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.576596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.576679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.576713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.576797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:02:01.577184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:02:01.577225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:02:01.577353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:02:01.577395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[2022-12-11 21:02:01.577484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:02:01.577527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.28 MB
[[[[2022-12-11 21:02:01[[2022-12-11 21:02:012022-12-11 21:02:012022-12-11 21:02:01.2022-12-11 21:02:012022-12-11 21:02:01...668655..668658668659668662: 668666[668667: : : E: 2022-12-11 21:02:01: EEE E.E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 668728[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980:E.:198019801980] 1980 6687851980] ] ] eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB:Eeager alloc mem 611.00 KB



1980 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[[2022-12-11 21:02:012022-12-11 21:02:01[..2022-12-11 21:02:01669695669698[.: : 2022-12-11 21:02:01669706EE.[:   [[6697232022-12-11 21:02:01E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 21:02:012022-12-11 21:02:01: . ::..E669757[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638669766669768 : 2022-12-11 21:02:01:] ] : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE.638eager release cuda mem 625663eager release cuda mem 625663EE: 669820] 

  638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :E
::eager release cuda mem 625663638 638638
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] [eager release cuda mem 625663:[eager release cuda mem 625663eager release cuda mem 6256632022-12-11 21:02:01
638
2022-12-11 21:02:01
.] .[669974eager release cuda mem 6256636699782022-12-11 21:02:01: 
[: .E2022-12-11 21:02:01E[670014 .[ [2022-12-11 21:02:01: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu670033[2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:02:01.E:: 2022-12-11 21:02:01.:.670054 1980E.6700621980670069: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  670102: ] : E:eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: Eeager alloc mem 611.00 KBE 1980
:E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980
eager alloc mem 611.00 KB:19801980] 
1980] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[[2022-12-11 21:02:012022-12-11 21:02:01..670976670979: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[::2022-12-11 21:02:01[638638.2022-12-11 21:02:01] ] [671032.eager release cuda mem 625663eager release cuda mem 6256632022-12-11 21:02:01: 671052

[.[E[: 2022-12-11 21:02:016710742022-12-11 21:02:012022-12-11 21:02:01E.: .. 671107 E671112671116/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : [[:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE:2022-12-11 21:02:012022-12-11 21:02:01638 :  638..] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 671182eager release cuda mem 625663671182eager release cuda mem 625663:] ::: 
: 
638eager release cuda mem 625663638638EE] 
] ]   eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu


::[198019802022-12-11 21:02:01[[] ] .2022-12-11 21:02:01[2022-12-11 21:02:01eager alloc mem 611.00 KBeager alloc mem 611.00 KB671360[.2022-12-11 21:02:01[.

: 2022-12-11 21:02:01671372.2022-12-11 21:02:01671375E.: 671406.:  671404E: 671411E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  E:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB:] 1980:] 
1980eager alloc mem 611.00 KB] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-11 21:02:01.672186: E[ 2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:672198638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.672293[: 2022-12-11 21:02:01E. 672302/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: [:E2022-12-11 21:02:011980 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[672330[eager alloc mem 611.00 KB:2022-12-11 21:02:01: 2022-12-11 21:02:01
1980.E.[] 672357[[ 6723722022-12-11 21:02:01eager alloc mem 611.00 KB: 2022-12-11 21:02:012022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: .
E..:E672413 672409672411638 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:EEeager release cuda mem 625663: 638  
638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663::eager release cuda mem 625663638
638638
] [] ] eager release cuda mem 6256632022-12-11 21:02:01eager release cuda mem 625663eager release cuda mem 625663
.

672572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 21:02:01[.2022-12-11 21:02:01672608.[: 672614[[2022-12-11 21:02:01E: 2022-12-11 21:02:012022-12-11 21:02:01. E..672633/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 672636672635: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : E1980:EE ] 1980  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
eager alloc mem 611.00 KB::1980
19801980] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-11 21:02:01.673141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.[6732122022-12-11 21:02:01: .E673219 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.673329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.673373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.673440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.673508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 21:02:01
.673529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 21:02:012022-12-11 21:02:01..[[673582[6735822022-12-11 21:02:012022-12-11 21:02:01: 2022-12-11 21:02:01: ..E.E673587673587 673607 : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE:E:  638 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ::eager release cuda mem 625663:eager release cuda mem 6256636381980
1980
] ] ] eager release cuda mem 625663eager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-11 21:02:01.[6737762022-12-11 21:02:01: .E673782 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: [1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:02:01] :.eager alloc mem 611.00 KB1980673804
] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.674031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.674102: E[ 2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:6741091980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.674211[: 2022-12-11 21:02:01E. 674216/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-11 21:02:01.674299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.674520: E[ 2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:674533638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.674621[: [2022-12-11 21:02:01E2022-12-11 21:02:01[. [.2022-12-11 21:02:01674628/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:02:01674632.: :.: 674644E1980674651E:  ] :  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KBE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :
 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] :] 638eager release cuda mem 625663638eager alloc mem 611.00 KB] 
] 
eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 21:02:01.674818: E[[ 2022-12-11 21:02:012022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..:6748296748291980: : ] EEeager alloc mem 611.00 KB  
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
[
2022-12-11 21:02:01.674893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.674973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.675009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 21:02:012022-12-11 21:02:01..675072675076: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-11 21:02:01.675180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.675500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.675546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:02:01.675570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.675620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:02:01.675660: E[ 2022-12-11 21:02:01/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:675671638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.675711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01[.2022-12-11 21:02:01[675742.2022-12-11 21:02:01: 675742.E[: 675753 2022-12-11 21:02:01E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu. E:675779/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 1980: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] E638:eager alloc mem 611.00 KB ] 1980
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663] :
eager alloc mem 611.00 KB1980[
] 2022-12-11 21:02:01eager alloc mem 611.00 KB.[
6758892022-12-11 21:02:01: .E675902 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] [:eager release cuda mem 6256632022-12-11 21:02:01638
.] 675960eager release cuda mem 4399996: [
E2022-12-11 21:02:01 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc675994:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 4399996
[2022-12-11 21:02:01.676053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:02:01.676355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.676393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 43999962022-12-11 21:02:01
.676409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.676455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:02:01.676623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:02:01.676659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:02:01.676699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:02:01eager release cuda mem 625663.
676716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:02:01eager release cuda mem 625663.
676741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 21:02:01eager release cuda mem 4399996.
676765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:02:01.679883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.212454 secs 
[2022-12-11 21:02:01.680284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.213239 secs 
[2022-12-11 21:02:01.680676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.213633 secs 
[2022-12-11 21:02:01.681072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.213424 secs 
[2022-12-11 21:02:01.681483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.230049 secs 
[2022-12-11 21:02:01.681883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.214236 secs 
[2022-12-11 21:02:01.682279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.215807 secs 
[2022-12-11 21:02:01.682668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 99000001 / 100000000 nodes ( 99.00 %) | 488.28 MB | 0.215271 secs 
[HCTR][21:02:01.682][ERROR][RK0][tid #140156443023104]: replica 5 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][tid #140156434630400]: replica 4 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][tid #140156711458560]: replica 2 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][tid #140157248329472]: replica 1 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][tid #140156635956992]: replica 0 calling init per replica done, doing barrier
[HCTR][21:02:01.682][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156711458560]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156434630400]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156443023104]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156635956992]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][tid #140157248329472]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:02:01.682][ERROR][RK0][main]: init per replica done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156711458560]: init per replica done
[HCTR][21:02:01.682][ERROR][RK0][main]: init per replica done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156434630400]: init per replica done
[HCTR][21:02:01.682][ERROR][RK0][tid #140156443023104]: init per replica done
[HCTR][21:02:01.682][ERROR][RK0][tid #140157248329472]: init per replica done
[HCTR][21:02:01.682][ERROR][RK0][main]: init per replica done
[HCTR][21:02:01.685][ERROR][RK0][tid #140156635956992]: init per replica done
