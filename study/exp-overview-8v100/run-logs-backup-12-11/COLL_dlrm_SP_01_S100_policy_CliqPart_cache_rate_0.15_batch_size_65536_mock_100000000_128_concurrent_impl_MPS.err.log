2022-12-12 03:32:54.932963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.940730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.946262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.951972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.955959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.967839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.982576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:54.988944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.025864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.037892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.041307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.045092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.047893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.048366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.049708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.049909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.051395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.051639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.052938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.053178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.054462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.054543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.055893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.055936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.057155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.058252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.059212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.060118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.061184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.062266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.063233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.064161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.065903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.067084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.068120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.069147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.070080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.071012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.071942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.072863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.077465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.078115: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.078616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.079679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.080751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.081799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.082830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.083944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.084965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.088057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.088926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.089854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.091051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.091448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.091816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.093139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.093503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.093831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.096050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.096219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.099175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.099351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.099898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.102383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.102556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.103321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.103668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.105468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.105551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.106620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.107198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.108831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.109796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.110262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.111109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.111527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.112422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.112889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.114016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.114978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.115362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.116300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.117206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.117579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.118403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.119559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.120067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.130139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.130676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.131480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.133504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.134351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.134611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.135112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.136874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.137308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.137565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.154297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.158973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.172991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.173668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.173709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.174789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.174828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.176048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.177200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.177374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.178632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.178821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.179772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.181742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.181943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.182121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.182899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.184051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.184806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.186202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.186342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.186619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.187725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.187856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.188696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.190001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.190181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.191035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.191959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.192048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.192877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.194589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.194721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.195400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.196529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.196616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.197347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.198782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.198962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.199333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.200398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.200578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.201529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.202699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.202956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.203393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.204190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.204498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.205343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.206708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.206936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.207232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.207963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.208284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.209088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.210602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.210783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.211066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.212198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.212913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.214504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.216235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.216482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.216543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.217330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.217853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.218914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.220345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.220565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.220807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.221951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.222086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.222447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.223771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.225078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.225490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.225748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.226481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.226542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.227089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.229592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.229792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.230038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.230303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.230697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.231011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.231328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.234380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.234695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.235055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.235351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.235581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.235856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.235988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.239338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.239509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.239832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.240215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.240333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.241793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.241988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.245185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.245326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.245470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.246005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.246011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.246764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.247157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.250119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.250347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.250936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.251029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.251463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.251746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.252022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.254605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.255364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.255499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.255980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.255998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.256174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.256318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.259253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.259836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.260636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.260881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.260990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.261085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.261229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.264603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.265563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.266222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.267029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.267194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.267451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.267523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.270869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.271607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.271755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.272126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.272172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.274199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.274564: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.275078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.275099: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.275184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.275380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.275421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.277743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.278424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.278643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.280833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.281981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.282085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.282087: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.282428: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.285193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.285502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.285645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.285739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.288467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.288706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.288901: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.289006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.291050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.291183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.291604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.292465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.292569: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.293463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.294996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.296718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.298192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.299447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.300055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.300983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.302145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.303060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.303297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.304027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.307587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.307749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.308393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.312086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.340696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.345410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.351297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.359481: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:32:55.369889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.388819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:55.393162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.454466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.455498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.456031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.456505: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.456561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:32:56.475039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.475708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.476441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.477677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.478254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.478912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:32:56.525781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.525991: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.572510: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:32:56.744442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.745049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.745580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.746045: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.746096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:32:56.753520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.754126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.754874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.755386: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.755451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:32:56.760660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.761260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.761789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.762398: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.762452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:32:56.764228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.764946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.765471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.766039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.766560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.767025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:32:56.772938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.773362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.773362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.774066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.775304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.775338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.775570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.776881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.776952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.777231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.778202: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.778259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:32:56.778387: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.778433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:32:56.778722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.779333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:32:56.779839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.780462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.780970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.781561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.782223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.782704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:32:56.787016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.787678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.788238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.788720: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.788765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:32:56.789441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.790028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.790551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.791012: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:32:56.791056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:32:56.796714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.796726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.797858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.797954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.798749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.798887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.799755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.799879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.800661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.800786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.801527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:32:56.801646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:32:56.806688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.808957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.813080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.813206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.814153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.814229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.815340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.815418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.816408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.816482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:32:56.817483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:32:56.817559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:32:56.824471: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.824631: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.826461: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 03:32:56.848104: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.848328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.849159: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.849309: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.850120: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 03:32:56.851175: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 03:32:56.860406: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.860592: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.862327: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:32:56.862377: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.862520: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.864447: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:32:56.864692: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.864883: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.866082: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.866224: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:32:56.866782: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 03:32:56.868381: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][03:32:58.091][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.092][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.138][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.138][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.138][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.138][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.139][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:32:58.139][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 101it [00:01, 85.31it/s]warmup run: 98it [00:01, 82.99it/s]warmup run: 203it [00:01, 186.16it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 197it [00:01, 181.03it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 305it [00:01, 297.20it/s]warmup run: 99it [00:01, 83.79it/s]warmup run: 99it [00:01, 85.02it/s]warmup run: 296it [00:01, 289.27it/s]warmup run: 93it [00:01, 79.99it/s]warmup run: 102it [00:01, 88.60it/s]warmup run: 87it [00:01, 74.89it/s]warmup run: 100it [00:01, 86.50it/s]warmup run: 407it [00:01, 412.50it/s]warmup run: 199it [00:01, 182.80it/s]warmup run: 198it [00:01, 184.03it/s]warmup run: 396it [00:01, 403.17it/s]warmup run: 191it [00:01, 178.64it/s]warmup run: 204it [00:01, 191.48it/s]warmup run: 179it [00:01, 167.55it/s]warmup run: 200it [00:01, 187.15it/s]warmup run: 509it [00:02, 524.94it/s]warmup run: 300it [00:01, 293.34it/s]warmup run: 296it [00:01, 291.64it/s]warmup run: 496it [00:02, 513.56it/s]warmup run: 293it [00:01, 292.33it/s]warmup run: 307it [00:01, 305.45it/s]warmup run: 271it [00:01, 269.35it/s]warmup run: 300it [00:01, 297.51it/s]warmup run: 612it [00:02, 629.79it/s]warmup run: 401it [00:01, 407.69it/s]warmup run: 395it [00:01, 404.28it/s]warmup run: 598it [00:02, 618.97it/s]warmup run: 394it [00:01, 408.35it/s]warmup run: 406it [00:01, 416.60it/s]warmup run: 362it [00:01, 373.03it/s]warmup run: 398it [00:01, 408.42it/s]warmup run: 715it [00:02, 720.39it/s]warmup run: 500it [00:02, 515.91it/s]warmup run: 495it [00:02, 515.22it/s]warmup run: 699it [00:02, 708.59it/s]warmup run: 495it [00:02, 520.82it/s]warmup run: 506it [00:01, 526.54it/s]warmup run: 454it [00:02, 475.65it/s]warmup run: 497it [00:02, 517.62it/s]warmup run: 816it [00:02, 791.75it/s]warmup run: 602it [00:02, 621.02it/s]warmup run: 596it [00:02, 619.13it/s]warmup run: 801it [00:02, 783.97it/s]warmup run: 598it [00:02, 628.45it/s]warmup run: 610it [00:02, 634.54it/s]warmup run: 553it [00:02, 583.91it/s]warmup run: 600it [00:02, 625.20it/s]warmup run: 918it [00:02, 849.94it/s]warmup run: 702it [00:02, 707.12it/s]warmup run: 696it [00:02, 706.83it/s]warmup run: 901it [00:02, 839.70it/s]warmup run: 701it [00:02, 720.25it/s]warmup run: 713it [00:02, 726.39it/s]warmup run: 653it [00:02, 678.91it/s]warmup run: 702it [00:02, 715.39it/s]warmup run: 1020it [00:02, 893.73it/s]warmup run: 803it [00:02, 781.38it/s]warmup run: 795it [00:02, 776.53it/s]warmup run: 1000it [00:02, 877.53it/s]warmup run: 804it [00:02, 797.21it/s]warmup run: 816it [00:02, 801.71it/s]warmup run: 753it [00:02, 758.36it/s]warmup run: 804it [00:02, 789.92it/s]warmup run: 1121it [00:02, 887.72it/s]warmup run: 902it [00:02, 833.48it/s]warmup run: 893it [00:02, 829.64it/s]warmup run: 1100it [00:02, 909.52it/s]warmup run: 907it [00:02, 856.30it/s]warmup run: 919it [00:02, 860.19it/s]warmup run: 853it [00:02, 820.01it/s]warmup run: 905it [00:02, 845.59it/s]warmup run: 1223it [00:02, 923.41it/s]warmup run: 1002it [00:02, 876.44it/s]warmup run: 991it [00:02, 867.70it/s]warmup run: 1199it [00:02, 923.63it/s]warmup run: 1009it [00:02, 900.74it/s]warmup run: 1021it [00:02, 902.14it/s]warmup run: 953it [00:02, 866.53it/s]warmup run: 1005it [00:02, 886.86it/s]warmup run: 1323it [00:02, 944.08it/s]warmup run: 1103it [00:02, 913.39it/s]warmup run: 1089it [00:02, 895.09it/s]warmup run: 1299it [00:02, 945.50it/s]warmup run: 1112it [00:02, 936.51it/s]warmup run: 1123it [00:02, 934.32it/s]warmup run: 1053it [00:02, 903.67it/s]warmup run: 1105it [00:02, 916.14it/s]warmup run: 1424it [00:02, 962.85it/s]warmup run: 1204it [00:02, 940.83it/s]warmup run: 1187it [00:02, 912.53it/s]warmup run: 1401it [00:02, 966.51it/s]warmup run: 1214it [00:02, 958.42it/s]warmup run: 1225it [00:02, 957.07it/s]warmup run: 1152it [00:02, 926.95it/s]warmup run: 1205it [00:02, 933.20it/s]warmup run: 1527it [00:03, 981.41it/s]warmup run: 1306it [00:02, 962.84it/s]warmup run: 1285it [00:02, 929.08it/s]warmup run: 1504it [00:03, 982.40it/s]warmup run: 1317it [00:02, 977.72it/s]warmup run: 1327it [00:02, 965.72it/s]warmup run: 1251it [00:02, 944.88it/s]warmup run: 1304it [00:02, 943.72it/s]warmup run: 1630it [00:03, 994.70it/s]warmup run: 1407it [00:02, 974.12it/s]warmup run: 1384it [00:02, 944.44it/s]warmup run: 1607it [00:03, 994.51it/s]warmup run: 1419it [00:02, 989.79it/s]warmup run: 1430it [00:02, 984.27it/s]warmup run: 1351it [00:02, 958.65it/s]warmup run: 1403it [00:02, 925.07it/s]warmup run: 1733it [00:03, 1004.77it/s]warmup run: 1508it [00:03, 984.60it/s]warmup run: 1483it [00:03, 957.22it/s]warmup run: 1709it [00:03, 999.59it/s]warmup run: 1521it [00:03, 998.19it/s]warmup run: 1532it [00:03, 991.46it/s]warmup run: 1451it [00:03, 970.59it/s]warmup run: 1837it [00:03, 1013.91it/s]warmup run: 1499it [00:03, 877.71it/s]warmup run: 1609it [00:03, 991.09it/s]warmup run: 1583it [00:03, 967.08it/s]warmup run: 1811it [00:03, 1002.21it/s]warmup run: 1624it [00:03, 1005.05it/s]warmup run: 1551it [00:03, 975.54it/s]warmup run: 1634it [00:03, 985.13it/s]warmup run: 1942it [00:03, 1022.36it/s]warmup run: 1599it [00:03, 911.45it/s]warmup run: 1710it [00:03, 996.68it/s]warmup run: 1684it [00:03, 978.51it/s]warmup run: 1912it [00:03, 995.56it/s] warmup run: 1726it [00:03, 998.88it/s] warmup run: 1651it [00:03, 982.20it/s]warmup run: 1734it [00:03, 964.02it/s]warmup run: 2053it [00:03, 1046.27it/s]warmup run: 1698it [00:03, 932.74it/s]warmup run: 1811it [00:03, 994.90it/s]warmup run: 1783it [00:03, 972.78it/s]warmup run: 2013it [00:03, 999.14it/s]warmup run: 1753it [00:03, 992.61it/s]warmup run: 1827it [00:03, 989.73it/s]warmup run: 1832it [00:03, 953.43it/s]warmup run: 2176it [00:03, 1099.02it/s]warmup run: 1798it [00:03, 952.08it/s]warmup run: 1912it [00:03, 991.41it/s]warmup run: 1881it [00:03, 962.62it/s]warmup run: 2132it [00:03, 1054.74it/s]warmup run: 1856it [00:03, 1002.04it/s]warmup run: 1927it [00:03, 983.64it/s]warmup run: 1936it [00:03, 977.24it/s]warmup run: 2299it [00:03, 1136.21it/s]warmup run: 1899it [00:03, 966.98it/s]warmup run: 2013it [00:03, 996.41it/s]warmup run: 1978it [00:03, 954.00it/s]warmup run: 2254it [00:03, 1101.28it/s]warmup run: 1959it [00:03, 1007.79it/s]warmup run: 2031it [00:03, 999.70it/s]warmup run: 2047it [00:03, 1015.18it/s]warmup run: 2421it [00:03, 1161.02it/s]warmup run: 1999it [00:03, 975.94it/s]warmup run: 2132it [00:03, 1052.68it/s]warmup run: 2090it [00:03, 1002.41it/s]warmup run: 2376it [00:03, 1134.87it/s]warmup run: 2072it [00:03, 1043.45it/s]warmup run: 2153it [00:03, 1064.21it/s]warmup run: 2169it [00:03, 1074.95it/s]warmup run: 2543it [00:03, 1178.12it/s]warmup run: 2120it [00:03, 1044.05it/s]warmup run: 2251it [00:03, 1092.40it/s]warmup run: 2209it [00:03, 1055.77it/s]warmup run: 2498it [00:03, 1160.03it/s]warmup run: 2195it [00:03, 1097.40it/s]warmup run: 2275it [00:03, 1109.73it/s]warmup run: 2289it [00:03, 1109.87it/s]warmup run: 2665it [00:04, 1188.11it/s]warmup run: 2242it [00:03, 1094.97it/s]warmup run: 2370it [00:03, 1118.36it/s]warmup run: 2328it [00:03, 1094.63it/s]warmup run: 2621it [00:04, 1178.78it/s]warmup run: 2318it [00:03, 1135.18it/s]warmup run: 2397it [00:03, 1142.35it/s]warmup run: 2412it [00:03, 1142.81it/s]warmup run: 2788it [00:04, 1199.19it/s]warmup run: 2364it [00:03, 1130.17it/s]warmup run: 2488it [00:03, 1134.24it/s]warmup run: 2448it [00:03, 1124.45it/s]warmup run: 2743it [00:04, 1189.37it/s]warmup run: 2440it [00:03, 1159.55it/s]warmup run: 2519it [00:03, 1164.25it/s]warmup run: 2534it [00:03, 1164.90it/s]warmup run: 2911it [00:04, 1206.19it/s]warmup run: 2482it [00:03, 1143.20it/s]warmup run: 2606it [00:04, 1145.56it/s]warmup run: 2568it [00:04, 1146.01it/s]warmup run: 2864it [00:04, 1194.06it/s]warmup run: 3000it [00:04, 689.64it/s] warmup run: 2562it [00:04, 1176.45it/s]warmup run: 2640it [00:04, 1175.96it/s]warmup run: 2654it [00:04, 1175.32it/s]warmup run: 2603it [00:04, 1162.02it/s]warmup run: 2723it [00:04, 1150.21it/s]warmup run: 2684it [00:04, 1148.45it/s]warmup run: 2984it [00:04, 1194.87it/s]warmup run: 3000it [00:04, 685.09it/s] warmup run: 2681it [00:04, 1178.67it/s]warmup run: 2762it [00:04, 1186.39it/s]warmup run: 2773it [00:04, 1177.65it/s]warmup run: 2722it [00:04, 1168.48it/s]warmup run: 2843it [00:04, 1163.16it/s]warmup run: 2803it [00:04, 1159.89it/s]warmup run: 2802it [00:04, 1185.12it/s]warmup run: 2884it [00:04, 1194.92it/s]warmup run: 2892it [00:04, 1178.70it/s]warmup run: 2842it [00:04, 1175.50it/s]warmup run: 2963it [00:04, 1171.21it/s]warmup run: 2922it [00:04, 1165.89it/s]warmup run: 3000it [00:04, 682.65it/s] warmup run: 2922it [00:04, 1186.60it/s]warmup run: 3000it [00:04, 692.09it/s] warmup run: 3000it [00:04, 692.96it/s] warmup run: 3000it [00:04, 679.34it/s] warmup run: 2960it [00:04, 1174.22it/s]warmup run: 3000it [00:04, 682.22it/s] warmup run: 3000it [00:04, 682.47it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.37it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1669.09it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.90it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.60it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1664.35it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1632.39it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1653.36it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1652.39it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1660.74it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1632.19it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1669.68it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1660.98it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1628.58it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1668.17it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1637.64it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1663.10it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1662.90it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1627.82it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1664.27it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1660.46it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1635.06it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1664.24it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1661.31it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1621.18it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1664.57it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1627.53it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1664.10it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1661.97it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1632.70it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1659.21it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1661.83it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1616.85it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1663.99it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1631.70it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1662.12it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1656.20it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1660.32it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1630.34it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1659.17it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1610.46it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1659.41it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1629.97it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1650.13it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1655.81it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1654.75it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1623.75it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1649.45it/s]warmup should be done:  33%|      | 977/3000 [00:00<00:01, 1605.00it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1632.36it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1659.77it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1654.72it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1649.44it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1653.16it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1623.34it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1612.91it/s]warmup should be done:  39%|      | 1166/3000 [00:00<00:01, 1640.15it/s]warmup should be done:  44%|     | 1333/3000 [00:00<00:01, 1659.64it/s]warmup should be done:  44%|     | 1310/3000 [00:00<00:01, 1633.53it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1654.61it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1648.44it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1624.03it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1652.68it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1619.98it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1599.50it/s]warmup should be done:  50%|     | 1499/3000 [00:00<00:00, 1658.67it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1633.23it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1647.29it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1650.56it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1620.45it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1624.06it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1648.06it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1592.33it/s]warmup should be done:  56%|    | 1665/3000 [00:01<00:00, 1658.68it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1632.98it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1646.20it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1625.41it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1611.54it/s]warmup should be done:  56%|    | 1667/3000 [00:01<00:00, 1637.48it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1636.58it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1595.53it/s]warmup should be done:  61%|    | 1831/3000 [00:01<00:00, 1658.55it/s]warmup should be done:  60%|    | 1802/3000 [00:01<00:00, 1632.92it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1645.26it/s]warmup should be done:  60%|    | 1794/3000 [00:01<00:00, 1618.87it/s]warmup should be done:  60%|    | 1799/3000 [00:01<00:00, 1606.81it/s]warmup should be done:  61%|    | 1831/3000 [00:01<00:00, 1631.11it/s]warmup should be done:  61%|    | 1830/3000 [00:01<00:00, 1629.79it/s]warmup should be done:  60%|    | 1813/3000 [00:01<00:00, 1577.10it/s]warmup should be done:  67%|   | 1997/3000 [00:01<00:00, 1657.76it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1632.47it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1644.84it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1618.62it/s]warmup should be done:  66%|   | 1995/3000 [00:01<00:00, 1630.85it/s]warmup should be done:  66%|   | 1993/3000 [00:01<00:00, 1627.67it/s]warmup should be done:  65%|   | 1960/3000 [00:01<00:00, 1603.51it/s]warmup should be done:  66%|   | 1979/3000 [00:01<00:00, 1599.15it/s]warmup should be done:  72%|  | 2163/3000 [00:01<00:00, 1658.36it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1633.08it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1644.45it/s]warmup should be done:  71%|   | 2120/3000 [00:01<00:00, 1622.15it/s]warmup should be done:  72%|  | 2156/3000 [00:01<00:00, 1627.61it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1631.11it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1601.05it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1616.47it/s]warmup should be done:  78%|  | 2329/3000 [00:01<00:00, 1656.75it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1630.87it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1641.27it/s]warmup should be done:  77%|  | 2321/3000 [00:01<00:00, 1633.00it/s]warmup should be done:  77%|  | 2323/3000 [00:01<00:00, 1633.00it/s]warmup should be done:  76%|  | 2282/3000 [00:01<00:00, 1598.05it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1592.47it/s]warmup should be done:  77%|  | 2307/3000 [00:01<00:00, 1600.65it/s]warmup should be done:  83%| | 2495/3000 [00:01<00:00, 1655.52it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1630.05it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1643.55it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1637.48it/s]warmup should be done:  83%| | 2487/3000 [00:01<00:00, 1638.22it/s]warmup should be done:  81%| | 2442/3000 [00:01<00:00, 1596.42it/s]warmup should be done:  82%| | 2445/3000 [00:01<00:00, 1598.18it/s]warmup should be done:  82%| | 2471/3000 [00:01<00:00, 1611.79it/s]warmup should be done:  89%| | 2661/3000 [00:01<00:00, 1654.42it/s]warmup should be done:  87%| | 2622/3000 [00:01<00:00, 1630.17it/s]warmup should be done:  88%| | 2650/3000 [00:01<00:00, 1643.60it/s]warmup should be done:  88%| | 2653/3000 [00:01<00:00, 1641.18it/s]warmup should be done:  88%| | 2652/3000 [00:01<00:00, 1641.39it/s]warmup should be done:  87%| | 2602/3000 [00:01<00:00, 1594.01it/s]warmup should be done:  87%| | 2607/3000 [00:01<00:00, 1603.46it/s]warmup should be done:  88%| | 2633/3000 [00:01<00:00, 1612.32it/s]warmup should be done:  94%|| 2827/3000 [00:01<00:00, 1655.75it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1630.33it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1645.03it/s]warmup should be done:  94%|| 2817/3000 [00:01<00:00, 1643.92it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1643.97it/s]warmup should be done:  92%|| 2762/3000 [00:01<00:00, 1593.99it/s]warmup should be done:  92%|| 2768/3000 [00:01<00:00, 1604.07it/s]warmup should be done:  93%|| 2798/3000 [00:01<00:00, 1621.21it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1660.54it/s]warmup should be done:  98%|| 2951/3000 [00:01<00:00, 1635.41it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.71it/s]warmup should be done:  99%|| 2982/3000 [00:01<00:00, 1650.20it/s]warmup should be done: 100%|| 2985/3000 [00:01<00:00, 1652.19it/s]warmup should be done: 100%|| 2986/3000 [00:01<00:00, 1649.64it/s]warmup should be done:  97%|| 2924/3000 [00:01<00:00, 1598.97it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1607.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1648.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.76it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.39it/s]warmup should be done:  99%|| 2963/3000 [00:01<00:00, 1628.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1621.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.21it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.52it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.86it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1695.40it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1694.94it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.08it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.13it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1694.00it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.82it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1647.84it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1701.96it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1666.28it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1641.88it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1681.02it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1699.97it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1699.56it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1649.21it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1651.45it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1704.06it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1644.20it/s]warmup should be done:  17%|        | 502/3000 [00:00<00:01, 1670.02it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1683.27it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1702.13it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1706.17it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.76it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1653.30it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1674.13it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1682.03it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1641.63it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1700.97it/s]warmup should be done:  23%|       | 685/3000 [00:00<00:01, 1708.48it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1655.51it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1700.79it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1653.10it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1676.13it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1653.58it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1699.95it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1680.17it/s]warmup should be done:  29%|       | 856/3000 [00:00<00:01, 1703.46it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1698.65it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1635.30it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1654.90it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1676.25it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1655.29it/s]warmup should be done:  34%|      | 1013/3000 [00:00<00:01, 1681.01it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1700.41it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1699.18it/s]warmup should be done:  33%|      | 988/3000 [00:00<00:01, 1633.99it/s]warmup should be done:  34%|      | 1027/3000 [00:00<00:01, 1690.64it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1657.28it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1674.19it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1656.02it/s]warmup should be done:  39%|      | 1182/3000 [00:00<00:01, 1681.52it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1701.09it/s]warmup should be done:  40%|      | 1195/3000 [00:00<00:01, 1699.39it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1632.35it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1683.42it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1663.77it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1675.33it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1652.24it/s]warmup should be done:  46%|     | 1367/3000 [00:00<00:00, 1702.12it/s]warmup should be done:  45%|     | 1351/3000 [00:00<00:00, 1681.18it/s]warmup should be done:  46%|     | 1366/3000 [00:00<00:00, 1700.07it/s]warmup should be done:  44%|     | 1316/3000 [00:00<00:01, 1626.88it/s]warmup should be done:  46%|     | 1366/3000 [00:00<00:00, 1678.70it/s]warmup should be done:  50%|     | 1498/3000 [00:00<00:00, 1670.01it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1674.09it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1652.28it/s]warmup should be done:  51%|     | 1520/3000 [00:00<00:00, 1681.44it/s]warmup should be done:  51%|    | 1538/3000 [00:00<00:00, 1698.56it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1627.77it/s]warmup should be done:  51%|     | 1537/3000 [00:00<00:00, 1692.06it/s]warmup should be done:  51%|     | 1534/3000 [00:00<00:00, 1676.79it/s]warmup should be done:  56%|    | 1667/3000 [00:01<00:00, 1675.72it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1674.31it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1653.93it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1684.06it/s]warmup should be done:  57%|    | 1708/3000 [00:01<00:00, 1694.26it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1630.37it/s]warmup should be done:  57%|    | 1707/3000 [00:01<00:00, 1688.19it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1676.88it/s]warmup should be done:  61%|    | 1836/3000 [00:01<00:00, 1679.84it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1675.13it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1654.41it/s]warmup should be done:  62%|   | 1859/3000 [00:01<00:00, 1685.83it/s]warmup should be done:  63%|   | 1878/3000 [00:01<00:00, 1694.81it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1632.22it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1686.00it/s]warmup should be done:  62%|   | 1871/3000 [00:01<00:00, 1678.46it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1680.76it/s]warmup should be done:  67%|   | 2015/3000 [00:01<00:00, 1675.44it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  68%|   | 2049/3000 [00:01<00:00, 1699.33it/s]warmup should be done:  68%|   | 2028/3000 [00:01<00:00, 1680.01it/s]warmup should be done:  66%|   | 1973/3000 [00:01<00:00, 1638.05it/s]warmup should be done:  68%|   | 2045/3000 [00:01<00:00, 1684.01it/s]warmup should be done:  68%|   | 2040/3000 [00:01<00:00, 1681.63it/s]warmup should be done:  72%|  | 2174/3000 [00:01<00:00, 1680.70it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1673.96it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1651.46it/s]warmup should be done:  74%|  | 2219/3000 [00:01<00:00, 1699.10it/s]warmup should be done:  71%|  | 2139/3000 [00:01<00:00, 1642.42it/s]warmup should be done:  74%|  | 2214/3000 [00:01<00:00, 1684.95it/s]warmup should be done:  73%|  | 2197/3000 [00:01<00:00, 1669.28it/s]warmup should be done:  74%|  | 2209/3000 [00:01<00:00, 1682.47it/s]warmup should be done:  78%|  | 2344/3000 [00:01<00:00, 1684.12it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1676.26it/s]warmup should be done:  80%|  | 2389/3000 [00:01<00:00, 1699.16it/s]warmup should be done:  78%|  | 2325/3000 [00:01<00:00, 1654.22it/s]warmup should be done:  77%|  | 2305/3000 [00:01<00:00, 1647.48it/s]warmup should be done:  80%|  | 2385/3000 [00:01<00:00, 1689.68it/s]warmup should be done:  79%|  | 2379/3000 [00:01<00:00, 1685.92it/s]warmup should be done:  79%|  | 2364/3000 [00:01<00:00, 1661.85it/s]warmup should be done:  84%| | 2514/3000 [00:01<00:00, 1686.25it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1677.52it/s]warmup should be done:  85%| | 2560/3000 [00:01<00:00, 1701.94it/s]warmup should be done:  83%| | 2491/3000 [00:01<00:00, 1655.08it/s]warmup should be done:  82%| | 2472/3000 [00:01<00:00, 1651.29it/s]warmup should be done:  85%| | 2556/3000 [00:01<00:00, 1695.10it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1693.28it/s]warmup should be done:  84%| | 2531/3000 [00:01<00:00, 1662.92it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1685.65it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1676.20it/s]warmup should be done:  91%| | 2731/3000 [00:01<00:00, 1703.03it/s]warmup should be done:  89%| | 2657/3000 [00:01<00:00, 1653.49it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1648.93it/s]warmup should be done:  91%| | 2727/3000 [00:01<00:00, 1698.49it/s]warmup should be done:  90%| | 2700/3000 [00:01<00:00, 1670.05it/s]warmup should be done:  91%| | 2721/3000 [00:01<00:00, 1688.29it/s]warmup should be done:  95%|| 2852/3000 [00:01<00:00, 1685.46it/s]warmup should be done:  95%|| 2857/3000 [00:01<00:00, 1674.66it/s]warmup should be done:  97%|| 2902/3000 [00:01<00:00, 1703.09it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1653.08it/s]warmup should be done:  93%|| 2804/3000 [00:01<00:00, 1651.32it/s]warmup should be done:  97%|| 2897/3000 [00:01<00:00, 1698.16it/s]warmup should be done:  96%|| 2869/3000 [00:01<00:00, 1673.69it/s]warmup should be done:  96%|| 2890/3000 [00:01<00:00, 1684.45it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1700.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1694.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.49it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.85it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.86it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1673.68it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1655.54it/s]warmup should be done:  99%|| 2971/3000 [00:01<00:00, 1655.64it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1653.49it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1641.89it/s]2022-12-12 03:34:33.292102: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f326382cac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:33.292154: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.172363: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f14980297b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.172427: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.178654: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f32638303e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.178710: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.782843: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f32638305d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.782910: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.798804: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f32677961e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.798869: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.881826: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3263834600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.881906: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.893620: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3263831070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.893688: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:34.896764: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3263832d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:34:34.896829: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:34:35.529294: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:36.442129: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:36.442487: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:37.073820: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:37.089357: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:37.133907: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:37.141063: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:37.216350: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:34:38.509360: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:39.306793: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:39.314650: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:40.003957: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:40.004211: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:40.031768: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:40.055227: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:34:40.112277: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:35:05.966][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:35:05.966][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:05.976][ERROR][RK0][main]: coll ps creation done
[HCTR][03:35:05.976][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][03:35:06.064][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:35:06.064][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.067][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:35:06.067][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.073][ERROR][RK0][main]: coll ps creation done
[HCTR][03:35:06.073][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][03:35:06.076][ERROR][RK0][main]: coll ps creation done
[HCTR][03:35:06.076][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][03:35:06.152][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:35:06.152][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.153][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:35:06.153][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.157][ERROR][RK0][main]: coll ps creation done
[HCTR][03:35:06.157][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][03:35:06.160][ERROR][RK0][main]: coll ps creation done
[HCTR][03:35:06.160][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][03:35:06.166][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:35:06.166][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.174][ERROR][RK0][main]: coll ps creation done
[HCTR][03:35:06.174][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][03:35:06.253][ERROR][RK0][tid #139854537029376]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:35:06.253][ERROR][RK0][tid #139854537029376]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.255][ERROR][RK0][tid #139855006791424]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:35:06.255][ERROR][RK0][tid #139855006791424]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][03:35:06.258][ERROR][RK0][tid #139854537029376]: coll ps creation done
[HCTR][03:35:06.258][ERROR][RK0][tid #139854537029376]: replica 1 waits for coll ps creation barrier
[HCTR][03:35:06.258][ERROR][RK0][tid #139855006791424]: coll ps creation done
[HCTR][03:35:06.258][ERROR][RK0][tid #139855006791424]: replica 0 waits for coll ps creation barrier
[HCTR][03:35:06.258][ERROR][RK0][tid #139855006791424]: replica 0 preparing frequency
[HCTR][03:35:07.128][ERROR][RK0][tid #139855006791424]: replica 0 preparing frequency done
[HCTR][03:35:07.168][ERROR][RK0][tid #139855006791424]: replica 0 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][tid #139854537029376]: replica 1 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][03:35:07.168][ERROR][RK0][tid #139855006791424]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][main]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][main]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][main]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][main]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][main]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][tid #139854537029376]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][tid #139855006791424]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][main]: Calling build_v2
[HCTR][03:35:07.168][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][tid #139854537029376]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:35:07.168][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 03:35:07[[[2022-12-12 03:35:072022-12-12 03:35:072022-12-12 03:35:07.2022-12-12 03:35:072022-12-12 03:35:07...2022-12-12 03:35:07168340..2022-12-12 03:35:07168346168346168353.: 168358168363.: : : 168359E: : 168381EEE:  EE:    E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136:] 136136:] ] ] 136] using concurrent impl MPS] ] 136using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS] 





using concurrent impl MPS
[2022-12-12 03:35:07.172590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:35:07.172628: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:35:07:.196172634] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:35:07[.2022-12-12 03:35:07172689.: [172686E2022-12-12 03:35:07:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc172703 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196E:] [ 178assigning 8 to cpu2022-12-12 03:35:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
.:v100x8, slow pcie172731212
: ] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:35:07[:.2022-12-12 03:35:07178[172776.] [2022-12-12 03:35:07: 172777v100x8, slow pcie2022-12-12 03:35:07.E: 
.172794 E172800: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ : E[:2022-12-12 03:35:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 2022-12-12 03:35:07196.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] 172831178[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:172830assigning 8 to cpu: ] 2022-12-12 03:35:07:212: 
Ev100x8, slow pcie[.213] E 
2022-12-12 03:35:07172872] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[: [remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:1729152022-12-12 03:35:07E2022-12-12 03:35:07
:196: [. .178[] E2022-12-12 03:35:07172967/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc172969] 2022-12-12 03:35:07assigning 8 to cpu .: :: v100x8, slow pcie.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc173014E178E
173033:[:  ]  : 1782022-12-12 03:35:07[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] .2022-12-12 03:35:07 :
: v100x8, slow pcie173143./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
: 173149:] ] :2022-12-12 03:35:07E: [213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 8 to cpu214. E2022-12-12 03:35:07] 

] 173216/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .remote time is 8.68421cpu time is 97.0588: :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc173245

E1962022-12-12 03:35:07:: [ [] .212E2022-12-12 03:35:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:35:07assigning 8 to cpu173311]  .:.
: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc173344196173350E
:: ] :  196E[assigning 8 to cpuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  [2022-12-12 03:35:07
 :assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:35:07./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
[:.173469:] 2022-12-12 03:35:07212173482[: 214remote time is 8.68421.] : 2022-12-12 03:35:07E] 
173552build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E. cpu time is 97.0588: [
 173586/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E2022-12-12 03:35:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :[ .:E2132022-12-12 03:35:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc173626212 ] .:: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421173665212Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:
: ]  
212Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  
2022-12-12 03:35:07:[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2142022-12-12 03:35:07
[:173770] .2022-12-12 03:35:07213: [cpu time is 97.0588173791.] E2022-12-12 03:35:07
: 173817remote time is 8.68421 .E: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc173843 E:: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214E2022-12-12 03:35:07:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  .213:cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc173914] 213
:: remote time is 8.68421] 213E
remote time is 8.68421]  
remote time is 8.68421[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 03:35:07[:.2022-12-12 03:35:07[214174023.2022-12-12 03:35:07] : 174037.cpu time is 97.0588E: 174049
 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 214:cpu time is 97.0588] 214
cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 03:36:26.784954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:36:26.825229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 03:36:26.933031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:36:26.933090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:36:27. 23299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:36:27. 23339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:36:27. 23883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:36:27. 23933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 24910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 25753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 38540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 03:36:27. 38601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 03:36:27. 39028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:36:27. 39083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 03:36:27.2022-12-12 03:36:27 39275.:  39280E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 202] 1 solved5 solved

[[[2022-12-12 03:36:272022-12-12 03:36:272022-12-12 03:36:27... 39339 39354 39356: : : EEE  [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 03:36:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::.:202205 39368205] ] : ] 3 solvedworker 0 thread 1 initing device 1Eworker 0 thread 5 initing device 5

 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 03:36:27202.]  394892 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 3 initing device 32022-12-12 03:36:27
. 39533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 03:36:27.[ 398902022-12-12 03:36:27: .E 39898 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Building Coll Cache with ... num gpu device is 81815
] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:36:27.[ 399482022-12-12 03:36:27: [.E2022-12-12 03:36:27 39956 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 39962E::  1815E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu]  [:Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:36:271980
:.] [1980 39994eager alloc mem 381.47 MB[2022-12-12 03:36:27] : 
2022-12-12 03:36:27.[eager alloc mem 381.47 MBE. 400142022-12-12 03:36:27
  40043: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E 40056:E : [1815 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE2022-12-12 03:36:27] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: .Building Coll Cache with ... num gpu device is 8:202/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 40108
1980] :: ] 7 solved1980Eeager alloc mem 381.47 MB
] [ 
eager alloc mem 381.47 MB2022-12-12 03:36:27[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
.2022-12-12 03:36:27: 40216.202:  40230] E: 4 solved E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[1980:2022-12-12 03:36:27] 205.eager alloc mem 381.47 MB]  40317
worker 0 thread 7 initing device 7: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 03:36:27. 40772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:36:27. 40804: [E2022-12-12 03:36:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 40816:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 40892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 44514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 44803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 44843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 44876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 44959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 45505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 45569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 49069: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 49260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 49297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 49450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 49507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27. 50003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:36:27.103657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 03:36:27.108971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.109105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.109936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.110588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.111677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.111729: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:36:27.129486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-12 03:36:272022-12-12 03:36:27..134513[134513: [2022-12-12 03:36:27: E2022-12-12 03:36:27.E .134561 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu134565: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: E:1980E 1980]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 5.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 5.00 Bytes
:1980
1980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 03:36:27.134837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.134928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.136006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.136714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 03:36:272022-12-12 03:36:27..137324137324: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 03:36:27.137765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.137815: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:36:27.141159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.141244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.141277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.141363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.141376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.141456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 03:36:27eager release cuda mem 400000000.
141460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.141553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.142214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.142543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 03:36:27.142599: E[ 2022-12-12 03:36:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:142626638: ] Eeager release cuda mem 5 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.142685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:36:27.143247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.143810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.144444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.145554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.145880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.146449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:36:27.146587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.146634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:36:27.147222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.147279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.147314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.147650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.147740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:27.148251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.148297: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 03:36:27:.43148305] : WORKER[0] alloc host memory 57.22 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 03:36:27
.148338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.148369: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 03:36:27WORKER[0] alloc host memory 57.22 MB.
148387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:36:27.148684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.148736: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:36:27.148772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:27.148821: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:36:27.152126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.152770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.152816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.174937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.175578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.175623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.185170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.185801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.185845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.186499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.186665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.186862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.186951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.187141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.187186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.187283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.187326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.187352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:36:27.187482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.187527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.187577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.187623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:36:27.187965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:36:27.188011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[[[[[[[[2022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:29........921037921036921038921037921037921037921037921037: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 7 init p2p of link 4Device 4 init p2p of link 5Device 0 init p2p of link 3Device 5 init p2p of link 6Device 3 init p2p of link 2Device 1 init p2p of link 7Device 6 init p2p of link 0







[[2022-12-12 03:36:292022-12-12 03:36:29[.[[[.[2022-12-12 03:36:29[9215942022-12-12 03:36:292022-12-12 03:36:292022-12-12 03:36:299215942022-12-12 03:36:29.2022-12-12 03:36:29: ...: .921600.E921602921603921603E921610: 921613 : : :  : E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E:   : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :::] :1980:eager alloc mem 611.00 KB198019801980eager alloc mem 611.00 KB1980] 1980
] ] ] 
] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB




[[2022-12-12 03:36:292022-12-12 03:36:29..922685922687: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 03:36:29[.2022-12-12 03:36:29922751.: 922757E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[ [2022-12-12 03:36:29:2022-12-12 03:36:29/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:36:29.[638.:.9227762022-12-12 03:36:29] 922781638922785: .eager release cuda mem 625663: ] : E922807
Eeager release cuda mem 625663E :  
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ::638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638] :] ] eager release cuda mem 625663638eager release cuda mem 625663eager release cuda mem 625663
] 

eager release cuda mem 625663
[2022-12-12 03:36:29.935965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 03:36:29.936133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.936263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 03:36:29.936423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.937023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.937333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.943342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 03:36:29.943524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.943808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 03:36:29.[9439572022-12-12 03:36:29: .E943951 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1926
] Device 2 init p2p of link 3
[2022-12-12 03:36:29.944039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 03:36:29.944181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.944213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.944429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.944725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 03:36:29.[9448982022-12-12 03:36:29: .E944905 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.944988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.945102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.945836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.946981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 03:36:29.947202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.948004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.948333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 03:36:29.948376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 03:36:29.948467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.948495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.949343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.949372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.957189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 03:36:29.957320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.958234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.959370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 03:36:29.959665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.960596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.963609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 03:36:29.963753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.963848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 03:36:29.963970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.964259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 03:36:29.964400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.964546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.964750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.965177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 03:36:29.965276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.965318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.966202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.971889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 03:36:29.972009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.972174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 03:36:29.972291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.972908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.973192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.973648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 03:36:29.973769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.973928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 03:36:29.974041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.974656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.974931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.976654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 03:36:29.976768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.977423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 03:36:29.977555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.977660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.978453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.986580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 03:36:29.986701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.986951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 03:36:29.987075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:36:29.987473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.987854: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:36:29.991383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.991723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.992090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.95189 secs 
[2022-12-12 03:36:29.992327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.95238 secs 
[2022-12-12 03:36:29.994673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.995138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.996808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.95593 secs 
[2022-12-12 03:36:29.996950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.95691 secs 
[2022-12-12 03:36:29.997280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.997665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.998337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.95926 secs 
[2022-12-12 03:36:29.999188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.999400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:36:29.999707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.95975 secs 
[2022-12-12 03:36:30.  2008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.9612 secs 
[2022-12-12 03:36:30.  2272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 15000000 / 100000000 nodes ( 15.00 %~15.00 %) | remote 45000000 / 100000000 nodes ( 45.00 %) | cpu 40000000 / 100000000 nodes ( 40.00 %) | 7.16 GB | 2.97835 secs 
[2022-12-12 03:36:30.  3761: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.80 GB
[2022-12-12 03:36:31.626402: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.06 GB
[2022-12-12 03:36:31.626766: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.06 GB
[2022-12-12 03:36:31.627822: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.06 GB
[2022-12-12 03:36:33.334487: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.32 GB
[2022-12-12 03:36:33.334871: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.32 GB
[2022-12-12 03:36:33.340397: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.32 GB
[2022-12-12 03:36:35.177554: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.54 GB
[2022-12-12 03:36:35.177725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.54 GB
[2022-12-12 03:36:35.178012: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.54 GB
[2022-12-12 03:36:37.322561: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.75 GB
[2022-12-12 03:36:37.323698: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.75 GB
[2022-12-12 03:36:37.325168: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.75 GB
[2022-12-12 03:36:39.744310: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.21 GB
[2022-12-12 03:36:39.750027: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.21 GB
[2022-12-12 03:36:39.754724: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.21 GB
[2022-12-12 03:36:42.188833: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.41 GB
[2022-12-12 03:36:42.189110: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.41 GB
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][tid #139854537029376]: replica 1 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][tid #139855006791424]: replica 0 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][tid #139855006791424]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][tid #139854537029376]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:36:42.241][ERROR][RK0][main]: init per replica done
[HCTR][03:36:42.241][ERROR][RK0][main]: init per replica done
[HCTR][03:36:42.241][ERROR][RK0][main]: init per replica done
[HCTR][03:36:42.241][ERROR][RK0][main]: init per replica done
[HCTR][03:36:42.241][ERROR][RK0][tid #139854537029376]: init per replica done
[HCTR][03:36:42.241][ERROR][RK0][main]: init per replica done
[HCTR][03:36:42.241][ERROR][RK0][main]: init per replica done
[HCTR][03:36:42.243][ERROR][RK0][tid #139855006791424]: init per replica done
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 5 allocated 3276800 at 0x7f3459320000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 5 allocated 6553600 at 0x7f178a800000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 5 allocated 3276800 at 0x7f178ae40000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 5 allocated 6553600 at 0x7f178b160000
[HCTR][03:36:42.247][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f3459320000
[HCTR][03:36:42.247][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f18ea800000
[HCTR][03:36:42.247][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f18eae40000
[HCTR][03:36:42.247][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f18eb160000
[HCTR][03:36:42.247][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f3459320000
[HCTR][03:36:42.247][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f1918800000
[HCTR][03:36:42.247][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f1918e40000
[HCTR][03:36:42.247][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f1919160000
[HCTR][03:36:42.247][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3459320000
[HCTR][03:36:42.247][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f185e800000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 6 allocated 3276800 at 0x7f3459320000
[HCTR][03:36:42.247][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f185ee40000
[HCTR][03:36:42.247][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f185f160000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 6 allocated 6553600 at 0x7f18ea800000
[HCTR][03:36:42.247][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3453320000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 6 allocated 3276800 at 0x7f18eae40000
[HCTR][03:36:42.247][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f17c2800000
[HCTR][03:36:42.247][ERROR][RK0][tid #139854537029376]: 6 allocated 6553600 at 0x7f18eb160000
[HCTR][03:36:42.247][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f17c2e40000
[HCTR][03:36:42.247][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f17c3160000
[HCTR][03:36:42.247][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f3459320000
[HCTR][03:36:42.247][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f18e2800000
[HCTR][03:36:42.247][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f18e2e40000
[HCTR][03:36:42.247][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f18e3160000
[HCTR][03:36:42.250][ERROR][RK0][tid #139855006791424]: 0 allocated 3276800 at 0x7f1b84d20000
[HCTR][03:36:42.250][ERROR][RK0][tid #139855006791424]: 0 allocated 6553600 at 0x7f1b85200000
[HCTR][03:36:42.250][ERROR][RK0][tid #139855006791424]: 0 allocated 3276800 at 0x7f180250e800
[HCTR][03:36:42.250][ERROR][RK0][tid #139855006791424]: 0 allocated 6553600 at 0x7f180282e800








