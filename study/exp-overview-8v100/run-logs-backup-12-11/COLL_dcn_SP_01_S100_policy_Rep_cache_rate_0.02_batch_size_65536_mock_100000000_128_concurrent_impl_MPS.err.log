2022-12-11 22:17:20.431830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.437791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.445204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.452143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.463207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.469168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.473764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.487045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.545341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.546827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.547118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.548217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.548993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.549710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.550857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.551368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.552430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.552883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.554104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.554434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.555885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.555978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.557492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.557535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.559298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.559348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.561064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.562162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.563252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.564326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.565388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.566416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.568248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.569275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.570233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.571158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.572146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.573113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.574140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.575153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.580615: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.582365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.583749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.584216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.585935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.586040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.586287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.588231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.588313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.588387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.589725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.590686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.590957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.590990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.592263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.593729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.594209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.594236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.595125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.597047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.597539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.597646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.599050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.600197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.600743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.600792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.602836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.603788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.604596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.606052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.606591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.607490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.607918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.609111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.609745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.610687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.611915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.612770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.613451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.614372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.614392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.615306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.616124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.616932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.617109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.618009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.618867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.619507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.619841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.620779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.621627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.622035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.622415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.623920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.624014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.624554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.627236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.627285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.628022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.629206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.629871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.630943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.649503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.651112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.656423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.665123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.666513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.667020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.667059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.667169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.667179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.670748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.671426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.671571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.671634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.671678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.672944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.674734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.675652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.675697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.676556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.676612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.678219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.679491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.680562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.680604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.680698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.680807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.682285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.684338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.685116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.685158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.685254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.685315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.686663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.689558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.689588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.689602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.689651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.689765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.691261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.693743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.693942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.693957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.694006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.694213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.695357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.697963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.698100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.698118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.698124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.698278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.699279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.701760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.701875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.701940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.702027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.702070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.703209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.706141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.706391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.706426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.706501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.706580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.707538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.710285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.710427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.710474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.710563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.710628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.711671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.712847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.714511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.714659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.714704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.714787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.714865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.715801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.717410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.719471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.719565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.719662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.719810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.719884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.720849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.722559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.724266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.724360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.724451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.724677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.725533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.726938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.728318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.728413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.728504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.728631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.729161: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.729688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.730855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.733019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.734047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.734114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.734186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.734270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.734762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.736884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.737891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.738114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.738241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.738395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.738553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.738793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.741717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.742867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.742955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.743104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.743110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.743345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.743440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.746143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.747584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.747642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.747843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.747914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.747928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.748035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.751059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.752445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.752712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.752848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.753016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.753250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.755900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.757244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.757619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.757750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.757858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.757966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.760712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.761990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.762255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.762380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.762540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.762613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.766033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.766413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.766933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.767210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.767335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.767433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.771045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.771827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.772107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.774945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.775016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.775170: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.775800: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.776070: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.777461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.777886: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.780724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.780766: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.782602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.784306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.784442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.784914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.785730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.787038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.789322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.789505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.789564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.789918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.791244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.791634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.794279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.794358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.823552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.823864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.824729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.825260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.856690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.857266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.863067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.868487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.879784: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:17:20.888405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.894037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:20.898817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.895878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.896494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.897030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.897488: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:21.897545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:17:21.915598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.921923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.922446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.923041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.923565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:21.924215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:17:21.970460: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:21.970681: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.019908: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:17:22.087517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.088570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.089141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.089602: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.089663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:17:22.107158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.108415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.108937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.109506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.110039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.110501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:17:22.159716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.160427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.160966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.161426: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.161478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:17:22.179213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.179865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.180645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.181297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.181828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.182536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:17:22.193183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.193183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.194442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.194527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.195375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.195504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.196221: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.196277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:17:22.196337: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.196384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:17:22.196916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.197524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.197516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.198758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.198897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.199774: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.199824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:17:22.200037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.200565: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.200614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:17:22.202189: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.202344: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.204114: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:17:22.204183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.204784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.205304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.205778: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:17:22.205829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:17:22.214142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.214585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.214896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.215643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.215742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.216548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.216741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.217505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.217597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.218204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.218317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.218404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.218769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:17:22.219914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.219958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:17:22.219999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.220818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.220942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.221839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.221936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.222809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.222906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.223686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:17:22.223803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:17:22.223985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.224630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.225161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.225746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.226272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:17:22.226751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:17:22.263781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.264012: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.264111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.264261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.265798: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 22:17:22.266318: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:17:22.268203: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.268347: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.269504: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.269639: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.270162: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:17:22.271556: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:17:22.272913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.273044: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.275003: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 22:17:22.293130: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.293322: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:17:22.295320: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][22:17:23.566][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.566][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.566][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.566][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.566][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.566][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.620][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:17:23.620][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.58s/it]warmup run: 100it [00:01, 82.48it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 96it [00:01, 79.41it/s]warmup run: 200it [00:01, 179.34it/s]warmup run: 102it [00:01, 87.72it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 192it [00:01, 172.63it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 299it [00:01, 285.57it/s]warmup run: 205it [00:01, 191.01it/s]warmup run: 100it [00:01, 86.77it/s]warmup run: 285it [00:01, 272.12it/s]warmup run: 100it [00:01, 87.11it/s]warmup run: 100it [00:01, 86.29it/s]warmup run: 98it [00:01, 84.93it/s]warmup run: 101it [00:01, 86.16it/s]warmup run: 398it [00:01, 396.27it/s]warmup run: 307it [00:01, 303.28it/s]warmup run: 199it [00:01, 186.58it/s]warmup run: 379it [00:01, 377.35it/s]warmup run: 200it [00:01, 188.40it/s]warmup run: 199it [00:01, 185.66it/s]warmup run: 197it [00:01, 184.75it/s]warmup run: 203it [00:01, 187.71it/s]warmup run: 495it [00:02, 501.65it/s]warmup run: 410it [00:01, 420.85it/s]warmup run: 299it [00:01, 297.15it/s]warmup run: 473it [00:02, 480.89it/s]warmup run: 301it [00:01, 300.72it/s]warmup run: 298it [00:01, 294.88it/s]warmup run: 296it [00:01, 294.14it/s]warmup run: 301it [00:01, 294.25it/s]warmup run: 593it [00:02, 600.73it/s]warmup run: 514it [00:02, 536.31it/s]warmup run: 390it [00:01, 396.63it/s]warmup run: 570it [00:02, 581.53it/s]warmup run: 402it [00:01, 416.59it/s]warmup run: 398it [00:01, 408.89it/s]warmup run: 396it [00:01, 408.40it/s]warmup run: 399it [00:01, 404.68it/s]warmup run: 691it [00:02, 686.00it/s]warmup run: 620it [00:02, 646.59it/s]warmup run: 490it [00:02, 509.55it/s]warmup run: 667it [00:02, 669.53it/s]warmup run: 502it [00:01, 527.00it/s]warmup run: 498it [00:02, 519.37it/s]warmup run: 496it [00:02, 510.31it/s]warmup run: 497it [00:02, 520.94it/s]warmup run: 787it [00:02, 751.46it/s]warmup run: 726it [00:02, 740.95it/s]warmup run: 588it [00:02, 608.37it/s]warmup run: 763it [00:02, 740.41it/s]warmup run: 604it [00:02, 631.40it/s]warmup run: 596it [00:02, 616.05it/s]warmup run: 598it [00:02, 623.46it/s]warmup run: 589it [00:02, 593.77it/s]warmup run: 884it [00:02, 806.82it/s]warmup run: 831it [00:02, 816.46it/s]warmup run: 686it [00:02, 693.53it/s]warmup run: 860it [00:02, 799.91it/s]warmup run: 705it [00:02, 719.57it/s]warmup run: 696it [00:02, 704.66it/s]warmup run: 699it [00:02, 712.52it/s]warmup run: 681it [00:02, 665.90it/s]warmup run: 983it [00:02, 855.26it/s]warmup run: 936it [00:02, 876.46it/s]warmup run: 784it [00:02, 764.14it/s]warmup run: 956it [00:02, 842.61it/s]warmup run: 808it [00:02, 795.19it/s]warmup run: 797it [00:02, 779.30it/s]warmup run: 800it [00:02, 785.88it/s]warmup run: 773it [00:02, 724.78it/s]warmup run: 1080it [00:02, 886.99it/s]warmup run: 1040it [00:02, 920.32it/s]warmup run: 882it [00:02, 820.28it/s]warmup run: 1053it [00:02, 877.43it/s]warmup run: 909it [00:02, 851.28it/s]warmup run: 900it [00:02, 842.84it/s]warmup run: 901it [00:02, 843.27it/s]warmup run: 864it [00:02, 770.38it/s]warmup run: 1179it [00:02, 914.68it/s]warmup run: 1145it [00:02, 954.25it/s]warmup run: 980it [00:02, 863.29it/s]warmup run: 1150it [00:02, 903.25it/s]warmup run: 1011it [00:02, 896.48it/s]warmup run: 1002it [00:02, 889.00it/s]warmup run: 964it [00:02, 830.67it/s]warmup run: 1277it [00:02, 925.30it/s]warmup run: 1249it [00:02, 975.91it/s]warmup run: 1000it [00:02, 837.44it/s]warmup run: 1080it [00:02, 899.64it/s]warmup run: 1247it [00:02, 920.14it/s]warmup run: 1114it [00:02, 932.32it/s]warmup run: 1103it [00:02, 921.43it/s]warmup run: 1060it [00:02, 866.03it/s]warmup run: 1377it [00:02, 946.16it/s]warmup run: 1354it [00:02, 995.07it/s]warmup run: 1094it [00:02, 861.65it/s]warmup run: 1182it [00:02, 932.50it/s]warmup run: 1348it [00:02, 944.22it/s]warmup run: 1218it [00:02, 961.90it/s]warmup run: 1206it [00:02, 951.36it/s]warmup run: 1157it [00:02, 895.34it/s]warmup run: 1478it [00:03, 963.19it/s]warmup run: 1458it [00:02, 1006.49it/s]warmup run: 1189it [00:02, 883.90it/s]warmup run: 1281it [00:02, 948.44it/s]warmup run: 1449it [00:03, 962.26it/s]warmup run: 1320it [00:02, 975.32it/s]warmup run: 1307it [00:02, 963.42it/s]warmup run: 1252it [00:02, 897.27it/s]warmup run: 1578it [00:03, 974.01it/s]warmup run: 1563it [00:03, 1018.63it/s]warmup run: 1283it [00:02, 899.66it/s]warmup run: 1380it [00:02, 957.62it/s]warmup run: 1552it [00:03, 979.34it/s]warmup run: 1422it [00:02, 987.96it/s]warmup run: 1411it [00:02, 985.01it/s]warmup run: 1352it [00:02, 925.79it/s]warmup run: 1678it [00:03, 979.75it/s]warmup run: 1669it [00:03, 1028.98it/s]warmup run: 1381it [00:02, 920.59it/s]warmup run: 1480it [00:03, 967.61it/s]warmup run: 1655it [00:03, 993.17it/s]warmup run: 1524it [00:03, 986.25it/s]warmup run: 1515it [00:03, 999.79it/s]warmup run: 1453it [00:03, 948.61it/s]warmup run: 1778it [00:03, 985.34it/s]warmup run: 1774it [00:03, 1033.49it/s]warmup run: 1478it [00:03, 933.28it/s]warmup run: 1582it [00:03, 981.38it/s]warmup run: 1757it [00:03, 1000.26it/s]warmup run: 1619it [00:03, 1010.76it/s]warmup run: 1628it [00:03, 1000.49it/s]warmup run: 1553it [00:03, 961.59it/s]warmup run: 1879it [00:03, 990.37it/s]warmup run: 1879it [00:03, 1036.06it/s]warmup run: 1574it [00:03, 926.69it/s]warmup run: 1685it [00:03, 994.22it/s]warmup run: 1860it [00:03, 1008.39it/s]warmup run: 1730it [00:03, 1006.02it/s]warmup run: 1723it [00:03, 1018.67it/s]warmup run: 1651it [00:03, 961.59it/s]warmup run: 1980it [00:03, 994.20it/s]warmup run: 1984it [00:03, 1034.11it/s]warmup run: 1669it [00:03, 930.79it/s]warmup run: 1788it [00:03, 1004.22it/s]warmup run: 1963it [00:03, 1012.41it/s]warmup run: 1834it [00:03, 1015.27it/s]warmup run: 1826it [00:03, 1020.32it/s]warmup run: 2095it [00:03, 1039.92it/s]warmup run: 1749it [00:03, 958.89it/s]warmup run: 2104it [00:03, 1080.78it/s]warmup run: 1765it [00:03, 938.06it/s]warmup run: 1891it [00:03, 1011.62it/s]warmup run: 2076it [00:03, 1045.92it/s]warmup run: 1937it [00:03, 1011.42it/s]warmup run: 1929it [00:03, 1009.25it/s]warmup run: 2215it [00:03, 1085.00it/s]warmup run: 1846it [00:03, 958.32it/s]warmup run: 2227it [00:03, 1125.07it/s]warmup run: 1862it [00:03, 945.02it/s]warmup run: 1995it [00:03, 1018.49it/s]warmup run: 2195it [00:03, 1088.38it/s]warmup run: 2044it [00:03, 1026.53it/s]warmup run: 2036it [00:03, 1025.02it/s]warmup run: 2336it [00:03, 1119.63it/s]warmup run: 1943it [00:03, 955.19it/s]warmup run: 2351it [00:03, 1156.78it/s]warmup run: 1959it [00:03, 951.74it/s]warmup run: 2114it [00:03, 1069.65it/s]warmup run: 2315it [00:03, 1120.16it/s]warmup run: 2164it [00:03, 1076.32it/s]warmup run: 2156it [00:03, 1075.65it/s]warmup run: 2457it [00:03, 1145.22it/s]warmup run: 2050it [00:03, 986.98it/s]warmup run: 2475it [00:03, 1178.75it/s]warmup run: 2068it [00:03, 991.97it/s]warmup run: 2235it [00:03, 1110.77it/s]warmup run: 2435it [00:03, 1142.57it/s]warmup run: 2281it [00:03, 1103.09it/s]warmup run: 2274it [00:03, 1104.14it/s]warmup run: 2578it [00:04, 1164.11it/s]warmup run: 2172it [00:03, 1055.84it/s]warmup run: 2598it [00:03, 1193.33it/s]warmup run: 2184it [00:03, 1041.23it/s]warmup run: 2356it [00:03, 1140.37it/s]warmup run: 2555it [00:04, 1158.04it/s]warmup run: 2401it [00:03, 1129.20it/s]warmup run: 2394it [00:03, 1130.48it/s]warmup run: 2700it [00:04, 1178.16it/s]warmup run: 2294it [00:03, 1103.61it/s]warmup run: 2722it [00:04, 1204.76it/s]warmup run: 2301it [00:03, 1076.95it/s]warmup run: 2477it [00:03, 1160.61it/s]warmup run: 2675it [00:04, 1168.03it/s]warmup run: 2522it [00:03, 1152.08it/s]warmup run: 2515it [00:03, 1152.26it/s]warmup run: 2820it [00:04, 1182.85it/s]warmup run: 2416it [00:03, 1137.14it/s]warmup run: 2843it [00:04, 1205.07it/s]warmup run: 2418it [00:03, 1102.10it/s]warmup run: 2598it [00:04, 1174.50it/s]warmup run: 2794it [00:04, 1171.58it/s]warmup run: 2643it [00:04, 1169.13it/s]warmup run: 2637it [00:04, 1171.86it/s]warmup run: 2941it [00:04, 1189.16it/s]warmup run: 2538it [00:04, 1160.03it/s]warmup run: 2965it [00:04, 1207.23it/s]warmup run: 2534it [00:04, 1119.07it/s]warmup run: 2719it [00:04, 1183.96it/s]warmup run: 3000it [00:04, 703.48it/s] warmup run: 3000it [00:04, 673.19it/s] warmup run: 2914it [00:04, 1179.53it/s]warmup run: 2761it [00:04, 1164.43it/s]warmup run: 2758it [00:04, 1183.21it/s]warmup run: 2660it [00:04, 1175.45it/s]warmup run: 2652it [00:04, 1136.34it/s]warmup run: 2839it [00:04, 1186.22it/s]warmup run: 3000it [00:04, 671.33it/s] warmup run: 2883it [00:04, 1179.81it/s]warmup run: 2880it [00:04, 1193.61it/s]warmup run: 2781it [00:04, 1183.78it/s]warmup run: 2771it [00:04, 1151.60it/s]warmup run: 2961it [00:04, 1195.53it/s]warmup run: 3000it [00:04, 689.85it/s] warmup run: 3000it [00:04, 1193.91it/s]warmup run: 3000it [00:04, 695.22it/s] warmup run: 3000it [00:04, 692.42it/s] warmup run: 2903it [00:04, 1192.92it/s]warmup run: 2892it [00:04, 1166.86it/s]warmup run: 3000it [00:04, 675.00it/s] warmup run: 3000it [00:04, 676.35it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.97it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.29it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.42it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1598.32it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.79it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.50it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.87it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.10it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1659.02it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1636.35it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1655.42it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1636.60it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.18it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1686.89it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1654.07it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1595.50it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1657.79it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1664.70it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1638.66it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1645.29it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1664.59it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1653.38it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1683.34it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1614.87it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1666.49it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1655.93it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1638.62it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1649.73it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1664.38it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1652.55it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1679.39it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1624.25it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1665.88it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1652.04it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1634.09it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1662.39it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1647.75it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1648.04it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1675.42it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1630.25it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1665.10it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1648.96it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1654.90it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1644.05it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1640.24it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1623.44it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1672.48it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1631.47it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1662.31it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1644.93it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1623.27it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1640.98it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1668.57it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1633.64it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1645.72it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1629.67it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1662.78it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1643.30it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1663.45it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1631.44it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1641.69it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1605.45it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1622.71it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1563.56it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1662.91it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1643.35it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1630.34it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1657.68it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1638.50it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1624.01it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1588.25it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1537.20it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1665.35it/s]warmup should be done:  55%|█████▌    | 1651/3000 [00:01<00:00, 1637.66it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1630.20it/s]warmup should be done:  56%|█████▌    | 1681/3000 [00:01<00:00, 1655.10it/s]warmup should be done:  55%|█████▌    | 1663/3000 [00:01<00:00, 1639.00it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1623.16it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1592.44it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1566.48it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1663.99it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1638.14it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1628.89it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1637.80it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1648.30it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1624.52it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1618.10it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1587.08it/s]warmup should be done:  67%|██████▋   | 2003/3000 [00:01<00:00, 1663.12it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1638.49it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1627.10it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1627.40it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1640.83it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1636.57it/s]warmup should be done:  66%|██████▋   | 1991/3000 [00:01<00:00, 1616.13it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1598.30it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1663.22it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1639.26it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1624.76it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1628.91it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1649.12it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1623.49it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1606.08it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1460.44it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1664.19it/s]warmup should be done:  77%|███████▋  | 2309/3000 [00:01<00:00, 1639.44it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1628.80it/s]warmup should be done:  77%|███████▋  | 2296/3000 [00:01<00:00, 1628.95it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1650.22it/s]warmup should be done:  77%|███████▋  | 2322/3000 [00:01<00:00, 1632.54it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1609.74it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1409.95it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1662.67it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1637.01it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1633.11it/s]warmup should be done:  82%|████████▏ | 2459/3000 [00:01<00:00, 1626.28it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1648.39it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1636.62it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1583.26it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1459.65it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1664.30it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1634.51it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1637.18it/s]warmup should be done:  87%|████████▋ | 2622/3000 [00:01<00:00, 1625.80it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1649.42it/s]warmup should be done:  88%|████████▊ | 2653/3000 [00:01<00:00, 1640.82it/s]warmup should be done:  87%|████████▋ | 2622/3000 [00:01<00:00, 1589.24it/s]warmup should be done:  88%|████████▊ | 2649/3000 [00:01<00:00, 1504.30it/s]warmup should be done:  95%|█████████▍| 2839/3000 [00:01<00:00, 1665.96it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1635.06it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1641.64it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1624.31it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1652.76it/s]warmup should be done:  94%|█████████▍| 2820/3000 [00:01<00:00, 1646.69it/s]warmup should be done:  93%|█████████▎| 2783/3000 [00:01<00:00, 1593.14it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1537.28it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.63it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1631.24it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1649.02it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1629.88it/s]warmup should be done: 100%|█████████▉| 2987/3000 [00:01<00:00, 1653.40it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1660.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1644.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.37it/s]warmup should be done:  98%|█████████▊| 2946/3000 [00:01<00:00, 1603.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.10it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1569.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1591.38it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1699.90it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.28it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.34it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.81it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1653.06it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1692.33it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1710.80it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1671.28it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.13it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1680.52it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1675.13it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1691.10it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1649.41it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1709.56it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1685.04it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1645.62it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1684.39it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1678.63it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1648.64it/s]warmup should be done:  17%|█▋        | 515/3000 [00:00<00:01, 1709.15it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1691.38it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1647.60it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1673.62it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1672.16it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1687.57it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1681.53it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1693.06it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1653.33it/s]warmup should be done:  23%|██▎       | 688/3000 [00:00<00:01, 1713.68it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.72it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1681.14it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1672.30it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1687.42it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1683.66it/s]warmup should be done:  29%|██▊       | 862/3000 [00:00<00:01, 1722.45it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1655.90it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1691.58it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1652.90it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1684.79it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1671.91it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1685.96it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1681.21it/s]warmup should be done:  35%|███▍      | 1036/3000 [00:00<00:01, 1725.54it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1691.40it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1652.88it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1650.60it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1684.34it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1667.37it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1687.62it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1680.33it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1653.61it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1688.12it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1649.01it/s]warmup should be done:  40%|████      | 1209/3000 [00:00<00:01, 1711.07it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1660.73it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1658.30it/s]warmup should be done:  45%|████▌     | 1354/3000 [00:00<00:00, 1690.72it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1689.98it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1654.88it/s]warmup should be done:  45%|████▌     | 1360/3000 [00:00<00:00, 1690.58it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1645.64it/s]warmup should be done:  46%|████▌     | 1381/3000 [00:00<00:00, 1702.99it/s]warmup should be done:  45%|████▍     | 1348/3000 [00:00<00:00, 1665.20it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1660.47it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1690.43it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1658.14it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1696.58it/s]warmup should be done:  51%|█████     | 1530/3000 [00:00<00:00, 1691.75it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1643.45it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1675.49it/s]warmup should be done:  52%|█████▏    | 1552/3000 [00:00<00:00, 1695.65it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1657.55it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1691.81it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1661.72it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1702.25it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1693.66it/s]warmup should be done:  55%|█████▌    | 1659/3000 [00:01<00:00, 1646.16it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1684.44it/s]warmup should be done:  57%|█████▋    | 1722/3000 [00:01<00:00, 1695.76it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1659.26it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1693.32it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1665.60it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1706.12it/s]warmup should be done:  62%|██████▏   | 1870/3000 [00:01<00:00, 1695.09it/s]warmup should be done:  61%|██████    | 1824/3000 [00:01<00:00, 1646.95it/s]warmup should be done:  62%|██████▏   | 1860/3000 [00:01<00:00, 1690.71it/s]warmup should be done:  63%|██████▎   | 1893/3000 [00:01<00:00, 1697.61it/s]warmup should be done:  62%|██████▏   | 1853/3000 [00:01<00:00, 1660.82it/s]warmup should be done:  68%|██████▊   | 2034/3000 [00:01<00:00, 1693.61it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1669.42it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1707.05it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1694.26it/s]warmup should be done:  66%|██████▋   | 1989/3000 [00:01<00:00, 1641.83it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1692.46it/s]warmup should be done:  69%|██████▉   | 2063/3000 [00:01<00:00, 1694.76it/s]warmup should be done:  67%|██████▋   | 2020/3000 [00:01<00:00, 1662.85it/s]warmup should be done:  73%|███████▎  | 2204/3000 [00:01<00:00, 1692.46it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1660.06it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1692.82it/s]warmup should be done:  72%|███████▏  | 2154/3000 [00:01<00:00, 1639.84it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1692.69it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1677.59it/s]warmup should be done:  74%|███████▍  | 2233/3000 [00:01<00:00, 1691.11it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1659.71it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1692.83it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1670.37it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1696.90it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1639.96it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1695.29it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1675.64it/s]warmup should be done:  80%|████████  | 2403/3000 [00:01<00:00, 1690.13it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1659.15it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1694.50it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1677.75it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1701.97it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1699.31it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1643.00it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1675.55it/s]warmup should be done:  84%|████████▍ | 2522/3000 [00:01<00:00, 1668.37it/s]warmup should be done:  86%|████████▌ | 2573/3000 [00:01<00:00, 1692.24it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1694.55it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1679.80it/s]warmup should be done:  91%|█████████ | 2726/3000 [00:01<00:00, 1705.30it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1700.42it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1676.22it/s]warmup should be done:  88%|████████▊ | 2650/3000 [00:01<00:00, 1643.41it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1677.06it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1694.43it/s]warmup should be done:  96%|█████████▌| 2884/3000 [00:01<00:00, 1689.86it/s]warmup should be done:  95%|█████████▍| 2843/3000 [00:01<00:00, 1681.86it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1706.67it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1707.90it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1643.11it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1674.74it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1681.53it/s]warmup should be done:  97%|█████████▋| 2914/3000 [00:01<00:00, 1695.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1695.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1689.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1671.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.61it/s]warmup should be done:  99%|█████████▉| 2981/3000 [00:01<00:00, 1647.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.21it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d912c3e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d90f90040>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d90f91310>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d912cbe50>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d912c4730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d90f7f280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d90f91100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4d90f84100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 22:18:52.817702: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48bf02de00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:52.817771: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:52.827349: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:52.939100: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48be834440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:52.939181: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:52.948897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:53.816633: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48ca830390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:53.816693: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:53.818245: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48ca82ba30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:53.818285: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:53.818503: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48c7028ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:53.818541: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:53.824945: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:53.825730: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:53.828245: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:53.881701: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48b6f91e60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:53.881766: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:53.889481: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:53.938481: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48c2959220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:53.938547: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:53.947423: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:18:53.953378: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f48ca837ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:18:53.953424: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:18:53.963025: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:19:00.114563: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.259733: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.520752: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.604182: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.636498: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.662385: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.670948: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:19:00.673504: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:20:02.145][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:20:02.145][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.145][ERROR][RK0][tid #139950896965376]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:20:02.145][ERROR][RK0][tid #139950896965376]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.152][ERROR][RK0][tid #139950896965376]: coll ps creation done
[HCTR][22:20:02.152][ERROR][RK0][tid #139950896965376]: replica 3 waits for coll ps creation barrier
[HCTR][22:20:02.156][ERROR][RK0][main]: coll ps creation done
[HCTR][22:20:02.156][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][22:20:02.160][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:20:02.160][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.167][ERROR][RK0][main]: coll ps creation done
[HCTR][22:20:02.167][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][22:20:02.241][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:20:02.241][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.247][ERROR][RK0][main]: coll ps creation done
[HCTR][22:20:02.247][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][22:20:02.302][ERROR][RK0][tid #139951157008128]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:20:02.302][ERROR][RK0][tid #139951157008128]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.307][ERROR][RK0][tid #139951157008128]: coll ps creation done
[HCTR][22:20:02.307][ERROR][RK0][tid #139951157008128]: replica 1 waits for coll ps creation barrier
[HCTR][22:20:02.515][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:20:02.515][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.520][ERROR][RK0][main]: coll ps creation done
[HCTR][22:20:02.520][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][22:20:02.745][ERROR][RK0][tid #139950813071104]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:20:02.745][ERROR][RK0][tid #139950813071104]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.753][ERROR][RK0][tid #139950813071104]: coll ps creation done
[HCTR][22:20:02.753][ERROR][RK0][tid #139950813071104]: replica 4 waits for coll ps creation barrier
[HCTR][22:20:02.843][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:20:02.843][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][22:20:02.848][ERROR][RK0][main]: coll ps creation done
[HCTR][22:20:02.848][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][22:20:02.848][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][22:20:03.684][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][22:20:03.716][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][tid #139951157008128]: replica 1 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][tid #139950813071104]: replica 4 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][tid #139950813071104]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][tid #139950896965376]: replica 3 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][22:20:03.716][ERROR][RK0][main]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][main]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][main]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][tid #139951157008128]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][tid #139950813071104]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][main]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][tid #139950896965376]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][main]: Calling build_v2
[HCTR][22:20:03.716][ERROR][RK0][tid #139951157008128]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][tid #139950896965376]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:20:03.716][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-11 22:20:032022-12-11 22:20:03.2022-12-11 22:20:03.2022-12-11 22:20:037163712022-12-11 22:20:03.716387.: .[[716372: 716393E[716394: E: 2022-12-11 22:20:03 2022-12-11 22:20:03: E E./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc.E2022-12-11 22:20:03 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 716411:716410 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: 136: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc716433:136:E] E:: 136] 136 using concurrent impl MPS 136E] using concurrent impl MPS] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc]  using concurrent impl MPS
using concurrent impl MPS::using concurrent impl MPS/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc

136136
:] ] 136using concurrent impl MPSusing concurrent impl MPS] 

using concurrent impl MPS
[2022-12-11 22:20:03.720356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:20:03.720392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-11 22:20:03.720449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-11 22:20:03.720490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-11 22:20:03.720518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 22:20:03.720539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:20:03.720575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-11 22:20:03] .assigning 8 to cpu720585
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 22:20:03.720628[: 2022-12-11 22:20:03E. 720628/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:E2022-12-11 22:20:03196 .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc720652assigning 8 to cpu:[: 
1782022-12-11 22:20:03E] . v100x8, slow pcie720673/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: :E212 [] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 22:20:03build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 22:20:03:2022-12-11 22:20:03.
.178.720713720715] 720717: [: v100x8, slow pcie: E2022-12-11 22:20:03E
E . [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[720754/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:20:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 22:20:03: :.:196.E212720775178[] 720767 ] : ] 2022-12-11 22:20:03assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8Ev100x8, slow pcie.
E:
 
720810 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[] :E:2022-12-11 22:20:032022-12-11 22:20:03remote time is 8.68421196[ 178..
] 2022-12-11 22:20:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 720902720902assigning 8 to cpu.[:v100x8, slow pcie: : 
7209272022-12-11 22:20:03178
EE: .]   E[720963v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 22:20:03: [
::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.E2022-12-11 22:20:03213196:2022-12-11 22:20:03721011 .] ] 212.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc721035remote time is 8.68421assigning 8 to cpu] 721069E:: 

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  214E
[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  2022-12-11 22:20:03 :[cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1962022-12-11 22:20:03
:[721145:] .2122022-12-11 22:20:03: 196assigning 8 to cpu721168] .E] 
: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8721188 assigning 8 to cpuE
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 214:2022-12-11 22:20:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 213.:2022-12-11 22:20:03cpu time is 97.0588] 721277[212.
remote time is 8.68421: 2022-12-11 22:20:03] 721285
E.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  721308[
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 22:20:03 :E[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 2022-12-11 22:20:03721372:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 212remote time is 8.68421:721407E] 
212:  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-11 22:20:03:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.214[:721499] 2022-12-11 22:20:03[213: cpu time is 97.0588.2022-12-11 22:20:03] E
721536.remote time is 8.68421 : 721550
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: : [E214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:20:03 ] :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588213721615:
] : 213remote time is 8.68421E] 
 remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[:2022-12-11 22:20:03214[.] 2022-12-11 22:20:03721731cpu time is 97.0588.: 
721743E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 22:21:22.785355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:21:22.834259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 22:21:22.834373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 1999999
[2022-12-11 22:21:22.946817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:21:22.946908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:21:22.947755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:21:22.947787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:21:22.948250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.949167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.949980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.962963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 22:21:22.963026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:21:22.963041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 22:21:22.963116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 22:21:22.963198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 22:21:22.963267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[[2022-12-11 22:21:222022-12-11 22:21:22..963415963429: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2021980] ] 4 solvedeager alloc mem 381.47 MB

[2022-12-11 22:21:22.963533[: 2022-12-11 22:21:22E. 963539/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[worker 0 thread 4 initing device 4:2022-12-11 22:21:22
1980.] 963561eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[[2022-12-11 22:21:222022-12-11 22:21:22..963624963636: : E[E 2022-12-11 22:21:22 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:963663:202: 205] E] 5 solved worker 0 thread 1 initing device 1
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980[] 2022-12-11 22:21:22eager alloc mem 381.47 MB.
963723[: 2022-12-11 22:21:22E. 963726/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 5 initing device 5:
202] 3 solved
[2022-12-11 22:21:22.963802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 22:21:22.963939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.964079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.964142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.964182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.967891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.968214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.968273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.968323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.968879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.968933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.968994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.972437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.972669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.972718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.972818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.972889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.973380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:22.973440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:21:23. 25064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:21:23. 25417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:21:23. 30370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:21:23. 30437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:21:23. 30480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:21:23. 31267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 31708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 32662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23. 32750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 33427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 33469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:21:23. 53874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-11 22:21:23. 54195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 22:21:23. 59252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:21:23. 59320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:21:23. 59376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:21:23. 60860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 61399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 62344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23. 62429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 63106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 63160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[[[[[[2022-12-11 22:21:232022-12-11 22:21:232022-12-11 22:21:232022-12-11 22:21:232022-12-11 22:21:232022-12-11 22:21:23.. 65858.... 65858:  65858 65858 65858 65858: E: : : : E EEEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::::1980] 1980198019801980] eager alloc mem 2.00 Bytes] ] ] ] eager alloc mem 2.00 Bytes
eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes




[2022-12-11 22:21:23. 66331[[: 2022-12-11 22:21:232022-12-11 22:21:23E[.[.[ 2022-12-11 22:21:23 663382022-12-11 22:21:23 663372022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: .: .: 66347E 66348E 663511980:  :  : ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEeager alloc mem 1024.00 Bytes : : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :] :1980eager alloc mem 1024.00 Bytes1980eager alloc mem 1024.00 Bytes1980] 
] 
] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes


[2022-12-11 22:21:23. 72887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-11 22:21:232022-12-11 22:21:23.. 72967 72953: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 2eager release cuda mem 1024

[[2022-12-11 22:21:232022-12-11 22:21:23.. 73039 73042: : EE[  2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:: 73046638638: ] ] Eeager release cuda mem 400000000eager release cuda mem 2 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 22:21:23. 73125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:21:23] .eager release cuda mem 400000000 73141
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2[
2022-12-11 22:21:23. 73157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10242022-12-11 22:21:23
. 73198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 22:21:23[.2022-12-11 22:21:23 73210.:  73230E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638] eager release cuda mem 1024eager release cuda mem 2

[[2022-12-11 22:21:232022-12-11 22:21:23.. 73317[ 73319: 2022-12-11 22:21:23: E.E  73316 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E:638 638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] eager release cuda mem 2:eager release cuda mem 400000000
638
] eager release cuda mem 1024
[2022-12-11 22:21:23. 73413: E[ 2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 73423638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-11 22:21:23. 73481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:21:23. 74216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 75014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 75768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 76512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 77062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 77692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 22:21:23. 78296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 78563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 78822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 78941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 79045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 79097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23. 79248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23. 79339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 79504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23. 79588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 79763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23. 79846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 79891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.[ 799812022-12-11 22:21:23: .E 79984 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: [1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:21:23] :.eager alloc mem 25.25 KB638 80009
] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 80071[: 2022-12-11 22:21:23E.  80073/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:21:23eager alloc mem 976.56 MB:.
638 80101] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 80202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:21:23. 80268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 80312: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:21:23. 80523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 80565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:21:23. 80690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 80735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:21:23. 80795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 80838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[2022-12-11 22:21:23. 80895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:21:23. 80939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 976.56 MB
[[[[2022-12-11 22:21:23[[2022-12-11 22:21:232022-12-11 22:21:232022-12-11 22:21:23.2022-12-11 22:21:232022-12-11 22:21:23...271005..271010271013271015: 271017[271020: : : [E: 2022-12-11 22:21:23: EEE2022-12-11 22:21:23 E.E   ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 271073 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu271068:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::: 1980:E:198019801980E] 1980 1980] ] ]  eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager alloc mem 611.00 KB:eager alloc mem 611.00 KB


:
1980
1980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 22:21:23.272071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 22:21:23.272096: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:21:23:.[638[2721122022-12-11 22:21:23] 2022-12-11 22:21:23: .[eager release cuda mem 625663.E2721202022-12-11 22:21:23[
272120 [: .2022-12-11 22:21:23: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:21:23[E272136.E:.2022-12-11 22:21:23 : 272145 638272152.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] : 2721692022-12-11 22:21:23: E:eager release cuda mem 625663E: .638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638
 E272217] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : eager release cuda mem 625663638:eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE
] 638
1980:[ eager release cuda mem 625663] ] 6382022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager release cuda mem 625663eager alloc mem 611.00 KB] .[:

eager release cuda mem 6256632723592022-12-11 22:21:23[1980
: .2022-12-11 22:21:23] E272419.[eager alloc mem 611.00 KB : 2724252022-12-11 22:21:23
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: .: E272460[1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : 2022-12-11 22:21:23] :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE.eager alloc mem 611.00 KB19802022-12-11 22:21:23: 272486
] .1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: eager alloc mem 611.00 KB272506] :E
: eager alloc mem 611.00 KB1980 E
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager alloc mem 611.00 KB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 22:21:23.273162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.273207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.273240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.273283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.273330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.273376: E[ 2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:273387[638[: 2022-12-11 22:21:23] 2022-12-11 22:21:23E[.eager release cuda mem 625663. 2022-12-11 22:21:23273400
[273400/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 2022-12-11 22:21:23: :273416E.E638:  273435 ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[eager release cuda mem 625663 :E:2022-12-11 22:21:23
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980 638.:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 273499638eager alloc mem 611.00 KB:eager release cuda mem 625663: ] 
638
Eeager release cuda mem 625663[]  
2022-12-11 22:21:23eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.
:2735821980: ] Eeager alloc mem 611.00 KB[ 
2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[:2736342022-12-11 22:21:231980[: .] 2022-12-11 22:21:23273653Eeager alloc mem 611.00 KB.:  
273668E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB:] 
1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-11 22:21:23.273985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.274035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 22:21:23
.274053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.274105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.274336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.[2744032022-12-11 22:21:23: .E274406 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-11 22:21:23.274473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.274498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:21:23] .eager alloc mem 611.00 KB274512
: [E2022-12-11 22:21:23 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2745412022-12-11 22:21:23:2022-12-11 22:21:23: .638.E274553] 274556 : eager release cuda mem 625663: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE
E:  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ::eager alloc mem 611.00 KB638638[
] ] 2022-12-11 22:21:23eager release cuda mem 625663eager release cuda mem 625663.

274674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:21:232022-12-11 22:21:23..274762274762: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:21:23::.19801980274809] ] : [eager alloc mem 611.00 KBeager alloc mem 611.00 KBE2022-12-11 22:21:23

 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc274858:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 22:21:23.274949: [E2022-12-11 22:21:23 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu274958:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.275198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.275271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.275298: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.275368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.275448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:21:23] .eager release cuda mem 625663275465
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.275539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 22:21:231980.] 275554eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.275634: [E2022-12-11 22:21:23 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc275644:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 22:21:23.275714[: 2022-12-11 22:21:23E. 275723/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 6256632022-12-11 22:21:23:[
.6382022-12-11 22:21:23275747] .: eager release cuda mem 625663275755E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ :2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980.:] 2758111980[eager alloc mem 611.00 KB: ] 2022-12-11 22:21:23
Eeager alloc mem 611.00 KB. 
275847/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.276042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.276106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.276147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.276214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.276327: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:21:23:.638276341] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.276423: E[ 2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2764351980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.276628: [E2022-12-11 22:21:23 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-11 22:21:23276638:2022-12-11 22:21:23.: 638.276644E] 276648:  eager release cuda mem 625663: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] :638eager release cuda mem 625663638] 
] [eager release cuda mem 625663eager release cuda mem 6256632022-12-11 22:21:23

.276762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 22:21:23.276810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 22:21:23[
.2022-12-11 22:21:23276831.: 276835E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ :2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980.:] 2768741980eager alloc mem 611.00 KB: ] 
Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23[.2022-12-11 22:21:23276998.: 276999E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 22:21:23.277091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.277216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.277256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.277291: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.277324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:21:23.[2775642022-12-11 22:21:23: .E277570 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 22:21:23.277619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[[2022-12-11 22:21:232022-12-11 22:21:23..277656277657: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-11 22:21:23.277702: E[ 2022-12-11 22:21:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:277715638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:21:23.277791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:21:23] .eager release cuda mem 625663277808
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 22:21:23
.277846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:21:23] .eager release cuda mem 8399996277871
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.277917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:21:23.278039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.278077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:21:23.278100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.278142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:21:23.278454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.278490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:21:23.278612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:21:23.278651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 22:21:23.290980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.327046 secs 
[2022-12-11 22:21:23.291393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.32774 secs 
[2022-12-11 22:21:23.291801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328275 secs 
[2022-12-11 22:21:23.292208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328138 secs 
[2022-12-11 22:21:23.292623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.344383 secs 
[2022-12-11 22:21:23.293026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.328851 secs 
[2022-12-11 22:21:23.293434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.329298 secs 
[2022-12-11 22:21:23.293845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 98000001 / 100000000 nodes ( 98.00 %) | 976.56 MB | 0.330423 secs 
[2022-12-11 22:21:23.296915: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.27 GB
[2022-12-11 22:21:24.840025: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.54 GB
[2022-12-11 22:21:24.840591: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.54 GB
[2022-12-11 22:21:24.842048: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.54 GB
[2022-12-11 22:21:26.462106: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.80 GB
[2022-12-11 22:21:26.463454: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.80 GB
[2022-12-11 22:21:26.464022: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.80 GB
[2022-12-11 22:21:27.329798: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.02 GB
[2022-12-11 22:21:27.329966: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.02 GB
[2022-12-11 22:21:27.330255: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.02 GB
[2022-12-11 22:21:29.296204: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.23 GB
[2022-12-11 22:21:29.297411: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.23 GB
[2022-12-11 22:21:29.299782: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.23 GB
[2022-12-11 22:21:30.934269: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.69 GB
[2022-12-11 22:21:30.934413: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.69 GB
[2022-12-11 22:21:30.934879: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.69 GB
[2022-12-11 22:21:32.575218: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.89 GB
[2022-12-11 22:21:32.575402: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.89 GB
[HCTR][22:21:32.773][ERROR][RK0][tid #139950896965376]: replica 3 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][tid #139951157008128]: replica 1 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][tid #139950813071104]: replica 4 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][22:21:32.773][ERROR][RK0][tid #139950896965376]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][tid #139950813071104]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][tid #139951157008128]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:21:32.773][ERROR][RK0][tid #139950896965376]: init per replica done
[HCTR][22:21:32.773][ERROR][RK0][main]: init per replica done
[HCTR][22:21:32.773][ERROR][RK0][main]: init per replica done
[HCTR][22:21:32.773][ERROR][RK0][main]: init per replica done
[HCTR][22:21:32.773][ERROR][RK0][tid #139950813071104]: init per replica done
[HCTR][22:21:32.773][ERROR][RK0][tid #139951157008128]: init per replica done
[HCTR][22:21:32.773][ERROR][RK0][main]: init per replica done
[HCTR][22:21:32.776][ERROR][RK0][main]: init per replica done
[HCTR][22:21:32.811][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f2d5a238400
[HCTR][22:21:32.811][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f2d5a558400
[HCTR][22:21:32.811][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f2d5ab98400
[HCTR][22:21:32.811][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f2d5aeb8400
[HCTR][22:21:32.812][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f2d5a238400
[HCTR][22:21:32.812][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f2c18238400
[HCTR][22:21:32.812][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f2d5a558400
[HCTR][22:21:32.812][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f2c18558400
[HCTR][22:21:32.812][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f2d5ab98400
[HCTR][22:21:32.812][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f2c18b98400
[HCTR][22:21:32.812][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f2d5aeb8400
[HCTR][22:21:32.812][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f2c18eb8400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950896965376]: 3 allocated 3276800 at 0x7f2d58238400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950896965376]: 3 allocated 6553600 at 0x7f2d58558400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950896965376]: 3 allocated 3276800 at 0x7f2d58b98400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950896965376]: 3 allocated 6553600 at 0x7f2d58eb8400
[HCTR][22:21:32.812][ERROR][RK0][tid #139951157008128]: 1 allocated 3276800 at 0x7f2c20238400
[HCTR][22:21:32.812][ERROR][RK0][tid #139951157008128]: 1 allocated 6553600 at 0x7f2c20558400
[HCTR][22:21:32.812][ERROR][RK0][tid #139951157008128]: 1 allocated 3276800 at 0x7f2c20b98400
[HCTR][22:21:32.812][ERROR][RK0][tid #139951157008128]: 1 allocated 6553600 at 0x7f2c20eb8400
[HCTR][22:21:32.812][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f2c24238400
[HCTR][22:21:32.812][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f2c24558400
[HCTR][22:21:32.812][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f2c24b98400
[HCTR][22:21:32.812][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f2c24eb8400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950762747648]: 5 allocated 3276800 at 0x7f2cb8238400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950762747648]: 5 allocated 6553600 at 0x7f2cb8558400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950762747648]: 5 allocated 3276800 at 0x7f2cb8b98400
[HCTR][22:21:32.812][ERROR][RK0][tid #139950762747648]: 5 allocated 6553600 at 0x7f2cb8eb8400
[HCTR][22:21:32.815][ERROR][RK0][tid #139950762747648]: 0 allocated 3276800 at 0x7f2d28320000
[HCTR][22:21:32.815][ERROR][RK0][tid #139950762747648]: 0 allocated 6553600 at 0x7f2d28640000
[HCTR][22:21:32.815][ERROR][RK0][tid #139950762747648]: 0 allocated 3276800 at 0x7f2d28c80000
[HCTR][22:21:32.815][ERROR][RK0][tid #139950762747648]: 0 allocated 6553600 at 0x7f2d28fa0000
