2022-12-11 22:58:30.836809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.842306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.849735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.855147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.859623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.873202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.880061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.892708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.942929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.948914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.952653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.954037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.954441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.955243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.956210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.956821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.957921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.958419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.959631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.959937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.961127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.961264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.962871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.962927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.964657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.964697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.966367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.966468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.967804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.968773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.969689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.970591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.972456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.973596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.974847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.976475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.977255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.977736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.978889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.979281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.980489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.980703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.982057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.982227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.983660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.984832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.985835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.986840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.988206: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:30.990958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.992147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.993398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.994443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.995546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.996650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.997531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.997686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.999018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:30.999154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.000427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.007278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.009772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.009807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.012135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.012630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.012693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.012870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.015189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.016002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.016170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.016353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.016728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.018146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.019170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.019351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.019497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.019968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.021275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.022406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.022630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.022717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.028646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.030377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.031954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.032044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.032105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.032236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.032964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.033605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.035820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.035833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.035930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.036142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.036953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.037395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.045438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.077604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.077917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.078320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.079007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.079100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.080439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.081503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.082981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.083533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.083634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.084392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.085956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.087302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.088515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.088897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.089419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.090492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.091682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.092203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.093458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.094232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.094743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.096187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.097003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.097673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.098391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.099472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.099885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.100906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.101604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.102111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.103198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.103788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.104578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.106628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.107022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.107948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.109509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.109509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.110248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.112179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.112322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.112810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.114457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.114821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.114983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.116798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.117133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.118009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.118705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.119335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.120726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.122321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.122923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.123447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.123891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.124951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.125678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.125827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.126761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.127006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.128337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.129382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.130010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.130100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.130285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.131371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.132455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.133200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.133520: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.133543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.133773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.134479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.134651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.136685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.137921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.138604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.138787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.139709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.139798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.141203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.141891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.142400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.142786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.142806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.143875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.143967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.145485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.146632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.147575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.147719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.148329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.148529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.149989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.150801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.151477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.151472: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.151701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.152207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.152359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.153968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.156268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.157676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.157706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.158335: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.159295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.160343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.160418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.161519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.161856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.163178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.165000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.165021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.165801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.166431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.167069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.167330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.169374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.169385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.170542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.171114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.171810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.171976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.174234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.175156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.175669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.176896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.177054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.180328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.181339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.181838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.182568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.184876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.186126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.186663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.192708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.194976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.196140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.197241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.197287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.200041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.229553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.231125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.231491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.232709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.233838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.235836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.236426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.238908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.240555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.243211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.243747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.245054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.247038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.248503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.249265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.251513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.252293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.283105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.283638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.284545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.285435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.287286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.287830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.290852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.293152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.295520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.296129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.297318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.299699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.302578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.303955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.304628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.305636: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.315195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.340195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.341169: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.341210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.341507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.349975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.377079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.377515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.378151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.378880: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.383661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.385886: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:58:31.387978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.393024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.394521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.399164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.400595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:31.406665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.349479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.350467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.351186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.351662: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.351721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:58:32.369920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.371241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.371767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.372335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.372986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.373459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:58:32.420446: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.420672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.467721: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:58:32.590856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.591518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.592064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.592526: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.592592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:58:32.597038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.597647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.598166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.598784: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.598851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:58:32.601825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.602477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.603018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.603495: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.603558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:58:32.610423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.611072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.612161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.612795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.613318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.613987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:58:32.616850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.617440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.617954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.618513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.619030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.619507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:58:32.621212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.622110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.622659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.623494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.624021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.624492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:58:32.669919: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.670124: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.671882: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:58:32.702701: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.702904: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.704514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.704826: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:58:32.705137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.705882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.706346: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.706401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:58:32.709493: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.709653: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.710176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.710789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.711332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.711447: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:58:32.711802: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.711859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:58:32.722258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.722882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.723420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.723889: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.723943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:58:32.725085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.725676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.726174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.726743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.727271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.727763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:58:32.729130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.729764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.730264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.731047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.731593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.732061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:58:32.743590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.744239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.744759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.745330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.745852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.746315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:58:32.771677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.772293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.772821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.773174: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.773276: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:58:32.773346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:58:32.773368: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.775080: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:58:32.778212: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.778379: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.780136: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 22:58:32.790048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.790665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.791181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.791756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.792259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:58:32.792726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:58:32.793133: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.793297: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.795207: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:58:32.837119: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.837329: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:58:32.839120: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][22:58:34.113][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.113][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.113][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.113][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.113][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.114][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.114][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:58:34.114][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 97it [00:01, 80.71it/s]warmup run: 96it [00:01, 81.39it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 192it [00:01, 173.28it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 193it [00:01, 177.59it/s]warmup run: 98it [00:01, 82.69it/s]warmup run: 287it [00:01, 275.60it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 96it [00:01, 83.86it/s]warmup run: 95it [00:01, 81.66it/s]warmup run: 293it [00:01, 287.45it/s]warmup run: 196it [00:01, 179.32it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 387it [00:01, 390.31it/s]warmup run: 92it [00:01, 78.52it/s]warmup run: 94it [00:01, 80.35it/s]warmup run: 194it [00:01, 183.36it/s]warmup run: 191it [00:01, 177.95it/s]warmup run: 393it [00:01, 401.50it/s]warmup run: 294it [00:01, 285.98it/s]warmup run: 88it [00:01, 77.69it/s]warmup run: 484it [00:02, 496.46it/s]warmup run: 187it [00:01, 173.36it/s]warmup run: 190it [00:01, 176.16it/s]warmup run: 289it [00:01, 288.26it/s]warmup run: 267it [00:01, 256.35it/s]warmup run: 491it [00:02, 508.55it/s]warmup run: 392it [00:01, 396.51it/s]warmup run: 179it [00:01, 171.13it/s]warmup run: 584it [00:02, 601.37it/s]warmup run: 285it [00:01, 281.85it/s]warmup run: 286it [00:01, 281.63it/s]warmup run: 383it [00:01, 394.22it/s]warmup run: 358it [00:01, 361.72it/s]warmup run: 590it [00:02, 609.17it/s]warmup run: 490it [00:02, 504.11it/s]warmup run: 272it [00:01, 275.80it/s]warmup run: 685it [00:02, 694.00it/s]warmup run: 382it [00:01, 392.69it/s]warmup run: 382it [00:01, 390.44it/s]warmup run: 482it [00:01, 506.51it/s]warmup run: 455it [00:02, 474.86it/s]warmup run: 689it [00:02, 695.84it/s]warmup run: 589it [00:02, 604.88it/s]warmup run: 366it [00:01, 385.13it/s]warmup run: 785it [00:02, 768.80it/s]warmup run: 480it [00:02, 502.01it/s]warmup run: 477it [00:02, 494.77it/s]warmup run: 582it [00:02, 611.19it/s]warmup run: 554it [00:02, 582.41it/s]warmup run: 788it [00:02, 768.39it/s]warmup run: 688it [00:02, 693.37it/s]warmup run: 463it [00:01, 496.32it/s]warmup run: 885it [00:02, 828.48it/s]warmup run: 579it [00:02, 604.10it/s]warmup run: 573it [00:02, 591.93it/s]warmup run: 682it [00:02, 700.11it/s]warmup run: 653it [00:02, 675.89it/s]warmup run: 888it [00:02, 827.37it/s]warmup run: 786it [00:02, 764.21it/s]warmup run: 561it [00:02, 599.04it/s]warmup run: 985it [00:02, 872.71it/s]warmup run: 678it [00:02, 692.79it/s]warmup run: 671it [00:02, 681.04it/s]warmup run: 778it [00:02, 762.16it/s]warmup run: 752it [00:02, 753.42it/s]warmup run: 987it [00:02, 870.58it/s]warmup run: 884it [00:02, 820.20it/s]warmup run: 661it [00:02, 691.71it/s]warmup run: 1084it [00:02, 903.76it/s]warmup run: 776it [00:02, 762.67it/s]warmup run: 771it [00:02, 758.62it/s]warmup run: 875it [00:02, 815.83it/s]warmup run: 851it [00:02, 813.37it/s]warmup run: 1088it [00:02, 907.58it/s]warmup run: 981it [00:02, 860.54it/s]warmup run: 758it [00:02, 761.09it/s]warmup run: 1183it [00:02, 924.27it/s]warmup run: 876it [00:02, 824.53it/s]warmup run: 869it [00:02, 816.24it/s]warmup run: 972it [00:02, 856.74it/s]warmup run: 948it [00:02, 854.54it/s]warmup run: 1080it [00:02, 895.57it/s]warmup run: 854it [00:02, 813.23it/s]warmup run: 1187it [00:02, 920.59it/s]warmup run: 1282it [00:02, 939.66it/s]warmup run: 976it [00:02, 869.96it/s]warmup run: 967it [00:02, 859.10it/s]warmup run: 1070it [00:02, 891.09it/s]warmup run: 1046it [00:02, 888.01it/s]warmup run: 1179it [00:02, 921.99it/s]warmup run: 1287it [00:02, 942.50it/s]warmup run: 950it [00:02, 840.24it/s]warmup run: 1380it [00:02, 937.38it/s]warmup run: 1076it [00:02, 904.16it/s]warmup run: 1066it [00:02, 894.78it/s]warmup run: 1168it [00:02, 915.50it/s]warmup run: 1144it [00:02, 911.89it/s]warmup run: 1278it [00:02, 939.89it/s]warmup run: 1388it [00:02, 959.80it/s]warmup run: 1044it [00:02, 851.04it/s]warmup run: 1477it [00:03, 943.19it/s]warmup run: 1177it [00:02, 931.82it/s]warmup run: 1164it [00:02, 914.52it/s]warmup run: 1267it [00:02, 935.58it/s]warmup run: 1245it [00:02, 938.49it/s]warmup run: 1377it [00:02, 952.57it/s]warmup run: 1487it [00:03, 968.45it/s]warmup run: 1140it [00:02, 879.63it/s]warmup run: 1575it [00:03, 952.42it/s]warmup run: 1277it [00:02, 950.62it/s]warmup run: 1262it [00:02, 933.26it/s]warmup run: 1367it [00:02, 953.06it/s]warmup run: 1345it [00:02, 954.02it/s]warmup run: 1477it [00:03, 965.85it/s]warmup run: 1588it [00:03, 979.85it/s]warmup run: 1237it [00:02, 905.22it/s]warmup run: 1673it [00:03, 959.18it/s]warmup run: 1377it [00:02, 962.53it/s]warmup run: 1361it [00:02, 949.77it/s]warmup run: 1465it [00:02, 958.71it/s]warmup run: 1445it [00:03, 965.70it/s]warmup run: 1577it [00:03, 972.83it/s]warmup run: 1688it [00:03, 985.12it/s]warmup run: 1334it [00:02, 923.92it/s]warmup run: 1771it [00:03, 962.84it/s]warmup run: 1477it [00:03, 971.22it/s]warmup run: 1461it [00:03, 961.91it/s]warmup run: 1563it [00:03, 961.27it/s]warmup run: 1544it [00:03, 968.03it/s]warmup run: 1676it [00:03, 977.33it/s]warmup run: 1788it [00:03, 988.77it/s]warmup run: 1433it [00:02, 942.11it/s]warmup run: 1870it [00:03, 970.36it/s]warmup run: 1577it [00:03, 975.08it/s]warmup run: 1562it [00:03, 974.76it/s]warmup run: 1661it [00:03, 965.15it/s]warmup run: 1643it [00:03, 970.39it/s]warmup run: 1891it [00:03, 999.90it/s]warmup run: 1775it [00:03, 971.48it/s]warmup run: 1531it [00:03, 950.24it/s]warmup run: 1970it [00:03, 978.24it/s]warmup run: 1662it [00:03, 980.33it/s]warmup run: 1677it [00:03, 979.96it/s]warmup run: 1759it [00:03, 964.99it/s]warmup run: 1741it [00:03, 971.74it/s]warmup run: 1994it [00:03, 1006.85it/s]warmup run: 1873it [00:03, 957.35it/s]warmup run: 1628it [00:03, 955.46it/s]warmup run: 2081it [00:03, 1015.45it/s]warmup run: 1777it [00:03, 984.83it/s]warmup run: 1763it [00:03, 986.31it/s]warmup run: 1858it [00:03, 970.56it/s]warmup run: 1839it [00:03, 974.13it/s]warmup run: 2108it [00:03, 1046.38it/s]warmup run: 1970it [00:03, 959.27it/s]warmup run: 1726it [00:03, 960.20it/s]warmup run: 2198it [00:03, 1059.09it/s]warmup run: 1877it [00:03, 987.97it/s]warmup run: 1864it [00:03, 991.76it/s]warmup run: 1956it [00:03, 973.08it/s]warmup run: 1937it [00:03, 974.39it/s]warmup run: 2079it [00:03, 996.98it/s]warmup run: 2213it [00:03, 1008.84it/s]warmup run: 1823it [00:03, 962.66it/s]warmup run: 2313it [00:03, 1086.17it/s]warmup run: 1965it [00:03, 995.70it/s]warmup run: 1977it [00:03, 988.50it/s]warmup run: 2062it [00:03, 997.30it/s]warmup run: 2044it [00:03, 1001.32it/s]warmup run: 2190it [00:03, 1029.67it/s]warmup run: 2330it [00:03, 1055.65it/s]warmup run: 1920it [00:03, 963.28it/s]warmup run: 2426it [00:03, 1096.99it/s]warmup run: 2076it [00:03, 1029.09it/s]warmup run: 2090it [00:03, 1028.39it/s]warmup run: 2176it [00:03, 1037.60it/s]warmup run: 2164it [00:03, 1060.34it/s]warmup run: 2302it [00:03, 1054.48it/s]warmup run: 2448it [00:03, 1090.77it/s]warmup run: 2021it [00:03, 975.79it/s]warmup run: 2536it [00:04, 1085.95it/s]warmup run: 2195it [00:03, 1076.02it/s]warmup run: 2207it [00:03, 1069.44it/s]warmup run: 2290it [00:03, 1066.61it/s]warmup run: 2284it [00:03, 1101.42it/s]warmup run: 2416it [00:03, 1078.54it/s]warmup run: 2566it [00:04, 1115.45it/s]warmup run: 2141it [00:03, 1040.87it/s]warmup run: 2647it [00:04, 1091.47it/s]warmup run: 2315it [00:03, 1111.15it/s]warmup run: 2325it [00:03, 1099.72it/s]warmup run: 2403it [00:03, 1085.49it/s]warmup run: 2402it [00:03, 1123.05it/s]warmup run: 2529it [00:04, 1091.90it/s]warmup run: 2682it [00:04, 1127.83it/s]warmup run: 2261it [00:03, 1087.09it/s]warmup run: 2758it [00:04, 1094.15it/s]warmup run: 2435it [00:03, 1135.53it/s]warmup run: 2443it [00:03, 1120.99it/s]warmup run: 2517it [00:04, 1100.40it/s]warmup run: 2518it [00:04, 1133.30it/s]warmup run: 2643it [00:04, 1104.73it/s]warmup run: 2799it [00:04, 1139.69it/s]warmup run: 2381it [00:03, 1119.81it/s]warmup run: 2874it [00:04, 1110.90it/s]warmup run: 2555it [00:04, 1151.95it/s]warmup run: 2560it [00:04, 1132.89it/s]warmup run: 2635it [00:04, 1121.40it/s]warmup run: 2634it [00:04, 1138.62it/s]warmup run: 2754it [00:04, 1105.98it/s]warmup run: 2916it [00:04, 1148.45it/s]warmup run: 2501it [00:03, 1142.86it/s]warmup run: 2673it [00:04, 1159.50it/s]warmup run: 2988it [00:04, 1117.36it/s]warmup run: 2674it [00:04, 1134.23it/s]warmup run: 3000it [00:04, 665.03it/s] warmup run: 2753it [00:04, 1138.79it/s]warmup run: 3000it [00:04, 675.25it/s] warmup run: 2752it [00:04, 1149.23it/s]warmup run: 2867it [00:04, 1112.46it/s]warmup run: 2620it [00:04, 1156.17it/s]warmup run: 2793it [00:04, 1171.55it/s]warmup run: 2790it [00:04, 1140.70it/s]warmup run: 2871it [00:04, 1148.54it/s]warmup run: 2870it [00:04, 1158.16it/s]warmup run: 2987it [00:04, 1136.38it/s]warmup run: 2740it [00:04, 1168.11it/s]warmup run: 3000it [00:04, 669.13it/s] warmup run: 2912it [00:04, 1175.29it/s]warmup run: 2907it [00:04, 1147.15it/s]warmup run: 2991it [00:04, 1161.04it/s]warmup run: 3000it [00:04, 678.74it/s] warmup run: 2988it [00:04, 1162.38it/s]warmup run: 3000it [00:04, 678.90it/s] warmup run: 2858it [00:04, 1170.74it/s]warmup run: 3000it [00:04, 674.37it/s] warmup run: 3000it [00:04, 677.17it/s] warmup run: 2978it [00:04, 1178.83it/s]warmup run: 3000it [00:04, 679.05it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1618.82it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1615.66it/s]warmup should be done:   4%|         | 106/3000 [00:00<00:02, 1056.31it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1584.52it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.28it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.33it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1601.25it/s]warmup should be done:   5%|         | 157/3000 [00:00<00:01, 1560.89it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1619.26it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1622.57it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1620.64it/s]warmup should be done:  11%|         | 322/3000 [00:00<00:01, 1605.94it/s]warmup should be done:   7%|         | 217/3000 [00:00<00:02, 1085.01it/s]warmup should be done:  10%|         | 315/3000 [00:00<00:01, 1571.12it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1618.37it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1599.61it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1617.84it/s]warmup should be done:  13%|        | 378/3000 [00:00<00:01, 1322.99it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1617.95it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1601.23it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1615.57it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1614.26it/s]warmup should be done:  16%|        | 481/3000 [00:00<00:01, 1590.06it/s]warmup should be done:  16%|        | 473/3000 [00:00<00:01, 1563.20it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1613.30it/s]warmup should be done:  18%|        | 538/3000 [00:00<00:01, 1431.99it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1624.53it/s]warmup should be done:  21%|       | 644/3000 [00:00<00:01, 1602.38it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1618.40it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1611.77it/s]warmup should be done:  21%|        | 630/3000 [00:00<00:01, 1559.01it/s]warmup should be done:  21%|       | 641/3000 [00:00<00:01, 1580.08it/s]warmup should be done:  23%|       | 699/3000 [00:00<00:01, 1495.23it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1628.71it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1607.96it/s]warmup should be done:  27%|       | 805/3000 [00:00<00:01, 1602.10it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1616.08it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1610.27it/s]warmup should be done:  26%|       | 786/3000 [00:00<00:01, 1557.92it/s]warmup should be done:  27%|       | 802/3000 [00:00<00:01, 1587.29it/s]warmup should be done:  29%|       | 859/3000 [00:00<00:01, 1529.77it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1627.69it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1616.02it/s]warmup should be done:  32%|      | 966/3000 [00:00<00:01, 1598.81it/s]warmup should be done:  32%|      | 971/3000 [00:00<00:01, 1600.97it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1604.71it/s]warmup should be done:  32%|      | 962/3000 [00:00<00:01, 1586.49it/s]warmup should be done:  31%|      | 942/3000 [00:00<00:01, 1481.88it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1548.80it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1628.70it/s]warmup should be done:  38%|      | 1127/3000 [00:00<00:01, 1599.73it/s]warmup should be done:  38%|      | 1132/3000 [00:00<00:01, 1593.09it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1601.13it/s]warmup should be done:  38%|      | 1125/3000 [00:00<00:01, 1599.50it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1598.85it/s]warmup should be done:  36%|      | 1091/3000 [00:00<00:01, 1332.24it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1561.87it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1628.56it/s]warmup should be done:  43%|     | 1288/3000 [00:00<00:01, 1600.85it/s]warmup should be done:  43%|     | 1288/3000 [00:00<00:01, 1608.32it/s]warmup should be done:  43%|     | 1292/3000 [00:00<00:01, 1589.64it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1604.42it/s]warmup should be done:  43%|     | 1296/3000 [00:00<00:01, 1599.49it/s]warmup should be done:  41%|     | 1244/3000 [00:00<00:01, 1387.83it/s]warmup should be done:  45%|     | 1337/3000 [00:00<00:01, 1571.43it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1629.42it/s]warmup should be done:  48%|     | 1449/3000 [00:00<00:00, 1601.07it/s]warmup should be done:  48%|     | 1450/3000 [00:00<00:00, 1610.23it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1605.26it/s]warmup should be done:  49%|     | 1457/3000 [00:00<00:00, 1601.26it/s]warmup should be done:  48%|     | 1451/3000 [00:00<00:00, 1581.88it/s]warmup should be done:  47%|     | 1402/3000 [00:00<00:01, 1442.16it/s]warmup should be done:  50%|     | 1497/3000 [00:01<00:00, 1577.50it/s]warmup should be done:  54%|    | 1632/3000 [00:01<00:00, 1629.06it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1601.78it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1613.31it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1613.21it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1592.04it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1578.03it/s]warmup should be done:  52%|    | 1560/3000 [00:01<00:00, 1481.77it/s]warmup should be done:  55%|    | 1657/3000 [00:01<00:00, 1582.08it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1628.77it/s]warmup should be done:  59%|    | 1771/3000 [00:01<00:00, 1602.66it/s]warmup should be done:  59%|    | 1776/3000 [00:01<00:00, 1618.17it/s]warmup should be done:  60%|    | 1788/3000 [00:01<00:00, 1619.06it/s]warmup should be done:  59%|    | 1779/3000 [00:01<00:00, 1595.24it/s]warmup should be done:  59%|    | 1768/3000 [00:01<00:00, 1575.82it/s]warmup should be done:  57%|    | 1718/3000 [00:01<00:00, 1510.16it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1628.73it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1581.58it/s]warmup should be done:  64%|   | 1932/3000 [00:01<00:00, 1602.91it/s]warmup should be done:  65%|   | 1939/3000 [00:01<00:00, 1620.66it/s]warmup should be done:  65%|   | 1951/3000 [00:01<00:00, 1621.35it/s]warmup should be done:  65%|   | 1940/3000 [00:01<00:00, 1598.88it/s]warmup should be done:  64%|   | 1926/3000 [00:01<00:00, 1573.59it/s]warmup should be done:  63%|   | 1876/3000 [00:01<00:00, 1529.89it/s]warmup should be done:  66%|   | 1975/3000 [00:01<00:00, 1583.34it/s]warmup should be done:  70%|   | 2093/3000 [00:01<00:00, 1603.33it/s]warmup should be done:  70%|   | 2102/3000 [00:01<00:00, 1623.24it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1606.65it/s]warmup should be done:  70%|   | 2115/3000 [00:01<00:00, 1625.17it/s]warmup should be done:  70%|   | 2100/3000 [00:01<00:00, 1598.12it/s]warmup should be done:  69%|   | 2084/3000 [00:01<00:00, 1567.39it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1544.20it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1585.57it/s]warmup should be done:  75%|  | 2254/3000 [00:01<00:00, 1600.69it/s]warmup should be done:  76%|  | 2265/3000 [00:01<00:00, 1621.81it/s]warmup should be done:  76%|  | 2278/3000 [00:01<00:00, 1624.32it/s]warmup should be done:  75%|  | 2260/3000 [00:01<00:00, 1593.00it/s]warmup should be done:  75%|  | 2241/3000 [00:01<00:00, 1562.50it/s]warmup should be done:  76%|  | 2282/3000 [00:01<00:00, 1573.45it/s]warmup should be done:  73%|  | 2192/3000 [00:01<00:00, 1553.11it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1582.30it/s]warmup should be done:  80%|  | 2415/3000 [00:01<00:00, 1601.33it/s]warmup should be done:  81%|  | 2428/3000 [00:01<00:00, 1621.22it/s]warmup should be done:  81%| | 2442/3000 [00:01<00:00, 1626.08it/s]warmup should be done:  81%|  | 2420/3000 [00:01<00:00, 1594.75it/s]warmup should be done:  80%|  | 2398/3000 [00:01<00:00, 1562.45it/s]warmup should be done:  81%| | 2440/3000 [00:01<00:00, 1556.26it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1557.83it/s]warmup should be done:  82%| | 2453/3000 [00:01<00:00, 1582.60it/s]warmup should be done:  86%| | 2576/3000 [00:01<00:00, 1601.55it/s]warmup should be done:  87%| | 2606/3000 [00:01<00:00, 1627.38it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1614.85it/s]warmup should be done:  86%| | 2580/3000 [00:01<00:00, 1585.35it/s]warmup should be done:  85%| | 2556/3000 [00:01<00:00, 1565.36it/s]warmup should be done:  87%| | 2596/3000 [00:01<00:00, 1544.51it/s]warmup should be done:  84%| | 2507/3000 [00:01<00:00, 1563.75it/s]warmup should be done:  87%| | 2613/3000 [00:01<00:00, 1585.98it/s]warmup should be done:  91%| | 2737/3000 [00:01<00:00, 1603.34it/s]warmup should be done:  92%|| 2770/3000 [00:01<00:00, 1628.72it/s]warmup should be done:  92%|| 2754/3000 [00:01<00:00, 1618.77it/s]warmup should be done:  91%|| 2740/3000 [00:01<00:00, 1589.65it/s]warmup should be done:  90%| | 2714/3000 [00:01<00:00, 1567.71it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1535.70it/s]warmup should be done:  89%| | 2666/3000 [00:01<00:00, 1568.82it/s]warmup should be done:  92%|| 2773/3000 [00:01<00:00, 1588.11it/s]warmup should be done:  97%|| 2899/3000 [00:01<00:00, 1607.72it/s]warmup should be done:  98%|| 2936/3000 [00:01<00:00, 1635.60it/s]warmup should be done:  97%|| 2919/3000 [00:01<00:00, 1625.78it/s]warmup should be done:  97%|| 2902/3000 [00:01<00:00, 1596.83it/s]warmup should be done:  96%|| 2873/3000 [00:01<00:00, 1572.70it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1544.20it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.86it/s]warmup should be done:  94%|| 2825/3000 [00:01<00:00, 1574.95it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1611.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1602.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1599.57it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1592.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1582.15it/s]warmup should be done:  98%|| 2934/3000 [00:01<00:00, 1593.09it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1539.87it/s]warmup should be done: 100%|| 2985/3000 [00:01<00:00, 1581.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1524.48it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.49it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1629.00it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.50it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1673.39it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1605.29it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1622.02it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1672.00it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.18it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1626.01it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1676.11it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1636.93it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1635.40it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1615.42it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1638.19it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1669.99it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1614.65it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1676.35it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1640.61it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1628.66it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1642.69it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1638.74it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1661.59it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1602.14it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1618.26it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1677.32it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1628.03it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1638.43it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1644.47it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1636.84it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1608.35it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1620.65it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1653.66it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1646.19it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1678.45it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1639.90it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1625.29it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1630.83it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1612.90it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1621.94it/s]warmup should be done:  28%|       | 836/3000 [00:00<00:01, 1648.61it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1678.03it/s]warmup should be done:  33%|      | 990/3000 [00:00<00:01, 1648.14it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1626.50it/s]warmup should be done:  33%|      | 988/3000 [00:00<00:01, 1643.66it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1630.10it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1633.31it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1618.79it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1649.52it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1625.52it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1649.70it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1674.55it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1643.57it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1621.26it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1640.16it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1630.53it/s]warmup should be done:  39%|      | 1168/3000 [00:00<00:01, 1651.99it/s]warmup should be done:  45%|     | 1346/3000 [00:00<00:00, 1677.61it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1622.62it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1645.29it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1640.25it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1643.09it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1626.63it/s]warmup should be done:  43%|     | 1301/3000 [00:00<00:01, 1616.39it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1657.32it/s]warmup should be done:  50%|     | 1514/3000 [00:00<00:00, 1677.69it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1624.59it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1644.61it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1640.70it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1643.38it/s]warmup should be done:  50%|     | 1502/3000 [00:00<00:00, 1660.22it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1618.80it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1628.47it/s]warmup should be done:  56%|    | 1683/3000 [00:01<00:00, 1679.11it/s]warmup should be done:  54%|    | 1632/3000 [00:01<00:00, 1626.48it/s]warmup should be done:  55%|    | 1652/3000 [00:01<00:00, 1646.54it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1641.72it/s]warmup should be done:  55%|    | 1645/3000 [00:01<00:00, 1645.97it/s]warmup should be done:  55%|    | 1642/3000 [00:01<00:00, 1630.90it/s]warmup should be done:  54%|    | 1627/3000 [00:01<00:00, 1620.77it/s]warmup should be done:  56%|    | 1670/3000 [00:01<00:00, 1663.21it/s]warmup should be done:  62%|   | 1852/3000 [00:01<00:00, 1680.02it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1625.23it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1646.66it/s]warmup should be done:  60%|    | 1813/3000 [00:01<00:00, 1640.60it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1646.42it/s]warmup should be done:  60%|    | 1806/3000 [00:01<00:00, 1630.26it/s]warmup should be done:  61%|   | 1838/3000 [00:01<00:00, 1665.28it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1619.57it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1679.53it/s]warmup should be done:  65%|   | 1958/3000 [00:01<00:00, 1622.48it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1642.41it/s]warmup should be done:  66%|   | 1975/3000 [00:01<00:00, 1645.19it/s]warmup should be done:  66%|   | 1978/3000 [00:01<00:00, 1637.79it/s]warmup should be done:  67%|   | 2005/3000 [00:01<00:00, 1664.05it/s]warmup should be done:  65%|   | 1952/3000 [00:01<00:00, 1617.07it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1626.72it/s]warmup should be done:  73%|  | 2189/3000 [00:01<00:00, 1677.68it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1623.81it/s]warmup should be done:  72%|  | 2147/3000 [00:01<00:00, 1642.58it/s]warmup should be done:  71%|  | 2140/3000 [00:01<00:00, 1645.63it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1638.44it/s]warmup should be done:  72%|  | 2172/3000 [00:01<00:00, 1663.55it/s]warmup should be done:  71%|   | 2116/3000 [00:01<00:00, 1621.59it/s]warmup should be done:  71%|   | 2133/3000 [00:01<00:00, 1626.81it/s]warmup should be done:  79%|  | 2357/3000 [00:01<00:00, 1678.06it/s]warmup should be done:  76%|  | 2285/3000 [00:01<00:00, 1626.48it/s]warmup should be done:  77%|  | 2312/3000 [00:01<00:00, 1644.63it/s]warmup should be done:  77%|  | 2306/3000 [00:01<00:00, 1647.15it/s]warmup should be done:  77%|  | 2308/3000 [00:01<00:00, 1639.75it/s]warmup should be done:  76%|  | 2280/3000 [00:01<00:00, 1626.38it/s]warmup should be done:  78%|  | 2340/3000 [00:01<00:00, 1666.86it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1627.51it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1679.76it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1645.44it/s]warmup should be done:  82%| | 2448/3000 [00:01<00:00, 1624.56it/s]warmup should be done:  82%| | 2472/3000 [00:01<00:00, 1648.45it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1640.63it/s]warmup should be done:  82%| | 2445/3000 [00:01<00:00, 1633.25it/s]warmup should be done:  82%| | 2459/3000 [00:01<00:00, 1628.04it/s]warmup should be done:  84%| | 2508/3000 [00:01<00:00, 1669.61it/s]warmup should be done:  90%| | 2695/3000 [00:01<00:00, 1679.72it/s]warmup should be done:  87%| | 2611/3000 [00:01<00:00, 1625.78it/s]warmup should be done:  88%| | 2642/3000 [00:01<00:00, 1644.05it/s]warmup should be done:  88%| | 2637/3000 [00:01<00:00, 1647.54it/s]warmup should be done:  89%| | 2675/3000 [00:01<00:00, 1668.91it/s]warmup should be done:  87%| | 2611/3000 [00:01<00:00, 1638.78it/s]warmup should be done:  87%| | 2622/3000 [00:01<00:00, 1626.70it/s]warmup should be done:  88%| | 2638/3000 [00:01<00:00, 1636.28it/s]warmup should be done:  95%|| 2863/3000 [00:01<00:00, 1677.75it/s]warmup should be done:  92%|| 2775/3000 [00:01<00:00, 1629.83it/s]warmup should be done:  94%|| 2807/3000 [00:01<00:00, 1644.34it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1647.70it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1647.59it/s]warmup should be done:  95%|| 2842/3000 [00:01<00:00, 1668.00it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1628.50it/s]warmup should be done:  93%|| 2802/3000 [00:01<00:00, 1633.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1663.00it/s]warmup should be done:  98%|| 2940/3000 [00:01<00:00, 1633.30it/s]warmup should be done:  99%|| 2973/3000 [00:01<00:00, 1646.48it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1647.97it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1653.61it/s]warmup should be done:  98%|| 2950/3000 [00:01<00:00, 1630.48it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1635.82it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.67it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1641.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.37it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.99it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1626.75it/s]2022-12-11 23:00:09.078671: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5f8c02d850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:09.078730: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.234267: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b87796340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.234331: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.545056: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b8b830eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.545122: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.670515: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5fbc02fa50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.670580: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.844341: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b8f831200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.844411: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.866356: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f5f0402a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.866433: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.867163: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b8782cf60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.867224: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:10.893080: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7b8f8329b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:00:10.893156: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:00:11.286910: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:12.487718: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:12.839663: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:12.907667: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:13.139577: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:13.153738: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:13.183839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:13.223944: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:00:14.136805: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:15.410459: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:15.753355: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:15.779519: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:16.027435: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:16.041948: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:16.114027: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:00:16.116808: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:00:53.865][ERROR][RK0][tid #140169629898496]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:00:53.866][ERROR][RK0][tid #140169629898496]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:53.871][ERROR][RK0][tid #140169629898496]: coll ps creation done
[HCTR][23:00:53.871][ERROR][RK0][tid #140169629898496]: replica 2 waits for coll ps creation barrier
[HCTR][23:00:53.884][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:00:53.884][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:53.891][ERROR][RK0][main]: coll ps creation done
[HCTR][23:00:53.891][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][23:00:53.900][ERROR][RK0][tid #140169294354176]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:00:53.900][ERROR][RK0][tid #140169294354176]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:53.907][ERROR][RK0][tid #140169294354176]: coll ps creation done
[HCTR][23:00:53.907][ERROR][RK0][tid #140169294354176]: replica 5 waits for coll ps creation barrier
[HCTR][23:00:53.914][ERROR][RK0][tid #140169294354176]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:00:53.914][ERROR][RK0][tid #140169294354176]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:53.920][ERROR][RK0][tid #140169294354176]: coll ps creation done
[HCTR][23:00:53.920][ERROR][RK0][tid #140169294354176]: replica 4 waits for coll ps creation barrier
[HCTR][23:00:53.963][ERROR][RK0][tid #140168799446784]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:00:53.963][ERROR][RK0][tid #140168799446784]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:53.968][ERROR][RK0][tid #140168799446784]: coll ps creation done
[HCTR][23:00:53.968][ERROR][RK0][tid #140168799446784]: replica 7 waits for coll ps creation barrier
[HCTR][23:00:54.007][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:00:54.007][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:54.012][ERROR][RK0][main]: coll ps creation done
[HCTR][23:00:54.012][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][23:00:54.064][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:00:54.064][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:54.068][ERROR][RK0][main]: coll ps creation done
[HCTR][23:00:54.068][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:00:54.076][ERROR][RK0][tid #140169294354176]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:00:54.076][ERROR][RK0][tid #140169294354176]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:00:54.081][ERROR][RK0][tid #140169294354176]: coll ps creation done
[HCTR][23:00:54.081][ERROR][RK0][tid #140169294354176]: replica 0 waits for coll ps creation barrier
[HCTR][23:00:54.081][ERROR][RK0][tid #140169294354176]: replica 0 preparing frequency
[HCTR][23:00:55.034][ERROR][RK0][tid #140169294354176]: replica 0 preparing frequency done
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: replica 0 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: replica 4 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: replica 5 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][tid #140168799446784]: replica 7 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][tid #140169629898496]: replica 2 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][tid #140168799446784]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][main]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][main]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][main]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][tid #140169629898496]: Calling build_v2
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][tid #140169294354176]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][tid #140168799446784]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:00:55.066][ERROR][RK0][tid #140169629898496]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 23:00:552022-12-11 23:00:552022-12-11 23:00:552022-12-11 23:00:552022-12-11 23:00:552022-12-11 23:00:55..2022-12-11 23:00:55....2022-12-11 23:00:55 66350 66351. 66351 66365 66365 66373.: :  66376: : : :  66372EE: EEEE:   E    E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136:136136136136:] ] 136] ] ] ] 136using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] 

using concurrent impl MPS



using concurrent impl MPS

[2022-12-11 23:00:55. 70603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:00:55. 70641: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:00:55:.196 70647] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:00:55[.2022-12-11 23:00:55 70695.:  70693[E: 2022-12-11 23:00:55 E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  70711:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 196:E] 178[ assigning 8 to cpu] 2022-12-11 23:00:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
v100x8, slow pcie.:
 70737212: ] E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-11 23:00:55
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 70775178: [[] [E2022-12-11 23:00:552022-12-11 23:00:55v100x8, slow pcie2022-12-11 23:00:55 ..
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 70789 70804 70799[:[: : : 2022-12-11 23:00:551962022-12-11 23:00:55EEE.] [.    70836assigning 8 to cpu2022-12-11 23:00:55 70845/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: 
.: :::2022-12-11 23:00:55E 70881E178213212[. :  ] ] ] 2022-12-11 23:00:55 70924/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcieremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: : :


 70984E[178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196[:  2022-12-11 23:00:55] [:] 2022-12-11 23:00:55E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.v100x8, slow pcie2022-12-11 23:00:55178assigning 8 to cpu. : 71085
.] 
 71082: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:  71094v100x8, slow pcie[E:] E: 
2022-12-11 23:00:55 212v100x8, slow pcie E[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-11 23:00:55 71204:2022-12-11 23:00:55build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: 196.
2022-12-11 23:00:55214: 71255E]  71265.] 213: [ assigning 8 to cpu:  71304cpu time is 97.0588] E2022-12-11 23:00:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E: 
remote time is 8.68421 .: E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 71361196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :: [] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[E2022-12-11 23:00:55assigning 8 to cpu196:] 2022-12-11 23:00:55 .
] 196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 71441assigning 8 to cpu] 
 71451:: 
assigning 8 to cpu[: 213[E
2022-12-11 23:00:55E] 2022-12-11 23:00:55 . remote time is 8.68421./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 71539/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 71547:[: :[: 214[2022-12-11 23:00:55E2122022-12-11 23:00:55E] 2022-12-11 23:00:55. ] . cpu time is 97.0588. 71602/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 71607/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 71616: :
: :: E212E[213E ]  2022-12-11 23:00:55]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
: 71711
:212212[: 214[] ] 2022-12-11 23:00:55E] 2022-12-11 23:00:55build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. cpu time is 97.0588.

 71772/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 71770: :[[: E2132022-12-11 23:00:552022-12-11 23:00:55E ] .. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 71843 71846/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
: : :[214EE2132022-12-11 23:00:55]   ] .cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 71925
::
: 213213E] ] [ remote time is 8.68421remote time is 8.684212022-12-11 23:00:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

.: 71989214[: [] 2022-12-11 23:00:55E2022-12-11 23:00:55cpu time is 97.0588. .
 72013/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 72015: :: E214E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
:214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-11 23:02:14.249851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:02:14.291225: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:02:14.291355: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:02:14.292598: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 23:02:14.372175: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 23:02:14.752635: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 23:02:14.752725: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 23:02:21.762780: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-11 23:02:21.762876: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 23:02:23.471147: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 23:02:23.471238: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-11 23:02:23.474055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 23:02:23.474113: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-11 23:02:23.756768: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 23:02:23.791414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 23:02:23.793660: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 23:02:23.829027: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 23:02:24.382371: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 23:02:41.225790: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 23:02:41.233612: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 23:02:41.235032: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 23:02:41.283617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:02:41.283715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:02:41.283747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:02:41.283785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:02:41.284415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:02:41.284469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.285432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.286119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.299143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 23:02:41.299220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 23:02:41.299372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 23:02:41.299434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 23:02:41.299674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:02:41.299728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.299833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[[2022-12-11 23:02:412022-12-11 23:02:41..299889299888: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2051815] ] worker 0 thread 1 initing device 1Building Coll Cache with ... num gpu device is 8

[2022-12-11 23:02:41.[2999172022-12-11 23:02:41: .E299960[ : 2022-12-11 23:02:41/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE.: 299967202/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: ] :E2 solved1980 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MB:
[2022022-12-11 23:02:41] .3000657 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-11 23:02:41] .worker 0 thread 2 initing device 2300105
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:02:41.300349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:02:41.300395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.300562: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:02:41:.1815300572] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:02:41.300635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-11 23:02:41.300658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.302697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 23:02:41.302758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 23:02:41.302800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:02:41.302856: E[ 2022-12-11 23:02:41/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:302856205: ] Eworker 0 thread 3 initing device 3 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.303160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.303204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 23:02:41
.303220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.303266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.303319: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:02:41.303369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.303705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.303768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.307255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.307435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.307495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.307573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.307639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-11 23:02:41
.307667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.308158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.311798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.311860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:02:41.363187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 23:02:41.368825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:02:41.368950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:02:41.369761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.370397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.371426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.371476: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.41 MB
[2022-12-11 23:02:41.374051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:02:41.374844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.374891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[[2022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:41......387021387021387021387021387024387023: : : : : : EEEEEE      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::[1980198019801980198019802022-12-11 23:02:41] ] ] ] ] ] .eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes387232





: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-11 23:02:41.401642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:02:41.401730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:02:41.402122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:02:41.402200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:02:41.402226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:02:41.[4023202022-12-11 23:02:41: .E402309 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1023
[2022-12-11 23:02:41.402419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-11 23:02:41.402426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-11 23:02:41.[4024872022-12-11 23:02:41: .[E4025072022-12-11 23:02:41 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE402524: : 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE[] : 2022-12-11 23:02:41eager release cuda mem 10231980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.
] :402562eager alloc mem 11.83 MB638: 
] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10232022-12-11 23:02:41
.402656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:02:41.402703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:02:41.403878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.404397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.405056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.405938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.406515: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.407026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:02:41.407347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.408018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.408070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.408110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.408322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.408341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:02:411980.] 408368eager alloc mem 611.00 KB[: 
2022-12-11 23:02:41W. 408383/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc[: :2022-12-11 23:02:41E43. ] 408413/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuWORKER[0] alloc host memory 11.44 MB: :
E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.408990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.409036: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-11 23:02:41] .WORKER[0] alloc host memory 11.41 MB409046
[: 2022-12-11 23:02:41E. 409071/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-11 23:02:41.409133[: 2022-12-11 23:02:41W. 409139/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: :W43 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccWORKER[0] alloc host memory 11.43 MB:
43] WORKER[0] alloc host memory 11.43 MB
[2022-12-11 23:02:41.409329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.409375: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.43 MB
[2022-12-11 23:02:41.[4094392022-12-11 23:02:41: .E409445 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 23:02:41.[4095192022-12-11 23:02:41: .W409524 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccW: 43/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc] :WORKER[0] alloc host memory 11.42 MB43
] WORKER[0] alloc host memory 11.39 MB
[2022-12-11 23:02:41.417716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:02:41.417894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:02:41.418301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 25.25 KB2022-12-11 23:02:41[
.2022-12-11 23:02:41418326.: 418342E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 25.25 KB] 
eager release cuda mem 25855
[2022-12-11 23:02:41.418411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:02:41.418500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.418544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:02:41.418960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.419005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:02:41eager alloc mem 1.43 GB.
419016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.419068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:02:41.419144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:02:41.419226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:02:41.419761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.419816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:02:41.419834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.419882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:02:41.419893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:02:41.420505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:02:41.420545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[[[[2022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:412022-12-11 23:02:41........867368867367867367867371867373867370867366867373: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] Device 0 init p2p of link 3] Device 2 init p2p of link 1Device 4 init p2p of link 5Device 7 init p2p of link 4Device 6 init p2p of link 0Device 1 init p2p of link 7Device 5 init p2p of link 6
Device 3 init p2p of link 2






[[[2022-12-11 23:02:412022-12-11 23:02:41[[[2022-12-11 23:02:41[..2022-12-11 23:02:41[2022-12-11 23:02:412022-12-11 23:02:41.2022-12-11 23:02:41867914867914.2022-12-11 23:02:41..867919.: : 867922.867922867922: 867928EE: 867937: : E:   E: EE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980:] ] 1980:19801980] 1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB] 1980] ] eager alloc mem 611.00 KB] 

eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB



[[[2022-12-11 23:02:412022-12-11 23:02:41[2022-12-11 23:02:41[.[.[2022-12-11 23:02:41.2022-12-11 23:02:41[8689552022-12-11 23:02:418689572022-12-11 23:02:41.868957.2022-12-11 23:02:41: .: .868966: 868968.E868975E868977: E: 868985 :  : E E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :] :638] 638:eager release cuda mem 625663638eager release cuda mem 625663638] eager release cuda mem 625663] 638
] 
] eager release cuda mem 625663
eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663

eager release cuda mem 625663


[2022-12-11 23:02:41.882417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:02:41.882490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:02:41.882567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.882650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.882732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:02:41.882836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:02:41.882884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.882996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.883327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 23:02:41.883393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41[.2022-12-11 23:02:41883468.: 883475E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-11 23:02:41.883566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:02:41.[8837012022-12-11 23:02:41: .E883699: E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[::2022-12-11 23:02:416381926.] ] 883755[eager release cuda mem 625663Device 3 init p2p of link 0: 2022-12-11 23:02:41

E. 883800/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-11 23:02:41.883934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:02:41eager alloc mem 611.00 KB.
883939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 23:02:41.884130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.884324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.884627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.884735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.884924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.896108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:02:41.896224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.896435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 23:02:41.896558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.896702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:02:41.896824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.896884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 23:02:41.897003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:02:41eager alloc mem 611.00 KB.
897018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.897148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:02:41.897278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.897325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:02:411926.] 897351Device 5 init p2p of link 7: 
[E2022-12-11 23:02:41 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc897371:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 0 init p2p of link 1
[2022-12-11 23:02:41.897460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.897541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.897577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:02:41.897618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.897695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.897804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.898073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.898254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.898334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.898484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.911940: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:02:41.912063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.912431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 23:02:41.912547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.912862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.913163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 23:02:41.913276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:02:412022-12-11 23:02:41..913338913351: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 1 init p2p of link 0eager release cuda mem 625663

[2022-12-11 23:02:41.913478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.913722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:02:41.913846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.914059: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:02:41:.638914063] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:02:41.914207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.914283: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:02:41:.638914289] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 23:02:41.914412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.914567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 23:02:41.914644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.914691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:02:41.914996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.915192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.915488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:02:41.927802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.928253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.928554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.928898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.929166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.929639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.929803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2990219 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8982500 / 100000000 nodes ( 8.98 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.626548 secs 
[2022-12-11 23:02:41.929885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2998371 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8974348 / 100000000 nodes ( 8.97 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.629246 secs 
[2022-12-11 23:02:41.929977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2997348 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8975371 / 100000000 nodes ( 8.98 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.630267 secs 
[2022-12-11 23:02:41.930032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.930147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2993525 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8979194 / 100000000 nodes ( 8.98 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.6302 secs 
[2022-12-11 23:02:41.930288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2986222 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8986497 / 100000000 nodes ( 8.99 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.629903 secs 
[2022-12-11 23:02:41.930457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:02:41.930644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2997549 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8975170 / 100000000 nodes ( 8.98 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.63002 secs 
[2022-12-11 23:02:41.930797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2996945 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8975774 / 100000000 nodes ( 8.98 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.627441 secs 
[2022-12-11 23:02:41.931961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2991181 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8981538 / 100000000 nodes ( 8.98 %) | cpu 88027281 / 100000000 nodes ( 88.03 %) | 1.43 GB | 0.647506 secs 
[2022-12-11 23:02:41.934488: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.07 GB
[2022-12-11 23:02:43.673244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.33 GB
[2022-12-11 23:02:43.674187: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.33 GB
[2022-12-11 23:02:43.675624: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.33 GB
[2022-12-11 23:02:45.125231: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.59 GB
[2022-12-11 23:02:45.125807: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.59 GB
[2022-12-11 23:02:45.127757: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.59 GB
[2022-12-11 23:02:46.317539: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 8.81 GB
[2022-12-11 23:02:46.317660: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 8.81 GB
[2022-12-11 23:02:46.318779: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.81 GB
[2022-12-11 23:02:47.622815: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.02 GB
[2022-12-11 23:02:47.622998: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.02 GB
[2022-12-11 23:02:47.623344: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.02 GB
[2022-12-11 23:02:48.931509: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.48 GB
[2022-12-11 23:02:48.932462: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.48 GB
[2022-12-11 23:02:48.933065: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.48 GB
[2022-12-11 23:02:50.203799: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.68 GB
[2022-12-11 23:02:50.204220: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.68 GB
[HCTR][23:02:51.207][ERROR][RK0][tid #140169629898496]: replica 2 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: replica 4 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: replica 0 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: replica 5 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][tid #140168799446784]: replica 7 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][tid #140169629898496]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][tid #140168799446784]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: init per replica done
[HCTR][23:02:51.207][ERROR][RK0][tid #140169294354176]: init per replica done
[HCTR][23:02:51.207][ERROR][RK0][main]: init per replica done
[HCTR][23:02:51.207][ERROR][RK0][main]: init per replica done
[HCTR][23:02:51.207][ERROR][RK0][tid #140169629898496]: init per replica done
[HCTR][23:02:51.207][ERROR][RK0][tid #140168799446784]: init per replica done
[HCTR][23:02:51.207][ERROR][RK0][main]: init per replica done
[HCTR][23:02:51.210][ERROR][RK0][tid #140169294354176]: init per replica done
[HCTR][23:02:51.214][ERROR][RK0][tid #140169629898496]: 2 allocated 3276800 at 0x7f7d7ad20000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169629898496]: 2 allocated 6553600 at 0x7f7d7b200000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169629898496]: 2 allocated 3276800 at 0x7f7d7b840000
[HCTR][23:02:51.214][ERROR][RK0][tid #140168799446784]: 7 allocated 3276800 at 0x7f7d6ad20000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169629898496]: 2 allocated 6553600 at 0x7f7d7bb60000
[HCTR][23:02:51.214][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7d7ad20000
[HCTR][23:02:51.214][ERROR][RK0][tid #140168799446784]: 7 allocated 6553600 at 0x7f7d6b200000
[HCTR][23:02:51.214][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7d7b200000
[HCTR][23:02:51.214][ERROR][RK0][tid #140168799446784]: 7 allocated 3276800 at 0x7f7d6b840000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169302746880]: 1 allocated 3276800 at 0x7f7d7ed20000
[HCTR][23:02:51.214][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7d7b840000
[HCTR][23:02:51.214][ERROR][RK0][tid #140168799446784]: 7 allocated 6553600 at 0x7f7d6bb60000
[HCTR][23:02:51.214][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7d7ad20000
[HCTR][23:02:51.214][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7d7bb60000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169302746880]: 1 allocated 6553600 at 0x7f7d7f200000
[HCTR][23:02:51.214][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f7d7ed20000
[HCTR][23:02:51.214][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7d7b200000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169302746880]: 1 allocated 3276800 at 0x7f7d7f840000
[HCTR][23:02:51.214][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f7d7f200000
[HCTR][23:02:51.214][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7d7b840000
[HCTR][23:02:51.214][ERROR][RK0][tid #140169302746880]: 1 allocated 6553600 at 0x7f7d7fb60000
[HCTR][23:02:51.214][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f7d7f840000
[HCTR][23:02:51.214][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7d7bb60000
[HCTR][23:02:51.214][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f7d7ed20000
[HCTR][23:02:51.214][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f7d7fb60000
[HCTR][23:02:51.214][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f7d7f200000
[HCTR][23:02:51.214][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f7d7f840000
[HCTR][23:02:51.214][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f7d7fb60000
[HCTR][23:02:51.217][ERROR][RK0][tid #140169294354176]: 0 allocated 3276800 at 0x7f7d81920000
[HCTR][23:02:51.217][ERROR][RK0][tid #140169294354176]: 0 allocated 6553600 at 0x7f7d81e00000
[HCTR][23:02:51.217][ERROR][RK0][tid #140169294354176]: 0 allocated 3276800 at 0x7f7d82b0e800
[HCTR][23:02:51.217][ERROR][RK0][tid #140169294354176]: 0 allocated 6553600 at 0x7f7d82e2e800








