2022-12-12 05:36:41.055604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.062174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.070092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.074711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.088477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.094275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.098947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.110835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.163303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.170199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.173177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.174260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.175121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.175992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.177069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.178085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.179333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.181036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.182037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.182179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.183522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.183536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.184839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.184953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.186248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.186439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.187564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.187946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.188916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.189639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.190949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.191231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.192259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.192956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.194426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.195524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.196448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.197366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.198314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.199429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.203482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.204561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.204714: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.205508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.206441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.207411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.208520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.209572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.210990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.214936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.215431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.216958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.216980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.217202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.219023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.219179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.219181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.221578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.221703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.224469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.224635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.225074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.227095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.227342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.227939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.229695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.230036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.230718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.232470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.232711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.233607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.235435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.236210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.237913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.238849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.240605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.241116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.241540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.243092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.243854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.243983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.244153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.244999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.246576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.257028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.260150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.261700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.263170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.263316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.263913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.264168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.265790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.266147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.266694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.266874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.279156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.281596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.302916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.303248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.303509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.304182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.304291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.305522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.305594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.307084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.307737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.308226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.309042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.309555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.310777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.310952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.312232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.313935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.314829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.315039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.315766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.317232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.317400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.318595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.319207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.320496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.321584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.321877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.321964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.322890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.323383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.325043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.326253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.326358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.327171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.327579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.329569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.330469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.330590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.331545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.331873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.332786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.334015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.334141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.334828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.335175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.336364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.337452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.337538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.338168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.338733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.339804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.340892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.340983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.341801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.342246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.344322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.345233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.345378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.346306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.346858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.348460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.348869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.349012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.349550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.350269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.352229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.352736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.352834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.353451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.354002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.355742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.356181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.356280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.357239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.357674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.359007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.359511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.359561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.360574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.361425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.362433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.362920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.363015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.364254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.364547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.364951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.366523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.368708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.368837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.369538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.369562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.369792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.369859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.370995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.373068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.373102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.374082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.374325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.374510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.374858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.377737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.377904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.378385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.378566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.378790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.379093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.379910: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.381652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.381700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.382186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.382494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.382717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.382955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.386372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.386475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.387552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.387823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.388516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.388643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.390006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.391740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.391905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.392454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.392674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.393319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.394799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.396561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.396694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.396869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.396951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.397293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.397557: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.398675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.400401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.400597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.401090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.401188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.401603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.404982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.405238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.406998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.408495: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.408899: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.408900: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.409306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.409470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.410591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.412538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.412583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.413623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.415491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.415565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.418274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.418332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.418344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.418473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.418650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.423884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.424091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.424195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.424322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.424422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.428749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.428923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.429018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.429067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.429115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.433836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.433999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.438301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.439351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.475055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.485028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.489681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.491081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.495928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.496870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.529057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.529590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.535029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.536245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.550164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.551871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.556998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.559612: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.569580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.593497: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:36:41.603052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.610503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.619948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.620455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:41.626342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.643366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.644032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.644576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.645043: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.645101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:36:42.662785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.663651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.664505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.665533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.666451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.667014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:36:42.714348: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:42.714548: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:42.751801: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:36:42.881291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.882214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.883342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.884825: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.884885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:36:42.902714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.905251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.906244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.907329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.909114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.910051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:36:42.911614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.912688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.913676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.914613: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.914663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:36:42.932143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.933194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.934314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.935759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.936812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.938179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:36:42.958385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.958385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.959713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.959732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.960723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.960777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.961667: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.961705: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.961722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:36:42.961753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:36:42.978276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.978906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.979301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.979466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.979570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.980734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.980923: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.980991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:36:42.981188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.982033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.982535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.983078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.983183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.983659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.984517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.984813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.985092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.985534: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:42.985698: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:42.985820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:36:42.986186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.986234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:36:42.986643: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:36:42.986657: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:42.986718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:36:42.997815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:42.999251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.000374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.001531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.002645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.003695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:36:43.004147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.005381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.006096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.006806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.007708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.008581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.009191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.010175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.010699: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:36:43.010756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:36:43.011614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:36:43.028168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.029317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.030322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.031539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.032556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:36:43.032752: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.032819: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.032933: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.032980: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.033484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:36:43.034698: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 05:36:43.034728: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:36:43.036045: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.036185: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.037153: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:36:43.050120: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.050312: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.051250: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:36:43.057730: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.057875: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.058804: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:36:43.078941: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.079153: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:36:43.080877: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][05:36:44.354][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.354][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.355][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.355][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.355][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.355][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.355][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:36:44.355][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 101it [00:01, 84.14it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 98it [00:01, 83.47it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 203it [00:01, 183.77it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 101it [00:01, 84.91it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 197it [00:01, 182.06it/s]warmup run: 99it [00:01, 84.72it/s]warmup run: 303it [00:01, 291.39it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 95it [00:01, 79.68it/s]warmup run: 199it [00:01, 181.09it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 97it [00:01, 84.83it/s]warmup run: 298it [00:01, 292.99it/s]warmup run: 199it [00:01, 184.64it/s]warmup run: 404it [00:01, 405.26it/s]warmup run: 100it [00:01, 86.42it/s]warmup run: 189it [00:01, 171.81it/s]warmup run: 297it [00:01, 287.17it/s]warmup run: 96it [00:01, 83.31it/s]warmup run: 195it [00:01, 184.43it/s]warmup run: 396it [00:01, 403.08it/s]warmup run: 301it [00:01, 296.96it/s]warmup run: 505it [00:02, 516.83it/s]warmup run: 200it [00:01, 187.06it/s]warmup run: 281it [00:01, 271.12it/s]warmup run: 395it [00:01, 397.57it/s]warmup run: 193it [00:01, 181.32it/s]warmup run: 292it [00:01, 292.13it/s]warmup run: 493it [00:02, 508.47it/s]warmup run: 402it [00:01, 411.40it/s]warmup run: 608it [00:02, 622.65it/s]warmup run: 294it [00:01, 289.08it/s]warmup run: 373it [00:01, 374.57it/s]warmup run: 494it [00:02, 506.66it/s]warmup run: 290it [00:01, 288.79it/s]warmup run: 391it [00:01, 406.35it/s]warmup run: 591it [00:02, 607.22it/s]warmup run: 503it [00:02, 523.20it/s]warmup run: 711it [00:02, 715.14it/s]warmup run: 393it [00:01, 402.49it/s]warmup run: 465it [00:02, 475.51it/s]warmup run: 595it [00:02, 610.67it/s]warmup run: 386it [00:01, 398.20it/s]warmup run: 490it [00:01, 516.62it/s]warmup run: 689it [00:02, 692.73it/s]warmup run: 605it [00:02, 627.72it/s]warmup run: 813it [00:02, 788.69it/s]warmup run: 484it [00:02, 497.46it/s]warmup run: 557it [00:02, 567.98it/s]warmup run: 693it [00:02, 695.50it/s]warmup run: 485it [00:01, 509.41it/s]warmup run: 592it [00:02, 623.04it/s]warmup run: 787it [00:02, 763.02it/s]warmup run: 705it [00:02, 713.06it/s]warmup run: 914it [00:02, 845.42it/s]warmup run: 583it [00:02, 601.17it/s]warmup run: 650it [00:02, 651.24it/s]warmup run: 793it [00:02, 769.14it/s]warmup run: 589it [00:02, 621.52it/s]warmup run: 695it [00:02, 716.17it/s]warmup run: 885it [00:02, 818.98it/s]warmup run: 807it [00:02, 788.57it/s]warmup run: 1016it [00:02, 890.48it/s]warmup run: 682it [00:02, 690.46it/s]warmup run: 743it [00:02, 718.83it/s]warmup run: 891it [00:02, 821.89it/s]warmup run: 690it [00:02, 710.93it/s]warmup run: 796it [00:02, 787.88it/s]warmup run: 986it [00:02, 869.81it/s]warmup run: 909it [00:02, 847.06it/s]warmup run: 1119it [00:02, 928.92it/s]warmup run: 777it [00:02, 752.21it/s]warmup run: 836it [00:02, 772.18it/s]warmup run: 990it [00:02, 867.24it/s]warmup run: 788it [00:02, 777.26it/s]warmup run: 898it [00:02, 846.93it/s]warmup run: 1088it [00:02, 909.81it/s]warmup run: 1010it [00:02, 889.27it/s]warmup run: 1223it [00:02, 958.47it/s]warmup run: 929it [00:02, 814.11it/s]warmup run: 872it [00:02, 797.91it/s]warmup run: 1090it [00:02, 903.79it/s]warmup run: 886it [00:02, 829.70it/s]warmup run: 999it [00:02, 891.14it/s]warmup run: 1188it [00:02, 935.19it/s]warmup run: 1112it [00:02, 923.91it/s]warmup run: 1325it [00:02, 973.00it/s]warmup run: 1022it [00:02, 844.93it/s]warmup run: 966it [00:02, 836.22it/s]warmup run: 1190it [00:02, 929.49it/s]warmup run: 984it [00:02, 867.77it/s]warmup run: 1101it [00:02, 926.20it/s]warmup run: 1291it [00:02, 961.72it/s]warmup run: 1214it [00:02, 950.04it/s]warmup run: 1428it [00:02, 989.10it/s]warmup run: 1115it [00:02, 866.17it/s]warmup run: 1063it [00:02, 871.93it/s]warmup run: 1289it [00:02, 946.86it/s]warmup run: 1082it [00:02, 897.10it/s]warmup run: 1202it [00:02, 948.24it/s]warmup run: 1395it [00:02, 982.01it/s]warmup run: 1316it [00:02, 968.10it/s]warmup run: 1532it [00:03, 1001.89it/s]warmup run: 1161it [00:02, 902.19it/s]warmup run: 1208it [00:02, 882.06it/s]warmup run: 1389it [00:02, 962.20it/s]warmup run: 1181it [00:02, 920.95it/s]warmup run: 1303it [00:02, 964.36it/s]warmup run: 1498it [00:03, 995.44it/s]warmup run: 1418it [00:02, 981.85it/s]warmup run: 1636it [00:03, 1011.69it/s]warmup run: 1300it [00:02, 892.74it/s]warmup run: 1257it [00:02, 915.74it/s]warmup run: 1489it [00:03, 964.48it/s]warmup run: 1279it [00:02, 932.57it/s]warmup run: 1405it [00:02, 979.46it/s]warmup run: 1600it [00:03, 1000.39it/s]warmup run: 1520it [00:03, 992.04it/s]warmup run: 1739it [00:03, 1014.30it/s]warmup run: 1353it [00:02, 928.19it/s]warmup run: 1589it [00:03, 972.91it/s]warmup run: 1392it [00:03, 861.26it/s]warmup run: 1377it [00:02, 942.74it/s]warmup run: 1507it [00:02, 989.98it/s]warmup run: 1702it [00:03, 1001.34it/s]warmup run: 1622it [00:03, 988.96it/s]warmup run: 1842it [00:03, 1008.48it/s]warmup run: 1449it [00:03, 937.20it/s]warmup run: 1689it [00:03, 980.46it/s]warmup run: 1494it [00:03, 904.32it/s]warmup run: 1475it [00:03, 950.88it/s]warmup run: 1608it [00:03, 995.37it/s]warmup run: 1804it [00:03, 1004.70it/s]warmup run: 1723it [00:03, 981.90it/s]warmup run: 1944it [00:03, 994.75it/s] warmup run: 1545it [00:03, 941.57it/s]warmup run: 1789it [00:03, 979.85it/s]warmup run: 1596it [00:03, 935.97it/s]warmup run: 1572it [00:03, 943.79it/s]warmup run: 1711it [00:03, 1003.59it/s]warmup run: 1906it [00:03, 1004.30it/s]warmup run: 1823it [00:03, 978.66it/s]warmup run: 2048it [00:03, 1007.44it/s]warmup run: 1641it [00:03, 946.14it/s]warmup run: 1888it [00:03, 982.32it/s]warmup run: 1697it [00:03, 956.47it/s]warmup run: 1670it [00:03, 953.06it/s]warmup run: 1814it [00:03, 1008.81it/s]warmup run: 2008it [00:03, 1007.54it/s]warmup run: 1924it [00:03, 984.84it/s]warmup run: 2168it [00:03, 1062.55it/s]warmup run: 1739it [00:03, 953.54it/s]warmup run: 1988it [00:03, 985.81it/s]warmup run: 1799it [00:03, 973.41it/s]warmup run: 1770it [00:03, 965.53it/s]warmup run: 1916it [00:03, 1012.05it/s]warmup run: 2129it [00:03, 1067.21it/s]warmup run: 2030it [00:03, 1005.57it/s]warmup run: 2289it [00:03, 1103.99it/s]warmup run: 1838it [00:03, 961.73it/s]warmup run: 2103it [00:03, 1033.96it/s]warmup run: 1902it [00:03, 987.57it/s]warmup run: 1868it [00:03, 968.23it/s]warmup run: 2021it [00:03, 1023.31it/s]warmup run: 2250it [00:03, 1109.22it/s]warmup run: 2153it [00:03, 1071.11it/s]warmup run: 2411it [00:03, 1137.48it/s]warmup run: 1936it [00:03, 964.44it/s]warmup run: 2222it [00:03, 1078.12it/s]warmup run: 2004it [00:03, 996.70it/s]warmup run: 1966it [00:03, 969.50it/s]warmup run: 2144it [00:03, 1082.91it/s]warmup run: 2372it [00:03, 1139.75it/s]warmup run: 2276it [00:03, 1118.19it/s]warmup run: 2533it [00:03, 1160.29it/s]warmup run: 2041it [00:03, 987.49it/s]warmup run: 2341it [00:03, 1109.80it/s]warmup run: 2127it [00:03, 1063.92it/s]warmup run: 2075it [00:03, 1004.79it/s]warmup run: 2267it [00:03, 1125.03it/s]warmup run: 2494it [00:03, 1161.04it/s]warmup run: 2400it [00:03, 1152.81it/s]warmup run: 2655it [00:04, 1176.48it/s]warmup run: 2163it [00:03, 1056.48it/s]warmup run: 2459it [00:03, 1130.15it/s]warmup run: 2250it [00:03, 1111.38it/s]warmup run: 2193it [00:03, 1055.95it/s]warmup run: 2390it [00:03, 1155.17it/s]warmup run: 2616it [00:04, 1176.03it/s]warmup run: 2524it [00:03, 1176.07it/s]warmup run: 2774it [00:04, 1177.65it/s]warmup run: 2287it [00:03, 1108.94it/s]warmup run: 2578it [00:04, 1145.18it/s]warmup run: 2373it [00:03, 1144.88it/s]warmup run: 2313it [00:03, 1096.30it/s]warmup run: 2513it [00:03, 1176.27it/s]warmup run: 2738it [00:04, 1186.30it/s]warmup run: 2648it [00:04, 1193.63it/s]warmup run: 2896it [00:04, 1187.66it/s]warmup run: 2411it [00:03, 1145.22it/s]warmup run: 2697it [00:04, 1156.37it/s]warmup run: 2496it [00:04, 1169.25it/s]warmup run: 2432it [00:03, 1123.01it/s]warmup run: 2636it [00:03, 1191.27it/s]warmup run: 2858it [00:04, 1189.34it/s]warmup run: 2770it [00:04, 1201.39it/s]warmup run: 3000it [00:04, 684.50it/s] warmup run: 2535it [00:04, 1170.70it/s]warmup run: 2814it [00:04, 1157.69it/s]warmup run: 2619it [00:04, 1184.85it/s]warmup run: 2552it [00:04, 1143.69it/s]warmup run: 2758it [00:04, 1197.08it/s]warmup run: 2980it [00:04, 1195.82it/s]warmup run: 3000it [00:04, 686.61it/s] warmup run: 2894it [00:04, 1210.07it/s]warmup run: 2658it [00:04, 1187.68it/s]warmup run: 2933it [00:04, 1166.04it/s]warmup run: 2742it [00:04, 1195.89it/s]warmup run: 2671it [00:04, 1157.41it/s]warmup run: 2878it [00:04, 1181.53it/s]warmup run: 3000it [00:04, 691.56it/s] warmup run: 3000it [00:04, 677.07it/s] warmup run: 2777it [00:04, 1187.42it/s]warmup run: 2863it [00:04, 1198.40it/s]warmup run: 2789it [00:04, 1161.33it/s]warmup run: 3000it [00:04, 696.45it/s] warmup run: 2896it [00:04, 1183.23it/s]warmup run: 2986it [00:04, 1206.00it/s]warmup run: 2906it [00:04, 1149.40it/s]warmup run: 3000it [00:04, 666.03it/s] warmup run: 3000it [00:04, 679.34it/s] warmup run: 3000it [00:04, 680.18it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1609.76it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.20it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1664.49it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1654.00it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1631.66it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.51it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.82it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1650.46it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1612.84it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1657.24it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1685.03it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1670.27it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1675.91it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.47it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1660.13it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1662.74it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1637.80it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1677.24it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1670.03it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1642.62it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1659.67it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1660.68it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1645.44it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1674.10it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1650.80it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1678.10it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1657.96it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1641.44it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1660.09it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1665.67it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1647.90it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1664.40it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1674.71it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1648.30it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1654.73it/s]warmup should be done:  28%|       | 825/3000 [00:00<00:01, 1640.19it/s]warmup should be done:  28%|       | 831/3000 [00:00<00:01, 1654.19it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1657.09it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1654.68it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1656.39it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1673.37it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1643.22it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1653.66it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1643.51it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1656.83it/s]warmup should be done:  33%|      | 1001/3000 [00:00<00:01, 1654.04it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1649.14it/s]warmup should be done:  34%|      | 1008/3000 [00:00<00:01, 1649.93it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1669.62it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1643.27it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1655.20it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1649.39it/s]warmup should be done:  38%|      | 1154/3000 [00:00<00:01, 1636.59it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1648.63it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1641.21it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1642.88it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1671.01it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1647.65it/s]warmup should be done:  44%|     | 1329/3000 [00:00<00:01, 1649.32it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1658.84it/s]warmup should be done:  44%|     | 1318/3000 [00:00<00:01, 1635.91it/s]warmup should be done:  44%|     | 1332/3000 [00:00<00:01, 1648.57it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1639.35it/s]warmup should be done:  45%|     | 1338/3000 [00:00<00:01, 1641.74it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1649.80it/s]warmup should be done:  50%|     | 1499/3000 [00:00<00:00, 1664.26it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1669.17it/s]warmup should be done:  50%|     | 1495/3000 [00:00<00:00, 1650.26it/s]warmup should be done:  49%|     | 1482/3000 [00:00<00:00, 1635.47it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1647.82it/s]warmup should be done:  50%|     | 1498/3000 [00:00<00:00, 1639.00it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1641.20it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1669.15it/s]warmup should be done:  55%|    | 1655/3000 [00:01<00:00, 1653.32it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1669.22it/s]warmup should be done:  55%|    | 1661/3000 [00:01<00:00, 1651.17it/s]warmup should be done:  55%|    | 1646/3000 [00:01<00:00, 1634.35it/s]warmup should be done:  55%|    | 1662/3000 [00:01<00:00, 1643.73it/s]warmup should be done:  55%|    | 1663/3000 [00:01<00:00, 1639.58it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1640.00it/s]warmup should be done:  62%|   | 1846/3000 [00:01<00:00, 1667.62it/s]warmup should be done:  61%|    | 1821/3000 [00:01<00:00, 1654.65it/s]warmup should be done:  61%|    | 1827/3000 [00:01<00:00, 1651.59it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1673.00it/s]warmup should be done:  60%|    | 1810/3000 [00:01<00:00, 1633.96it/s]warmup should be done:  61%|    | 1827/3000 [00:01<00:00, 1639.41it/s]warmup should be done:  61%|    | 1827/3000 [00:01<00:00, 1639.40it/s]warmup should be done:  61%|    | 1833/3000 [00:01<00:00, 1640.34it/s]warmup should be done:  66%|   | 1987/3000 [00:01<00:00, 1656.14it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1667.02it/s]warmup should be done:  66%|   | 1993/3000 [00:01<00:00, 1652.78it/s]warmup should be done:  67%|   | 2006/3000 [00:01<00:00, 1675.98it/s]warmup should be done:  66%|   | 1974/3000 [00:01<00:00, 1635.05it/s]warmup should be done:  66%|   | 1993/3000 [00:01<00:00, 1642.98it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1640.49it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1641.63it/s]warmup should be done:  72%|  | 2153/3000 [00:01<00:00, 1656.75it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1666.41it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1653.04it/s]warmup should be done:  71%|  | 2138/3000 [00:01<00:00, 1635.77it/s]warmup should be done:  72%|  | 2175/3000 [00:01<00:00, 1677.66it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  72%|  | 2157/3000 [00:01<00:00, 1640.87it/s]warmup should be done:  72%|  | 2163/3000 [00:01<00:00, 1641.85it/s]warmup should be done:  77%|  | 2319/3000 [00:01<00:00, 1656.83it/s]warmup should be done:  78%|  | 2325/3000 [00:01<00:00, 1654.24it/s]warmup should be done:  77%|  | 2302/3000 [00:01<00:00, 1636.81it/s]warmup should be done:  78%|  | 2343/3000 [00:01<00:00, 1676.67it/s]warmup should be done:  78%|  | 2347/3000 [00:01<00:00, 1656.12it/s]warmup should be done:  77%|  | 2323/3000 [00:01<00:00, 1645.12it/s]warmup should be done:  77%|  | 2322/3000 [00:01<00:00, 1641.64it/s]warmup should be done:  78%|  | 2328/3000 [00:01<00:00, 1642.57it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1655.51it/s]warmup should be done:  82%| | 2466/3000 [00:01<00:00, 1634.51it/s]warmup should be done:  84%| | 2512/3000 [00:01<00:00, 1678.20it/s]warmup should be done:  83%| | 2491/3000 [00:01<00:00, 1649.73it/s]warmup should be done:  84%| | 2513/3000 [00:01<00:00, 1650.20it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1645.43it/s]warmup should be done:  83%| | 2487/3000 [00:01<00:00, 1639.80it/s]warmup should be done:  83%| | 2493/3000 [00:01<00:00, 1639.52it/s]warmup should be done:  88%| | 2652/3000 [00:01<00:00, 1657.07it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1635.43it/s]warmup should be done:  89%| | 2681/3000 [00:01<00:00, 1678.99it/s]warmup should be done:  89%| | 2657/3000 [00:01<00:00, 1650.36it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1647.58it/s]warmup should be done:  89%| | 2679/3000 [00:01<00:00, 1645.59it/s]warmup should be done:  88%| | 2651/3000 [00:01<00:00, 1637.45it/s]warmup should be done:  89%| | 2658/3000 [00:01<00:00, 1639.92it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1658.60it/s]warmup should be done:  93%|| 2794/3000 [00:01<00:00, 1636.27it/s]warmup should be done:  95%|| 2850/3000 [00:01<00:00, 1681.69it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1651.91it/s]warmup should be done:  94%|| 2820/3000 [00:01<00:00, 1650.44it/s]warmup should be done:  95%|| 2844/3000 [00:01<00:00, 1644.02it/s]warmup should be done:  94%|| 2815/3000 [00:01<00:00, 1634.05it/s]warmup should be done:  94%|| 2823/3000 [00:01<00:00, 1640.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1670.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1662.09it/s]warmup should be done: 100%|| 2987/3000 [00:01<00:00, 1662.17it/s]warmup should be done: 100%|| 2990/3000 [00:01<00:00, 1655.83it/s]warmup should be done:  99%|| 2958/3000 [00:01<00:00, 1624.75it/s]warmup should be done: 100%|| 2987/3000 [00:01<00:00, 1655.82it/s]warmup should be done:  99%|| 2980/3000 [00:01<00:00, 1637.87it/s]warmup should be done: 100%|| 2989/3000 [00:01<00:00, 1645.62it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1653.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1652.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.92it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1643.41it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1633.59it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.88it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.26it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1675.40it/s]warmup should be done:   6%|         | 171/3000 [00:00<00:01, 1705.71it/s]warmup should be done:   6%|         | 173/3000 [00:00<00:01, 1721.15it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1680.21it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1681.82it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1670.40it/s]warmup should be done:  12%|        | 346/3000 [00:00<00:01, 1724.82it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1682.91it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1701.47it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1671.76it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1681.09it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1659.88it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1659.20it/s]warmup should be done:  11%|        | 342/3000 [00:00<00:01, 1661.84it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1686.01it/s]warmup should be done:  17%|        | 520/3000 [00:00<00:01, 1729.22it/s]warmup should be done:  17%|        | 513/3000 [00:00<00:01, 1703.55it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1675.81it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1679.52it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1662.14it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1669.63it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1670.92it/s]warmup should be done:  23%|       | 694/3000 [00:00<00:01, 1732.07it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1689.24it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1695.66it/s]warmup should be done:  23%|       | 684/3000 [00:00<00:01, 1700.86it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1685.85it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1666.65it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1675.02it/s]warmup should be done:  23%|       | 681/3000 [00:00<00:01, 1679.08it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1692.18it/s]warmup should be done:  29%|       | 868/3000 [00:00<00:01, 1732.36it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1706.43it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1690.55it/s]warmup should be done:  28%|       | 855/3000 [00:00<00:01, 1700.15it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1669.46it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1678.23it/s]warmup should be done:  28%|       | 852/3000 [00:00<00:01, 1688.58it/s]warmup should be done:  35%|      | 1042/3000 [00:00<00:01, 1731.29it/s]warmup should be done:  34%|      | 1023/3000 [00:00<00:01, 1709.87it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1689.81it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1689.36it/s]warmup should be done:  34%|      | 1026/3000 [00:00<00:01, 1698.06it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1681.66it/s]warmup should be done:  34%|      | 1006/3000 [00:00<00:01, 1653.60it/s]warmup should be done:  34%|      | 1025/3000 [00:00<00:01, 1700.77it/s]warmup should be done:  40%|      | 1195/3000 [00:00<00:01, 1710.58it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1687.50it/s]warmup should be done:  41%|      | 1216/3000 [00:00<00:01, 1730.34it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1685.93it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1682.45it/s]warmup should be done:  40%|      | 1196/3000 [00:00<00:01, 1692.09it/s]warmup should be done:  39%|      | 1174/3000 [00:00<00:01, 1659.75it/s]warmup should be done:  40%|      | 1197/3000 [00:00<00:01, 1704.47it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1689.99it/s]warmup should be done:  46%|     | 1390/3000 [00:00<00:00, 1732.62it/s]warmup should be done:  46%|     | 1368/3000 [00:00<00:00, 1713.99it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1688.16it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1683.86it/s]warmup should be done:  46%|     | 1367/3000 [00:00<00:00, 1694.95it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1666.93it/s]warmup should be done:  46%|     | 1370/3000 [00:00<00:00, 1712.06it/s]warmup should be done:  52%|    | 1564/3000 [00:00<00:00, 1732.63it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1690.45it/s]warmup should be done:  51%|    | 1541/3000 [00:00<00:00, 1715.97it/s]warmup should be done:  51%|     | 1525/3000 [00:00<00:00, 1687.55it/s]warmup should be done:  51%|     | 1519/3000 [00:00<00:00, 1683.48it/s]warmup should be done:  51%|     | 1537/3000 [00:00<00:00, 1695.44it/s]warmup should be done:  51%|    | 1542/3000 [00:00<00:00, 1713.91it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1671.40it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1692.59it/s]warmup should be done:  58%|    | 1738/3000 [00:01<00:00, 1732.87it/s]warmup should be done:  57%|    | 1714/3000 [00:01<00:00, 1718.02it/s]warmup should be done:  56%|    | 1694/3000 [00:01<00:00, 1685.92it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1684.57it/s]warmup should be done:  57%|    | 1707/3000 [00:01<00:00, 1696.38it/s]warmup should be done:  56%|    | 1681/3000 [00:01<00:00, 1677.07it/s]warmup should be done:  57%|    | 1715/3000 [00:01<00:00, 1715.90it/s]warmup should be done:  62%|   | 1867/3000 [00:01<00:00, 1697.53it/s]warmup should be done:  64%|   | 1912/3000 [00:01<00:00, 1734.12it/s]warmup should be done:  63%|   | 1887/3000 [00:01<00:00, 1720.29it/s]warmup should be done:  62%|   | 1864/3000 [00:01<00:00, 1688.30it/s]warmup should be done:  63%|   | 1878/3000 [00:01<00:00, 1698.87it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1680.60it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1677.78it/s]warmup should be done:  63%|   | 1888/3000 [00:01<00:00, 1718.57it/s]warmup should be done:  70%|   | 2086/3000 [00:01<00:00, 1735.03it/s]warmup should be done:  69%|   | 2060/3000 [00:01<00:00, 1721.50it/s]warmup should be done:  68%|   | 2034/3000 [00:01<00:00, 1690.16it/s]warmup should be done:  68%|   | 2050/3000 [00:01<00:00, 1704.66it/s]warmup should be done:  68%|   | 2037/3000 [00:01<00:00, 1682.01it/s]warmup should be done:  68%|   | 2026/3000 [00:01<00:00, 1675.70it/s]warmup should be done:  67%|   | 2018/3000 [00:01<00:00, 1679.10it/s]warmup should be done:  69%|   | 2061/3000 [00:01<00:00, 1721.89it/s]warmup should be done:  75%|  | 2260/3000 [00:01<00:00, 1734.87it/s]warmup should be done:  74%|  | 2233/3000 [00:01<00:00, 1720.03it/s]warmup should be done:  73%|  | 2204/3000 [00:01<00:00, 1687.47it/s]warmup should be done:  74%|  | 2221/3000 [00:01<00:00, 1701.60it/s]warmup should be done:  74%|  | 2206/3000 [00:01<00:00, 1684.01it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1678.60it/s]warmup should be done:  73%|  | 2194/3000 [00:01<00:00, 1672.13it/s]warmup should be done:  74%|  | 2234/3000 [00:01<00:00, 1722.21it/s]warmup should be done:  81%|  | 2434/3000 [00:01<00:00, 1733.57it/s]warmup should be done:  79%|  | 2374/3000 [00:01<00:00, 1688.24it/s]warmup should be done:  79%|  | 2377/3000 [00:01<00:00, 1691.01it/s]warmup should be done:  80%|  | 2392/3000 [00:01<00:00, 1698.70it/s]warmup should be done:  78%|  | 2355/3000 [00:01<00:00, 1680.57it/s]warmup should be done:  80%|  | 2406/3000 [00:01<00:00, 1705.98it/s]warmup should be done:  79%|  | 2362/3000 [00:01<00:00, 1670.65it/s]warmup should be done:  80%|  | 2407/3000 [00:01<00:00, 1721.61it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1730.94it/s]warmup should be done:  85%| | 2544/3000 [00:01<00:00, 1690.47it/s]warmup should be done:  85%| | 2549/3000 [00:01<00:00, 1696.90it/s]warmup should be done:  85%| | 2563/3000 [00:01<00:00, 1699.80it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1683.05it/s]warmup should be done:  84%| | 2530/3000 [00:01<00:00, 1671.28it/s]warmup should be done:  86%| | 2580/3000 [00:01<00:00, 1722.41it/s]warmup should be done:  86%| | 2577/3000 [00:01<00:00, 1696.45it/s]warmup should be done:  93%|| 2782/3000 [00:01<00:00, 1733.46it/s]warmup should be done:  90%| | 2715/3000 [00:01<00:00, 1695.35it/s]warmup should be done:  91%| | 2721/3000 [00:01<00:00, 1701.83it/s]warmup should be done:  91%| | 2733/3000 [00:01<00:00, 1699.74it/s]warmup should be done:  90%| | 2693/3000 [00:01<00:00, 1682.67it/s]warmup should be done:  90%| | 2698/3000 [00:01<00:00, 1670.44it/s]warmup should be done:  92%|| 2753/3000 [00:01<00:00, 1715.30it/s]warmup should be done:  92%|| 2750/3000 [00:01<00:00, 1704.03it/s]warmup should be done:  99%|| 2956/3000 [00:01<00:00, 1734.74it/s]warmup should be done:  96%|| 2886/3000 [00:01<00:00, 1698.18it/s]warmup should be done:  96%|| 2892/3000 [00:01<00:00, 1702.30it/s]warmup should be done:  97%|| 2903/3000 [00:01<00:00, 1697.33it/s]warmup should be done:  95%|| 2862/3000 [00:01<00:00, 1681.21it/s]warmup should be done:  96%|| 2866/3000 [00:01<00:00, 1667.95it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1708.40it/s]warmup should be done:  98%|| 2925/3000 [00:01<00:00, 1706.22it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1732.24it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1707.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1706.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1698.45it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1693.20it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.44it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.85it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1674.39it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9bb7b730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9b047040>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9b047190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9b055280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9bb7ae80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9bb7dd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9b0571f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fde9b056190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 05:38:14.058180: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9ca82f780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.058243: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.064422: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9cf0289d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.064461: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.066193: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.074056: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.158533: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9cf02dd10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.158595: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.167462: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.621969: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9ca8300c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.622031: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.632375: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.733930: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9d2830390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.733986: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.744048: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.758102: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9cf0309c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.758163: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.768199: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.771018: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9cf030b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.771081: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.777334: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9ca794710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:38:14.777391: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:38:14.779022: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:14.784976: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:38:21.202582: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.247251: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.249970: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.526121: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.716386: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.739348: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.747098: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:38:21.816013: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:39:12.150][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:39:12.150][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.156][ERROR][RK0][main]: coll ps creation done
[HCTR][05:39:12.156][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][05:39:12.406][ERROR][RK0][tid #140573843371776]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:39:12.406][ERROR][RK0][tid #140573843371776]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.411][ERROR][RK0][tid #140574103414528]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:39:12.411][ERROR][RK0][tid #140574103414528]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.413][ERROR][RK0][tid #140573843371776]: coll ps creation done
[HCTR][05:39:12.413][ERROR][RK0][tid #140573843371776]: replica 5 waits for coll ps creation barrier
[HCTR][05:39:12.416][ERROR][RK0][tid #140574103414528]: coll ps creation done
[HCTR][05:39:12.416][ERROR][RK0][tid #140574103414528]: replica 3 waits for coll ps creation barrier
[HCTR][05:39:12.421][ERROR][RK0][tid #140573843371776]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:39:12.421][ERROR][RK0][tid #140573843371776]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.425][ERROR][RK0][tid #140573843371776]: coll ps creation done
[HCTR][05:39:12.425][ERROR][RK0][tid #140573843371776]: replica 4 waits for coll ps creation barrier
[HCTR][05:39:12.501][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:39:12.501][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.502][ERROR][RK0][tid #140573902087936]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:39:12.502][ERROR][RK0][tid #140573902087936]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.508][ERROR][RK0][main]: coll ps creation done
[HCTR][05:39:12.508][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:39:12.509][ERROR][RK0][tid #140573902087936]: coll ps creation done
[HCTR][05:39:12.509][ERROR][RK0][tid #140573902087936]: replica 1 waits for coll ps creation barrier
[HCTR][05:39:12.550][ERROR][RK0][tid #140573910480640]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:39:12.550][ERROR][RK0][tid #140573910480640]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.555][ERROR][RK0][tid #140573910480640]: coll ps creation done
[HCTR][05:39:12.555][ERROR][RK0][tid #140573910480640]: replica 0 waits for coll ps creation barrier
[HCTR][05:39:12.625][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:39:12.625][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][05:39:12.629][ERROR][RK0][main]: coll ps creation done
[HCTR][05:39:12.629][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][05:39:12.629][ERROR][RK0][tid #140573910480640]: replica 0 preparing frequency
[HCTR][05:39:13.508][ERROR][RK0][tid #140573910480640]: replica 0 preparing frequency done
[HCTR][05:39:13.577][ERROR][RK0][tid #140573910480640]: replica 0 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][tid #140574103414528]: replica 3 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][tid #140573843371776]: replica 5 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][tid #140573902087936]: replica 1 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][tid #140573843371776]: replica 4 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:39:13.577][ERROR][RK0][tid #140573910480640]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][tid #140574103414528]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][main]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][tid #140573843371776]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][main]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][tid #140573902087936]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][tid #140573843371776]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][tid #140573910480640]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][main]: Calling build_v2
[HCTR][05:39:13.577][ERROR][RK0][tid #140573843371776]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][tid #140574103414528]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][tid #140573843371776]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][tid #140573902087936]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:39:13.577][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 05:39:13.[581867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 05:39:13178.581906: E]  v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:178] v100x8, slow pcie
[2022-12-12 05:39:13.581974: [E2022-12-12 05:39:13 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc5819842022-12-12 05:39:13:: .196E[581946]  : assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 05:39:13
: .196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc581995] :: assigning 0 to cpu178E
]  v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
:[1782022-12-12 05:39:13] .2022-12-12 05:39:13[v100x8, slow pcie582073.2022-12-12 05:39:13
: [582046.E[[: 582085 2022-12-12 05:39:132022-12-12 05:39:132022-12-12 05:39:13E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[... E:5820915821175821222022-12-12 05:39:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [212: : : .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E2022-12-12 05:39:13EE582134178:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 .  : ] 196
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc582181/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie] :: :: [
assigning 0 to cpu178E212196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:39:13
] [ ] ] :.v100x8, slow pcie2022-12-12 05:39:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8assigning 0 to cpu178582322
.:

] [: 582360178[v100x8, slow pcie2022-12-12 05:39:13[E: ] 2022-12-12 05:39:13
.2022-12-12 05:39:13 Ev100x8, slow pcie.[582423./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 
5824352022-12-12 05:39:13: 582450:2022-12-12 05:39:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .[E: 213.:E5824782022-12-12 05:39:13 E] 582487196 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc remote time is 8.68421: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE582531:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
Eassigning 0 to cpu: : 212: 
[196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:39:13] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] :.assigning 0 to cpu212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421196582646
] 2022-12-12 05:39:13:
] : [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.196assigning 0 to cpuE[2022-12-12 05:39:13[
582709] 
 2022-12-12 05:39:13.2022-12-12 05:39:13: assigning 0 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.582756.E
:2022-12-12 05:39:13[582783: 582786 214.2022-12-12 05:39:13: E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [582824.E E:cpu time is 97.05882022-12-12 05:39:13: 582860 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 212
.E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 582904 E:213:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214] 212
E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421]  213:cpu time is 97.0588[
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 212
2022-12-12 05:39:13
[:remote time is 8.68421] .2022-12-12 05:39:13212
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[583049.] 
2022-12-12 05:39:13[: 583084build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-12 05:39:13E[: 
583118. 2022-12-12 05:39:13E: 583152/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[. E: :2022-12-12 05:39:13583174/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc E213.: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 583212E214:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:  ] 213:
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588] 214 :[
remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2132022-12-12 05:39:13
cpu time is 97.0588:] .
213[remote time is 8.68421583342] 2022-12-12 05:39:13
: remote time is 8.68421.E
583391[ : 2022-12-12 05:39:13/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.:2022-12-12 05:39:13 583422214./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 583431:Ecpu time is 97.0588: 214 
E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 05:40:32.676688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:40:32.725646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:40:32.725758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 20000000
[2022-12-12 05:40:32.842788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:40:32.842926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:40:32.918304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:40:32.918394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:40:32.918970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.920138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.921158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.933484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 05:40:32.933538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 05:40:32.933822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 05:40:32.933875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 05:40:32.933906: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:40:32[:.2022-12-12 05:40:321980933909.] : 933901eager alloc mem 381.47 MBE: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 2022 solved] 
1 solved
[2022-12-12 05:40:32.934048[: 2022-12-12 05:40:32E. 934055/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 2 initing device 2:
205] worker 0 thread 1 initing device 1
[2022-12-12 05:40:32.934240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.934347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 05:40:32[.2022-12-12 05:40:32934413.: 934402: EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[::2022-12-12 05:40:32202205.] ] 9344677 solvedworker 0 thread 3 initing device 3: 
[
E2022-12-12 05:40:32[ .2022-12-12 05:40:32/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu934522.:: 9345331980E: ]  Eeager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1980:] 205eager alloc mem 381.47 MB] 
worker 0 thread 7 initing device 7
[2022-12-12 05:40:32.934757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 05:40:32.934812: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 05:40:32.934897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.935018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.935196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.938156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.938556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.939242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.939306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.939365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.939433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.940014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.942707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.943051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.943642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.943701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.943754: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.943816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:32.944342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:40:33.   944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:40:33.  1332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:40:33.  6696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:40:33.  6790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:40:33.  6836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:40:33.  7699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33.  8318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33.  9390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33.  9484: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 10157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 10196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 25955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:40:33. 26300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:40:33. 27730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[[2022-12-12 05:40:332022-12-12 05:40:332022-12-12 05:40:33... 27809 27812 27813: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes


[2022-12-12 05:40:33. 28035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:40:33[.[2022-12-12 05:40:33 281482022-12-12 05:40:33.: . 28153E 28153:  : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] :1980eager alloc mem 1024.00 Bytes1980] 
] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes

[2022-12-12 05:40:33. 30888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:40:33. 30988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 05:40:33. 31228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:40:33. 31304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:40:33. 34417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:40:33. 34490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:40:33. 34502: [E2022-12-12 05:40:33 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 34533:: 638E]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[[2022-12-12 05:40:332022-12-12 05:40:33.. 34572 34585: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 2

[[[2022-12-12 05:40:332022-12-12 05:40:332022-12-12 05:40:33... 34643 34656 34659: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:::638638638] ] ] eager release cuda mem 1024eager release cuda mem 2eager release cuda mem 400000000


[2022-12-12 05:40:33[.2022-12-12 05:40:33 34750.:  34755E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[638:2022-12-12 05:40:33] 638.eager release cuda mem 2]  34769
eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10242022-12-12 05:40:33
. 34827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 05:40:33. 34852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:40:33. 34896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:40:33. 35437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 36243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:40:33. 36305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:40:33. 36345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:40:33. 36360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:40:33. 36424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 05:40:33. 36472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:40:33. 36607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 37268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 37819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 38567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 39708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 40395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 40748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33. 40841: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 40927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:40:33. 41508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 41548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 41818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 41958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 42009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 42860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33. 42941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 42993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33.[ 430692022-12-12 05:40:33: .E 43077 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 6256631980
] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 43187: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 43612: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 43653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 43785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 43825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 43855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 43894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 52641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 53670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33. 53749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 54301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 54337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 85574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 86608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33. 86691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 87245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 87284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:40:33. 96548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:33. 97593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:33. 97691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:40:33. 98250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:40:33. 98289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:35........ 60906 60910 60906 60906 60905 60906 60906 60906: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] ] ] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB







[2022-12-12 05:40:35[.[2022-12-12 05:40:35[ 62107[[[2022-12-12 05:40:35.[2022-12-12 05:40:35: 2022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:35. 621092022-12-12 05:40:35.E... 62111: . 62114  62118 62120 62122: E 62128: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : : E : E:EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 638   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663:::eager release cuda mem 625663] :638
638638638
eager release cuda mem 625663638] ] ] ] 
] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663




[2022-12-12 05:40:35[.2022-12-12 05:40:35 62436.:  62450[E: 2022-12-12 05:40:35 E./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[[ [[ 62473:2022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:40:352022-12-12 05:40:35: 1980...:..E]  62496 62499 624971980 62500 62503 eager alloc mem 611.00 KB: : : ] : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
EEEeager alloc mem 611.00 KBEE:   
  1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :::::eager alloc mem 611.00 KB19801980198019801980
] ] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB




[2022-12-12 05:40:35. 63370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 63408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 63441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 63468: E[ 2022-12-12 05:40:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[:[[[ 63479[2022-12-12 05:40:356382022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:35: 2022-12-12 05:40:35.] ...E. 63503eager release cuda mem 625663 63505 63505 63507  63511: 
: : : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: EEEE:E    1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 05:40:35:::eager alloc mem 611.00 KB:638.638638638
638]  63658] ] ] ] eager release cuda mem 625663: eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663
E



 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35.[ 638262022-12-12 05:40:35[: .[[2022-12-12 05:40:35E 638322022-12-12 05:40:352022-12-12 05:40:35. : .. 63836/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE 63838 63840: : : : E1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEE ] :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
] ::1980eager alloc mem 611.00 KB19801980] 
] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 05:40:35. 64193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 64258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 64471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35[.2022-12-12 05:40:35 64534.:  64540E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1980eager release cuda mem 625663] 
eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 64653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 64695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 64720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 05:40:35[2022-12-12 05:40:35.[2022-12-12 05:40:35. 647482022-12-12 05:40:35. 64749: .[ 64750: E 647642022-12-12 05:40:35: E : .E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE 64788 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:638] : 638] eager release cuda mem 6256631980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager release cuda mem 625663
] :eager release cuda mem 625663
eager alloc mem 611.00 KB1980

] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 64965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 05:40:35[1980.2022-12-12 05:40:35]  64978[.eager alloc mem 611.00 KB: 2022-12-12 05:40:35 64984
E.:   65004E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980 :] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980eager alloc mem 611.00 KB:] 
638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-12 05:40:35. 65177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 65343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 65404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 65431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 65473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:40:352022-12-12 05:40:35.. 65671 65672: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[[2022-12-12 05:40:352022-12-12 05:40:352022-12-12 05:40:35... 65764 65764 65766: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::19806381980] ] ] eager alloc mem 611.00 KBeager release cuda mem 625663eager alloc mem 611.00 KB


[[2022-12-12 05:40:352022-12-12 05:40:35.. 65855 65856[: : 2022-12-12 05:40:35EE.  [ 65883/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:40:35: ::.E638638 65922 ] ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663eager release cuda mem 625663E:

 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[[2022-12-12 05:40:352022-12-12 05:40:35..[ 66044 660452022-12-12 05:40:35: : .EE 66065  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:: 19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] :eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980

] [eager alloc mem 611.00 KB2022-12-12 05:40:35
. 66184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 66223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 66264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 66289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:40:352022-12-12 05:40:35.. 66564 66566: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-12 05:40:352022-12-12 05:40:35.. 66654 66655: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 05:40:35. 66741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 66807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 66906[: 2022-12-12 05:40:35E.[  669132022-12-12 05:40:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: .:E 66927638 : ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEeager release cuda mem 625663: 
638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[] :2022-12-12 05:40:35eager release cuda mem 625663638.[
]  67011[2022-12-12 05:40:35eager release cuda mem 625663: 2022-12-12 05:40:35.
E. 67039  67054: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E2022-12-12 05:40:35:E .[638 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 671012022-12-12 05:40:35] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: .eager release cuda mem 625663:638E 67137
1980]  : ] eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuEeager alloc mem 611.00 KB
: 
[1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:40:35] :.eager alloc mem 611.00 KB1980 67260
[] : 2022-12-12 05:40:35eager alloc mem 611.00 KBE.
  67299/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[[2022-12-12 05:40:352022-12-12 05:40:35.. 67428 67430: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[[2022-12-12 05:40:352022-12-12 05:40:35.. 67521 67521: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::[198019802022-12-12 05:40:35] ] .eager alloc mem 611.00 KBeager alloc mem 611.00 KB 67552

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 67638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:40:35. 67992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 68038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-12 05:40:35
2022-12-12 05:40:35.. 68060 68065: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[:1980[2022-12-12 05:40:35638] [2022-12-12 05:40:35.] eager alloc mem 611.00 KB2022-12-12 05:40:35. 68103eager release cuda mem 625663
. 68111: 
 68123: E: E [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:40:35 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:638 68205:638] : 1980] eager release cuda mem 625663E] eager release cuda mem 625663
[[ eager alloc mem 611.00 KB
2022-12-12 05:40:35.2022-12-12 05:40:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[2022-12-12 05:40:35. 68351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[
 68302.] 2022-12-12 05:40:35: [ 68303eager release cuda mem 80400000.E2022-12-12 05:40:35: 
 68330 .E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 68386 E:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  638:eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638
:eager release cuda mem 625663] 638
eager release cuda mem 80400000[] 
2022-12-12 05:40:35eager release cuda mem 625663[.
2022-12-12 05:40:35 68523.:  68539E: [ E2022-12-12 05:40:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 68565638:: ] 638Eeager release cuda mem 80400000]  
eager release cuda mem 80400000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 80400000
[2022-12-12 05:40:35. 68899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 68936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:40:35. 69109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:40:35. 69147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:40:35. 69222: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13403 secs 
[2022-12-12 05:40:35. 69509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13528 secs 
[2022-12-12 05:40:35. 70098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13521 secs 
[2022-12-12 05:40:35. 70261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13581 secs 
[2022-12-12 05:40:35. 70849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13695 secs 
[2022-12-12 05:40:35. 71006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13599 secs 
[2022-12-12 05:40:35. 71606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.15265 secs 
[2022-12-12 05:40:35. 71783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 80000000 / 100000000 nodes ( 80.00 %) | 9.54 GB | 2.13728 secs 
[HCTR][05:40:35.071][ERROR][RK0][tid #140573910480640]: replica 0 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][tid #140573843371776]: replica 5 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][tid #140573843371776]: replica 4 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][tid #140573902087936]: replica 1 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][tid #140574103414528]: replica 3 calling init per replica done, doing barrier
[HCTR][05:40:35.071][ERROR][RK0][tid #140574103414528]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][tid #140573843371776]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][tid #140573902087936]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][tid #140573910480640]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:40:35.071][ERROR][RK0][tid #140573843371776]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:40:35.072][ERROR][RK0][tid #140574103414528]: init per replica done
[HCTR][05:40:35.072][ERROR][RK0][main]: init per replica done
[HCTR][05:40:35.072][ERROR][RK0][tid #140573843371776]: init per replica done
[HCTR][05:40:35.072][ERROR][RK0][tid #140573902087936]: init per replica done
[HCTR][05:40:35.072][ERROR][RK0][main]: init per replica done
[HCTR][05:40:35.072][ERROR][RK0][main]: init per replica done
[HCTR][05:40:35.072][ERROR][RK0][tid #140573843371776]: init per replica done
[HCTR][05:40:35.074][ERROR][RK0][tid #140573910480640]: init per replica done
