2022-12-12 07:46:39.392582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.400369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.415821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.421586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.428015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.433088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.436779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.448998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.500177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.508680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.513053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.513475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.514491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.515227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.516181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.516979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.517924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.518742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.519413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.520470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.520946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.522215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.522417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.523981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.524144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.525675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.525919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.527350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.528350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.529342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.530291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.531182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.533055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.534216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.535252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.536258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.537358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.538392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.539441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.540458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.545628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.545764: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.546819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.548374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.549763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.549806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.551277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.551466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.552595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.553003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.554031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.554494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.555556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.555951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.556721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.558351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.559424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.559836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.560317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.561479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.562091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.563614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.564452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.567183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.568151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.569349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.570501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.571023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.571806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.573524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.573966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.575028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.577163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.577336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.578333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.578351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.580222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.580283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.581616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.582931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.583266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.584583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.585277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.585436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.585574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.586739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.587443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.595308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.595448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.597397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.598055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.598853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.599621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.600038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.600911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.602464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.609992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.625221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.636447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.636830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.638162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.639236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.639532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.639560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.639746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.640153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.642370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.642499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.643557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.643647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.643844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.644155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.647044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.647105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.648856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.648902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.649120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.649359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.651179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.651653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.652245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.652513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.652798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.652972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.655353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.655984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.656382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.656566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.656738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.658886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.659530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.659923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.660120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.660164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.661992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.662475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.663235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.663405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.664259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.665010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.665328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.666207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.666248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.667616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.668064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.668332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.669103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.669269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.670779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.671160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.671568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.672562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.672687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.674162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.674323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.674614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.676523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.676563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.678292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.678335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.678826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.679686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.679770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.681398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.681437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.681781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.682876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.682916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.684430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.684755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.685013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.685947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.686052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.687697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.687802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.688095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.688150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.689159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.689219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.691244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.691732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.692557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.693431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.694015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.694062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.694445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.695345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.695626: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.696409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.697426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.697633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.697900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.698236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.699139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.699298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.700640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.700961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.701345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.702045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.702913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.702981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.704641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.705008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.705314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.705691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.705931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.707176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.707250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.708752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.709050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.709648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.710036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.710245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.711248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.711453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.712778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.713243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.713674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.714264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.714368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.715265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.716108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.716994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.717114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.717625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.718242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.719147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.720245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.720978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.721166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.721703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.723302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.724868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.725458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.726352: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.726508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.727388: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.727664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.728262: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.729323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.730440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.731837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.732843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.732931: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.734167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.735387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.736641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.736781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.737313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.738341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.738827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.740338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.740395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.741051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.742646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.742721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.743600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.743891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.744016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.744832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.746518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.746633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.747633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.747923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.787718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.790001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.790122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.794830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.795627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.799645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.829252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.834241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.835118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.839663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.842730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.846944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.848025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.853435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.856191: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.858125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.862410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.866073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.888785: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 07:46:39.896590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.898515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.901735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.903791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:39.998618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.869758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.870780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.871369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.871853: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:40.871909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:46:40.890321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.890941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.891704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.892311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.893045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:40.893520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 07:46:40.940649: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:40.940857: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:40.971797: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 07:46:41.077455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.078263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.078785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.079272: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.079325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:46:41.096979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.097598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.098113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.098677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.099222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.099697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 07:46:41.153702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.154336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.154858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.155554: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.155611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:46:41.163282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.163882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.164414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.165097: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.165150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:46:41.173260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.174110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.174616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.175221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.175741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.175787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.176827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 07:46:41.176887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.177440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.177907: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.177949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:46:41.183307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.183903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.184418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.184987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.185499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.185969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 07:46:41.195844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.196061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.196917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.197061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.197677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.197933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.198418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.199177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.199630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.200082: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.200140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:46:41.200723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.200756: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.200966: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.201025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.201590: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.201636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:46:41.201785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 07:46:41.202003: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 07:46:41.207693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.208308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.208830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.209375: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 07:46:41.209430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:46:41.217425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.218105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.218615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.219214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.219451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.220098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.220483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.221053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 07:46:41.221336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.221916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.222458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.222929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 07:46:41.227747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.228378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.228885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.229469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.229988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 07:46:41.230452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 07:46:41.232941: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.233145: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.234878: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 07:46:41.248898: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.249087: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.250116: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 07:46:41.267080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.267295: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.268377: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 07:46:41.268428: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.268603: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.269543: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 07:46:41.270197: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.270350: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.271337: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 07:46:41.276941: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.277104: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 07:46:41.278861: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][07:46:42.535][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.535][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.535][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.540][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.540][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.541][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.541][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][07:46:42.541][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 95it [00:01, 80.86it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 192it [00:01, 177.35it/s]warmup run: 93it [00:01, 79.73it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 288it [00:01, 282.33it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 189it [00:01, 175.98it/s]warmup run: 98it [00:01, 82.94it/s]warmup run: 94it [00:01, 79.95it/s]warmup run: 100it [00:01, 83.94it/s]warmup run: 97it [00:01, 81.13it/s]warmup run: 384it [00:01, 391.04it/s]warmup run: 98it [00:01, 85.11it/s]warmup run: 292it [00:01, 291.23it/s]warmup run: 196it [00:01, 179.92it/s]warmup run: 187it [00:01, 172.09it/s]warmup run: 199it [00:01, 181.11it/s]warmup run: 194it [00:01, 176.23it/s]warmup run: 479it [00:02, 494.66it/s]warmup run: 196it [00:01, 184.06it/s]warmup run: 394it [00:01, 409.03it/s]warmup run: 295it [00:01, 288.05it/s]warmup run: 280it [00:01, 273.41it/s]warmup run: 299it [00:01, 289.65it/s]warmup run: 294it [00:01, 285.35it/s]warmup run: 575it [00:02, 591.65it/s]warmup run: 294it [00:01, 292.56it/s]warmup run: 498it [00:02, 527.04it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 392it [00:01, 397.05it/s]warmup run: 373it [00:01, 378.48it/s]warmup run: 400it [00:01, 404.09it/s]warmup run: 394it [00:01, 398.93it/s]warmup run: 674it [00:02, 682.49it/s]warmup run: 391it [00:01, 402.68it/s]warmup run: 603it [00:02, 636.52it/s]warmup run: 85it [00:01, 74.98it/s]warmup run: 489it [00:02, 503.35it/s]warmup run: 470it [00:02, 488.54it/s]warmup run: 501it [00:02, 515.49it/s]warmup run: 494it [00:02, 509.42it/s]warmup run: 773it [00:02, 757.50it/s]warmup run: 480it [00:02, 491.91it/s]warmup run: 706it [00:02, 727.18it/s]warmup run: 185it [00:01, 178.78it/s]warmup run: 587it [00:02, 602.35it/s]warmup run: 569it [00:02, 593.30it/s]warmup run: 602it [00:02, 617.59it/s]warmup run: 596it [00:02, 615.15it/s]warmup run: 871it [00:02, 813.88it/s]warmup run: 575it [00:02, 587.62it/s]warmup run: 806it [00:02, 792.79it/s]warmup run: 285it [00:01, 292.12it/s]warmup run: 684it [00:02, 686.07it/s]warmup run: 661it [00:02, 658.31it/s]warmup run: 703it [00:02, 706.37it/s]warmup run: 697it [00:02, 705.20it/s]warmup run: 968it [00:02, 855.74it/s]warmup run: 672it [00:02, 675.16it/s]warmup run: 906it [00:02, 842.23it/s]warmup run: 377it [00:01, 395.51it/s]warmup run: 783it [00:02, 760.05it/s]warmup run: 803it [00:02, 778.56it/s]warmup run: 799it [00:02, 781.67it/s]warmup run: 1067it [00:02, 891.49it/s]warmup run: 751it [00:02, 684.14it/s]warmup run: 771it [00:02, 752.27it/s]warmup run: 1005it [00:02, 881.54it/s]warmup run: 464it [00:01, 485.37it/s]warmup run: 881it [00:02, 816.31it/s]warmup run: 902it [00:02, 832.76it/s]warmup run: 901it [00:02, 841.88it/s]warmup run: 1165it [00:02, 914.08it/s]warmup run: 854it [00:02, 769.59it/s]warmup run: 869it [00:02, 810.88it/s]warmup run: 1106it [00:02, 915.17it/s]warmup run: 551it [00:02, 563.73it/s]warmup run: 981it [00:02, 863.96it/s]warmup run: 1001it [00:02, 875.25it/s]warmup run: 1003it [00:02, 887.62it/s]warmup run: 956it [00:02, 833.57it/s]warmup run: 1262it [00:02, 919.12it/s]warmup run: 967it [00:02, 855.80it/s]warmup run: 1207it [00:02, 941.49it/s]warmup run: 648it [00:02, 657.63it/s]warmup run: 1082it [00:02, 903.72it/s]warmup run: 1100it [00:02, 892.87it/s]warmup run: 1105it [00:02, 923.49it/s]warmup run: 1058it [00:02, 884.24it/s]warmup run: 1361it [00:02, 936.86it/s]warmup run: 1069it [00:02, 900.74it/s]warmup run: 1310it [00:02, 965.02it/s]warmup run: 738it [00:02, 717.41it/s]warmup run: 1182it [00:02, 929.63it/s]warmup run: 1197it [00:02, 910.03it/s]warmup run: 1206it [00:02, 943.21it/s]warmup run: 1156it [00:02, 909.65it/s]warmup run: 1459it [00:03, 947.71it/s]warmup run: 1171it [00:02, 933.15it/s]warmup run: 1412it [00:02, 979.80it/s]warmup run: 840it [00:02, 794.99it/s]warmup run: 1281it [00:02, 946.38it/s]warmup run: 1294it [00:02, 926.08it/s]warmup run: 1307it [00:02, 958.07it/s]warmup run: 1255it [00:02, 931.66it/s]warmup run: 1559it [00:03, 960.80it/s]warmup run: 1271it [00:02, 952.25it/s]warmup run: 1515it [00:03, 993.00it/s]warmup run: 935it [00:02, 834.55it/s]warmup run: 1380it [00:02, 958.17it/s]warmup run: 1394it [00:02, 944.79it/s]warmup run: 1407it [00:02, 970.11it/s]warmup run: 1356it [00:02, 952.68it/s]warmup run: 1660it [00:03, 973.11it/s]warmup run: 1373it [00:02, 970.93it/s]warmup run: 1617it [00:03, 999.95it/s]warmup run: 1029it [00:02, 855.77it/s]warmup run: 1480it [00:03, 968.89it/s]warmup run: 1493it [00:03, 956.15it/s]warmup run: 1507it [00:03, 976.12it/s]warmup run: 1456it [00:03, 964.94it/s]warmup run: 1761it [00:03, 983.49it/s]warmup run: 1473it [00:03, 979.37it/s]warmup run: 1720it [00:03, 1007.72it/s]warmup run: 1122it [00:02, 872.88it/s]warmup run: 1580it [00:03, 975.48it/s]warmup run: 1592it [00:03, 963.49it/s]warmup run: 1607it [00:03, 981.88it/s]warmup run: 1557it [00:03, 975.21it/s]warmup run: 1861it [00:03, 979.83it/s]warmup run: 1573it [00:03, 985.27it/s]warmup run: 1822it [00:03, 1010.38it/s]warmup run: 1217it [00:02, 894.86it/s]warmup run: 1680it [00:03, 979.78it/s]warmup run: 1691it [00:03, 970.51it/s]warmup run: 1708it [00:03, 987.82it/s]warmup run: 1658it [00:03, 983.99it/s]warmup run: 1960it [00:03, 981.99it/s]warmup run: 1673it [00:03, 979.52it/s]warmup run: 1924it [00:03, 1007.16it/s]warmup run: 1311it [00:02, 877.61it/s]warmup run: 1780it [00:03, 983.39it/s]warmup run: 1791it [00:03, 976.13it/s]warmup run: 1809it [00:03, 992.32it/s]warmup run: 1758it [00:03, 987.72it/s]warmup run: 2071it [00:03, 1019.46it/s]warmup run: 1772it [00:03, 981.42it/s]warmup run: 2028it [00:03, 1016.63it/s]warmup run: 1410it [00:02, 908.72it/s]warmup run: 1880it [00:03, 984.90it/s]warmup run: 1891it [00:03, 981.97it/s]warmup run: 1909it [00:03, 990.17it/s]warmup run: 1860it [00:03, 994.59it/s]warmup run: 2191it [00:03, 1072.91it/s]warmup run: 1871it [00:03, 980.56it/s]warmup run: 2143it [00:03, 1054.96it/s]warmup run: 1509it [00:03, 932.02it/s]warmup run: 1979it [00:03, 985.20it/s]warmup run: 1990it [00:03, 983.46it/s]warmup run: 2010it [00:03, 994.20it/s]warmup run: 1960it [00:03, 994.15it/s]warmup run: 2311it [00:03, 1110.12it/s]warmup run: 1970it [00:03, 971.23it/s]warmup run: 2257it [00:03, 1079.52it/s]warmup run: 1609it [00:03, 950.82it/s]warmup run: 2094it [00:03, 1033.24it/s]warmup run: 2106it [00:03, 1034.30it/s]warmup run: 2127it [00:03, 1045.73it/s]warmup run: 2071it [00:03, 1027.57it/s]warmup run: 2432it [00:03, 1137.75it/s]warmup run: 2083it [00:03, 1016.73it/s]warmup run: 2371it [00:03, 1095.17it/s]warmup run: 1708it [00:03, 959.51it/s]warmup run: 2216it [00:03, 1087.53it/s]warmup run: 2225it [00:03, 1079.60it/s]warmup run: 2245it [00:03, 1083.96it/s]warmup run: 2191it [00:03, 1076.78it/s]warmup run: 2553it [00:04, 1157.28it/s]warmup run: 2204it [00:03, 1072.98it/s]warmup run: 2486it [00:03, 1110.39it/s]warmup run: 1807it [00:03, 965.88it/s]warmup run: 2338it [00:03, 1125.27it/s]warmup run: 2345it [00:03, 1115.43it/s]warmup run: 2365it [00:03, 1116.15it/s]warmup run: 2311it [00:03, 1111.93it/s]warmup run: 2673it [00:04, 1167.78it/s]warmup run: 2325it [00:03, 1112.61it/s]warmup run: 2602it [00:04, 1123.39it/s]warmup run: 1906it [00:03, 972.77it/s]warmup run: 2459it [00:03, 1149.20it/s]warmup run: 2465it [00:03, 1139.89it/s]warmup run: 2485it [00:03, 1139.32it/s]warmup run: 2431it [00:03, 1137.49it/s]warmup run: 2794it [00:04, 1178.63it/s]warmup run: 2446it [00:03, 1140.65it/s]warmup run: 2719it [00:04, 1136.49it/s]warmup run: 2007it [00:03, 982.55it/s]warmup run: 2578it [00:04, 1160.08it/s]warmup run: 2586it [00:04, 1160.83it/s]warmup run: 2605it [00:04, 1156.15it/s]warmup run: 2551it [00:04, 1155.76it/s]warmup run: 2914it [00:04, 1184.67it/s]warmup run: 2564it [00:04, 1151.84it/s]warmup run: 2839it [00:04, 1152.92it/s]warmup run: 2123it [00:03, 1034.98it/s]warmup run: 2696it [00:04, 1164.16it/s]warmup run: 3000it [00:04, 677.30it/s] warmup run: 2705it [00:04, 1166.81it/s]warmup run: 2723it [00:04, 1162.26it/s]warmup run: 2671it [00:04, 1167.53it/s]warmup run: 2956it [00:04, 1156.03it/s]warmup run: 2680it [00:04, 1109.83it/s]warmup run: 3000it [00:04, 685.10it/s] warmup run: 2227it [00:03, 1001.32it/s]warmup run: 2814it [00:04, 1168.60it/s]warmup run: 2826it [00:04, 1178.61it/s]warmup run: 2842it [00:04, 1169.30it/s]warmup run: 2791it [00:04, 1174.74it/s]warmup run: 2792it [00:04, 1106.31it/s]warmup run: 2345it [00:03, 1052.99it/s]warmup run: 2935it [00:04, 1179.62it/s]warmup run: 2946it [00:04, 1183.93it/s]warmup run: 2964it [00:04, 1183.95it/s]warmup run: 2912it [00:04, 1182.58it/s]warmup run: 2915it [00:04, 1142.27it/s]warmup run: 3000it [00:04, 679.82it/s] warmup run: 3000it [00:04, 679.61it/s] warmup run: 3000it [00:04, 677.69it/s] warmup run: 2464it [00:04, 1090.92it/s]warmup run: 3000it [00:04, 673.65it/s] warmup run: 3000it [00:04, 680.43it/s] warmup run: 2583it [00:04, 1119.21it/s]warmup run: 2698it [00:04, 1126.48it/s]warmup run: 2815it [00:04, 1137.68it/s]warmup run: 2932it [00:04, 1145.27it/s]warmup run: 3000it [00:04, 669.99it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.28it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.47it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.19it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.25it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1636.91it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1604.81it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.26it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1633.44it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1628.51it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1670.93it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1611.44it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1647.75it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1638.28it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.46it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.56it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.81it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1639.01it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1647.53it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1622.75it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.54it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1655.17it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1644.95it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.02it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1606.07it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1647.63it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1656.05it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1639.41it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1666.80it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1643.05it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1617.24it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1639.35it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1602.83it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1642.94it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1643.41it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1653.59it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1638.09it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1640.25it/s]warmup should be done:  27%|██▋       | 814/3000 [00:00<00:01, 1614.65it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1658.62it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1609.53it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1641.00it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1609.53it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1647.68it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1635.77it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1631.62it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1652.06it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1607.94it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1631.67it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1642.07it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1612.55it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1639.22it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1646.29it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1632.22it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1606.48it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1646.12it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1629.17it/s]warmup should be done:  43%|████▎     | 1294/3000 [00:00<00:01, 1612.78it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1640.91it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1645.09it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1640.05it/s]warmup should be done:  44%|████▍     | 1316/3000 [00:00<00:01, 1630.41it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1605.18it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1630.68it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1643.31it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1646.58it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1614.98it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1640.25it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1641.46it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1629.59it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1632.21it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1643.96it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:01, 1493.31it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1642.67it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1645.86it/s]warmup should be done:  54%|█████▍    | 1620/3000 [00:01<00:00, 1616.62it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1640.96it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1629.10it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1647.29it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1630.61it/s]warmup should be done:  54%|█████▎    | 1610/3000 [00:01<00:01, 1352.71it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1644.74it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1643.24it/s]warmup should be done:  59%|█████▉    | 1783/3000 [00:01<00:00, 1617.54it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1628.21it/s]warmup should be done:  60%|██████    | 1815/3000 [00:01<00:00, 1638.21it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1650.29it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1629.26it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1419.84it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1645.01it/s]warmup should be done:  65%|██████▍   | 1946/3000 [00:01<00:00, 1618.84it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1641.14it/s]warmup should be done:  66%|██████▌   | 1969/3000 [00:01<00:00, 1627.03it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1632.59it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1652.21it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1631.09it/s]warmup should be done:  64%|██████▍   | 1931/3000 [00:01<00:00, 1471.54it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1645.79it/s]warmup should be done:  70%|███████   | 2109/3000 [00:01<00:00, 1619.88it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1641.98it/s]warmup should be done:  71%|███████   | 2132/3000 [00:01<00:00, 1626.62it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1654.58it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1630.21it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1631.68it/s]warmup should be done:  70%|██████▉   | 2092/3000 [00:01<00:00, 1509.39it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1643.69it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1617.15it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1640.85it/s]warmup should be done:  76%|███████▋  | 2295/3000 [00:01<00:00, 1625.20it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1655.19it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1627.56it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1629.35it/s]warmup should be done:  75%|███████▌  | 2252/3000 [00:01<00:00, 1535.63it/s]warmup should be done:  82%|████████▏ | 2473/3000 [00:01<00:00, 1644.08it/s]warmup should be done:  81%|████████  | 2434/3000 [00:01<00:00, 1618.64it/s]warmup should be done:  82%|████████▏ | 2459/3000 [00:01<00:00, 1626.69it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1651.92it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1629.39it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1630.07it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1628.88it/s]warmup should be done:  80%|████████  | 2413/3000 [00:01<00:00, 1556.91it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1644.51it/s]warmup should be done:  87%|████████▋ | 2597/3000 [00:01<00:00, 1619.55it/s]warmup should be done:  87%|████████▋ | 2623/3000 [00:01<00:00, 1628.52it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1652.19it/s]warmup should be done:  88%|████████▊ | 2646/3000 [00:01<00:00, 1635.71it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1627.88it/s]warmup should be done:  88%|████████▊ | 2635/3000 [00:01<00:00, 1630.08it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1571.92it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1646.25it/s]warmup should be done:  92%|█████████▏| 2760/3000 [00:01<00:00, 1620.67it/s]warmup should be done:  93%|█████████▎| 2787/3000 [00:01<00:00, 1628.98it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1654.62it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1627.18it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1640.09it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1630.57it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1583.09it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1648.96it/s]warmup should be done:  97%|█████████▋| 2924/3000 [00:01<00:00, 1626.30it/s]warmup should be done:  98%|█████████▊| 2953/3000 [00:01<00:00, 1636.03it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1633.82it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1657.89it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1635.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1653.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1643.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1636.35it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1528.67it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.67it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1595.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1553.34it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.50it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1688.59it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.88it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.91it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.87it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.71it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.06it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.50it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1687.48it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.04it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1638.11it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1659.25it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1657.28it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1628.70it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1622.20it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1612.07it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1692.55it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1694.31it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1654.67it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1669.08it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1658.55it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1637.00it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1610.69it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1593.68it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1698.05it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1692.43it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1658.62it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1670.87it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1664.33it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1647.32it/s]warmup should be done:  22%|██▏       | 663/3000 [00:00<00:01, 1636.72it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1624.37it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1700.81it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1660.72it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1691.32it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1670.35it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1667.29it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1648.82it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1649.88it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1641.45it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1665.81it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1692.88it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1698.15it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1670.42it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1670.89it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1659.58it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1623.04it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1650.63it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1668.83it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1691.97it/s]warmup should be done:  40%|███▉      | 1192/3000 [00:00<00:01, 1694.37it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1675.19it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1671.04it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1666.30it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1630.93it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1659.73it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:00, 1667.65it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1693.81it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1696.50it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1682.33it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1672.66it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:00, 1670.01it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1634.16it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1664.34it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1667.77it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1692.86it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1697.61it/s]warmup should be done:  50%|█████     | 1513/3000 [00:00<00:00, 1680.37it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1669.69it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1667.94it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1634.76it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1666.99it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1667.90it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1692.41it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1695.11it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1678.74it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1670.83it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1640.00it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1657.96it/s]warmup should be done:  56%|█████▌    | 1672/3000 [00:01<00:00, 1671.54it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1665.99it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1694.00it/s]warmup should be done:  62%|██████▏   | 1850/3000 [00:01<00:00, 1679.07it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1663.65it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1680.95it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1657.64it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1641.45it/s]warmup should be done:  61%|██████▏   | 1840/3000 [00:01<00:00, 1666.72it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1695.37it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1661.79it/s]warmup should be done:  67%|██████▋   | 2018/3000 [00:01<00:00, 1674.35it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1663.20it/s]warmup should be done:  68%|██████▊   | 2043/3000 [00:01<00:00, 1677.57it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1661.31it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1642.87it/s]warmup should be done:  67%|██████▋   | 2007/3000 [00:01<00:00, 1665.82it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1694.44it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1658.99it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1673.64it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1662.15it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1672.13it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1665.70it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1645.84it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1668.62it/s]warmup should be done:  79%|███████▉  | 2378/3000 [00:01<00:00, 1694.13it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1660.98it/s]warmup should be done:  78%|███████▊  | 2355/3000 [00:01<00:00, 1676.29it/s]warmup should be done:  78%|███████▊  | 2348/3000 [00:01<00:00, 1663.97it/s]warmup should be done:  79%|███████▉  | 2379/3000 [00:01<00:00, 1670.07it/s]warmup should be done:  78%|███████▊  | 2341/3000 [00:01<00:00, 1670.89it/s]warmup should be done:  77%|███████▋  | 2317/3000 [00:01<00:00, 1647.42it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1673.75it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1695.15it/s]warmup should be done:  83%|████████▎ | 2502/3000 [00:01<00:00, 1662.34it/s]warmup should be done:  84%|████████▍ | 2524/3000 [00:01<00:00, 1678.75it/s]warmup should be done:  84%|████████▍ | 2517/3000 [00:01<00:00, 1670.10it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1675.82it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1669.73it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1646.00it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1677.97it/s]warmup should be done:  91%|█████████ | 2718/3000 [00:01<00:00, 1695.54it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1660.19it/s]warmup should be done:  90%|████████▉ | 2692/3000 [00:01<00:00, 1678.37it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1675.96it/s]warmup should be done:  90%|█████████ | 2714/3000 [00:01<00:00, 1669.33it/s]warmup should be done:  89%|████████▉ | 2679/3000 [00:01<00:00, 1677.30it/s]warmup should be done:  88%|████████▊ | 2647/3000 [00:01<00:00, 1644.84it/s]warmup should be done:  89%|████████▉ | 2681/3000 [00:01<00:00, 1677.44it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1692.80it/s]warmup should be done:  95%|█████████▍| 2836/3000 [00:01<00:00, 1658.87it/s]warmup should be done:  95%|█████████▌| 2860/3000 [00:01<00:00, 1676.83it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1677.03it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1676.24it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1666.22it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1646.41it/s]warmup should be done:  95%|█████████▌| 2850/3000 [00:01<00:00, 1681.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.37it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1660.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.27it/s]warmup should be done:  99%|█████████▉| 2979/3000 [00:01<00:00, 1647.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.74it/s]2022-12-12 07:48:18.488909: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f475f830af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:18.488983: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:18.846192: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x1dcf3880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:18.846258: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:18.886781: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f475b82b110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:18.886846: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:18.887493: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f475b82b710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:18.887559: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:18.930516: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f475f795c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:18.930589: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:19.044602: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f4763830040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:19.044675: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:19.206686: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f47638309d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:19.206759: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:19.278447: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f27f40304c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 07:48:19.278522: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 07:48:20.805795: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.132681: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.172712: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.184004: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.226253: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.376691: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.569251: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:21.585059: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 07:48:23.728284: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.049134: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.075909: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.138868: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.252177: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.335814: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.470816: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 07:48:24.546733: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][07:48:47.694][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][07:48:47.694][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.704][ERROR][RK0][main]: coll ps creation done
[HCTR][07:48:47.704][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][07:48:47.714][ERROR][RK0][tid #139944966207232]: replica 2 reaches 1000, calling init pre replica
[HCTR][07:48:47.715][ERROR][RK0][tid #139944966207232]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.719][ERROR][RK0][tid #139944966207232]: coll ps creation done
[HCTR][07:48:47.719][ERROR][RK0][tid #139944966207232]: replica 2 waits for coll ps creation barrier
[HCTR][07:48:47.721][ERROR][RK0][tid #139944840382208]: replica 4 reaches 1000, calling init pre replica
[HCTR][07:48:47.721][ERROR][RK0][tid #139944840382208]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.724][ERROR][RK0][tid #139944840382208]: replica 5 reaches 1000, calling init pre replica
[HCTR][07:48:47.724][ERROR][RK0][tid #139944840382208]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.726][ERROR][RK0][tid #139944840382208]: coll ps creation done
[HCTR][07:48:47.726][ERROR][RK0][tid #139944840382208]: replica 4 waits for coll ps creation barrier
[HCTR][07:48:47.728][ERROR][RK0][tid #139944840382208]: coll ps creation done
[HCTR][07:48:47.728][ERROR][RK0][tid #139944840382208]: replica 5 waits for coll ps creation barrier
[HCTR][07:48:47.795][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][07:48:47.795][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.798][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][07:48:47.798][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.803][ERROR][RK0][main]: coll ps creation done
[HCTR][07:48:47.803][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][07:48:47.806][ERROR][RK0][main]: coll ps creation done
[HCTR][07:48:47.806][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][07:48:47.841][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][07:48:47.841][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.848][ERROR][RK0][main]: coll ps creation done
[HCTR][07:48:47.848][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][07:48:47.858][ERROR][RK0][tid #139945301751552]: replica 1 reaches 1000, calling init pre replica
[HCTR][07:48:47.858][ERROR][RK0][tid #139945301751552]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][07:48:47.866][ERROR][RK0][tid #139945301751552]: coll ps creation done
[HCTR][07:48:47.866][ERROR][RK0][tid #139945301751552]: replica 1 waits for coll ps creation barrier
[HCTR][07:48:47.866][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][07:48:48.755][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][07:48:48.803][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][tid #139944840382208]: replica 5 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][tid #139944840382208]: replica 4 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][tid #139945301751552]: replica 1 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][tid #139944966207232]: replica 2 calling init per replica
[HCTR][07:48:48.803][ERROR][RK0][main]: Calling build_v2
[HCTR][07:48:48.803][ERROR][RK0][main]: Calling build_v2
[HCTR][07:48:48.803][ERROR][RK0][main]: Calling build_v2
[HCTR][07:48:48.804][ERROR][RK0][tid #139944840382208]: Calling build_v2
[HCTR][07:48:48.804][ERROR][RK0][tid #139944840382208]: Calling build_v2
[HCTR][07:48:48.804][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][tid #139945301751552]: Calling build_v2
[HCTR][07:48:48.804][ERROR][RK0][main]: Calling build_v2
[HCTR][07:48:48.804][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][tid #139944966207232]: Calling build_v2
[HCTR][07:48:48.804][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][tid #139944840382208]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][tid #139944840382208]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][tid #139945301751552]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][07:48:48.804][ERROR][RK0][tid #139944966207232]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[2022-12-12 07:48:48[2022-12-12 07:48:48.2022-12-12 07:48:482022-12-12 07:48:48.2022-12-12 07:48:48804120[[..804128.: 2022-12-12 07:48:488041308041272022-12-12 07:48:48: 8041342022-12-12 07:48:48E.: : .E: . 804148EE804149 E804151/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:   : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc : :E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE136 :: 136: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccusing concurrent impl MPSPhase:] ] :using concurrent impl MPSPhase] :
136using concurrent impl MPSPhaseusing concurrent impl MPSPhase136
using concurrent impl MPSPhase136] 

] 
] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase


[2022-12-12 07:48:48.808380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 07:48:48.808423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 07:48:48.808477: E[ 2022-12-12 07:48:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:808481212: ] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[[2022-12-12 07:48:482022-12-12 07:48:482022-12-12 07:48:48...808525808532808533: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[::1782022-12-12 07:48:48213196] .] ] v100x8, slow pcie808571remote time is 8.68421assigning 8 to cpu
: 

E[ [2022-12-12 07:48:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 07:48:48.:2022-12-12 07:48:48.808617178.808622: ] 808618: Ev100x8, slow pcie: E 
E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 07:48:48:2022-12-12 07:48:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 07:48:48.196.:214.808664] 808664178] 808673: assigning 8 to cpu: ] cpu time is 97.0588: E
[Ev100x8, slow pcie
E 2022-12-12 07:48:48 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:[[808717::2022-12-12 07:48:482122022-12-12 07:48:482022-12-12 07:48:48: 178196.] ..E] ] 808760build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8808767808770 v100x8, slow pcieassigning 8 to cpu: 
: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

E[EE:[ 2022-12-12 07:48:48  1782022-12-12 07:48:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .:808897:2022-12-12 07:48:48:v100x8, slow pcie808892178: 196.212
: ] E] 808926] E[v100x8, slow pcie assigning 8 to cpu: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-12 07:48:48
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:[ :809011196[2022-12-12 07:48:48[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213: ] 2022-12-12 07:48:48.2022-12-12 07:48:48:] Eassigning 8 to cpu.809058.212remote time is 8.68421 
809071: 809076] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:[E[ E
1962022-12-12 07:48:48 2022-12-12 07:48:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu809180:2022-12-12 07:48:48809183196:
: 213.: ] 212E] 809227Eassigning 8 to cpu]  remote time is 8.68421:  
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2022-12-12 07:48:48 :[[214.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-12 07:48:482022-12-12 07:48:48] 8093522022-12-12 07:48:48:] ..cpu time is 97.0588: .213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8809368809382
E809379] 
: :  : remote time is 8.68421EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E
  :2022-12-12 07:48:48 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::2022-12-12 07:48:48] 809488:214212.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 213] ] 809516
E] cpu time is 97.0588build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:  [remote time is 8.68421

E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 07:48:48
 :.[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2138096202022-12-12 07:48:482022-12-12 07:48:48:] : ..214remote time is 8.68421E809657809651] 
 : : cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[EE
:2022-12-12 07:48:48  213./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 809739::remote time is 8.68421: 214213
E] ]  cpu time is 97.0588[remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 07:48:48
:.214809832[] : 2022-12-12 07:48:48cpu time is 97.0588E.
 809871/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E214 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 07:50:05.937829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 07:50:05.977823: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:50:05.977910: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 07:50:05.978948: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 07:50:06. 55980: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 07:50:06.450704: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 07:50:06.450795: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 07:50:13.501944: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 07:50:13.502044: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 07:50:15.188885: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 07:50:15.188984: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 07:50:15.191937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 07:50:15.191999: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 07:50:15.442103: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 07:50:15.470719: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 07:50:15.472135: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 07:50:15.492565: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 07:50:16. 62147: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 07:50:16. 64468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 07:50:16. 67543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 07:50:16. 70481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 07:50:16. 73355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 07:50:16. 76285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 07:50:16. 79170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 07:50:16. 82081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 07:50:16. 84976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 07:51:17.714025: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 07:51:17.724100: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 07:51:17.727403: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 07:51:17.771516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 07:51:17.771614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 07:51:17.771650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 07:51:17.771683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 07:51:17.772254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:51:17.772310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.773545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.774220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.787025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 07:51:17.787107: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 07:51:17.787113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 07:51:17.787200: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 07:51:17.787340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 07:51:17.787405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 07:51:17.787571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:51:17.787630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.787661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:51:17.787717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.787843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:51:17.787894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.788755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [1 solved2022-12-12 07:51:17
.788789: E[ 2022-12-12 07:51:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:788832202: ] E 4 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 1 initing device 1[
2022-12-12 07:51:17.788892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 07:51:17.789110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[[2022-12-12 07:51:172022-12-12 07:51:17..789145789166: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202205] ] 6 solvedworker 0 thread 2 initing device 2

[2022-12-12 07:51:17.789256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 07:51:17.789307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:51:17.789340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:51:171815.] 789356Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.789416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.789655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 07:51:17.789687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 07:51:17.1815789704] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.789771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.790780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.790843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.791935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.793915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.793977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.794194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.794269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.795252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.795306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.796288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.798168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.798220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.798438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.798501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 07:51:17.852551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 07:51:17.858131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:51:17.858277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:51:17.859150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.859949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.861123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.861181: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.42 MB
[[[[2022-12-12 07:51:172022-12-12 07:51:172022-12-12 07:51:172022-12-12 07:51:17....875765875765875764875764: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 07:51:17.877465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 07:51:17.881734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:51:17.881817: [E2022-12-12 07:51:17 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc881812:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[[2022-12-12 07:51:172022-12-12 07:51:17..881887881903: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 07:51:17.881984[: 2022-12-12 07:51:17E. 881977/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1024
[2022-12-12 07:51:17.882072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:51:17.882584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:51:17.882690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:51:17.882715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.882971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 07:51:17.883815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.884076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 07:51:17.884324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.884996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.885862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.886618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.887242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.887467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.887679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.887715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.887768: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.31 MB
[2022-12-12 07:51:17.887853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.888333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.888382: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.28 MB
[2022-12-12 07:51:17.888562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.888609: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.42 MB
[2022-12-12 07:51:17.888785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.888832: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.17 MB
[2022-12-12 07:51:17.888984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.889034: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [WORKER[0] alloc host memory 114.42 MB2022-12-12 07:51:17
.889041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:51:17.889114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:51:17.889390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 07:51:17.889463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 07:51:17.889891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.890807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 07:51:17.891114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.891463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:17.892227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.892274: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.15 MB
[2022-12-12 07:51:17.892568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:17.892616: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 114.33 MB
[2022-12-12 07:51:17.938809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.939462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.939514: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 07:51:17.960160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.960504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.960778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.960821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.29 GB
[2022-12-12 07:51:17.961114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.961159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.29 GB
[2022-12-12 07:51:17.961964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.962150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.962565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.962605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 07:51:17.962758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.962801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.28 GB
[2022-12-12 07:51:17.965667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.966338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.966380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 07:51:17.966740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.967361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.967403[: 2022-12-12 07:51:17E. 967394/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 14.27 GB:
1980] eager alloc mem 25.25 KB
[2022-12-12 07:51:17.968035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 07:51:17.968078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.30 GB
[[[[[[[[2022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:22........510100510100510100510100510100510101510100510101: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 5 init p2p of link 6Device 7 init p2p of link 4Device 4 init p2p of link 5Device 0 init p2p of link 3Device 6 init p2p of link 0Device 2 init p2p of link 1Device 3 init p2p of link 2Device 1 init p2p of link 7







[[[[2022-12-12 07:51:22[2022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:22[[[.2022-12-12 07:51:22...2022-12-12 07:51:222022-12-12 07:51:222022-12-12 07:51:22510668.510668510668510672...: 510673: : : 510681510681510683E: EEE: : :  E   EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:198019801980:::] 1980] ] ] 198019801980eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] ] 
eager alloc mem 611.00 KB


eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB



[2022-12-12 07:51:22.511833: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[[2022-12-12 07:51:22:2022-12-12 07:51:222022-12-12 07:51:22.638[[..511846[] 2022-12-12 07:51:222022-12-12 07:51:22511846511849: 2022-12-12 07:51:22[eager release cuda mem 625663..: : E.2022-12-12 07:51:22
511866511870EE 511877.: :   /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 511900EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:E:   ::638 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ::] ] eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638eager release cuda mem 625663eager release cuda mem 625663
638:] ] 

] 638eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663] 


eager release cuda mem 625663
[2022-12-12 07:51:22.533317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 07:51:22.533486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.533525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 07:51:22.533683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.534274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 07:51:22.534395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.534445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.534619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.535157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 07:51:22.535314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.535347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.535586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 07:51:22.535765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.535903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 07:51:22.536042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 07:51:22] .Device 7 init p2p of link 1536077
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.[5362192022-12-12 07:51:22: .E536227 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 07:51:22.536395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 07:51:22.536569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.536682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.536990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.537094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.537426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.554488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 07:51:22.554633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.555592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.555889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 07:51:22.556026[: 2022-12-12 07:51:22E. 556047/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 7 init p2p of link 6:
1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.556173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.556390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 07:51:22.556535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.556601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 07:51:22.556733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.556900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[[2022-12-12 07:51:222022-12-12 07:51:22..557036557035: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-12 07:51:22.557116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.557446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.557546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[[2022-12-12 07:51:222022-12-12 07:51:22..557631557646: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:2022-12-12 07:51:22:1926.638] 557691] Device 3 init p2p of link 5: eager release cuda mem 625663
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.557857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.557980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.558694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.558753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.584921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 07:51:22.585053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.586015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.587654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 07:51:22.587793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.588362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 07:51:22.588441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 07:51:22.588490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.588545[: 2022-12-12 07:51:22E. 588568/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1926 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuDevice 5 init p2p of link 3:
1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.588685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.588744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.589121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 07:51:22.589241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.589412: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.589509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.589530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 07:51:22.589602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.589648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.590154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.590552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.590750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 07:51:22.590876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 07:51:22.591758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 07:51:22.603546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.606828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29995331 / 100000000 nodes ( 30.00 %~30.00 %) | remote 70004669 / 100000000 nodes ( 70.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.81748 secs 
[2022-12-12 07:51:22.618608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.619020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.619102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.619347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.619599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.619758: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.620378: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 07:51:22.637605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29970368 / 100000000 nodes ( 29.97 %~30.00 %) | remote 70029632 / 100000000 nodes ( 70.03 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.30 GB | 4.84791 secs 
[2022-12-12 07:51:22.637808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29924268 / 100000000 nodes ( 29.92 %~30.00 %) | remote 70075732 / 100000000 nodes ( 70.08 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.27 GB | 4.85011 secs 
[2022-12-12 07:51:22.638019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29995536 / 100000000 nodes ( 30.00 %~30.00 %) | remote 70004464 / 100000000 nodes ( 70.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.86572 secs 
[2022-12-12 07:51:22.638227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29956616 / 100000000 nodes ( 29.96 %~30.00 %) | remote 70043384 / 100000000 nodes ( 70.04 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.29 GB | 4.85062 secs 
[2022-12-12 07:51:22.638440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29966570 / 100000000 nodes ( 29.97 %~30.00 %) | remote 70033430 / 100000000 nodes ( 70.03 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.29 GB | 4.84904 secs 
[2022-12-12 07:51:22.638653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29995315 / 100000000 nodes ( 30.00 %~30.00 %) | remote 70004685 / 100000000 nodes ( 70.00 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.31 GB | 4.85077 secs 
[2022-12-12 07:51:22.638919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 29928967 / 100000000 nodes ( 29.93 %~30.00 %) | remote 70071033 / 100000000 nodes ( 70.07 %) | cpu 0 / 100000000 nodes ( 0.00 %) | 14.28 GB | 4.84916 secs 
[2022-12-12 07:51:22.638993: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 20.95 GB
[2022-12-12 07:51:23.911835: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 21.21 GB
[2022-12-12 07:51:23.911973: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 21.21 GB
[2022-12-12 07:51:23.912496: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 21.21 GB
[2022-12-12 07:51:25.318605: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 21.47 GB
[2022-12-12 07:51:25.318962: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 21.47 GB
[2022-12-12 07:51:25.319973: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 21.47 GB
[2022-12-12 07:51:26.798276: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 21.69 GB
[2022-12-12 07:51:26.799279: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 21.69 GB
[2022-12-12 07:51:26.800268: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 21.69 GB
[2022-12-12 07:51:28.499811: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 21.90 GB
[2022-12-12 07:51:28.500193: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 21.90 GB
[2022-12-12 07:51:28.500657: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 21.90 GB
[2022-12-12 07:51:28.501658: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 21.90 GB
[2022-12-12 07:51:28.502558: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 21.90 GB
[2022-12-12 07:51:30.269916: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 22.10 GB
[2022-12-12 07:51:30.270092: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 22.10 GB
[HCTR][07:51:30.671][ERROR][RK0][tid #139944966207232]: replica 2 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][tid #139945301751552]: replica 1 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][tid #139944840382208]: replica 4 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][tid #139944840382208]: replica 5 calling init per replica done, doing barrier
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][tid #139945301751552]: replica 1 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][tid #139944840382208]: replica 4 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][tid #139944840382208]: replica 5 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][tid #139944966207232]: replica 2 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][07:51:30.671][ERROR][RK0][main]: init per replica done
[HCTR][07:51:30.672][ERROR][RK0][tid #139944840382208]: init per replica done
[HCTR][07:51:30.672][ERROR][RK0][tid #139945301751552]: init per replica done
[HCTR][07:51:30.672][ERROR][RK0][tid #139944840382208]: init per replica done
[HCTR][07:51:30.672][ERROR][RK0][main]: init per replica done
[HCTR][07:51:30.672][ERROR][RK0][tid #139944966207232]: init per replica done
[HCTR][07:51:30.672][ERROR][RK0][main]: init per replica done
[HCTR][07:51:30.674][ERROR][RK0][main]: init per replica done
[HCTR][07:51:30.678][ERROR][RK0][tid #139944840382208]: 5 allocated 3276800 at 0x7f30e3920000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944840382208]: 5 allocated 6553600 at 0x7f4953000000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944840382208]: 5 allocated 3276800 at 0x7f4953640000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944840382208]: 5 allocated 6553600 at 0x7f4953960000
[HCTR][07:51:30.678][ERROR][RK0][tid #139945301751552]: 1 allocated 3276800 at 0x7f30e3920000
[HCTR][07:51:30.678][ERROR][RK0][tid #139945301751552]: 1 allocated 6553600 at 0x7f4951000000
[HCTR][07:51:30.678][ERROR][RK0][tid #139945301751552]: 1 allocated 3276800 at 0x7f4951640000
[HCTR][07:51:30.678][ERROR][RK0][tid #139945301751552]: 1 allocated 6553600 at 0x7f4951960000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944966207232]: 2 allocated 3276800 at 0x7f30e3920000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944966207232]: 2 allocated 6553600 at 0x7f4953000000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944966207232]: 2 allocated 3276800 at 0x7f4953640000
[HCTR][07:51:30.678][ERROR][RK0][tid #139944966207232]: 2 allocated 6553600 at 0x7f4953960000
[HCTR][07:51:30.678][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f30e3920000
[HCTR][07:51:30.678][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f30e7920000
[HCTR][07:51:30.678][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4951000000
[HCTR][07:51:30.678][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f4953000000
[HCTR][07:51:30.678][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4951640000
[HCTR][07:51:30.678][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f4953640000
[HCTR][07:51:30.678][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4951960000
[HCTR][07:51:30.678][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f4953960000
[HCTR][07:51:30.678][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f30e7920000
[HCTR][07:51:30.678][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f4951000000
[HCTR][07:51:30.678][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f4951640000
[HCTR][07:51:30.678][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f4951960000
[HCTR][07:51:30.679][ERROR][RK0][tid #139944899098368]: 7 allocated 3276800 at 0x7f30e7920000
[HCTR][07:51:30.679][ERROR][RK0][tid #139944899098368]: 7 allocated 6553600 at 0x7f4951000000
[HCTR][07:51:30.679][ERROR][RK0][tid #139944899098368]: 7 allocated 3276800 at 0x7f4951640000
[HCTR][07:51:30.679][ERROR][RK0][tid #139944899098368]: 7 allocated 6553600 at 0x7f4951960000
[HCTR][07:51:30.681][ERROR][RK0][tid #139945024923392]: 0 allocated 3276800 at 0x7f4952f20000
[HCTR][07:51:30.681][ERROR][RK0][tid #139945024923392]: 0 allocated 6553600 at 0x7f4953400000
[HCTR][07:51:30.681][ERROR][RK0][tid #139945024923392]: 0 allocated 3276800 at 0x7f495410e800
[HCTR][07:51:30.681][ERROR][RK0][tid #139945024923392]: 0 allocated 6553600 at 0x7f495442e800








