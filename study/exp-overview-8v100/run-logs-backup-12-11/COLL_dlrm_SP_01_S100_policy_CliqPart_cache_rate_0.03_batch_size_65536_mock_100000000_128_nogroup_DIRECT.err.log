2022-12-11 22:37:39.905727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.914325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.920634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.926828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.939567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.945425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.950766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:39.955766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.013710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.022820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.026105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.027859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.028772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.029091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.030481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.030516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.032238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.032306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.033720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.034037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.035430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.035727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.037077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.037623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.038471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.039402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.039863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.041170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.041364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.042981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.044021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.045070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.046810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.047897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.049001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.049943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.050869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.051905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.053273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.054922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.055554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.056870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.057848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.058782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.059967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.060526: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.061012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.062036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.063000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.068471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.069985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.070391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.072021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.072194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.072207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.074231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.074330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.074519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.076677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.076954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.079661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.080078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.081369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.082800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.083294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.085042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.085191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.086074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.086590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.087947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.088484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.088744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.089945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.091294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.091862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.092283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.093344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.094312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.095230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.096108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.097520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.097510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.098340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.098955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.100049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.100321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.101158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.101709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.102224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.102716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.103356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.104104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.104282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.107352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.108453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.109903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.109969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.110926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.111143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.112299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.113506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.113665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.133970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.141594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.148014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.149384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.149434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.150091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.150180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.151207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.152670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.153281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.153289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.154021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.154203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.154948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.157495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.157875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.158187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.158567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.158733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.160156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.162120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.162614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.162801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.163075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.163545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.164294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.166191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.167706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.167879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.168055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.168238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.169021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.171251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.172438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.172615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.172699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.173025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.174404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.176269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.177111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.177524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.177533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.177657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.179312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.179972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.181185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.181538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.181637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.181712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.183435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.184295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.185285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.185824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.185872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.185916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.187934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.188523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.189341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.189976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.190031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.190108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.192288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.192855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.193689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.194167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.194219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.194433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.196474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.197459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.198428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.198976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.199091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.199120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.200689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.201354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.202245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.202591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.202738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.202785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.203094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.205685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.207141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.207192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.208087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.208200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.208359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.208611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.210176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.211981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.211992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.212505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.212604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.212759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.212996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.216249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.216253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.217004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.217180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.218104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.218293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.219185: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.219770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.219856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.220536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.220676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.222107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.222243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.223986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.224364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.224514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.224737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.225930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.226509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.227802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.228171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.228453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.229018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.229077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.230008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.230272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.231859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.232262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.232550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.233557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.233668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.234423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.234736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.236208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.236747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.237026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.237919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.238176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.238842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.239084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.240931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.241113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.242010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.242868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.243175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.244550: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.244585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.245183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.245548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.246487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.246983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.248583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.249039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.249525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.252011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.252479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.253431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.253908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.254049: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.254303: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.255401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.256558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.256677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.258251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.258319: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.259710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.260904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.262989: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.263907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.264047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.264291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.267576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.267767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.267837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.268042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.271281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.271369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.271564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.271810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.272340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.277961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.278367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.279353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.310947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.311932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.316172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.349565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.355356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.365872: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 22:37:40.375875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.381664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:40.415358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.400533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.401163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.401687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.402527: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.402590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:37:41.421036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.421690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.422307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.422890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.423647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.424140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 22:37:41.470060: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.470267: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.519948: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 22:37:41.619734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.620374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.620900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.621605: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.621661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:37:41.639589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.640313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.641419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.642237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.642773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.643308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 22:37:41.674674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.675316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.675839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.676305: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.676362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:37:41.693032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.693373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.693659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.694841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.694984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.695263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.696563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.696687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.696985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.698071: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.698128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:37:41.698334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.698388: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.698443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:37:41.699195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.699337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.700239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.700327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 22:37:41.700806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.701278: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.701321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:37:41.714654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.715297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.715330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.717226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.717236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.718470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.718606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.718997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.719560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.719827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.720488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.720990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.721265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 22:37:41.721918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.722163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 22:37:41.722607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.723157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.723639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 22:37:41.725885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.726463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.726989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.727468: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.727517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:37:41.729033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.729076: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.729229: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.729611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.730145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.730608: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 22:37:41.730655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:37:41.731056: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 22:37:41.745258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.745924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.746435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.747028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.747557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.748034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 22:37:41.748357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.748971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.749485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.750078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.750608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 22:37:41.751089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 22:37:41.767571: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.767793: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.768291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.768467: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.768768: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 22:37:41.769373: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 22:37:41.769834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.769988: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.771873: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 22:37:41.787772: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.787970: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.789736: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 22:37:41.794347: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.794556: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.796474: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 22:37:41.798224: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.798365: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 22:37:41.800001: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][22:37:43.050][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.057][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.057][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.058][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.058][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.058][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.090][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][22:37:43.090][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 90it [00:01, 76.27it/s]warmup run: 97it [00:01, 82.63it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 181it [00:01, 166.54it/s]warmup run: 194it [00:01, 179.12it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.47s/it]warmup run: 97it [00:01, 83.45it/s]warmup run: 96it [00:01, 82.22it/s]warmup run: 274it [00:01, 268.47it/s]warmup run: 292it [00:01, 286.48it/s]warmup run: 99it [00:01, 85.34it/s]warmup run: 97it [00:01, 83.73it/s]warmup run: 97it [00:01, 84.19it/s]warmup run: 95it [00:01, 83.86it/s]warmup run: 195it [00:01, 181.79it/s]warmup run: 193it [00:01, 179.06it/s]warmup run: 366it [00:01, 372.74it/s]warmup run: 391it [00:01, 399.14it/s]warmup run: 199it [00:01, 185.90it/s]warmup run: 195it [00:01, 182.36it/s]warmup run: 193it [00:01, 180.94it/s]warmup run: 190it [00:01, 180.94it/s]warmup run: 293it [00:01, 289.88it/s]warmup run: 291it [00:01, 286.82it/s]warmup run: 458it [00:02, 473.88it/s]warmup run: 489it [00:02, 507.32it/s]warmup run: 293it [00:01, 290.70it/s]warmup run: 301it [00:01, 298.86it/s]warmup run: 290it [00:01, 288.44it/s]warmup run: 286it [00:01, 288.48it/s]warmup run: 392it [00:01, 402.85it/s]warmup run: 389it [00:01, 398.66it/s]warmup run: 549it [00:02, 565.18it/s]warmup run: 592it [00:02, 616.27it/s]warmup run: 388it [00:01, 397.37it/s]warmup run: 399it [00:01, 408.94it/s]warmup run: 389it [00:01, 402.24it/s]warmup run: 383it [00:01, 400.47it/s]warmup run: 491it [00:02, 513.05it/s]warmup run: 487it [00:02, 507.29it/s]warmup run: 642it [00:02, 648.49it/s]warmup run: 695it [00:02, 711.71it/s]warmup run: 480it [00:02, 494.78it/s]warmup run: 490it [00:02, 499.42it/s]warmup run: 488it [00:02, 512.29it/s]warmup run: 482it [00:01, 512.33it/s]warmup run: 591it [00:02, 615.87it/s]warmup run: 586it [00:02, 608.88it/s]warmup run: 734it [00:02, 715.30it/s]warmup run: 800it [00:02, 793.59it/s]warmup run: 571it [00:02, 581.82it/s]warmup run: 587it [00:02, 597.60it/s]warmup run: 588it [00:02, 615.58it/s]warmup run: 582it [00:02, 615.94it/s]warmup run: 690it [00:02, 701.44it/s]warmup run: 685it [00:02, 696.91it/s]warmup run: 827it [00:02, 769.77it/s]warmup run: 902it [00:02, 852.68it/s]warmup run: 663it [00:02, 658.60it/s]warmup run: 686it [00:02, 687.71it/s]warmup run: 687it [00:02, 702.00it/s]warmup run: 676it [00:02, 691.14it/s]warmup run: 789it [00:02, 771.70it/s]warmup run: 781it [00:02, 760.11it/s]warmup run: 918it [00:02, 807.05it/s]warmup run: 1003it [00:02, 892.84it/s]warmup run: 753it [00:02, 718.50it/s]warmup run: 784it [00:02, 759.41it/s]warmup run: 786it [00:02, 773.14it/s]warmup run: 781it [00:02, 779.99it/s]warmup run: 888it [00:02, 828.78it/s]warmup run: 877it [00:02, 809.95it/s]warmup run: 1010it [00:02, 836.74it/s]warmup run: 1104it [00:02, 921.72it/s]warmup run: 843it [00:02, 763.46it/s]warmup run: 881it [00:02, 813.36it/s]warmup run: 885it [00:02, 829.02it/s]warmup run: 885it [00:02, 848.09it/s]warmup run: 986it [00:02, 868.77it/s]warmup run: 976it [00:02, 856.73it/s]warmup run: 1103it [00:02, 862.49it/s]warmup run: 1205it [00:02, 937.99it/s]warmup run: 936it [00:02, 806.82it/s]warmup run: 980it [00:02, 860.07it/s]warmup run: 983it [00:02, 870.03it/s]warmup run: 989it [00:02, 898.23it/s]warmup run: 1086it [00:02, 904.96it/s]warmup run: 1073it [00:02, 865.71it/s]warmup run: 1305it [00:02, 949.63it/s]warmup run: 1030it [00:02, 841.38it/s]warmup run: 1080it [00:02, 897.32it/s]warmup run: 1195it [00:02, 821.57it/s]warmup run: 1082it [00:02, 902.91it/s]warmup run: 1093it [00:02, 937.48it/s]warmup run: 1186it [00:02, 930.85it/s]warmup run: 1167it [00:02, 884.38it/s]warmup run: 1404it [00:02, 933.69it/s]warmup run: 1123it [00:02, 866.17it/s]warmup run: 1179it [00:02, 920.91it/s]warmup run: 1282it [00:02, 834.83it/s]warmup run: 1180it [00:02, 922.77it/s]warmup run: 1196it [00:02, 961.66it/s]warmup run: 1286it [00:02, 950.82it/s]warmup run: 1264it [00:02, 907.82it/s]warmup run: 1220it [00:02, 894.13it/s]warmup run: 1279it [00:02, 942.59it/s]warmup run: 1501it [00:03, 929.75it/s]warmup run: 1381it [00:03, 877.82it/s]warmup run: 1278it [00:02, 932.20it/s]warmup run: 1298it [00:02, 977.86it/s]warmup run: 1387it [00:02, 967.23it/s]warmup run: 1363it [00:02, 930.66it/s]warmup run: 1316it [00:02, 911.18it/s]warmup run: 1602it [00:03, 952.59it/s]warmup run: 1379it [00:02, 957.41it/s]warmup run: 1475it [00:03, 893.73it/s]warmup run: 1375it [00:02, 941.58it/s]warmup run: 1402it [00:02, 993.51it/s]warmup run: 1487it [00:03, 975.82it/s]warmup run: 1459it [00:03, 914.20it/s]warmup run: 1410it [00:03, 917.86it/s]warmup run: 1703it [00:03, 967.91it/s]warmup run: 1479it [00:03, 967.79it/s]warmup run: 1571it [00:03, 912.62it/s]warmup run: 1474it [00:03, 954.65it/s]warmup run: 1505it [00:02, 995.06it/s]warmup run: 1588it [00:03, 985.46it/s]warmup run: 1553it [00:03, 915.42it/s]warmup run: 1505it [00:03, 925.67it/s]warmup run: 1806it [00:03, 984.65it/s]warmup run: 1579it [00:03, 975.56it/s]warmup run: 1669it [00:03, 930.21it/s]warmup run: 1575it [00:03, 970.03it/s]warmup run: 1607it [00:03, 1002.00it/s]warmup run: 1689it [00:03, 991.58it/s]warmup run: 1654it [00:03, 942.29it/s]warmup run: 1599it [00:03, 929.75it/s]warmup run: 1679it [00:03, 982.63it/s]warmup run: 1909it [00:03, 995.45it/s]warmup run: 1765it [00:03, 937.92it/s]warmup run: 1676it [00:03, 981.00it/s]warmup run: 1710it [00:03, 1008.66it/s]warmup run: 1790it [00:03, 995.26it/s]warmup run: 1755it [00:03, 961.85it/s]warmup run: 1698it [00:03, 945.89it/s]warmup run: 2012it [00:03, 1005.11it/s]warmup run: 1781it [00:03, 991.92it/s]warmup run: 1863it [00:03, 947.50it/s]warmup run: 1780it [00:03, 997.13it/s]warmup run: 1816it [00:03, 1021.34it/s]warmup run: 1891it [00:03, 997.67it/s]warmup run: 1857it [00:03, 977.04it/s]warmup run: 1796it [00:03, 953.85it/s]warmup run: 2131it [00:03, 1058.44it/s]warmup run: 1881it [00:03, 988.39it/s]warmup run: 1962it [00:03, 957.52it/s]warmup run: 1885it [00:03, 1012.69it/s]warmup run: 1921it [00:03, 1027.32it/s]warmup run: 1992it [00:03, 997.83it/s]warmup run: 1957it [00:03, 982.52it/s]warmup run: 1896it [00:03, 964.80it/s]warmup run: 2250it [00:03, 1095.03it/s]warmup run: 1983it [00:03, 996.18it/s]warmup run: 2074it [00:03, 1004.06it/s]warmup run: 1990it [00:03, 1021.28it/s]warmup run: 2029it [00:03, 1042.55it/s]warmup run: 2109it [00:03, 1047.16it/s]warmup run: 2068it [00:03, 1019.41it/s]warmup run: 1996it [00:03, 974.06it/s]warmup run: 2369it [00:03, 1121.39it/s]warmup run: 2099it [00:03, 1044.13it/s]warmup run: 2197it [00:03, 1070.38it/s]warmup run: 2109it [00:03, 1069.76it/s]warmup run: 2152it [00:03, 1097.84it/s]warmup run: 2228it [00:03, 1088.28it/s]warmup run: 2188it [00:03, 1072.78it/s]warmup run: 2115it [00:03, 1038.43it/s]warmup run: 2488it [00:03, 1140.62it/s]warmup run: 2320it [00:03, 1117.16it/s]warmup run: 2219it [00:03, 1088.78it/s]warmup run: 2230it [00:03, 1110.79it/s]warmup run: 2275it [00:03, 1136.70it/s]warmup run: 2345it [00:03, 1112.29it/s]warmup run: 2309it [00:03, 1111.72it/s]warmup run: 2232it [00:03, 1076.71it/s]warmup run: 2605it [00:04, 1148.65it/s]warmup run: 2443it [00:04, 1150.31it/s]warmup run: 2339it [00:03, 1119.87it/s]warmup run: 2350it [00:03, 1135.36it/s]warmup run: 2398it [00:03, 1164.13it/s]warmup run: 2463it [00:03, 1131.76it/s]warmup run: 2430it [00:03, 1139.65it/s]warmup run: 2353it [00:03, 1114.75it/s]warmup run: 2721it [00:04, 1149.40it/s]warmup run: 2565it [00:04, 1170.73it/s]warmup run: 2459it [00:03, 1142.61it/s]warmup run: 2468it [00:03, 1146.94it/s]warmup run: 2521it [00:03, 1183.51it/s]warmup run: 2582it [00:04, 1147.56it/s]warmup run: 2551it [00:04, 1158.45it/s]warmup run: 2474it [00:04, 1140.38it/s]warmup run: 2837it [00:04, 1151.70it/s]warmup run: 2686it [00:04, 1179.86it/s]warmup run: 2579it [00:04, 1158.42it/s]warmup run: 2584it [00:04, 1148.85it/s]warmup run: 2643it [00:03, 1193.08it/s]warmup run: 2700it [00:04, 1155.06it/s]warmup run: 2667it [00:04, 1157.29it/s]warmup run: 2595it [00:04, 1158.56it/s]warmup run: 2954it [00:04, 1156.22it/s]warmup run: 2808it [00:04, 1190.05it/s]warmup run: 2698it [00:04, 1166.40it/s]warmup run: 2703it [00:04, 1160.65it/s]warmup run: 2766it [00:04, 1203.02it/s]warmup run: 3000it [00:04, 680.44it/s] warmup run: 2818it [00:04, 1161.03it/s]warmup run: 2788it [00:04, 1170.94it/s]warmup run: 2711it [00:04, 1158.60it/s]warmup run: 2928it [00:04, 1191.22it/s]warmup run: 2819it [00:04, 1178.38it/s]warmup run: 2823it [00:04, 1169.88it/s]warmup run: 2889it [00:04, 1209.89it/s]warmup run: 2935it [00:04, 1160.79it/s]warmup run: 3000it [00:04, 659.94it/s] warmup run: 2908it [00:04, 1177.89it/s]warmup run: 2829it [00:04, 1162.45it/s]warmup run: 2942it [00:04, 1191.03it/s]warmup run: 2944it [00:04, 1180.18it/s]warmup run: 3000it [00:04, 684.28it/s] warmup run: 3000it [00:04, 700.60it/s] warmup run: 3000it [00:04, 684.21it/s] warmup run: 3000it [00:04, 686.94it/s] warmup run: 3000it [00:04, 675.10it/s] warmup run: 2948it [00:04, 1169.52it/s]warmup run: 3000it [00:04, 670.40it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1657.48it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1599.08it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1638.03it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1595.68it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1596.84it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1624.13it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1642.25it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.60it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1644.66it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1603.88it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.85it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1636.17it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1630.47it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1647.42it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1608.43it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1591.81it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1634.20it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1655.84it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1642.84it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1646.87it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1636.25it/s]warmup should be done:  16%|        | 482/3000 [00:00<00:01, 1595.84it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1617.61it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1618.83it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1634.36it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1646.36it/s]warmup should be done:  22%|       | 660/3000 [00:00<00:01, 1649.95it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1640.85it/s]warmup should be done:  21%|       | 642/3000 [00:00<00:01, 1595.03it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1649.53it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1621.28it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1629.52it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1652.22it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1642.21it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1647.96it/s]warmup should be done:  27%|       | 808/3000 [00:00<00:01, 1614.98it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1624.49it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1628.67it/s]warmup should be done:  27%|       | 818/3000 [00:00<00:01, 1609.62it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1637.02it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1624.05it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1647.63it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1643.32it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1624.48it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1636.55it/s]warmup should be done:  33%|      | 987/3000 [00:00<00:01, 1627.78it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1607.96it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1637.92it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1633.71it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1648.95it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1636.90it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1623.12it/s]warmup should be done:  38%|      | 1151/3000 [00:00<00:01, 1631.53it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1637.72it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1610.70it/s]warmup should be done:  39%|      | 1159/3000 [00:00<00:01, 1642.23it/s]warmup should be done:  43%|     | 1303/3000 [00:00<00:01, 1635.66it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1655.11it/s]warmup should be done:  44%|     | 1315/3000 [00:00<00:01, 1631.90it/s]warmup should be done:  44%|     | 1319/3000 [00:00<00:01, 1632.03it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1641.00it/s]warmup should be done:  44%|     | 1305/3000 [00:00<00:01, 1617.48it/s]warmup should be done:  44%|     | 1306/3000 [00:00<00:01, 1618.22it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1644.49it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1652.51it/s]warmup should be done:  50%|     | 1487/3000 [00:00<00:00, 1643.24it/s]warmup should be done:  49%|     | 1479/3000 [00:00<00:00, 1632.39it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1613.86it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1620.11it/s]warmup should be done:  49%|     | 1467/3000 [00:00<00:00, 1616.40it/s]warmup should be done:  50%|     | 1489/3000 [00:00<00:00, 1644.99it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1602.37it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1646.17it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1649.00it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1625.65it/s]warmup should be done:  54%|    | 1629/3000 [00:01<00:00, 1612.10it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1628.91it/s]warmup should be done:  54%|    | 1633/3000 [00:01<00:00, 1623.44it/s]warmup should be done:  55%|    | 1654/3000 [00:01<00:00, 1645.74it/s]warmup should be done:  55%|    | 1645/3000 [00:01<00:00, 1605.80it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1646.93it/s]warmup should be done:  60%|    | 1806/3000 [00:01<00:00, 1625.77it/s]warmup should be done:  61%|    | 1818/3000 [00:01<00:00, 1637.74it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1611.68it/s]warmup should be done:  60%|    | 1797/3000 [00:01<00:00, 1626.19it/s]warmup should be done:  60%|    | 1796/3000 [00:01<00:00, 1623.80it/s]warmup should be done:  61%|    | 1819/3000 [00:01<00:00, 1646.99it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1620.18it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1645.09it/s]warmup should be done:  66%|   | 1969/3000 [00:01<00:00, 1626.75it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1614.36it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1634.21it/s]warmup should be done:  65%|   | 1961/3000 [00:01<00:00, 1627.57it/s]warmup should be done:  66%|   | 1984/3000 [00:01<00:00, 1647.31it/s]warmup should be done:  66%|   | 1977/3000 [00:01<00:00, 1631.15it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1594.27it/s]warmup should be done:  72%|  | 2155/3000 [00:01<00:00, 1642.81it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1634.58it/s]warmup should be done:  71%|   | 2117/3000 [00:01<00:00, 1616.74it/s]warmup should be done:  71%|   | 2124/3000 [00:01<00:00, 1626.97it/s]warmup should be done:  72%|  | 2146/3000 [00:01<00:00, 1630.65it/s]warmup should be done:  72%|  | 2149/3000 [00:01<00:00, 1646.88it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1638.14it/s]warmup should be done:  71%|   | 2123/3000 [00:01<00:00, 1605.01it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1637.76it/s]warmup should be done:  76%|  | 2279/3000 [00:01<00:00, 1616.98it/s]warmup should be done:  77%|  | 2320/3000 [00:01<00:00, 1637.75it/s]warmup should be done:  76%|  | 2287/3000 [00:01<00:00, 1624.50it/s]warmup should be done:  77%|  | 2314/3000 [00:01<00:00, 1644.36it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1624.40it/s]warmup should be done:  77%|  | 2308/3000 [00:01<00:00, 1640.29it/s]warmup should be done:  76%|  | 2285/3000 [00:01<00:00, 1607.26it/s]warmup should be done:  82%| | 2464/3000 [00:01<00:00, 1638.18it/s]warmup should be done:  81%| | 2441/3000 [00:01<00:00, 1615.37it/s]warmup should be done:  82%| | 2450/3000 [00:01<00:00, 1625.75it/s]warmup should be done:  83%| | 2484/3000 [00:01<00:00, 1634.27it/s]warmup should be done:  83%| | 2479/3000 [00:01<00:00, 1644.95it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1621.78it/s]warmup should be done:  82%| | 2474/3000 [00:01<00:00, 1644.87it/s]warmup should be done:  82%| | 2448/3000 [00:01<00:00, 1613.27it/s]warmup should be done:  88%| | 2628/3000 [00:01<00:00, 1635.67it/s]warmup should be done:  87%| | 2604/3000 [00:01<00:00, 1618.48it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1627.34it/s]warmup should be done:  88%| | 2644/3000 [00:01<00:00, 1646.22it/s]warmup should be done:  88%| | 2648/3000 [00:01<00:00, 1631.77it/s]warmup should be done:  88%| | 2636/3000 [00:01<00:00, 1620.75it/s]warmup should be done:  88%| | 2640/3000 [00:01<00:00, 1646.94it/s]warmup should be done:  87%| | 2611/3000 [00:01<00:00, 1616.82it/s]warmup should be done:  93%|| 2792/3000 [00:01<00:00, 1633.00it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1619.78it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1628.49it/s]warmup should be done:  94%|| 2810/3000 [00:01<00:00, 1647.57it/s]warmup should be done:  94%|| 2814/3000 [00:01<00:00, 1637.83it/s]warmup should be done:  93%|| 2799/3000 [00:01<00:00, 1620.25it/s]warmup should be done:  94%|| 2806/3000 [00:01<00:00, 1648.92it/s]warmup should be done:  92%|| 2774/3000 [00:01<00:00, 1618.17it/s]warmup should be done:  99%|| 2957/3000 [00:01<00:00, 1637.06it/s]warmup should be done:  98%|| 2931/3000 [00:01<00:00, 1625.58it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1639.40it/s]warmup should be done:  99%|| 2977/3000 [00:01<00:00, 1651.53it/s]warmup should be done:  99%|| 2981/3000 [00:01<00:00, 1644.67it/s]warmup should be done:  99%|| 2963/3000 [00:01<00:00, 1623.83it/s]warmup should be done:  99%|| 2973/3000 [00:01<00:00, 1653.99it/s]warmup should be done:  98%|| 2940/3000 [00:01<00:00, 1627.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1644.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1642.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1633.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1632.79it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1618.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.44it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1698.99it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1687.28it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.22it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.39it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1694.87it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1673.06it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1692.53it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.62it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1691.94it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1671.84it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1670.10it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1673.72it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1669.89it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1691.25it/s]warmup should be done:  11%|        | 341/3000 [00:00<00:01, 1697.68it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1687.33it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1693.86it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1677.32it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.22it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1671.12it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1670.50it/s]warmup should be done:  17%|        | 512/3000 [00:00<00:01, 1700.39it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1686.23it/s]warmup should be done:  17%|        | 510/3000 [00:00<00:01, 1686.89it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1692.50it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1678.66it/s]warmup should be done:  23%|       | 683/3000 [00:00<00:01, 1703.52it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1672.13it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1679.72it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1690.27it/s]warmup should be done:  22%|       | 671/3000 [00:00<00:01, 1668.69it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1691.12it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1673.52it/s]warmup should be done:  28%|       | 841/3000 [00:00<00:01, 1676.07it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1677.40it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1689.89it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1691.49it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1669.83it/s]warmup should be done:  28%|       | 854/3000 [00:00<00:01, 1686.27it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1673.67it/s]warmup should be done:  34%|      | 1010/3000 [00:00<00:01, 1677.87it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1675.08it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1692.16it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1671.77it/s]warmup should be done:  34%|      | 1007/3000 [00:00<00:01, 1671.09it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1686.68it/s]warmup should be done:  34%|      | 1024/3000 [00:00<00:01, 1689.29it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1671.70it/s]warmup should be done:  39%|      | 1178/3000 [00:00<00:01, 1678.53it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1693.80it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1673.86it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1669.82it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1669.81it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1681.79it/s]warmup should be done:  40%|      | 1194/3000 [00:00<00:01, 1690.20it/s]warmup should be done:  40%|      | 1187/3000 [00:00<00:01, 1675.12it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1679.98it/s]warmup should be done:  45%|     | 1360/3000 [00:00<00:00, 1696.76it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1671.95it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1670.96it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1670.69it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1687.63it/s]warmup should be done:  46%|     | 1365/3000 [00:00<00:00, 1693.55it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1677.69it/s]warmup should be done:  50%|     | 1515/3000 [00:00<00:00, 1679.46it/s]warmup should be done:  51%|     | 1530/3000 [00:00<00:00, 1695.70it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1670.19it/s]warmup should be done:  50%|     | 1513/3000 [00:00<00:00, 1669.10it/s]warmup should be done:  50%|     | 1511/3000 [00:00<00:00, 1670.46it/s]warmup should be done:  51%|     | 1529/3000 [00:00<00:00, 1689.32it/s]warmup should be done:  51%|     | 1535/3000 [00:00<00:00, 1695.08it/s]warmup should be done:  51%|     | 1524/3000 [00:00<00:00, 1677.59it/s]warmup should be done:  56%|    | 1684/3000 [00:01<00:00, 1680.33it/s]warmup should be done:  57%|    | 1700/3000 [00:01<00:00, 1696.60it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1670.56it/s]warmup should be done:  56%|    | 1682/3000 [00:01<00:00, 1672.64it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1671.52it/s]warmup should be done:  57%|    | 1699/3000 [00:01<00:00, 1691.71it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1696.62it/s]warmup should be done:  56%|    | 1692/3000 [00:01<00:00, 1677.41it/s]warmup should be done:  62%|   | 1853/3000 [00:01<00:00, 1682.04it/s]warmup should be done:  62%|   | 1870/3000 [00:01<00:00, 1697.53it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1669.59it/s]warmup should be done:  62%|   | 1851/3000 [00:01<00:00, 1675.05it/s]warmup should be done:  62%|   | 1869/3000 [00:01<00:00, 1693.94it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1670.61it/s]warmup should be done:  63%|   | 1877/3000 [00:01<00:00, 1699.00it/s]warmup should be done:  62%|   | 1860/3000 [00:01<00:00, 1677.47it/s]warmup should be done:  68%|   | 2040/3000 [00:01<00:00, 1697.49it/s]warmup should be done:  67%|   | 2022/3000 [00:01<00:00, 1681.27it/s]warmup should be done:  68%|   | 2039/3000 [00:01<00:00, 1694.80it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1666.09it/s]warmup should be done:  67%|   | 2015/3000 [00:01<00:00, 1666.75it/s]warmup should be done:  68%|   | 2048/3000 [00:01<00:00, 1700.83it/s]warmup should be done:  68%|   | 2028/3000 [00:01<00:00, 1674.34it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1657.35it/s]warmup should be done:  74%|  | 2210/3000 [00:01<00:00, 1694.26it/s]warmup should be done:  74%|  | 2209/3000 [00:01<00:00, 1692.57it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1664.12it/s]warmup should be done:  73%|  | 2191/3000 [00:01<00:00, 1665.89it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1664.96it/s]warmup should be done:  74%|  | 2219/3000 [00:01<00:00, 1700.33it/s]warmup should be done:  73%|  | 2196/3000 [00:01<00:00, 1669.35it/s]warmup should be done:  73%|  | 2185/3000 [00:01<00:00, 1647.73it/s]warmup should be done:  79%|  | 2380/3000 [00:01<00:00, 1695.34it/s]warmup should be done:  79%|  | 2379/3000 [00:01<00:00, 1693.43it/s]warmup should be done:  78%|  | 2348/3000 [00:01<00:00, 1663.81it/s]warmup should be done:  78%|  | 2349/3000 [00:01<00:00, 1664.62it/s]warmup should be done:  80%|  | 2390/3000 [00:01<00:00, 1697.87it/s]warmup should be done:  79%|  | 2358/3000 [00:01<00:00, 1657.13it/s]warmup should be done:  79%|  | 2363/3000 [00:01<00:00, 1668.84it/s]warmup should be done:  78%|  | 2353/3000 [00:01<00:00, 1656.45it/s]warmup should be done:  85%| | 2551/3000 [00:01<00:00, 1697.49it/s]warmup should be done:  85%| | 2550/3000 [00:01<00:00, 1696.16it/s]warmup should be done:  84%| | 2516/3000 [00:01<00:00, 1665.75it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1666.24it/s]warmup should be done:  85%| | 2560/3000 [00:01<00:00, 1697.19it/s]warmup should be done:  84%| | 2531/3000 [00:01<00:00, 1669.65it/s]warmup should be done:  84%| | 2524/3000 [00:01<00:00, 1651.33it/s]warmup should be done:  84%| | 2522/3000 [00:01<00:00, 1663.93it/s]warmup should be done:  91%| | 2722/3000 [00:01<00:00, 1699.19it/s]warmup should be done:  91%| | 2721/3000 [00:01<00:00, 1697.32it/s]warmup should be done:  89%| | 2684/3000 [00:01<00:00, 1666.02it/s]warmup should be done:  91%| | 2731/3000 [00:01<00:00, 1698.30it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1655.27it/s]warmup should be done:  90%| | 2698/3000 [00:01<00:00, 1669.17it/s]warmup should be done:  90%| | 2690/3000 [00:01<00:00, 1647.73it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1665.26it/s]warmup should be done:  96%|| 2892/3000 [00:01<00:00, 1697.76it/s]warmup should be done:  96%|| 2891/3000 [00:01<00:00, 1695.50it/s]warmup should be done:  95%|| 2851/3000 [00:01<00:00, 1663.07it/s]warmup should be done:  96%|| 2865/3000 [00:01<00:00, 1666.06it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1650.11it/s]warmup should be done:  95%|| 2855/3000 [00:01<00:00, 1643.38it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1662.44it/s]warmup should be done:  97%|| 2901/3000 [00:01<00:00, 1672.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1695.53it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1691.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1690.98it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1673.92it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1667.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1666.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.63it/s]2022-12-11 22:39:18.500484: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7027831b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:18.500545: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:18.966162: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f702f82ce90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:18.966223: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:19.122936: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f702b833290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:19.123004: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:19.277809: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f702b8310c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:19.277882: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:19.330285: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f702bf92460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:19.330349: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:19.370143: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f543c02e0e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:19.370227: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:19.378622: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f702395b310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:19.378678: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:19.392407: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f702782c950 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 22:39:19.392478: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 22:39:20.713010: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.295935: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.400826: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.580919: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.584786: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.635199: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.652947: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:21.688108: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 22:39:23.536365: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.179181: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.273626: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.465204: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.465342: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.510097: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.524693: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 22:39:24.536520: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][22:40:02.286][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][22:40:02.286][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.291][ERROR][RK0][main]: coll ps creation done
[HCTR][22:40:02.291][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][22:40:02.339][ERROR][RK0][tid #140120195843840]: replica 1 reaches 1000, calling init pre replica
[HCTR][22:40:02.339][ERROR][RK0][tid #140120195843840]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.344][ERROR][RK0][tid #140120195843840]: coll ps creation done
[HCTR][22:40:02.344][ERROR][RK0][tid #140120195843840]: replica 1 waits for coll ps creation barrier
[HCTR][22:40:02.356][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][22:40:02.356][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.356][ERROR][RK0][tid #140120254560000]: replica 2 reaches 1000, calling init pre replica
[HCTR][22:40:02.356][ERROR][RK0][tid #140120254560000]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.364][ERROR][RK0][main]: coll ps creation done
[HCTR][22:40:02.364][ERROR][RK0][tid #140120254560000]: coll ps creation done
[HCTR][22:40:02.364][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][22:40:02.364][ERROR][RK0][tid #140120254560000]: replica 2 waits for coll ps creation barrier
[HCTR][22:40:02.372][ERROR][RK0][tid #140120120342272]: replica 7 reaches 1000, calling init pre replica
[HCTR][22:40:02.372][ERROR][RK0][tid #140120120342272]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.372][ERROR][RK0][tid #140120657213184]: replica 3 reaches 1000, calling init pre replica
[HCTR][22:40:02.372][ERROR][RK0][tid #140120657213184]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.377][ERROR][RK0][tid #140120657213184]: coll ps creation done
[HCTR][22:40:02.377][ERROR][RK0][tid #140120657213184]: replica 3 waits for coll ps creation barrier
[HCTR][22:40:02.377][ERROR][RK0][tid #140120120342272]: coll ps creation done
[HCTR][22:40:02.377][ERROR][RK0][tid #140120120342272]: replica 7 waits for coll ps creation barrier
[HCTR][22:40:02.545][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][22:40:02.545][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.550][ERROR][RK0][main]: coll ps creation done
[HCTR][22:40:02.550][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][22:40:02.556][ERROR][RK0][tid #140120187451136]: replica 5 reaches 1000, calling init pre replica
[HCTR][22:40:02.556][ERROR][RK0][tid #140120187451136]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][22:40:02.559][ERROR][RK0][tid #140120187451136]: coll ps creation done
[HCTR][22:40:02.560][ERROR][RK0][tid #140120187451136]: replica 5 waits for coll ps creation barrier
[HCTR][22:40:02.560][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][22:40:03.422][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][22:40:03.462][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][tid #140120195843840]: replica 1 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][tid #140120254560000]: replica 2 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][tid #140120657213184]: replica 3 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][tid #140120187451136]: replica 5 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][tid #140120120342272]: replica 7 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][22:40:03.462][ERROR][RK0][main]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][tid #140120195843840]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][tid #140120254560000]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][tid #140120657213184]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][tid #140120187451136]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][main]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][tid #140120120342272]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][main]: Calling build_v2
[HCTR][22:40:03.462][ERROR][RK0][tid #140120195843840]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][tid #140120254560000]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][tid #140120657213184]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][tid #140120187451136]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][tid #140120120342272]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][22:40:03.462][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 22:40:03.466773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:178] v100x8, slow pcie
2022-12-11 22:40:03.466814: [E2022-12-11 22:40:03 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc466854:: [178E]  v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 22:40:03:.196466858[] : 2022-12-11 22:40:03assigning 0 to cpuE.
 [466909/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E178 2022-12-11 22:40:03[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 22:40:03v100x8, slow pcie:[466911.
196: 4669502022-12-11 22:40:03] E[: .[assigning 0 to cpu 2022-12-11 22:40:03E466955
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 22:40:03 : [:467003.2022-12-11 22:40:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE178: 466998.[: [] E: 4670482022-12-11 22:40:03212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie E: .] :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:40:03 E467102build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178:.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : 
] 1964671012022-12-11 22:40:03:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEv100x8, slow pcie] : .178: [
assigning 0 to cpuE467210] 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:40:03
 [: v100x8, slow pcie] :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:40:03E
v100x8, slow pcie212467274:. 
] : [178[467314/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[2022-12-11 22:40:03] 2022-12-11 22:40:03: :
 2022-12-11 22:40:03.v100x8, slow pcie.E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.467369[
467374 ] :467390: 2022-12-11 22:40:03: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu[213: E.E:
2022-12-11 22:40:03] E 467453 196.remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 467490
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E:[assigning 0 to cpu: :196 [2122022-12-11 22:40:03
E196] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 22:40:03] . ] assigning 0 to cpu:.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8467593/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu[
213467614
: :
2022-12-11 22:40:03] : E196[.[remote time is 8.68421E ] 2022-12-11 22:40:034676872022-12-11 22:40:03
 [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu.: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 22:40:03:
467737E467762:2022-12-11 22:40:03.212:  : 214.467776] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 467801: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :[ cpu time is 97.0588: E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-11 22:40:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E :] .[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 84678952022-12-11 22:40:03212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] 
: .] :212remote time is 8.68421E467963[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214] 
 : 2022-12-11 22:40:03
] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E.cpu time is 97.0588
[:2022-12-11 22:40:03[ 468054
2022-12-11 22:40:03212.2022-12-11 22:40:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .] 468097.:E468115build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 468146213 : 
E: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE Eremote time is 8.68421: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 22:40:03/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :[214.:remote time is 8.684212132022-12-11 22:40:03] 468252213
] .cpu time is 97.0588: ] remote time is 8.68421468289[
Eremote time is 8.68421
: 2022-12-11 22:40:03 
E.[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 4683452022-12-11 22:40:032022-12-11 22:40:03:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ..213:E468382468397] 214 : : remote time is 8.68421] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE
cpu time is 97.0588:  
214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :[:cpu time is 97.05882142022-12-11 22:40:03214
] .] cpu time is 97.0588468500cpu time is 97.0588
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 22:41:22.546608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 22:41:22.586765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 22:41:22.714454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 22:41:22.714525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 22:41:22.730357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 22:41:22.730390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 22:41:22.730881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:41:22.730933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.731886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.732683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.745633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 22:41:22.745695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 22:41:22.745870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 22:41:22.745930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 22:41:22.746128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:41:22.746179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.746356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:41:22.746405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.747745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.747847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.749108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.749202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.750950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 22:41:22.751044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 22:41:22.751614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 22:41:22.751692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.752789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.753791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 22:41:222022-12-11 22:41:22..755448755448: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved5 solved

[[2022-12-11 22:41:222022-12-11 22:41:22..755533755534: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 5 initing device 5

[[2022-12-11 22:41:22.2022-12-11 22:41:22755735.: 755735E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202:] 202] 7 solved
4 solved
[2022-12-11 22:41:22[.2022-12-11 22:41:22755811.: 755815E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 205worker 0 thread 7 initing device 7] 
worker 0 thread 4 initing device 4
[[2022-12-11 22:41:222022-12-11 22:41:22..756139756138: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 22:41:222022-12-11 22:41:22..756217756217: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[[2022-12-11 22:41:222022-12-11 22:41:22..756394756394: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[[2022-12-11 22:41:222022-12-11 22:41:22..756469756469: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 22:41:22.759032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.759101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.759199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.759264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.761689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.761760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.761845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.761905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 22:41:22.810236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:41:22.815669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:41:22.815802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:41:22.816633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.817229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.818230: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:22.818279: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.823416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:41:22.824238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:41:22.824284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[2022-12-11 22:41:222022-12-11 22:41:22..828537828537: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 22:41:22.833415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:41:22.834024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:41:22.834127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:41:22.834212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:41:22.834303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:41:22.834987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.835685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.836240: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.836330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.837241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:22.837293: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.837314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:22.837362: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.838528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:41:22.838631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:41:22.839450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.840051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.841064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:22.841117: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.846810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[2022-12-11 22:41:222022-12-11 22:41:22..846881846881: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 22:41:22.846973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:41:22.847414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 22:41:22.847704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:41:22.847751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:41:22.848333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:41:22.848977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:41:22.849021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:41:22.850117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:41:22.850870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:41:22.850915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:41:22.859232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:41:22.859316: [E2022-12-11 22:41:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc859311:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 5
[[2022-12-11 22:41:222022-12-11 22:41:22..859408859397: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-11 22:41:22.859506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:41:22.859537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 22:41:22.859619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 22:41:22.860258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.861190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.861768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.862340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 22:41:22.863416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.863954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.863988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.864040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:22.864397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:22.864442: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.864929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 22:41:222022-12-11 22:41:22..864969864974: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 625663WORKER[0] alloc host memory 11.44 MB

[2022-12-11 22:41:22.865024: [E2022-12-11 22:41:22 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc865037:: 638W]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.865084: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 22:41:22.873847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:41:22.[8744582022-12-11 22:41:22: .E874453 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 22:41:22] :.eager release cuda mem 258551980874480
] : eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 25.25 KB2022-12-11 22:41:22
.874544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:41:22.874721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 22:41:22.875104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:41:22] .eager release cuda mem 25855875119
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:41:22.875165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 22:41:22] .eager alloc mem 1.44 GB875182
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 22:41:22.875329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 22:41:22.875372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[[[[[[[2022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:23........407997407997407998408000407997407997407997407997: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 5 init p2p of link 6Device 0 init p2p of link 3Device 3 init p2p of link 2Device 1 init p2p of link 7Device 6 init p2p of link 0Device 4 init p2p of link 5Device 7 init p2p of link 4







[[[[2022-12-11 22:41:23[2022-12-11 22:41:23[2022-12-11 22:41:232022-12-11 22:41:23.2022-12-11 22:41:23.2022-12-11 22:41:23..408467[.408473.408471408467: 2022-12-11 22:41:23408477: 408478: : E.: E: EE 408497E E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-11 22:41:23E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1980. :1980:19801980] 408545/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] 1980] ] eager alloc mem 611.00 KB: :] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KB
E1980eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

 ] 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.409438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:41:23] .eager release cuda mem 625663409455
: E[ [[[2022-12-11 22:41:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 22:41:232022-12-11 22:41:232022-12-11 22:41:23.:...409478638409479[409487409477: ] : 2022-12-11 22:41:23: : Eeager release cuda mem 625663E.EE 
 409511  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::E::[638638 6386382022-12-11 22:41:23] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] .eager release cuda mem 625663eager release cuda mem 625663:eager release cuda mem 625663eager release cuda mem 625663409571

638

: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.421925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 22:41:23.422090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.422223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 22:41:23.422391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.422912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.423218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.423616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 22:41:23.423782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.423851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 22:41:23.424016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.424092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 22:41:23.424258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.424268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 22:41:23.424423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.424505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 22:41:23.424604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.424682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.424736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 22:41:23.424837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.424913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.425031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.425201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.425493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.425716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.436245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 22:41:23.436379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.436487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 22:41:23.436608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 22:41:232022-12-11 22:41:23..436721436725: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 1 init p2p of link 3Device 0 init p2p of link 1

[2022-12-11 22:41:23.436870[: 2022-12-11 22:41:23E. 436877/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.437178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.437243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 22:41:23.437373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.437422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.437529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 22:41:23.437652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.437684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.437712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.438173: [E2022-12-11 22:41:23 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc438174:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 4 init p2p of link 2
[2022-12-11 22:41:23.438320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.438345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 22:41:23.438444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-11 22:41:23] .eager release cuda mem 625663438460
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.439094: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.439242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.452688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 22:41:23.452819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.452958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 22:41:23.453001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 22:41:23.453071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.453122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.453447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 22:41:23.453565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.453627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.453795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 22:41:23.453889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.453912: E[ 2022-12-11 22:41:23/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:4539241980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.454133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 22:41:23.454249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.454372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.454416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 22:41:23.454538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.454726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.454746: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 22:41:23.454861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 22:41:23.455053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.455308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.455627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 22:41:23.469286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:41:23.469498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:41:23.469565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:41:23.469941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:41:23.469977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.713774 secs 
[2022-12-11 22:41:23.470148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.713693 secs 
[2022-12-11 22:41:23.470277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.723884 secs 
[2022-12-11 22:41:23.470346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 22:41:23.470449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.724284 secs 
[2022-12-11 22:41:23.[4706912022-12-11 22:41:23: .E470693 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1955/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.719016 secs 638
] eager release cuda mem 12399996
[2022-12-11 22:41:23[.2022-12-11 22:41:23471025.: 471038E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1955eager release cuda mem 12399996] 
Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.714586 secs 
[2022-12-11 22:41:23.471350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 22:41:23638.] 471374eager release cuda mem 12399996: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.715174 secs 
[2022-12-11 22:41:23.472006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.741087 secs 
[HCTR][22:41:23.472][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][tid #140120195843840]: replica 1 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][tid #140120187451136]: replica 5 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][tid #140120254560000]: replica 2 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][tid #140120120342272]: replica 7 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][tid #140120657213184]: replica 3 calling init per replica done, doing barrier
[HCTR][22:41:23.472][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120657213184]: replica 3 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120187451136]: replica 5 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120254560000]: replica 2 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120195843840]: replica 1 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120120342272]: replica 7 calling init per replica done, doing barrier done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120657213184]: init per replica done
[HCTR][22:41:23.472][ERROR][RK0][main]: init per replica done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120187451136]: init per replica done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120254560000]: init per replica done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120195843840]: init per replica done
[HCTR][22:41:23.472][ERROR][RK0][main]: init per replica done
[HCTR][22:41:23.472][ERROR][RK0][tid #140120120342272]: init per replica done
[HCTR][22:41:23.474][ERROR][RK0][main]: init per replica done








