2022-12-11 20:51:21.364080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.370016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.376437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.384098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.389307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.393412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.407275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.413583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.469890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.480241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.483900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.484363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.485185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.486141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.486683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.487728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.488245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.489339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.489779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.491031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.491389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.492791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.492889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.494354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.494450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.495739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.495873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.497302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.498313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.499152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.500152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.501211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.502975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.504157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.505070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.505962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.507006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.508069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.509007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.510071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.515523: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.516295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.517884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.519446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.519536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.521125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.521359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.522740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.523010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.524260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.524658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.525426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.526184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.526371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.527377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.528697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.528867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.529833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.531657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.532169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.533621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.534307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.536650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.539346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.540182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.541956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.541953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.543385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.543721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.545157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.545278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.546913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.547393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.548029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.548262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.548672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.550249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.550757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.551300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.551394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.551972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.553527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.554034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.554248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.554536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.556666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.556734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.556841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.557236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.560808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.560925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.561335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.561486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.563254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.563637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.564446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.564448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.566072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.576298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.579896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.599710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.601597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.601674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.603220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.603295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.603503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.603586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.605483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.607171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.607317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.607358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.607565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.611488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.611533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.611618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.611707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.614030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.614071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.614200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.614404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.616799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.616967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.617056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.617187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.619858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.619994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.620127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.620169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.622627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.622712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.622843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.622975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.625339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.625483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.625611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.626063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.627787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.627910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.628329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.628602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.630782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.630824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.631151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.631578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.633286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.633329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.633541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.634382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.635946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.635994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.636264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.636985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.638674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.638714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.639021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.639622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.641136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.641259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.641582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.642426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.643770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.643862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.644082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.645068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.645638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.647613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.647616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.648091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.648509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.650126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.650323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.650398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.650853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.651443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.652044: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.653605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.653737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.653923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.654032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.654309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.654887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.657365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.657522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.657700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.658000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.658210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.659056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.661606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.661871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.662088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.662282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.662493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.662667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.663649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.666569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.666765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.666969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.667200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.667711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.667787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.669033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.671835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.672020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.672183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.672214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.672768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.672799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.673850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.677089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.677212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.677388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.677515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.677639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.678583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.681888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.682106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.682156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.685242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.685296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.685335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.685545: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.685560: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.686279: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.688456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.688618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.688678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.691478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.691639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.691676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.694466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.694652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.695024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.695384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.695477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.696035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.699206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.699366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.699880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.700500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.700539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.701112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.703707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.703760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.704107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.704956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.705002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.705442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.707876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.708412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.710087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.711846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.712352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.745381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.748907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.749174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.751449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.754464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.754559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.757027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.788372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.788458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.792743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.796589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.796717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.798480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.803324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.803486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.805752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.809187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.809422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.811242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.814882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.815003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.819232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.819320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.820404: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.824512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.827592: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.829956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.833269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.835100: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:51:21.836987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.844494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.867960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.922420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.923061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.928314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:21.928598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.791613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.792441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.792976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.793449: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:22.793503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:51:22.811017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.814722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.815254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.815825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.816340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:22.816802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:51:22.863808: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:22.864035: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:22.899771: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:51:23.034998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.035744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.036286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.037040: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.037097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:51:23.055244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.056027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.056539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.057121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.057819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.058299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:51:23.073866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.073866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.074984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.075001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.076019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.076041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.077192: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.077230: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.077259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:51:23.077281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:51:23.090939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.091577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.092111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.092580: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.092629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:51:23.095599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.095599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.096792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.096818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.097806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.097839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.098945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.099040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.100178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.100216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.101124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:51:23.101160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:51:23.111353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.112590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.113111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.113677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.114198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.114662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:51:23.117755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.118390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.118574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.119425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.119602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.120550: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.120655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:51:23.120854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.121538: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.121584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:51:23.126681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.127319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.127840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.128316: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:51:23.128364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:51:23.138322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.138954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.139483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.139664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.140457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.140699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.141449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.141688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.142467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:51:23.142848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.143444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.143949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:51:23.145781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.145994: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.146313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.146925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.146956: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:51:23.147244: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.147392: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.147451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.148025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.148413: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:51:23.148544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:51:23.149015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:51:23.160782: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.160989: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.162050: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:51:23.188507: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.188729: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.189134: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.189343: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.189803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:51:23.190351: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 20:51:23.191479: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.191628: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.193285: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:51:23.196128: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.196270: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:51:23.198055: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][20:51:24.470][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.470][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:51:24.471][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 94it [00:01, 80.23it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 96it [00:01, 82.42it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 99it [00:01, 85.83it/s]warmup run: 1it [00:01,  1.46s/it]warmup run: 92it [00:01, 79.17it/s]warmup run: 189it [00:01, 174.81it/s]warmup run: 94it [00:01, 80.50it/s]warmup run: 192it [00:01, 178.46it/s]warmup run: 99it [00:01, 86.41it/s]warmup run: 95it [00:01, 80.06it/s]warmup run: 199it [00:01, 186.84it/s]warmup run: 96it [00:01, 85.19it/s]warmup run: 185it [00:01, 172.53it/s]warmup run: 286it [00:01, 281.51it/s]warmup run: 190it [00:01, 176.62it/s]warmup run: 289it [00:01, 285.26it/s]warmup run: 198it [00:01, 186.84it/s]warmup run: 191it [00:01, 174.88it/s]warmup run: 298it [00:01, 296.07it/s]warmup run: 193it [00:01, 184.86it/s]warmup run: 278it [00:01, 274.85it/s]warmup run: 383it [00:01, 392.11it/s]warmup run: 287it [00:01, 283.41it/s]warmup run: 298it [00:01, 298.16it/s]warmup run: 386it [00:01, 395.48it/s]warmup run: 287it [00:01, 279.59it/s]warmup run: 397it [00:01, 409.05it/s]warmup run: 290it [00:01, 293.54it/s]warmup run: 370it [00:01, 379.31it/s]warmup run: 482it [00:02, 503.12it/s]warmup run: 385it [00:01, 395.81it/s]warmup run: 484it [00:02, 505.02it/s]warmup run: 399it [00:01, 414.13it/s]warmup run: 380it [00:01, 383.22it/s]warmup run: 497it [00:02, 520.29it/s]warmup run: 388it [00:01, 406.63it/s]warmup run: 463it [00:02, 482.46it/s]warmup run: 580it [00:02, 603.15it/s]warmup run: 482it [00:02, 503.24it/s]warmup run: 583it [00:02, 607.37it/s]warmup run: 500it [00:01, 526.77it/s]warmup run: 476it [00:02, 490.06it/s]warmup run: 597it [00:02, 621.80it/s]warmup run: 486it [00:01, 515.94it/s]warmup run: 556it [00:02, 576.85it/s]warmup run: 677it [00:02, 687.72it/s]warmup run: 582it [00:02, 607.03it/s]warmup run: 682it [00:02, 695.58it/s]warmup run: 601it [00:02, 629.17it/s]warmup run: 577it [00:02, 598.80it/s]warmup run: 698it [00:02, 710.83it/s]warmup run: 584it [00:02, 615.02it/s]warmup run: 650it [00:02, 660.97it/s]warmup run: 772it [00:02, 750.99it/s]warmup run: 681it [00:02, 695.43it/s]warmup run: 781it [00:02, 768.66it/s]warmup run: 702it [00:02, 717.17it/s]warmup run: 678it [00:02, 692.38it/s]warmup run: 799it [00:02, 785.16it/s]warmup run: 683it [00:02, 702.08it/s]warmup run: 744it [00:02, 729.49it/s]warmup run: 867it [00:02, 800.85it/s]warmup run: 780it [00:02, 767.05it/s]warmup run: 880it [00:02, 825.30it/s]warmup run: 803it [00:02, 790.09it/s]warmup run: 779it [00:02, 769.90it/s]warmup run: 901it [00:02, 844.92it/s]warmup run: 783it [00:02, 775.51it/s]warmup run: 838it [00:02, 783.00it/s]warmup run: 963it [00:02, 842.66it/s]warmup run: 879it [00:02, 825.13it/s]warmup run: 903it [00:02, 844.32it/s]warmup run: 979it [00:02, 868.66it/s]warmup run: 879it [00:02, 828.78it/s]warmup run: 1002it [00:02, 888.50it/s]warmup run: 883it [00:02, 834.06it/s]warmup run: 932it [00:02, 823.23it/s]warmup run: 1058it [00:02, 871.73it/s]warmup run: 977it [00:02, 866.40it/s]warmup run: 1004it [00:02, 887.53it/s]warmup run: 1080it [00:02, 905.95it/s]warmup run: 979it [00:02, 874.24it/s]warmup run: 1103it [00:02, 921.41it/s]warmup run: 984it [00:02, 881.21it/s]warmup run: 1025it [00:02, 849.40it/s]warmup run: 1155it [00:02, 896.78it/s]warmup run: 1077it [00:02, 901.85it/s]warmup run: 1105it [00:02, 920.83it/s]warmup run: 1180it [00:02, 931.39it/s]warmup run: 1080it [00:02, 910.02it/s]warmup run: 1205it [00:02, 948.72it/s]warmup run: 1084it [00:02, 912.94it/s]warmup run: 1119it [00:02, 872.77it/s]warmup run: 1250it [00:02, 911.04it/s]warmup run: 1176it [00:02, 925.19it/s]warmup run: 1206it [00:02, 944.91it/s]warmup run: 1280it [00:02, 949.06it/s]warmup run: 1180it [00:02, 935.54it/s]warmup run: 1306it [00:02, 964.98it/s]warmup run: 1183it [00:02, 933.64it/s]warmup run: 1212it [00:02, 887.45it/s]warmup run: 1346it [00:02, 923.92it/s]warmup run: 1274it [00:02, 932.29it/s]warmup run: 1306it [00:02, 959.46it/s]warmup run: 1380it [00:02, 963.53it/s]warmup run: 1281it [00:02, 955.34it/s]warmup run: 1408it [00:02, 980.46it/s]warmup run: 1282it [00:02, 949.03it/s]warmup run: 1305it [00:02, 896.86it/s]warmup run: 1372it [00:02, 940.97it/s]warmup run: 1442it [00:03, 905.58it/s]warmup run: 1481it [00:03, 977.08it/s]warmup run: 1408it [00:02, 975.54it/s]warmup run: 1384it [00:02, 976.66it/s]warmup run: 1510it [00:03, 990.65it/s]warmup run: 1382it [00:02, 963.16it/s]warmup run: 1399it [00:03, 906.83it/s]warmup run: 1469it [00:03, 943.03it/s]warmup run: 1510it [00:02, 987.23it/s]warmup run: 1486it [00:03, 989.32it/s]warmup run: 1581it [00:03, 979.51it/s]warmup run: 1535it [00:03, 865.31it/s]warmup run: 1612it [00:03, 999.25it/s]warmup run: 1482it [00:02, 971.37it/s]warmup run: 1492it [00:03, 913.00it/s]warmup run: 1566it [00:03, 943.76it/s]warmup run: 1682it [00:03, 988.47it/s]warmup run: 1612it [00:03, 994.74it/s]warmup run: 1589it [00:03, 999.68it/s]warmup run: 1635it [00:03, 901.91it/s]warmup run: 1714it [00:03, 1004.67it/s]warmup run: 1582it [00:03, 978.65it/s]warmup run: 1585it [00:03, 915.70it/s]warmup run: 1662it [00:03, 939.78it/s]warmup run: 1783it [00:03, 994.79it/s]warmup run: 1715it [00:03, 1003.72it/s]warmup run: 1691it [00:03, 1000.18it/s]warmup run: 1737it [00:03, 934.14it/s]warmup run: 1817it [00:03, 1010.01it/s]warmup run: 1682it [00:03, 983.25it/s]warmup run: 1678it [00:03, 918.96it/s]warmup run: 1757it [00:03, 942.42it/s]warmup run: 1884it [00:03, 997.95it/s]warmup run: 1818it [00:03, 1008.87it/s]warmup run: 1793it [00:03, 1003.21it/s]warmup run: 1838it [00:03, 955.07it/s]warmup run: 1919it [00:03, 1012.54it/s]warmup run: 1783it [00:03, 988.77it/s]warmup run: 1773it [00:03, 928.13it/s]warmup run: 1853it [00:03, 944.99it/s]warmup run: 1985it [00:03, 1000.75it/s]warmup run: 1922it [00:03, 1015.72it/s]warmup run: 1896it [00:03, 1009.01it/s]warmup run: 1940it [00:03, 971.46it/s]warmup run: 2025it [00:03, 1025.76it/s]warmup run: 1883it [00:03, 991.97it/s]warmup run: 1871it [00:03, 940.89it/s]warmup run: 1948it [00:03, 943.99it/s]warmup run: 2100it [00:03, 1042.80it/s]warmup run: 2029it [00:03, 1031.80it/s]warmup run: 1999it [00:03, 1014.64it/s]warmup run: 2047it [00:03, 999.03it/s]warmup run: 2146it [00:03, 1078.65it/s]warmup run: 1983it [00:03, 994.19it/s]warmup run: 1968it [00:03, 946.60it/s]warmup run: 2050it [00:03, 966.11it/s]warmup run: 2218it [00:03, 1082.98it/s]warmup run: 2152it [00:03, 1089.39it/s]warmup run: 2118it [00:03, 1065.02it/s]warmup run: 2167it [00:03, 1057.85it/s]warmup run: 2265it [00:03, 1110.87it/s]warmup run: 2098it [00:03, 1038.66it/s]warmup run: 2082it [00:03, 1001.85it/s]warmup run: 2165it [00:03, 1019.43it/s]warmup run: 2336it [00:03, 1110.64it/s]warmup run: 2275it [00:03, 1130.66it/s]warmup run: 2238it [00:03, 1103.98it/s]warmup run: 2288it [00:03, 1100.71it/s]warmup run: 2382it [00:03, 1127.71it/s]warmup run: 2217it [00:03, 1081.51it/s]warmup run: 2205it [00:03, 1068.80it/s]warmup run: 2280it [00:03, 1057.54it/s]warmup run: 2454it [00:03, 1129.95it/s]warmup run: 2398it [00:03, 1159.16it/s]warmup run: 2358it [00:03, 1130.63it/s]warmup run: 2409it [00:03, 1132.71it/s]warmup run: 2500it [00:03, 1142.10it/s]warmup run: 2336it [00:03, 1111.75it/s]warmup run: 2328it [00:03, 1115.45it/s]warmup run: 2395it [00:03, 1083.43it/s]warmup run: 2572it [00:04, 1144.20it/s]warmup run: 2520it [00:03, 1177.20it/s]warmup run: 2478it [00:03, 1150.21it/s]warmup run: 2530it [00:04, 1154.74it/s]warmup run: 2618it [00:04, 1150.60it/s]warmup run: 2454it [00:03, 1131.78it/s]warmup run: 2451it [00:04, 1149.29it/s]warmup run: 2510it [00:04, 1102.66it/s]warmup run: 2690it [00:04, 1153.60it/s]warmup run: 2644it [00:03, 1193.30it/s]warmup run: 2598it [00:04, 1163.68it/s]warmup run: 2651it [00:04, 1169.82it/s]warmup run: 2735it [00:04, 1156.11it/s]warmup run: 2573it [00:03, 1147.04it/s]warmup run: 2574it [00:04, 1173.25it/s]warmup run: 2627it [00:04, 1122.45it/s]warmup run: 2766it [00:04, 1200.25it/s]warmup run: 2719it [00:04, 1174.68it/s]warmup run: 2771it [00:04, 1176.40it/s]warmup run: 2806it [00:04, 1088.38it/s]warmup run: 2853it [00:04, 1161.01it/s]warmup run: 2695it [00:04, 1166.53it/s]warmup run: 2698it [00:04, 1190.91it/s]warmup run: 2748it [00:04, 1148.41it/s]warmup run: 2889it [00:04, 1208.57it/s]warmup run: 2838it [00:04, 1177.48it/s]warmup run: 2892it [00:04, 1184.07it/s]warmup run: 2972it [00:04, 1168.51it/s]warmup run: 2916it [00:04, 1028.28it/s]warmup run: 2815it [00:04, 1174.33it/s]warmup run: 3000it [00:04, 691.17it/s] warmup run: 2820it [00:04, 1197.59it/s]warmup run: 2868it [00:04, 1163.01it/s]warmup run: 3000it [00:04, 698.08it/s] warmup run: 2957it [00:04, 1181.18it/s]warmup run: 3000it [00:04, 671.49it/s] warmup run: 3000it [00:04, 674.21it/s] warmup run: 3000it [00:04, 681.47it/s] warmup run: 2934it [00:04, 1178.29it/s]warmup run: 2943it [00:04, 1204.79it/s]warmup run: 2990it [00:04, 1179.02it/s]warmup run: 3000it [00:04, 673.05it/s] warmup run: 3000it [00:04, 691.97it/s] warmup run: 3000it [00:04, 669.31it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.21it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1599.72it/s]warmup should be done:   5%|▌         | 152/3000 [00:00<00:01, 1519.15it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1616.68it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.03it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1606.72it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.99it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.50it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1647.24it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1638.95it/s]warmup should be done:  10%|█         | 306/3000 [00:00<00:01, 1527.78it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1646.32it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1616.88it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1669.71it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1669.49it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1625.78it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1636.59it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1653.19it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1644.55it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1666.50it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1614.19it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1665.76it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1637.38it/s]warmup should be done:  15%|█▌        | 459/3000 [00:00<00:01, 1522.78it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1641.05it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1653.05it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1642.30it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1644.08it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1665.16it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1665.15it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1611.74it/s]warmup should be done:  20%|██        | 612/3000 [00:00<00:01, 1517.97it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1640.63it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1644.02it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1651.63it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1643.43it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1663.72it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1662.82it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1615.51it/s]warmup should be done:  26%|██▌       | 765/3000 [00:00<00:01, 1521.35it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1638.41it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1642.31it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1640.05it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1615.53it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1649.20it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1660.54it/s]warmup should be done:  31%|███       | 918/3000 [00:00<00:01, 1518.50it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1648.58it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1634.98it/s]warmup should be done:  38%|███▊      | 1135/3000 [00:00<00:01, 1613.23it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1645.80it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1639.50it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1638.00it/s]warmup should be done:  39%|███▉      | 1170/3000 [00:00<00:01, 1657.10it/s]warmup should be done:  36%|███▌      | 1070/3000 [00:00<00:01, 1513.80it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1648.17it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1635.95it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1647.04it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1639.12it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1640.59it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1613.53it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1656.26it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1651.52it/s]warmup should be done:  41%|████      | 1222/3000 [00:00<00:01, 1512.40it/s]warmup should be done:  49%|████▉     | 1479/3000 [00:00<00:00, 1637.44it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1643.21it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1648.22it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1637.76it/s]warmup should be done:  49%|████▊     | 1459/3000 [00:00<00:00, 1612.73it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1654.82it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1654.13it/s]warmup should be done:  46%|████▌     | 1374/3000 [00:00<00:01, 1510.66it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1646.54it/s]warmup should be done:  55%|█████▍    | 1644/3000 [00:01<00:00, 1639.03it/s]warmup should be done:  55%|█████▍    | 1647/3000 [00:01<00:00, 1636.50it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1648.92it/s]warmup should be done:  54%|█████▍    | 1621/3000 [00:01<00:00, 1612.26it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1654.51it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1656.34it/s]warmup should be done:  51%|█████     | 1526/3000 [00:01<00:00, 1508.30it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1639.16it/s]warmup should be done:  60%|██████    | 1811/3000 [00:01<00:00, 1636.63it/s]warmup should be done:  59%|█████▉    | 1783/3000 [00:01<00:00, 1611.67it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1639.77it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1654.59it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1641.20it/s]warmup should be done:  61%|██████    | 1833/3000 [00:01<00:00, 1656.41it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1508.14it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1637.23it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1639.58it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1612.57it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1654.48it/s]warmup should be done:  67%|██████▋   | 1999/3000 [00:01<00:00, 1657.12it/s]warmup should be done:  66%|██████▌   | 1977/3000 [00:01<00:00, 1636.85it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1636.47it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1507.17it/s]warmup should be done:  71%|███████   | 2137/3000 [00:01<00:00, 1639.41it/s]warmup should be done:  71%|███████▏  | 2139/3000 [00:01<00:00, 1636.42it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1613.37it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1657.19it/s]warmup should be done:  72%|███████▏  | 2166/3000 [00:01<00:00, 1653.53it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1634.44it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1507.02it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1632.61it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1635.63it/s]warmup should be done:  77%|███████▋  | 2302/3000 [00:01<00:00, 1640.04it/s]warmup should be done:  76%|███████▌  | 2269/3000 [00:01<00:00, 1614.31it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1657.48it/s]warmup should be done:  78%|███████▊  | 2332/3000 [00:01<00:00, 1652.93it/s]warmup should be done:  77%|███████▋  | 2305/3000 [00:01<00:00, 1634.01it/s]warmup should be done:  71%|███████   | 2130/3000 [00:01<00:00, 1506.57it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1631.24it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1632.18it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1637.62it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1655.80it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1611.03it/s]warmup should be done:  83%|████████▎ | 2498/3000 [00:01<00:00, 1649.93it/s]warmup should be done:  82%|████████▏ | 2469/3000 [00:01<00:00, 1631.39it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1505.43it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1626.85it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1633.72it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1637.26it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1656.78it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1650.98it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1606.42it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1630.51it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1502.37it/s]warmup should be done:  88%|████████▊ | 2641/3000 [00:01<00:00, 1626.97it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1638.05it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1659.30it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1651.82it/s]warmup should be done:  92%|█████████▏| 2754/3000 [00:01<00:00, 1602.67it/s]warmup should be done:  93%|█████████▎| 2795/3000 [00:01<00:00, 1618.20it/s]warmup should be done:  93%|█████████▎| 2797/3000 [00:01<00:00, 1627.62it/s]warmup should be done:  86%|████████▌ | 2583/3000 [00:01<00:00, 1502.97it/s]warmup should be done:  93%|█████████▎| 2804/3000 [00:01<00:00, 1627.65it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1642.89it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1663.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1658.04it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1655.32it/s]warmup should be done:  99%|█████████▊| 2960/3000 [00:01<00:00, 1627.00it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1639.90it/s]warmup should be done:  97%|█████████▋| 2917/3000 [00:01<00:00, 1608.43it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1503.69it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1633.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.04it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.45it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1635.22it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1610.81it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1509.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1510.95it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1698.13it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.17it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1698.60it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.00it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.15it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.25it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1551.99it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1694.21it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1698.95it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.97it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1688.29it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1697.15it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1701.17it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.29it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.16it/s]warmup should be done:  10%|█         | 312/3000 [00:00<00:01, 1548.77it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1701.31it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1660.59it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1692.49it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1704.02it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1700.12it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1648.55it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1678.98it/s]warmup should be done:  16%|█▌        | 469/3000 [00:00<00:01, 1554.87it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1704.50it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1708.11it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1652.74it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1692.02it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1672.45it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1704.64it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1680.22it/s]warmup should be done:  21%|██        | 627/3000 [00:00<00:01, 1563.85it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1705.59it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1684.96it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1656.37it/s]warmup should be done:  28%|██▊       | 848/3000 [00:00<00:01, 1692.11it/s]warmup should be done:  26%|██▌       | 786/3000 [00:00<00:01, 1572.97it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1682.38it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1700.39it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1692.84it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1704.42it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1689.39it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1702.54it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1654.56it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1683.82it/s]warmup should be done:  32%|███▏      | 946/3000 [00:00<00:01, 1579.78it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1702.64it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1686.00it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1691.01it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1701.14it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1707.06it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1685.37it/s]warmup should be done:  37%|███▋      | 1106/3000 [00:00<00:01, 1586.23it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1653.75it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1678.04it/s]warmup should be done:  40%|███▉      | 1196/3000 [00:00<00:01, 1676.48it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1703.72it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1691.64it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1711.02it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1687.47it/s]warmup should be done:  42%|████▏     | 1265/3000 [00:00<00:01, 1583.22it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1650.57it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1658.53it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:01, 1632.52it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1704.13it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1710.62it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1688.67it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1652.12it/s]warmup should be done:  47%|████▋     | 1424/3000 [00:00<00:00, 1582.67it/s]warmup should be done:  51%|█████     | 1520/3000 [00:00<00:00, 1682.35it/s]warmup should be done:  51%|█████     | 1530/3000 [00:00<00:00, 1645.46it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1643.49it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1703.80it/s]warmup should be done:  57%|█████▋    | 1709/3000 [00:01<00:00, 1710.23it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1689.59it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1662.24it/s]warmup should be done:  53%|█████▎    | 1583/3000 [00:01<00:00, 1581.97it/s]warmup should be done:  56%|█████▋    | 1689/3000 [00:01<00:00, 1676.49it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1637.26it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1648.91it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1705.05it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1710.63it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1690.74it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1672.46it/s]warmup should be done:  58%|█████▊    | 1742/3000 [00:01<00:00, 1578.04it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1673.75it/s]warmup should be done:  62%|██████▏   | 1864/3000 [00:01<00:00, 1657.95it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1620.43it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1703.99it/s]warmup should be done:  68%|██████▊   | 2053/3000 [00:01<00:00, 1712.47it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1676.82it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1690.35it/s]warmup should be done:  63%|██████▎   | 1900/3000 [00:01<00:00, 1576.58it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1670.65it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1663.59it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1619.10it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1703.01it/s]warmup should be done:  72%|███████▏  | 2170/3000 [00:01<00:00, 1680.51it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1687.77it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1704.98it/s]warmup should be done:  69%|██████▊   | 2058/3000 [00:01<00:00, 1577.03it/s]warmup should be done:  73%|███████▎  | 2193/3000 [00:01<00:00, 1666.83it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1666.19it/s]warmup should be done:  73%|███████▎  | 2184/3000 [00:01<00:00, 1616.39it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1702.24it/s]warmup should be done:  78%|███████▊  | 2340/3000 [00:01<00:00, 1685.45it/s]warmup should be done:  79%|███████▉  | 2372/3000 [00:01<00:00, 1688.92it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1574.27it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1694.63it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1666.26it/s]warmup should be done:  79%|███████▉  | 2368/3000 [00:01<00:00, 1668.76it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1617.23it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1689.35it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1700.68it/s]warmup should be done:  85%|████████▍ | 2542/3000 [00:01<00:00, 1689.92it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1572.49it/s]warmup should be done:  86%|████████▌ | 2567/3000 [00:01<00:00, 1696.92it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1666.83it/s]warmup should be done:  85%|████████▍ | 2536/3000 [00:01<00:00, 1670.49it/s]warmup should be done:  84%|████████▎ | 2509/3000 [00:01<00:00, 1621.00it/s]warmup should be done:  89%|████████▉ | 2680/3000 [00:01<00:00, 1691.03it/s]warmup should be done:  91%|█████████ | 2734/3000 [00:01<00:00, 1696.58it/s]warmup should be done:  90%|█████████ | 2712/3000 [00:01<00:00, 1691.57it/s]warmup should be done:  84%|████████▍ | 2533/3000 [00:01<00:00, 1575.00it/s]warmup should be done:  90%|████████▉ | 2694/3000 [00:01<00:00, 1660.58it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1684.79it/s]warmup should be done:  90%|█████████ | 2704/3000 [00:01<00:00, 1664.28it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1624.31it/s]warmup should be done:  95%|█████████▌| 2850/3000 [00:01<00:00, 1687.20it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1693.03it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1690.34it/s]warmup should be done:  90%|████████▉ | 2691/3000 [00:01<00:00, 1575.59it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1689.49it/s]warmup should be done:  95%|█████████▌| 2861/3000 [00:01<00:00, 1652.94it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1658.12it/s]warmup should be done:  95%|█████████▍| 2838/3000 [00:01<00:00, 1630.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.27it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1698.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1687.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.99it/s]warmup should be done:  95%|█████████▍| 2849/3000 [00:01<00:00, 1572.44it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.89it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1574.26it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39aaedd0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39aaeda190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39aaee82b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39ab24ce80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39aaeda0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39ab24fd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39ab24d730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f39aaedb1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 20:52:53.871719: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34c702d3a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:53.871780: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:53.872567: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34cb02c790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:53.872624: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:53.880635: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:53.882034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:53.887389: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34c682b690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:53.887432: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:53.895447: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:53.970179: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34be833d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:53.970240: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:53.971321: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34c67993e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:53.971363: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:53.977960: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:53.980123: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:54.427946: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34c2833a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:54.428006: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:54.430936: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34c7029840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:54.430980: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:54.436836: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:54.439834: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:52:54.451913: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f34be8339f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:52:54.451980: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:52:54.462191: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:53:00.834021: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:00.917808: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:00.932269: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:01.114234: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:01.128563: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:01.184579: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:01.237944: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:53:01.279916: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:54:06.055][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:54:06.055][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.061][ERROR][RK0][main]: coll ps creation done
[HCTR][20:54:06.061][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][20:54:06.070][ERROR][RK0][tid #139865106659072]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:54:06.070][ERROR][RK0][tid #139865106659072]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.075][ERROR][RK0][tid #139865106659072]: coll ps creation done
[HCTR][20:54:06.075][ERROR][RK0][tid #139865106659072]: replica 2 waits for coll ps creation barrier
[HCTR][20:54:06.101][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:54:06.101][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.110][ERROR][RK0][main]: coll ps creation done
[HCTR][20:54:06.110][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][20:54:06.145][ERROR][RK0][tid #139865249269504]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:54:06.146][ERROR][RK0][tid #139865249269504]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.146][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:54:06.146][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.151][ERROR][RK0][tid #139865249269504]: coll ps creation done
[HCTR][20:54:06.151][ERROR][RK0][tid #139865249269504]: replica 3 waits for coll ps creation barrier
[HCTR][20:54:06.153][ERROR][RK0][main]: coll ps creation done
[HCTR][20:54:06.153][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][20:54:06.162][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:54:06.162][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.169][ERROR][RK0][main]: coll ps creation done
[HCTR][20:54:06.169][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][20:54:06.236][ERROR][RK0][tid #139865165375232]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:54:06.236][ERROR][RK0][tid #139865165375232]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.243][ERROR][RK0][tid #139865165375232]: coll ps creation done
[HCTR][20:54:06.243][ERROR][RK0][tid #139865165375232]: replica 7 waits for coll ps creation barrier
[HCTR][20:54:06.369][ERROR][RK0][tid #139865173767936]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:54:06.370][ERROR][RK0][tid #139865173767936]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][20:54:06.377][ERROR][RK0][tid #139865173767936]: coll ps creation done
[HCTR][20:54:06.377][ERROR][RK0][tid #139865173767936]: replica 4 waits for coll ps creation barrier
[HCTR][20:54:06.377][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][20:54:07.243][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][20:54:07.278][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][tid #139865173767936]: replica 4 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][tid #139865249269504]: replica 3 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][tid #139865106659072]: replica 2 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][tid #139865165375232]: replica 7 calling init per replica
[HCTR][20:54:07.278][ERROR][RK0][main]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][main]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][main]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][tid #139865173767936]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][main]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][tid #139865249269504]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][tid #139865106659072]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][tid #139865165375232]: Calling build_v2
[HCTR][20:54:07.278][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][tid #139865173767936]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][tid #139865249269504]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][tid #139865106659072]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:54:07.278][ERROR][RK0][tid #139865165375232]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 20:54:072022-12-11 20:54:072022-12-11 20:54:072022-12-11 20:54:072022-12-11 20:54:07.2022-12-11 20:54:072022-12-11 20:54:07.2022-12-11 20:54:07...278683..278683.278698278683278699: 278694278687: 278698: : : E: : E: EEE EE E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136::136:136136136] 136136] 136] ] ] using concurrent impl MPS] ] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS





[2022-12-11 20:54:07.283054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:54:07.283092: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:54:07:.196283098] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:54:07.283156[: 2022-12-11 20:54:07E[. 2022-12-11 20:54:07283157/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: :283168E196:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178[:] 2022-12-11 20:54:07212v100x8, slow pcie.] 
283208build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [[[
E2022-12-11 20:54:072022-12-11 20:54:072022-12-11 20:54:07 .../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[283255283257283254:2022-12-11 20:54:072022-12-11 20:54:07: : : 178..EEE] [283299283304   v100x8, slow pcie2022-12-11 20:54:07: : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.E[E:::283347[ 2022-12-11 20:54:07 196212178: 2022-12-11 20:54:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] ] E.:283395:assigning 8 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8v100x8, slow pcie 283423178: 213


/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] E] :E[v100x8, slow pcie[[ remote time is 8.68421178 2022-12-11 20:54:07
2022-12-11 20:54:072022-12-11 20:54:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc...[:v100x8, slow pcie:[2835472835502835702022-12-11 20:54:07178
1962022-12-11 20:54:07[: : : .] ] .2022-12-11 20:54:07EEE283599v100x8, slow pcieassigning 8 to cpu283614.   : 

: 283667/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE[: :::[  2022-12-11 20:54:07E1962132122022-12-11 20:54:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. ] ] ] .::283742/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuremote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8283758196214: :


: ] ] [E196E[assigning 8 to cpucpu time is 97.05882022-12-11 20:54:07 ]  2022-12-11 20:54:07[

./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.2022-12-11 20:54:07283913:
:283906.: 196[212: 283937E] 2022-12-11 20:54:07] [E:  assigning 8 to cpu.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:54:07 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
284006
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :: 284033:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213[E: 214[:] 2022-12-11 20:54:07 E] 2022-12-11 20:54:07212remote time is 8.68421./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc cpu time is 97.0588.] 
284115:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
284128build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 212[:: 
E] 2022-12-11 20:54:07212E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
284210build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:54:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:: 
.:2022-12-11 20:54:07213E284250212.] [ : ] 284285remote time is 8.684212022-12-11 20:54:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 
.: 
E284308214[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : ] [2022-12-11 20:54:07:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.05882022-12-11 20:54:07.213: 
.284361] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc284377: remote time is 8.68421] :: E
remote time is 8.68421213E 
[]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:54:07[remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.2022-12-11 20:54:07
:214284483.213] : [284497] cpu time is 97.0588E2022-12-11 20:54:07: remote time is 8.68421
 .E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc284535 :: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E2022-12-11 20:54:07:]  .214cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc284587] 
:: cpu time is 97.0588214E
]  cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] cpu time is 97.0588
[2022-12-11 20:55:27.   618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:55:27. 40394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 20:55:27.164059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:55:27.164122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:55:27.164157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:55:27.164188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:55:27.164708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:55:27.164761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.165759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.166654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.179688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 20:55:27.179764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-11 20:55:272022-12-11 20:55:27..179880179896: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved5 solved

[[2022-12-11 20:55:27[2022-12-11 20:55:27.[2022-12-11 20:55:27.1799752022-12-11 20:55:27.179977: .179965: E179975: E : E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] :202] worker 0 thread 7 initing device 7202] worker 0 thread 5 initing device 5
] 
3 solved2 solved

[[2022-12-11 20:55:272022-12-11 20:55:27..180104180106: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 3 initing device 3worker 0 thread 2 initing device 2

[2022-12-11 20:55:27.180202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:55:27.180256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 20:55:272022-12-11 20:55:27..180491180492: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-11 20:55:27:1815.1815] 180511] Building Coll Cache with ... num gpu device is 8: Building Coll Cache with ... num gpu device is 8
E[[
[ 2022-12-11 20:55:27[2022-12-11 20:55:272022-12-11 20:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.2022-12-11 20:55:27..[:180552.1805691805702022-12-11 20:55:27202: 180595: : .] E: EE1806186 solved E  : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:: 202:2022-12-11 20:55:2718151815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980.] ] :1 solved] 180712Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 81980
eager alloc mem 381.47 MB: 

] 
E[eager alloc mem 381.47 MB 2022-12-11 20:55:27
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[[.:2022-12-11 20:55:272022-12-11 20:55:27180797205..: ] 180815180817Eworker 0 thread 6 initing device 6: :  
EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu205::] 19801980worker 0 thread 1 initing device 1] ] 
eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 20:55:27.181280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:55:27.181331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.181364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:55:27.181419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.184623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.184892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.185035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.185126: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.185637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.185687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.185756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.189196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.189445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.189537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.189587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.189646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.190127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.190199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:55:27.245270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:55:27.250362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:55:27.250444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:55:27.251244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.251788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.252752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.253847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.254561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.254605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:55:27.276005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 20:55:27.281256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:55:27.281345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[[[[[[2022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:27..283248283248....: : 283248283248283248283248EE: : : :   EEEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu    ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980::::] ] 1980198019801980eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes] ] ] ] 

eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[2022-12-11 20:55:27.283513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.284158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.285120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.286152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.286858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.286901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:55:27.290391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 20:55:272022-12-11 20:55:27..290467290487: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 20:55:27.290547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 20:55:27638.] 290578eager release cuda mem 5: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:55:27.290637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:55:27.290730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:55:27.290808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 20:55:27eager release cuda mem 400000000.
290811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 20:55:27[.2022-12-11 20:55:27290892.: 290913E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5] 
eager release cuda mem 400000000
[2022-12-11 20:55:27.290994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:55:27.291567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.299243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.300192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.310436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.311110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.311655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:55:27.312195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.312443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.312659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.312888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.312966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 20:55:27
.312992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.313105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.313334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.313561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.313847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.313910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.313944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.314054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.314641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.314684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:55:27.314910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.315621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.315665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:55:27.316876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.317044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.317426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:55:27.317467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.317519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:55:27.317636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.317680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[2022-12-11 20:55:27.318139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-11 20:55:272022-12-11 20:55:27..318165318181: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 493.16 MB

[2022-12-11 20:55:27.318899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:55:27.318943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[[[[2022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:272022-12-11 20:55:27........492897492897492897492897492897492897492900492900: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] Device 4 init p2p of link 5] ] ] ] Device 6 init p2p of link 0Device 7 init p2p of link 4Device 2 init p2p of link 1
Device 5 init p2p of link 6Device 3 init p2p of link 2Device 1 init p2p of link 7Device 0 init p2p of link 3






[2022-12-11 20:55:27.493335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[[1980[2022-12-11 20:55:27[2022-12-11 20:55:27] 2022-12-11 20:55:27[[.2022-12-11 20:55:27.eager alloc mem 611.00 KB.2022-12-11 20:55:272022-12-11 20:55:27493352.[493353
493356..: 4933612022-12-11 20:55:27: : 493361493363E: .EE: :  E493386  EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980: 19801980::] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] 19801980eager alloc mem 611.00 KB] :eager alloc mem 611.00 KBeager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KB1980

eager alloc mem 611.00 KBeager alloc mem 611.00 KB
] 

eager alloc mem 611.00 KB
[2022-12-11 20:55:27.494208: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 20:55:27[2022-12-11 20:55:27.[2022-12-11 20:55:27[.4943952022-12-11 20:55:27.2022-12-11 20:55:27494396: .494399.: E494408[: 494404[E : 2022-12-11 20:55:27E: 2022-12-11 20:55:27 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE. E./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 494440/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 494449:638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638] :E638:E] eager release cuda mem 625663638 ] 638 eager release cuda mem 625663
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
eager release cuda mem 625663:
eager release cuda mem 625663:
638
638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 20:55:27.507136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:55:27.507277: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.507348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 20:55:27.507486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.507757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:55:27.507897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.508000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 20:55:27.508073: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:55:27:.1926508101] : Device 1 init p2p of link 2E[
 2022-12-11 20:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:508134638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 20:55:272022-12-11 20:55:27..508245508237: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-11 20:55:27:1926.1980] 508303] Device 3 init p2p of link 0: eager alloc mem 611.00 KB
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.508458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.508692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.508705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 20:55:27.508848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.508893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 20:55:27.508939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.509038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.509134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.509256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.509652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.509843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.521064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:55:27.521178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.521517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:55:27.521550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:55:27.521634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.521662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.521765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 20:55:27.521889: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.521980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.522145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:55:27.522264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.522318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 20:55:27[.2022-12-11 20:55:27522432.: 522436E: [ E2022-12-11 20:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[ .:2022-12-11 20:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu522443638.:: ] 5224661980Eeager release cuda mem 625663: ]  
Eeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1926:] 638Device 0 init p2p of link 1] 
eager release cuda mem 625663
[2022-12-11 20:55:27.522591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:55:27.522670: [E2022-12-11 20:55:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu522680:: [1980E2022-12-11 20:55:27]  .eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc522710
:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.523058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.523315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27[.2022-12-11 20:55:27523543.: 523560E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 20:55:27.541522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 20:55:27.541639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.541652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:55:27.541763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.542421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.542577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.542626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 20:55:27.542738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.542826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 20:55:27.542941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 20:55:27.542952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:55:27.543074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.543314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:55:27.543446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.543528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.543741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.543850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.543883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 20:55:27.544002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.544183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:55:27.544232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.544306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:55:27.544810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.545102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:55:27.562728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.563079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.563388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.563525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.563864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.564474: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.564507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.564886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:55:27.565411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.385171 secs 
[2022-12-11 20:55:27.565632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.384318 secs 
[2022-12-11 20:55:27.565803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.384999 secs 
[2022-12-11 20:55:27.566199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.385593 secs 
[2022-12-11 20:55:27.566642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.385836 secs 
[2022-12-11 20:55:27.567039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.386457 secs 
[2022-12-11 20:55:27.567557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.386153 secs 
[2022-12-11 20:55:27.568084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 999999 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2999997 / 100000000 nodes ( 3.00 %) | cpu 96000004 / 100000000 nodes ( 96.00 %) | 493.16 MB | 0.403338 secs 
[2022-12-11 20:55:27.568955: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.80 GB
[2022-12-11 20:55:28.961165: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.06 GB
[2022-12-11 20:55:28.963519: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.06 GB
[2022-12-11 20:55:28.964107: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.06 GB
[2022-12-11 20:55:30.349007: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.33 GB
[2022-12-11 20:55:30.350122: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.33 GB
[2022-12-11 20:55:30.350851: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.33 GB
[2022-12-11 20:55:31.789356: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.54 GB
[2022-12-11 20:55:31.789583: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.54 GB
[2022-12-11 20:55:31.790653: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.54 GB
[2022-12-11 20:55:33.279522: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.76 GB
[2022-12-11 20:55:33.279697: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.76 GB
[2022-12-11 20:55:33.280003: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.76 GB
[2022-12-11 20:55:34.552657: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.21 GB
[2022-12-11 20:55:34.553062: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.21 GB
[2022-12-11 20:55:34.553617: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.21 GB
[2022-12-11 20:55:35.719466: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.41 GB
[2022-12-11 20:55:35.720375: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.41 GB
[HCTR][20:55:36.870][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][tid #139865165375232]: replica 7 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][tid #139865173767936]: replica 4 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][tid #139865249269504]: replica 3 calling init per replica done, doing barrier
[HCTR][20:55:36.870][ERROR][RK0][tid #139865106659072]: replica 2 calling init per replica done, doing barrier
[HCTR][20:55:36.871][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865106659072]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865165375232]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865249269504]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865173767936]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:55:36.871][ERROR][RK0][main]: init per replica done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865106659072]: init per replica done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865165375232]: init per replica done
[HCTR][20:55:36.871][ERROR][RK0][main]: init per replica done
[HCTR][20:55:36.871][ERROR][RK0][main]: init per replica done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865249269504]: init per replica done
[HCTR][20:55:36.871][ERROR][RK0][tid #139865173767936]: init per replica done
[HCTR][20:55:36.873][ERROR][RK0][main]: init per replica done
[HCTR][20:55:36.909][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f197a238400
[HCTR][20:55:36.909][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f197a558400
[HCTR][20:55:36.909][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f197ab98400
[HCTR][20:55:36.909][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f197aeb8400
[HCTR][20:55:36.909][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f18b0238400
[HCTR][20:55:36.909][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f18b0558400
[HCTR][20:55:36.909][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f18b0b98400
[HCTR][20:55:36.909][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f18b0eb8400
[HCTR][20:55:36.909][ERROR][RK0][tid #139865106659072]: 2 allocated 3276800 at 0x7f189c238400
[HCTR][20:55:36.909][ERROR][RK0][tid #139865106659072]: 2 allocated 6553600 at 0x7f189c558400
[HCTR][20:55:36.909][ERROR][RK0][tid #139865106659072]: 2 allocated 3276800 at 0x7f189cb98400
[HCTR][20:55:36.909][ERROR][RK0][tid #139865106659072]: 2 allocated 6553600 at 0x7f189ceb8400
[HCTR][20:55:36.909][ERROR][RK0][tid #139865903572736]: 1 allocated 3276800 at 0x7f1964238400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865903572736]: 1 allocated 6553600 at 0x7f1964558400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865903572736]: 1 allocated 3276800 at 0x7f1964b98400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865903572736]: 1 allocated 6553600 at 0x7f1964eb8400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865249269504]: 3 allocated 3276800 at 0x7f193c238400
[HCTR][20:55:36.910][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f18b8238400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865249269504]: 3 allocated 6553600 at 0x7f193c558400
[HCTR][20:55:36.910][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f18b8558400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865249269504]: 3 allocated 3276800 at 0x7f193cb98400
[HCTR][20:55:36.910][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f18b8b98400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865249269504]: 3 allocated 6553600 at 0x7f193ceb8400
[HCTR][20:55:36.910][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f18b8eb8400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865106659072]: 6 allocated 3276800 at 0x7f197a238400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865106659072]: 6 allocated 6553600 at 0x7f197a558400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865106659072]: 6 allocated 3276800 at 0x7f197ab98400
[HCTR][20:55:36.910][ERROR][RK0][tid #139865106659072]: 6 allocated 6553600 at 0x7f197aeb8400
[HCTR][20:55:36.912][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f18cc320000
[HCTR][20:55:36.912][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f18cc640000
[HCTR][20:55:36.912][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f18ccc80000
[HCTR][20:55:36.912][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f18ccfa0000
