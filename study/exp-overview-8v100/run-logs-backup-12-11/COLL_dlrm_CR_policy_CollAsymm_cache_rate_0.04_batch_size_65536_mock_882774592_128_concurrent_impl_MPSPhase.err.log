2022-12-11 19:27:48.507911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.514180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.519588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.524952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.528554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.543630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.550471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.556875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.609946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.618524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.621703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.630943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.642788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.647336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.656010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.660108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.661220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.662283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.663594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.665292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.666127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.666420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.667848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.667885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.669598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.669639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.671200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.671259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.672899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.673516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.674282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.675123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.676026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.676645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.677697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.678205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.679758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.680783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.681882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.682946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.687664: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:48.688009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.689344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.690501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.691629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.692725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.693840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.695304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.697097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.697131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.697692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.699167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.699820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.700674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.701334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.702958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.704719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.706386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.707941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.709497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.716235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.717867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.718775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.724271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.734032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.734674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.734893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.736954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.737610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.737897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.737976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.744167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.755977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.778569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.779401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.780350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.780402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.780871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.782522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.782541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.783591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.783825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.785530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.785760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.786173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.787329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.787513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.789391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.789580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.791991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.792116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.792678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.793126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.793573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.794385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.794964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.796829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.797061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.797457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.797598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.798067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.798779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.801594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.801866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.801908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.802142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.802375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.803031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.806347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.806382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.806962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.807066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.807355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.810012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.810184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.810341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.810662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.810777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.813253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.813631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.814730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.814941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.816561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.817205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.817567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.820405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.821454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.821593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.823169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.824314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.824374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.826002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.827050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.827099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.828648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.829495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.830251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.830810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.832078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.832711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.833092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.834367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.835860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.837349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.838700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.838918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.839310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.841006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.841848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.841887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.843501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.844167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.844276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.845197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.845964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.847271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.847335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.849039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.850201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.851056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.851176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.851249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.852279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.854532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.854557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.854596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.855141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.857248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.857343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.857359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.857926: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:48.858053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.858151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.860717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.860867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.861438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.861482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.862642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.865042: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:48.865721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.866341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.866377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.866730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.868021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.868446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.869360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.869592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.869844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.870327: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:48.871411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.871624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.872577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.872767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.872979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.874779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.874914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.875088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.876220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.876359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.876809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.878811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.879177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.880007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.881178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.881441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.881961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.884174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.884561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.885510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.886059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.886144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.886458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.889201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.890108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.890551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.890898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.891292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.894290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.895784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.895967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.896147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.930990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.932054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.932408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.932626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.935799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.936880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.937273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.937404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.941726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.943136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.943226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.943622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.947194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.948503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.948549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.948939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.952651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.959489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.959535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.959876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.962329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.963286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.963348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.963811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.974933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.976037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.976581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.976611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.983078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.984437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.984478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.984498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:48.995185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.025119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.025328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.025385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.035843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.038111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.038223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.038350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.041381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.070561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.070561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.073906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.075209: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:49.085326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.103683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.103683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.110849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.111045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.111098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.113500: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:49.117646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.117672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.117794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.122155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.123618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.127892: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:49.127926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.132880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.133666: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 19:27:49.137101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.143727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.143851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.148533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.148608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:49.155907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.027405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.028036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.028573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.029333: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.029392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:27:50.049154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.049795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.050295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.050887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.051417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.052287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 19:27:50.099505: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.099696: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.147886: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 19:27:50.293588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.294214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.294756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.295356: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.295415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:27:50.314168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.314814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.315336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.315998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.317124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.317602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 19:27:50.331926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.332555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.333080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.333546: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.333603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:27:50.341288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.342358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.343044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.343527: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.343583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:27:50.351795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.352417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.352944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.353529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.354047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.354524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 19:27:50.362087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.363087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.363951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.364540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.365054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.365526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 19:27:50.411571: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.411768: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.413562: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.413745: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.414262: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 19:27:50.414731: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 19:27:50.431611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.432241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.432793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.433256: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.433311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:27:50.445371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.446145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.446687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.447168: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.447223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:27:50.449144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.449745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.450231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.450301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.451269: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.451323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:27:50.451334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.452011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.452079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.452507: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.452637: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.453029: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 19:27:50.453072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:27:50.453078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.453787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.454344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.454412: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 19:27:50.454862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.455337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 19:27:50.466182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.466870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.467414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.468018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.468542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.469011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 19:27:50.469128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.469788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.470295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.470545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.471098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.471522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.472029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.472404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.472963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 19:27:50.473367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.473920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 19:27:50.474386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 19:27:50.499630: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.499830: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.501681: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 19:27:50.514570: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.514749: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.516096: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 19:27:50.517353: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.517532: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.519375: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 19:27:50.520092: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.520230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 19:27:50.521715: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][19:27:51.787][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.787][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.787][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.787][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.787][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.788][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.788][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
[HCTR][19:27:51.788][ERROR][RK0][main]: using mock embedding with 882774585 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:00,  2.67it/s]warmup run: 1it [00:00,  2.57it/s]warmup run: 1it [00:00,  2.50it/s]warmup run: 1it [00:00,  2.50it/s]warmup run: 1it [00:00,  2.49it/s]warmup run: 61it [00:00, 167.22it/s]warmup run: 68it [00:00, 181.34it/s]warmup run: 62it [00:00, 162.11it/s]warmup run: 72it [00:00, 188.14it/s]warmup run: 65it [00:00, 169.75it/s]warmup run: 113it [00:00, 266.13it/s]warmup run: 125it [00:00, 287.92it/s]warmup run: 110it [00:00, 248.69it/s]warmup run: 129it [00:00, 291.37it/s]warmup run: 118it [00:00, 266.75it/s]warmup run: 161it [00:00, 326.01it/s]warmup run: 182it [00:00, 366.29it/s]warmup run: 162it [00:00, 324.17it/s]warmup run: 184it [00:00, 362.73it/s]warmup run: 173it [00:00, 345.46it/s]warmup run: 206it [00:00, 359.17it/s]warmup run: 238it [00:00, 420.35it/s]warmup run: 216it [00:00, 385.11it/s]warmup run: 235it [00:00, 403.98it/s]warmup run: 226it [00:00, 398.00it/s]warmup run: 256it [00:00, 398.23it/s]warmup run: 292it [00:00, 454.70it/s]warmup run: 289it [00:00, 442.30it/s]warmup run: 279it [00:00, 434.24it/s]warmup run: 265it [00:00, 406.31it/s]warmup run: 309it [00:00, 434.96it/s]warmup run: 346it [00:00, 479.66it/s]warmup run: 330it [00:01, 449.57it/s]warmup run: 322it [00:01, 451.80it/s]warmup run: 341it [00:01, 446.97it/s]warmup run: 361it [00:01, 459.62it/s]warmup run: 400it [00:01, 497.37it/s]warmup run: 382it [00:01, 493.82it/s]warmup run: 384it [00:01, 473.92it/s]warmup run: 391it [00:01, 451.22it/s]warmup run: 414it [00:01, 479.60it/s]warmup run: 455it [00:01, 510.23it/s]warmup run: 439it [00:01, 496.15it/s]warmup run: 441it [00:01, 520.34it/s]warmup run: 440it [00:01, 452.76it/s]warmup run: 470it [00:01, 503.33it/s]warmup run: 510it [00:01, 519.59it/s]warmup run: 500it [00:01, 539.70it/s]warmup run: 494it [00:01, 509.97it/s]warmup run: 491it [00:01, 466.76it/s]warmup run: 524it [00:01, 514.07it/s]warmup run: 1it [00:01,  1.40s/it]warmup run: 1it [00:01,  1.40s/it]warmup run: 565it [00:01, 526.71it/s]warmup run: 1it [00:01,  1.40s/it]warmup run: 547it [00:01, 512.52it/s]warmup run: 557it [00:01, 541.85it/s]warmup run: 541it [00:01, 475.75it/s]warmup run: 577it [00:01, 513.50it/s]warmup run: 99it [00:01, 91.56it/s]warmup run: 93it [00:01, 85.89it/s]warmup run: 84it [00:01, 77.57it/s]warmup run: 619it [00:01, 525.88it/s]warmup run: 601it [00:01, 519.10it/s]warmup run: 613it [00:01, 544.24it/s]warmup run: 591it [00:01, 480.76it/s]warmup run: 633it [00:01, 526.97it/s]warmup run: 196it [00:01, 194.25it/s]warmup run: 190it [00:01, 189.11it/s]warmup run: 175it [00:01, 174.82it/s]warmup run: 673it [00:01, 523.13it/s]warmup run: 657it [00:01, 528.44it/s]warmup run: 670it [00:01, 549.93it/s]warmup run: 647it [00:01, 501.88it/s]warmup run: 691it [00:01, 540.41it/s]warmup run: 294it [00:01, 306.73it/s]warmup run: 287it [00:01, 300.48it/s]warmup run: 267it [00:01, 281.20it/s]warmup run: 731it [00:01, 537.20it/s]warmup run: 712it [00:01, 532.41it/s]warmup run: 727it [00:01, 553.89it/s]warmup run: 704it [00:01, 520.51it/s]warmup run: 748it [00:01, 547.02it/s]warmup run: 389it [00:01, 415.74it/s]warmup run: 385it [00:01, 415.00it/s]warmup run: 358it [00:01, 387.57it/s]warmup run: 790it [00:01, 551.67it/s]warmup run: 767it [00:01, 535.04it/s]warmup run: 784it [00:01, 558.55it/s]warmup run: 761it [00:01, 533.48it/s]warmup run: 806it [00:01, 555.42it/s]warmup run: 449it [00:01, 489.29it/s]warmup run: 483it [00:01, 524.06it/s]warmup run: 482it [00:01, 510.40it/s]warmup run: 847it [00:01, 556.88it/s]warmup run: 821it [00:01, 535.92it/s]warmup run: 841it [00:01, 559.25it/s]warmup run: 818it [00:01, 542.38it/s]warmup run: 863it [00:01, 559.55it/s]warmup run: 540it [00:01, 580.87it/s]warmup run: 582it [00:02, 624.56it/s]warmup run: 577it [00:02, 604.89it/s]warmup run: 903it [00:02, 555.55it/s]warmup run: 876it [00:02, 537.59it/s]warmup run: 900it [00:02, 566.14it/s]warmup run: 875it [00:02, 549.49it/s]warmup run: 920it [00:02, 555.71it/s]warmup run: 632it [00:02, 660.47it/s]warmup run: 681it [00:02, 710.89it/s]warmup run: 673it [00:02, 688.30it/s]warmup run: 959it [00:02, 550.05it/s]warmup run: 957it [00:02, 562.37it/s]warmup run: 930it [00:02, 520.40it/s]warmup run: 931it [00:02, 547.74it/s]warmup run: 976it [00:02, 552.72it/s]warmup run: 725it [00:02, 727.55it/s]warmup run: 782it [00:02, 785.00it/s]warmup run: 766it [00:02, 749.00it/s]warmup run: 1015it [00:02, 546.02it/s]warmup run: 1014it [00:02, 561.59it/s]warmup run: 983it [00:02, 523.08it/s]warmup run: 986it [00:02, 533.03it/s]warmup run: 1032it [00:02, 551.29it/s]warmup run: 818it [00:02, 781.03it/s]warmup run: 883it [00:02, 843.39it/s]warmup run: 859it [00:02, 795.79it/s]warmup run: 1070it [00:02, 546.91it/s]warmup run: 1072it [00:02, 564.42it/s]warmup run: 1039it [00:02, 532.11it/s]warmup run: 1040it [00:02, 523.44it/s]warmup run: 1088it [00:02, 550.49it/s]warmup run: 912it [00:02, 823.06it/s]warmup run: 984it [00:02, 886.96it/s]warmup run: 952it [00:02, 827.30it/s]warmup run: 1125it [00:02, 541.60it/s]warmup run: 1129it [00:02, 565.15it/s]warmup run: 1093it [00:02, 522.77it/s]warmup run: 1093it [00:02, 517.27it/s]warmup run: 1086it [00:02, 923.43it/s]warmup run: 1004it [00:02, 844.17it/s]warmup run: 1144it [00:02, 547.37it/s]warmup run: 1044it [00:02, 851.98it/s]warmup run: 1186it [00:02, 565.14it/s]warmup run: 1180it [00:02, 518.33it/s]warmup run: 1148it [00:02, 529.81it/s]warmup run: 1145it [00:02, 507.46it/s]warmup run: 1188it [00:02, 948.80it/s]warmup run: 1202it [00:02, 555.90it/s]warmup run: 1138it [00:02, 875.43it/s]warmup run: 1095it [00:02, 809.68it/s]warmup run: 1244it [00:02, 567.54it/s]warmup run: 1203it [00:02, 532.78it/s]warmup run: 1233it [00:02, 500.63it/s]warmup run: 1196it [00:02, 503.64it/s]warmup run: 1291it [00:02, 970.31it/s]warmup run: 1258it [00:02, 555.49it/s]warmup run: 1239it [00:02, 912.84it/s]warmup run: 1302it [00:02, 569.40it/s]warmup run: 1259it [00:02, 538.35it/s]warmup run: 1284it [00:02, 494.24it/s]warmup run: 1247it [00:02, 504.91it/s]warmup run: 1181it [00:02, 690.14it/s]warmup run: 1334it [00:02, 910.83it/s]warmup run: 1314it [00:02, 538.71it/s]warmup run: 1392it [00:02, 929.97it/s]warmup run: 1359it [00:02, 567.06it/s]warmup run: 1315it [00:02, 544.27it/s]warmup run: 1337it [00:02, 504.04it/s]warmup run: 1298it [00:02, 496.83it/s]warmup run: 1368it [00:02, 532.10it/s]warmup run: 1417it [00:02, 567.89it/s]warmup run: 1375it [00:02, 560.18it/s]warmup run: 1257it [00:02, 625.06it/s]warmup run: 1392it [00:02, 515.45it/s]warmup run: 1428it [00:02, 774.75it/s]warmup run: 1348it [00:02, 494.91it/s]warmup run: 1488it [00:02, 783.62it/s]warmup run: 1422it [00:03, 529.61it/s]warmup run: 1475it [00:03, 571.33it/s]warmup run: 1436it [00:03, 574.71it/s]warmup run: 1448it [00:03, 526.40it/s]warmup run: 1325it [00:03, 596.31it/s]warmup run: 1398it [00:03, 492.19it/s]warmup run: 1511it [00:03, 710.36it/s]warmup run: 1476it [00:03, 521.93it/s]warmup run: 1534it [00:03, 574.59it/s]warmup run: 1494it [00:03, 573.42it/s]warmup run: 1573it [00:03, 682.68it/s]warmup run: 1506it [00:03, 540.00it/s]warmup run: 1448it [00:03, 491.40it/s]warmup run: 1388it [00:03, 566.78it/s]warmup run: 1592it [00:03, 576.15it/s]warmup run: 1529it [00:03, 504.17it/s]warmup run: 1552it [00:03, 573.76it/s]warmup run: 1587it [00:03, 657.65it/s]warmup run: 1561it [00:03, 525.59it/s]warmup run: 1498it [00:03, 489.89it/s]warmup run: 1647it [00:03, 621.09it/s]warmup run: 1447it [00:03, 556.65it/s]warmup run: 1651it [00:03, 577.32it/s]warmup run: 1610it [00:03, 573.06it/s]warmup run: 1580it [00:03, 496.21it/s]warmup run: 1657it [00:03, 635.25it/s]warmup run: 1614it [00:03, 519.29it/s]warmup run: 1547it [00:03, 487.69it/s]warmup run: 1505it [00:03, 546.91it/s]warmup run: 1709it [00:03, 576.30it/s]warmup run: 1714it [00:03, 587.40it/s]warmup run: 1668it [00:03, 574.08it/s]warmup run: 1631it [00:03, 499.03it/s]warmup run: 1667it [00:03, 513.41it/s]warmup run: 1723it [00:03, 618.07it/s]warmup run: 1596it [00:03, 488.26it/s]warmup run: 1561it [00:03, 535.65it/s]warmup run: 1767it [00:03, 576.14it/s]warmup run: 1726it [00:03, 572.09it/s]warmup run: 1684it [00:03, 506.77it/s]warmup run: 1776it [00:03, 560.59it/s]warmup run: 1719it [00:03, 511.98it/s]warmup run: 1647it [00:03, 493.74it/s]warmup run: 1787it [00:03, 591.95it/s]warmup run: 1617it [00:03, 540.93it/s]warmup run: 1825it [00:03, 575.64it/s]warmup run: 1784it [00:03, 572.93it/s]warmup run: 1742it [00:03, 526.56it/s]warmup run: 1772it [00:03, 515.71it/s]warmup run: 1834it [00:03, 541.14it/s]warmup run: 1705it [00:03, 517.47it/s]warmup run: 1848it [00:03, 574.96it/s]warmup run: 1883it [00:03, 572.72it/s]warmup run: 1672it [00:03, 535.04it/s]warmup run: 1842it [00:03, 574.28it/s]warmup run: 1799it [00:03, 538.14it/s]warmup run: 1827it [00:03, 524.23it/s]warmup run: 1890it [00:03, 535.54it/s]warmup run: 1759it [00:03, 522.52it/s]warmup run: 1907it [00:03, 555.12it/s]warmup run: 1941it [00:03, 573.10it/s]warmup run: 1726it [00:03, 523.84it/s]warmup run: 1857it [00:03, 550.01it/s]warmup run: 1900it [00:03, 572.38it/s]warmup run: 1885it [00:03, 537.92it/s]warmup run: 1815it [00:03, 533.35it/s]warmup run: 1945it [00:03, 534.93it/s]warmup run: 1999it [00:03, 572.30it/s]warmup run: 1914it [00:03, 554.79it/s]warmup run: 1963it [00:03, 535.41it/s]warmup run: 1958it [00:03, 566.23it/s]warmup run: 1779it [00:03, 514.89it/s]warmup run: 1943it [00:03, 549.72it/s]warmup run: 1869it [00:04, 532.82it/s]warmup run: 1999it [00:04, 529.22it/s]warmup run: 2057it [00:04, 571.93it/s]warmup run: 1970it [00:04, 548.80it/s]warmup run: 2015it [00:04, 560.55it/s]warmup run: 2017it [00:04, 526.40it/s]warmup run: 1831it [00:04, 502.37it/s]warmup run: 2000it [00:04, 553.37it/s]warmup run: 1923it [00:04, 531.60it/s]warmup run: 2053it [00:04, 520.75it/s]warmup run: 2115it [00:04, 572.19it/s]warmup run: 2072it [00:04, 562.46it/s]warmup run: 2025it [00:04, 540.21it/s]warmup run: 2070it [00:04, 522.54it/s]warmup run: 1882it [00:04, 492.66it/s]warmup run: 2056it [00:04, 547.91it/s]warmup run: 1979it [00:04, 537.56it/s]warmup run: 2107it [00:04, 523.77it/s]warmup run: 2173it [00:04, 573.92it/s]warmup run: 2129it [00:04, 562.89it/s]warmup run: 2080it [00:04, 539.36it/s]warmup run: 2126it [00:04, 530.64it/s]warmup run: 1932it [00:04, 487.47it/s]warmup run: 2111it [00:04, 540.16it/s]warmup run: 2035it [00:04, 541.53it/s]warmup run: 2161it [00:04, 525.92it/s]warmup run: 2231it [00:04, 574.16it/s]warmup run: 2186it [00:04, 562.71it/s]warmup run: 2135it [00:04, 541.03it/s]warmup run: 2180it [00:04, 524.90it/s]warmup run: 1981it [00:04, 477.18it/s]warmup run: 2090it [00:04, 536.89it/s]warmup run: 2166it [00:04, 530.55it/s]warmup run: 2216it [00:04, 532.44it/s]warmup run: 2289it [00:04, 573.05it/s]warmup run: 2244it [00:04, 567.46it/s]warmup run: 2190it [00:04, 543.20it/s]warmup run: 2235it [00:04, 531.25it/s]warmup run: 2031it [00:04, 481.34it/s]warmup run: 2144it [00:04, 533.78it/s]warmup run: 2220it [00:04, 527.64it/s]warmup run: 2272it [00:04, 537.97it/s]warmup run: 2347it [00:04, 571.66it/s]warmup run: 2301it [00:04, 567.66it/s]warmup run: 2247it [00:04, 549.43it/s]warmup run: 2289it [00:04, 533.34it/s]warmup run: 2088it [00:04, 505.94it/s]warmup run: 2198it [00:04, 533.23it/s]warmup run: 2273it [00:04, 525.84it/s]warmup run: 2329it [00:04, 546.66it/s]warmup run: 2405it [00:04, 572.80it/s]warmup run: 2358it [00:04, 566.08it/s]warmup run: 2346it [00:04, 541.32it/s]warmup run: 2302it [00:04, 532.99it/s]warmup run: 2139it [00:04, 506.57it/s]warmup run: 2252it [00:04, 534.76it/s]warmup run: 2330it [00:04, 536.37it/s]warmup run: 2385it [00:04, 550.12it/s]warmup run: 2464it [00:04, 575.55it/s]warmup run: 2415it [00:04, 566.73it/s]warmup run: 2402it [00:04, 545.23it/s]warmup run: 2356it [00:04, 530.34it/s]warmup run: 2190it [00:04, 497.79it/s]warmup run: 2306it [00:04, 531.27it/s]warmup run: 2388it [00:04, 548.16it/s]warmup run: 2441it [00:04, 540.51it/s]warmup run: 2523it [00:04, 578.13it/s]warmup run: 2472it [00:04, 566.01it/s]warmup run: 2459it [00:04, 551.98it/s]warmup run: 2410it [00:04, 527.02it/s]warmup run: 2240it [00:04, 486.79it/s]warmup run: 2446it [00:04, 555.34it/s]warmup run: 2360it [00:04, 524.08it/s]warmup run: 2499it [00:04, 549.99it/s]warmup run: 2582it [00:04, 579.35it/s]warmup run: 2529it [00:04, 564.19it/s]warmup run: 2516it [00:04, 556.81it/s]warmup run: 2467it [00:04, 537.30it/s]warmup run: 2289it [00:05, 484.70it/s]warmup run: 2502it [00:05, 556.34it/s]warmup run: 2415it [00:05, 529.27it/s]warmup run: 2555it [00:05, 538.85it/s]warmup run: 2640it [00:05, 577.55it/s]warmup run: 2586it [00:05, 564.40it/s]warmup run: 2572it [00:05, 554.58it/s]warmup run: 2521it [00:05, 526.92it/s]warmup run: 2339it [00:05, 487.24it/s]warmup run: 2560it [00:05, 561.05it/s]warmup run: 2470it [00:05, 533.78it/s]warmup run: 2609it [00:05, 523.58it/s]warmup run: 2628it [00:05, 545.45it/s]warmup run: 2577it [00:05, 535.61it/s]warmup run: 2390it [00:05, 493.22it/s]warmup run: 2617it [00:05, 556.11it/s]warmup run: 2524it [00:05, 527.17it/s]warmup run: 2662it [00:05, 513.32it/s]warmup run: 2634it [00:05, 544.72it/s]warmup run: 2440it [00:05, 481.67it/s]warmup run: 2577it [00:05, 517.45it/s]warmup run: 2698it [00:05, 371.07it/s]warmup run: 2643it [00:05, 359.19it/s]warmup run: 2489it [00:05, 478.17it/s]warmup run: 2632it [00:05, 525.43it/s]warmup run: 2754it [00:05, 410.99it/s]warmup run: 2697it [00:05, 396.36it/s]warmup run: 2683it [00:05, 340.05it/s]warmup run: 2673it [00:05, 357.07it/s]warmup run: 2540it [00:05, 485.73it/s]warmup run: 2714it [00:05, 332.20it/s]warmup run: 2812it [00:05, 449.22it/s]warmup run: 2754it [00:05, 435.83it/s]warmup run: 2689it [00:05, 357.80it/s]warmup run: 2733it [00:05, 371.68it/s]warmup run: 2731it [00:05, 403.00it/s]warmup run: 2590it [00:05, 487.54it/s]warmup run: 2768it [00:05, 374.57it/s]warmup run: 2869it [00:05, 478.40it/s]warmup run: 2812it [00:05, 470.99it/s]warmup run: 2746it [00:05, 403.20it/s]warmup run: 2783it [00:05, 400.07it/s]warmup run: 2783it [00:05, 429.00it/s]warmup run: 2685it [00:05, 342.79it/s]warmup run: 2641it [00:05, 492.26it/s]warmup run: 2826it [00:05, 420.40it/s]warmup run: 2926it [00:05, 500.46it/s]warmup run: 2869it [00:05, 496.70it/s]warmup run: 2803it [00:05, 442.28it/s]warmup run: 2836it [00:05, 430.56it/s]warmup run: 2833it [00:05, 443.13it/s]warmup run: 2741it [00:05, 388.36it/s]warmup run: 2883it [00:05, 457.02it/s]warmup run: 2983it [00:05, 518.84it/s]warmup run: 2926it [00:05, 516.21it/s]warmup run: 2860it [00:05, 474.12it/s]warmup run: 3000it [00:05, 509.05it/s]warmup run: 2889it [00:05, 455.97it/s]warmup run: 2883it [00:05, 456.94it/s]warmup run: 2799it [00:05, 432.58it/s]warmup run: 2939it [00:05, 482.74it/s]warmup run: 2983it [00:05, 531.21it/s]warmup run: 2917it [00:05, 498.56it/s]warmup run: 2942it [00:06, 475.90it/s]warmup run: 3000it [00:06, 499.01it/s]warmup run: 2935it [00:06, 472.85it/s]warmup run: 2855it [00:06, 463.51it/s]warmup run: 2996it [00:06, 505.52it/s]warmup run: 3000it [00:06, 494.15it/s]warmup run: 2691it [00:06, 290.26it/s]warmup run: 2971it [00:06, 507.44it/s]warmup run: 2993it [00:06, 483.41it/s]warmup run: 3000it [00:06, 489.71it/s]warmup run: 2987it [00:06, 485.08it/s]warmup run: 2908it [00:06, 480.53it/s]warmup run: 3000it [00:06, 488.06it/s]warmup run: 3000it [00:06, 487.05it/s]warmup run: 2746it [00:06, 341.37it/s]warmup run: 2960it [00:06, 491.11it/s]warmup run: 2804it [00:06, 393.91it/s]warmup run: 3000it [00:06, 474.99it/s]warmup run: 2862it [00:06, 438.44it/s]warmup run: 2921it [00:06, 475.39it/s]warmup run: 2980it [00:06, 503.94it/s]warmup run: 3000it [00:06, 453.21it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.26it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1702.78it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.75it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.23it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1665.49it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1712.22it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.87it/s]warmup should be done:   5%|▍         | 136/3000 [00:00<00:02, 1353.36it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.83it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1665.68it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1715.07it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.40it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1696.03it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1670.89it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1714.69it/s]warmup should be done:  10%|█         | 303/3000 [00:00<00:01, 1536.16it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1673.18it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1675.49it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1693.40it/s]warmup should be done:  17%|█▋        | 516/3000 [00:00<00:01, 1715.89it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1718.15it/s]warmup should be done:  16%|█▌        | 471/3000 [00:00<00:01, 1600.40it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1662.94it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1684.86it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1675.82it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1672.03it/s]warmup should be done:  23%|██▎       | 690/3000 [00:00<00:01, 1720.54it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1691.13it/s]warmup should be done:  21%|██▏       | 639/3000 [00:00<00:01, 1628.33it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1684.13it/s]warmup should be done:  23%|██▎       | 688/3000 [00:00<00:01, 1699.12it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1656.97it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1673.96it/s]warmup should be done:  27%|██▋       | 807/3000 [00:00<00:01, 1644.97it/s]warmup should be done:  29%|██▉       | 863/3000 [00:00<00:01, 1719.23it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1669.47it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1686.83it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1686.69it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1657.93it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1701.82it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1668.47it/s]warmup should be done:  34%|███▍      | 1035/3000 [00:00<00:01, 1716.50it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1666.02it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1685.02it/s]warmup should be done:  34%|███▍      | 1018/3000 [00:00<00:01, 1684.48it/s]warmup should be done:  34%|███▍      | 1030/3000 [00:00<00:01, 1701.60it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1654.28it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1631.77it/s]warmup should be done:  40%|████      | 1207/3000 [00:00<00:01, 1715.90it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1664.67it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1685.54it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1661.29it/s]warmup should be done:  40%|████      | 1201/3000 [00:00<00:01, 1704.11it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1689.57it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1655.03it/s]warmup should be done:  38%|███▊      | 1138/3000 [00:00<00:01, 1639.94it/s]warmup should be done:  46%|████▌     | 1379/3000 [00:00<00:00, 1716.04it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1683.25it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:00, 1662.05it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1697.26it/s]warmup should be done:  46%|████▌     | 1373/3000 [00:00<00:00, 1706.01it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1654.44it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1649.55it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:01, 1650.73it/s]warmup should be done:  52%|█████▏    | 1551/3000 [00:00<00:00, 1714.25it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1684.06it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1662.34it/s]warmup should be done:  50%|█████     | 1501/3000 [00:00<00:00, 1659.15it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1702.06it/s]warmup should be done:  52%|█████▏    | 1545/3000 [00:00<00:00, 1707.37it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1652.24it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1648.81it/s]warmup should be done:  57%|█████▋    | 1723/3000 [00:01<00:00, 1711.40it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1683.65it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1703.44it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1660.23it/s]warmup should be done:  57%|█████▋    | 1716/3000 [00:01<00:00, 1706.37it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1656.67it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1649.84it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1646.55it/s]warmup should be done:  62%|██████▏   | 1863/3000 [00:01<00:00, 1682.36it/s]warmup should be done:  62%|██████▎   | 1875/3000 [00:01<00:00, 1703.27it/s]warmup should be done:  63%|██████▎   | 1887/3000 [00:01<00:00, 1707.01it/s]warmup should be done:  63%|██████▎   | 1895/3000 [00:01<00:00, 1706.35it/s]warmup should be done:  61%|██████▏   | 1839/3000 [00:01<00:00, 1652.68it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1649.99it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1641.77it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1639.69it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1684.27it/s]warmup should be done:  69%|██████▊   | 2059/3000 [00:01<00:00, 1708.57it/s]warmup should be done:  68%|██████▊   | 2047/3000 [00:01<00:00, 1705.45it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1701.06it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1651.06it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1648.47it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1642.82it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1648.56it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1685.48it/s]warmup should be done:  74%|███████▍  | 2219/3000 [00:01<00:00, 1707.03it/s]warmup should be done:  74%|███████▍  | 2231/3000 [00:01<00:00, 1709.05it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1648.89it/s]warmup should be done:  75%|███████▍  | 2237/3000 [00:01<00:00, 1697.28it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1650.15it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1647.13it/s]warmup should be done:  72%|███████▏  | 2169/3000 [00:01<00:00, 1654.87it/s]warmup should be done:  79%|███████▉  | 2370/3000 [00:01<00:00, 1685.45it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1709.39it/s]warmup should be done:  80%|████████  | 2402/3000 [00:01<00:00, 1708.14it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1650.02it/s]warmup should be done:  78%|███████▊  | 2337/3000 [00:01<00:00, 1649.70it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1654.07it/s]warmup should be done:  80%|████████  | 2407/3000 [00:01<00:00, 1688.94it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1655.06it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1685.59it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1711.44it/s]warmup should be done:  86%|████████▌ | 2573/3000 [00:01<00:00, 1706.91it/s]warmup should be done:  82%|████████▏ | 2467/3000 [00:01<00:00, 1651.55it/s]warmup should be done:  83%|████████▎ | 2503/3000 [00:01<00:00, 1650.35it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1664.13it/s]warmup should be done:  86%|████████▌ | 2576/3000 [00:01<00:00, 1688.78it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1663.45it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1685.54it/s]warmup should be done:  91%|█████████ | 2735/3000 [00:01<00:00, 1710.78it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1703.56it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1650.41it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1670.47it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1693.57it/s]warmup should be done:  89%|████████▉ | 2674/3000 [00:01<00:00, 1672.24it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1686.48it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1711.52it/s]warmup should be done:  97%|█████████▋| 2915/3000 [00:01<00:00, 1703.83it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1652.93it/s]warmup should be done:  97%|█████████▋| 2919/3000 [00:01<00:00, 1700.16it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1673.65it/s]warmup should be done:  95%|█████████▍| 2844/3000 [00:01<00:00, 1680.05it/s]warmup should be done:  94%|█████████▍| 2835/3000 [00:01<00:00, 1647.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1705.59it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1705.64it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1701.71it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.90it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1662.13it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1654.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.92it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 176/3000 [00:00<00:01, 1759.84it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1699.46it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1736.38it/s]warmup should be done:   6%|▌         | 174/3000 [00:00<00:01, 1738.43it/s]warmup should be done:   6%|▌         | 172/3000 [00:00<00:01, 1717.70it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1703.23it/s]warmup should be done:   6%|▌         | 175/3000 [00:00<00:01, 1741.29it/s]warmup should be done:   6%|▌         | 177/3000 [00:00<00:01, 1762.04it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1738.37it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1701.28it/s]warmup should be done:  12%|█▏        | 350/3000 [00:00<00:01, 1746.15it/s]warmup should be done:  12%|█▏        | 354/3000 [00:00<00:01, 1765.92it/s]warmup should be done:  11%|█▏        | 344/3000 [00:00<00:01, 1714.72it/s]warmup should be done:  12%|█▏        | 354/3000 [00:00<00:01, 1763.19it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1698.99it/s]warmup should be done:  12%|█▏        | 348/3000 [00:00<00:01, 1686.56it/s]warmup should be done:  17%|█▋        | 522/3000 [00:00<00:01, 1738.92it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1702.52it/s]warmup should be done:  18%|█▊        | 531/3000 [00:00<00:01, 1764.53it/s]warmup should be done:  17%|█▋        | 519/3000 [00:00<00:01, 1728.52it/s]warmup should be done:  18%|█▊        | 526/3000 [00:00<00:01, 1748.25it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1697.16it/s]warmup should be done:  18%|█▊        | 531/3000 [00:00<00:01, 1751.48it/s]warmup should be done:  17%|█▋        | 524/3000 [00:00<00:01, 1715.46it/s]warmup should be done:  23%|██▎       | 701/3000 [00:00<00:01, 1746.62it/s]warmup should be done:  24%|██▎       | 708/3000 [00:00<00:01, 1762.37it/s]warmup should be done:  23%|██▎       | 692/3000 [00:00<00:01, 1723.74it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1698.28it/s]warmup should be done:  23%|██▎       | 696/3000 [00:00<00:01, 1726.64it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1692.30it/s]warmup should be done:  24%|██▎       | 707/3000 [00:00<00:01, 1746.94it/s]warmup should be done:  23%|██▎       | 696/3000 [00:00<00:01, 1715.18it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1697.76it/s]warmup should be done:  30%|██▉       | 885/3000 [00:00<00:01, 1759.22it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1691.62it/s]warmup should be done:  29%|██▉       | 869/3000 [00:00<00:01, 1720.47it/s]warmup should be done:  29%|██▉       | 883/3000 [00:00<00:01, 1749.74it/s]warmup should be done:  29%|██▉       | 876/3000 [00:00<00:01, 1730.37it/s]warmup should be done:  29%|██▉       | 865/3000 [00:00<00:01, 1709.48it/s]warmup should be done:  29%|██▉       | 872/3000 [00:00<00:01, 1728.03it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1696.39it/s]warmup should be done:  34%|███▍      | 1022/3000 [00:00<00:01, 1689.93it/s]warmup should be done:  35%|███▌      | 1060/3000 [00:00<00:01, 1756.12it/s]warmup should be done:  35%|███▍      | 1042/3000 [00:00<00:01, 1717.42it/s]warmup should be done:  35%|███▌      | 1061/3000 [00:00<00:01, 1747.88it/s]warmup should be done:  35%|███▍      | 1039/3000 [00:00<00:01, 1718.79it/s]warmup should be done:  35%|███▌      | 1051/3000 [00:00<00:01, 1734.17it/s]warmup should be done:  35%|███▍      | 1049/3000 [00:00<00:01, 1739.39it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1701.00it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1687.59it/s]warmup should be done:  40%|████      | 1214/3000 [00:00<00:01, 1717.01it/s]warmup should be done:  41%|████      | 1236/3000 [00:00<00:01, 1754.28it/s]warmup should be done:  40%|████      | 1212/3000 [00:00<00:01, 1721.36it/s]warmup should be done:  41%|████      | 1225/3000 [00:00<00:01, 1734.68it/s]warmup should be done:  41%|████      | 1236/3000 [00:00<00:01, 1742.11it/s]warmup should be done:  41%|████      | 1226/3000 [00:00<00:01, 1746.51it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1703.18it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1695.17it/s]warmup should be done:  46%|████▌     | 1386/3000 [00:00<00:00, 1716.23it/s]warmup should be done:  46%|████▌     | 1386/3000 [00:00<00:00, 1725.92it/s]warmup should be done:  47%|████▋     | 1414/3000 [00:00<00:00, 1759.45it/s]warmup should be done:  47%|████▋     | 1399/3000 [00:00<00:00, 1733.58it/s]warmup should be done:  47%|████▋     | 1411/3000 [00:00<00:00, 1739.11it/s]warmup should be done:  47%|████▋     | 1404/3000 [00:00<00:00, 1755.25it/s]warmup should be done:  51%|█████     | 1537/3000 [00:00<00:00, 1701.63it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1698.18it/s]warmup should be done:  52%|█████▏    | 1559/3000 [00:00<00:00, 1725.56it/s]warmup should be done:  53%|█████▎    | 1591/3000 [00:00<00:00, 1760.96it/s]warmup should be done:  52%|█████▏    | 1573/3000 [00:00<00:00, 1730.66it/s]warmup should be done:  52%|█████▏    | 1558/3000 [00:00<00:00, 1709.07it/s]warmup should be done:  53%|█████▎    | 1581/3000 [00:00<00:00, 1758.65it/s]warmup should be done:  53%|█████▎    | 1585/3000 [00:00<00:00, 1727.53it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1702.17it/s]warmup should be done:  57%|█████▋    | 1706/3000 [00:01<00:00, 1701.78it/s]warmup should be done:  58%|█████▊    | 1732/3000 [00:01<00:00, 1724.88it/s]warmup should be done:  59%|█████▉    | 1768/3000 [00:01<00:00, 1761.59it/s]warmup should be done:  58%|█████▊    | 1729/3000 [00:01<00:00, 1705.97it/s]warmup should be done:  58%|█████▊    | 1747/3000 [00:01<00:00, 1728.00it/s]warmup should be done:  59%|█████▊    | 1758/3000 [00:01<00:00, 1759.41it/s]warmup should be done:  59%|█████▊    | 1760/3000 [00:01<00:00, 1731.69it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1703.93it/s]warmup should be done:  63%|██████▎   | 1878/3000 [00:01<00:00, 1705.21it/s]warmup should be done:  64%|██████▎   | 1906/3000 [00:01<00:00, 1726.73it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1760.99it/s]warmup should be done:  63%|██████▎   | 1900/3000 [00:01<00:00, 1705.07it/s]warmup should be done:  64%|██████▍   | 1920/3000 [00:01<00:00, 1726.17it/s]warmup should be done:  64%|██████▍   | 1935/3000 [00:01<00:00, 1759.58it/s]warmup should be done:  65%|██████▍   | 1937/3000 [00:01<00:00, 1741.14it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1704.39it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1707.04it/s]warmup should be done:  69%|██████▉   | 2082/3000 [00:01<00:00, 1734.04it/s]warmup should be done:  71%|███████   | 2123/3000 [00:01<00:00, 1763.84it/s]warmup should be done:  69%|██████▉   | 2071/3000 [00:01<00:00, 1704.52it/s]warmup should be done:  70%|██████▉   | 2093/3000 [00:01<00:00, 1726.27it/s]warmup should be done:  70%|███████   | 2112/3000 [00:01<00:00, 1761.80it/s]warmup should be done:  70%|███████   | 2115/3000 [00:01<00:00, 1750.28it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1701.18it/s]warmup should be done:  74%|███████▍  | 2221/3000 [00:01<00:00, 1704.72it/s]warmup should be done:  75%|███████▌  | 2256/3000 [00:01<00:00, 1733.80it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1764.75it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1725.77it/s]warmup should be done:  75%|███████▍  | 2242/3000 [00:01<00:00, 1702.09it/s]warmup should be done:  76%|███████▋  | 2292/3000 [00:01<00:00, 1754.54it/s]warmup should be done:  76%|███████▋  | 2289/3000 [00:01<00:00, 1752.24it/s]warmup should be done:  81%|████████  | 2431/3000 [00:01<00:00, 1737.78it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1763.67it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1700.76it/s]warmup should be done:  81%|████████▏ | 2439/3000 [00:01<00:00, 1723.04it/s]warmup should be done:  80%|████████  | 2413/3000 [00:01<00:00, 1700.82it/s]warmup should be done:  80%|███████▉  | 2392/3000 [00:01<00:00, 1682.40it/s]warmup should be done:  82%|████████▏ | 2469/3000 [00:01<00:00, 1756.43it/s]warmup should be done:  82%|████████▏ | 2466/3000 [00:01<00:00, 1755.30it/s]warmup should be done:  87%|████████▋ | 2607/3000 [00:01<00:00, 1741.43it/s]warmup should be done:  88%|████████▊ | 2654/3000 [00:01<00:00, 1761.65it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1699.65it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1722.21it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1687.52it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1699.32it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1756.80it/s]warmup should be done:  88%|████████▊ | 2643/3000 [00:01<00:00, 1758.05it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1748.20it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1761.50it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1698.81it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1726.82it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1692.96it/s]warmup should be done:  92%|█████████▏| 2755/3000 [00:01<00:00, 1700.11it/s]warmup should be done:  94%|█████████▍| 2822/3000 [00:01<00:00, 1758.62it/s]warmup should be done:  94%|█████████▍| 2820/3000 [00:01<00:00, 1760.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1760.07it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1752.89it/s]warmup should be done:  97%|█████████▋| 2903/3000 [00:01<00:00, 1696.85it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1733.42it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1695.50it/s]warmup should be done:  98%|█████████▊| 2926/3000 [00:01<00:00, 1701.19it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1761.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1751.73it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1763.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1749.47it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1733.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1731.60it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1709.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.48it/s]2022-12-11 19:28:57.220200: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9142c272f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.220264: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.308627: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:57.526153: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f913ef82510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.526209: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.617166: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:57.679786: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9142b894e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.679850: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.756598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:57.865536: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f913efa2f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.865594: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.914820: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f913ac26f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.914877: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.918146: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f913ac26d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.918202: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.927213: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f913ac221e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.927267: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.927844: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9136f9ef50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 19:28:57.927888: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 19:28:57.957559: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:57.998794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:58.000175: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:58.010978: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:58.012932: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 19:28:59.155903: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.312845: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.414164: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.580289: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.591403: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.606326: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.606999: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 19:28:59.622037: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][19:29:20.110][ERROR][RK0][tid #140262340810496]: replica 2 reaches 1000, calling init pre replica
[HCTR][19:29:20.110][ERROR][RK0][tid #140262340810496]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.121][ERROR][RK0][tid #140262340810496]: coll ps creation done
[HCTR][19:29:20.121][ERROR][RK0][tid #140262340810496]: replica 2 waits for coll ps creation barrier
[HCTR][19:29:20.236][ERROR][RK0][tid #140262131091200]: replica 6 reaches 1000, calling init pre replica
[HCTR][19:29:20.236][ERROR][RK0][tid #140262131091200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.249][ERROR][RK0][tid #140262131091200]: coll ps creation done
[HCTR][19:29:20.249][ERROR][RK0][tid #140262131091200]: replica 6 waits for coll ps creation barrier
[HCTR][19:29:20.299][ERROR][RK0][tid #140262072375040]: replica 7 reaches 1000, calling init pre replica
[HCTR][19:29:20.299][ERROR][RK0][tid #140262072375040]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.303][ERROR][RK0][tid #140262072375040]: coll ps creation done
[HCTR][19:29:20.303][ERROR][RK0][tid #140262072375040]: replica 7 waits for coll ps creation barrier
[HCTR][19:29:20.313][ERROR][RK0][tid #140262198200064]: replica 1 reaches 1000, calling init pre replica
[HCTR][19:29:20.313][ERROR][RK0][tid #140262198200064]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.322][ERROR][RK0][tid #140262198200064]: coll ps creation done
[HCTR][19:29:20.322][ERROR][RK0][tid #140262198200064]: replica 1 waits for coll ps creation barrier
[HCTR][19:29:20.367][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][19:29:20.367][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.374][ERROR][RK0][main]: coll ps creation done
[HCTR][19:29:20.375][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][19:29:20.396][ERROR][RK0][tid #140262131091200]: replica 5 reaches 1000, calling init pre replica
[HCTR][19:29:20.396][ERROR][RK0][tid #140262131091200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.402][ERROR][RK0][tid #140262131091200]: coll ps creation done
[HCTR][19:29:20.402][ERROR][RK0][tid #140262131091200]: replica 5 waits for coll ps creation barrier
[HCTR][19:29:20.402][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][19:29:20.402][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.411][ERROR][RK0][main]: coll ps creation done
[HCTR][19:29:20.411][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][19:29:20.420][ERROR][RK0][tid #140262072375040]: replica 4 reaches 1000, calling init pre replica
[HCTR][19:29:20.420][ERROR][RK0][tid #140262072375040]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][19:29:20.425][ERROR][RK0][tid #140262072375040]: coll ps creation done
[HCTR][19:29:20.425][ERROR][RK0][tid #140262072375040]: replica 4 waits for coll ps creation barrier
[HCTR][19:29:20.425][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][19:29:27.457][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][19:29:27.491][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][tid #140262131091200]: replica 5 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][tid #140262198200064]: replica 1 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][tid #140262340810496]: replica 2 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][tid #140262072375040]: replica 4 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][tid #140262131091200]: replica 6 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][tid #140262072375040]: replica 7 calling init per replica
[HCTR][19:29:27.491][ERROR][RK0][main]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][tid #140262131091200]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][tid #140262198200064]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][tid #140262340810496]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][tid #140262072375040]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][tid #140262131091200]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][main]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][tid #140262072375040]: Calling build_v2
[HCTR][19:29:27.491][ERROR][RK0][tid #140262131091200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][tid #140262198200064]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][tid #140262340810496]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][tid #140262072375040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][tid #140262131091200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][19:29:27.491][ERROR][RK0][tid #140262072375040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 19:29:272022-12-11 19:29:272022-12-11 19:29:272022-12-11 19:29:272022-12-11 19:29:27[.2022-12-11 19:29:272022-12-11 19:29:27....2022-12-11 19:29:27491594..491594491607491594491594.: 491607491615: : : : 491627E: : EEEE:  EE    E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::136136136136:] 136136] ] ] ] 136using concurrent impl MPSPhase] ] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase] 
using concurrent impl MPSPhaseusing concurrent impl MPSPhase



using concurrent impl MPSPhase


[2022-12-11 19:29:27.495843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 19:29:27.495882: [E2022-12-11 19:29:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc495885:: 196E]  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:178] v100x8, slow pcie
[[2022-12-11 19:29:272022-12-11 19:29:27..495929495937: : EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:29:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.:178495957196] [: ] v100x8, slow pcie2022-12-11 19:29:27Eassigning 8 to cpu
. 
495973/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-11 19:29:27E212. ] 496006[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 2022-12-11 19:29:27:
E.[178 4960192022-12-11 19:29:27] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .v100x8, slow pcie:[E496040
1962022-12-11 19:29:27 : ] [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[Eassigning 8 to cpu2022-12-11 19:29:27496060:2022-12-11 19:29:27 
.: 178./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[496070E] 496081:2022-12-11 19:29:27:  [v100x8, slow pcie: 212.[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:29:27
E] 4961152022-12-11 19:29:27 :. build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213496156/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 19:29:27E496161:] : :. : 178[remote time is 8.68421E196496205/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] 2022-12-11 19:29:27
 ] : : v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu[E178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
496256:
2022-12-11 19:29:27 ] :: 212[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie178E] 2022-12-11 19:29:27[496304:
]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.2022-12-11 19:29:27: 196v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
496340.E] 
:2022-12-11 19:29:27: 496366 [assigning 8 to cpu213.[E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:29:27
] 4964042022-12-11 19:29:27 E:.remote time is 8.68421: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 214496446
E496458:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [:  : [196:cpu time is 97.05882022-12-11 19:29:27E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-11 19:29:27] 212
. : .assigning 8 to cpu] 496531/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc496543
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :] :: 
E213assigning 8 to cpu196E ] 
[]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421[2022-12-11 19:29:27assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
2022-12-11 19:29:27.[
:212.496669[2022-12-11 19:29:27214] 496679: 2022-12-11 19:29:27.] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E.496712[cpu time is 97.0588
E 496728: 2022-12-11 19:29:27
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E2022-12-11 19:29:27 496774:213 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc496817:E] remote time is 8.68421:: 212 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
214E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
]  [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 19:29:27
212[
:.] 2022-12-11 19:29:27213[496919build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.] 2022-12-11 19:29:27: 
496942remote time is 8.68421.E: 
[496965 E2022-12-11 19:29:27: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .E2022-12-11 19:29:27:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc497014 .214:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc497032] 213E:: cpu time is 97.0588]  213E
remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  
:remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
[:] 2022-12-11 19:29:27214[remote time is 8.68421.] 2022-12-11 19:29:27
497141cpu time is 97.0588.: [
497157E2022-12-11 19:29:27:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc497190 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E:]  214cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
:cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 19:31:11.305832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 19:31:11.658826: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 3.29 GB
[2022-12-11 19:31:11.658939: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 3.29 GB
[2022-12-11 19:31:11.660013: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 19:31:12.344388: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 19:31:14.  9186: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 2
[2022-12-11 19:31:14.  9280: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 19:33:40.784248: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1001
[2022-12-11 19:33:40.784344: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 19:33:54.265983: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 19:33:54.266103: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1001
[2022-12-11 19:33:54.286976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 19:33:54.287059: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1001 blocks, 8 devices
[2022-12-11 19:33:54.553575: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 19:33:54.586143: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 19:33:54.587541: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 19:33:54.608984: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 19:33:55.143571: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 19:33:55.145853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 19:33:55.148885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 19:33:55.151789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 19:33:55.154668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 19:33:55.157582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 19:33:55.160504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 19:33:55.163398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 19:33:55.166284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 19:34:08.160726: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 19:34:08.168847: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 19:34:08.170940: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 19:34:08.216525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 19:34:08.216627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 19:34:08.216659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 19:34:08.216687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 19:34:08.217193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.217254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.221415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.225353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.350224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 19:34:08.350304: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 19:34:08.350700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.350749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.352110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 19:34:08.352169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 19:34:08.352548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.352588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.353765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 19:34:08.353840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 19:34:08.354245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.354292: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.355205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 19:34:08.355262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 19:34:08.355626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.355665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.358718[: 2022-12-11 19:34:08E. 358728/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E202 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2 solved:
202] [4 solved2022-12-11 19:34:08
.358822: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 19:34:08:.205358843] : worker 0 thread 2 initing device 2E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 19:34:08.359235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.359280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 19:34:08eager alloc mem 3.29 GB.
359294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.359355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.373583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.373745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.374034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.377673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.378686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 19:34:08.378749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 19:34:08.379163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 19:34:08.379210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.381411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.381534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.400229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.400472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.400731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.400801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.404498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.408132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.408242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.427777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 3.29 GB
[2022-12-11 19:34:08.926560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1001.00 Bytes
[2022-12-11 19:34:08.926784: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[2022-12-11 19:34:08.944182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 19:34:08.944341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:34:08.948655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:08.949461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:08.956752: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:08.957124: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:34:09. 51788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09. 56433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09. 56488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[2022-12-11 19:34:092022-12-11 19:34:09..100023100023: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes

[[2022-12-11 19:34:092022-12-11 19:34:09..100246100247: : WW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18551855] ] using empty feat=27using empty feat=27

[2022-12-11 19:34:09.118239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 19:34:09.[1183252022-12-11 19:34:09: .E118354 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1001638
] eager release cuda mem 3531098340
[2022-12-11 19:34:09.118433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:34:09.122934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.126915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.127883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 5.26 MB2022-12-11 19:34:09
.127909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:09.135362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:09.135408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:09.135812: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:34:09.135878: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:34:09.137113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1001.00 Bytes
[2022-12-11 19:34:09.137259: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1855] using empty feat=27
[[2022-12-11 19:34:092022-12-11 19:34:09..139573139573: : EE[[  2022-12-11 19:34:092022-12-11 19:34:09/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu..::13963613963619801980: : ] ] EEeager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes  

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1001.00 Byteseager alloc mem 1001.00 Bytes

[[2022-12-11 19:34:092022-12-11 19:34:09..139813139813: : W[[W 2022-12-11 19:34:092022-12-11 19:34:09 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu../hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:139831139833:1855: : 1855] WW] using empty feat=27  using empty feat=27
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
::18551855] ] using empty feat=27using empty feat=27

[2022-12-11 19:34:09.154499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 19:34:09.154582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:34:09.157763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1001
[2022-12-11 19:34:09.157843: [E2022-12-11 19:34:09 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc157872:: 638E]  eager release cuda mem 1001/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 3531098340
[[2022-12-11 19:34:092022-12-11 19:34:09..157924157941: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1001eager release cuda mem 3531098340

[[2022-12-11 19:34:092022-12-11 19:34:09..158015158029: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1001eager release cuda mem 3531098340

[2022-12-11 19:34:09.158115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 3531098340
[2022-12-11 19:34:09.159091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.163901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.167836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.171727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.175641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 138.07 MB
[2022-12-11 19:34:09.176809: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:09.177308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:09.177477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:09.177524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:09.177586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:09.184773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:09.185091: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 131.44 MB
[2022-12-11 19:34:09.185216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:09.185388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 19:34:09eager release cuda mem 5518079.
185410[: 2022-12-11 19:34:09E. 185427/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5518079:
638] eager release cuda mem 5518079
[2022-12-11 19:34:09.185537: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:34:09.185972: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:34:09.186045: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 134.70 MB
[2022-12-11 19:34:09.186118: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 131.44 MB
[2022-12-11 19:34:09.219426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.223705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.223927: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09.223971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:34:09.228206: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09.228254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:34:09.274448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.274764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.276377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.276428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.278782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 19:34:09.279012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09.279066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.47 GB
[2022-12-11 19:34:09.279303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09.279353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:34:09.280886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09.[2809232022-12-11 19:34:09: .E280930 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager release cuda mem 258551980
] eager alloc mem 16.47 GB
[2022-12-11 19:34:09.280985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[2022-12-11 19:34:09.283273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 19:34:09.283317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 16.88 GB
[[[[[[[[2022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:12........453906453906453911453906453906453913453907453906: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] Device 4 init p2p of link 5] ] ] ] ] ] Device 3 init p2p of link 2
Device 0 init p2p of link 3Device 5 init p2p of link 6Device 6 init p2p of link 0Device 1 init p2p of link 7Device 7 init p2p of link 4Device 2 init p2p of link 1






[2022-12-11 19:34:12.[4544662022-12-11 19:34:12: .E454472 [[[[[: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:122022-12-11 19:34:12E2022-12-11 19:34:12:..... .1980454481454493454481454481454483/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu454491] : : : : : :: eager alloc mem 5.26 MBEEEEE1980E
     ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 5.26 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::
:198019801980198019801980] ] ] ] ] ] eager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MBeager alloc mem 5.26 MB





[2022-12-11 19:34:12.463992[: 2022-12-11 19:34:12E. 464004/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 5518079:
638] eager release cuda mem 5518079
[[2022-12-11 19:34:122022-12-11 19:34:12..464072464080: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::[6386382022-12-11 19:34:12] ] .eager release cuda mem 5518079eager release cuda mem 5518079464115

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.464168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[[2022-12-11 19:34:122022-12-11 19:34:12..464295464296: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5518079eager release cuda mem 5518079

[2022-12-11 19:34:12.487302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 19:34:12.487380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 19:34:12.487466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.487541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.489442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 19:34:12.489576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.491160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 19:34:12.491324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.492796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 19:34:12.492959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.493468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 19:34:12.493647: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 19:34:12:.1980493646] : eager alloc mem 5.26 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 19:34:12.493817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.494618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 19:34:12.494787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.495180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.495233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.496429: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.498019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.499601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.500593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.500692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.501439: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.514749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 19:34:12.514873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.514885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 19:34:12.515009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.515322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 19:34:12.515443: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.520974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 19:34:12.521095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.[5215292022-12-11 19:34:12: .E521538 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 5 init p2p of link 71926
] Device 4 init p2p of link 2
[2022-12-11 19:34:12[.2022-12-11 19:34:12521703.: 521708E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 5.26 MB] 
eager alloc mem 5.26 MB
[2022-12-11 19:34:12.523459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 19:34:12.523587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.523967: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 19:34:12:.638523983] : eager release cuda mem 5518079E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.524136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.524349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 19:34:12.524470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.527726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.528657: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.528723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.530198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.533671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.550337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 19:34:12.550457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.552427: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 19:34:12.552544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[[2022-12-11 19:34:122022-12-11 19:34:12..554641554647: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 1 init p2p of link 0Device 6 init p2p of link 7

[2022-12-11 19:34:12[.2022-12-11 19:34:12554797.: 554801E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 5.26 MB] 
eager alloc mem 5.26 MB
[2022-12-11 19:34:12.557202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.558233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 19:34:12.558275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 19:34:12.558358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.558397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.559221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.560634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 19:34:12.560762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.561863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.561918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.562420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 19:34:12.562546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.26 MB
[2022-12-11 19:34:12.566843: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.566960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.569374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.572587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5518079
[2022-12-11 19:34:12.580507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.583213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 92719772 / 882774585 nodes ( 10.50 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.88 GB | 4.22893 secs 
[2022-12-11 19:34:12.584609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.585791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 34456688 / 882774585 nodes ( 3.90 %~4.00 %) | remote 93574044 / 882774585 nodes ( 10.60 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.47 GB | 4.20659 secs 
[2022-12-11 19:34:12.585867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.587118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 92719772 / 882774585 nodes ( 10.50 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.88 GB | 4.23639 secs 
[2022-12-11 19:34:12.587183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.589013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 92719772 / 882774585 nodes ( 10.50 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.88 GB | 4.22967 secs 
[2022-12-11 19:34:12.591098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.591390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.593790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 92719772 / 882774585 nodes ( 10.50 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.88 GB | 4.23813 secs 
[2022-12-11 19:34:12.594021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 92719772 / 882774585 nodes ( 10.50 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.88 GB | 4.23475 secs 
[2022-12-11 19:34:12.598415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.600027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 144775028
[2022-12-11 19:34:12.602163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 35310960 / 882774585 nodes ( 4.00 %~4.00 %) | remote 92719772 / 882774585 nodes ( 10.50 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.88 GB | 4.38493 secs 
[2022-12-11 19:34:12.602795: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 27.79 GB
[2022-12-11 19:34:12.603321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 34456688 / 882774585 nodes ( 3.90 %~4.00 %) | remote 93574044 / 882774585 nodes ( 10.60 %) | cpu 754743853 / 882774585 nodes ( 85.50 %) | 16.47 GB | 4.25074 secs 
[2022-12-11 19:34:14.112414: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.06 GB
[2022-12-11 19:34:14.113719: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.06 GB
[2022-12-11 19:34:14.119362: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.06 GB
[2022-12-11 19:34:15.788016: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.32 GB
[2022-12-11 19:34:15.788360: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.32 GB
[2022-12-11 19:34:15.788668: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.32 GB
[2022-12-11 19:34:17.126419: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.53 GB
[2022-12-11 19:34:17.126570: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.53 GB
[2022-12-11 19:34:17.126886: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.53 GB
[2022-12-11 19:34:18.694438: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.75 GB
[2022-12-11 19:34:18.694676: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.75 GB
[2022-12-11 19:34:18.695026: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 28.75 GB
[2022-12-11 19:34:18.695302: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 28.75 GB
[2022-12-11 19:34:18.695499: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 28.75 GB
[2022-12-11 19:34:20.142056: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 28.95 GB
[2022-12-11 19:34:20.142674: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 28.95 GB
[HCTR][19:34:21.172][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][tid #140262340810496]: replica 2 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][tid #140262198200064]: replica 1 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][tid #140262131091200]: replica 5 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][tid #140262131091200]: replica 6 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][tid #140262072375040]: replica 7 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][tid #140262072375040]: replica 4 calling init per replica done, doing barrier
[HCTR][19:34:21.172][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262072375040]: replica 7 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262072375040]: replica 4 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262131091200]: replica 5 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262131091200]: replica 6 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262340810496]: replica 2 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262198200064]: replica 1 calling init per replica done, doing barrier done
[HCTR][19:34:21.172][ERROR][RK0][main]: init per replica done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262072375040]: init per replica done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262072375040]: init per replica done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262131091200]: init per replica done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262131091200]: init per replica done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262340810496]: init per replica done
[HCTR][19:34:21.172][ERROR][RK0][tid #140262198200064]: init per replica done
[HCTR][19:34:21.191][ERROR][RK0][main]: init per replica done








