2022-12-12 03:18:29.451772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.457663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.477623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.500810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.509375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.511730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.521471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.532464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.550158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.551301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.552358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.553373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.554370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.555368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.556433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.557466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.563246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.564346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.564558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.566454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.566610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.568509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.568633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.568714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.570680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.570774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.570908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.573317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.573438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.573501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.576547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.576618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.576711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.577341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.579496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.579661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.579728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.580488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.580700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.581023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.583216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.583349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.584087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.584488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.584802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.585355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.586576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.586960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.587464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.587851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.588359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.589410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.589466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.590187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.590603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.591222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.591967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.592894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.593237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.593610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.594120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.595038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.595529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.595901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.596189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.597295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.597662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.598183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.598320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.599876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.600174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.600264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.601753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.601823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.602391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.602949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.603786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.603914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.604714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.604819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.605616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.605730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.606517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.606636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.607490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.607571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.608415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.608920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.609301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.609434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.610356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.610372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.611386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.611390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.612355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.612441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.613238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.613412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.614109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.614349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.614967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.615306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.615841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.616247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.617461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.618060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.618608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.619158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.619703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.620249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.620776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.621317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.621519: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.625933: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.627432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.628071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.628633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.629153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.629651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.630156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.630334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.631243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.631353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.631731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.632434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.632600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.633186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.633839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.634007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.634557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.635263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.635425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.635991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.636258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.636969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.637382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.638558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.638980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.639489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.640081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.641157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.641581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.642029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.642695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.642846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.644517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.644702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.644999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.646031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.646141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.646339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.649078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.649431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.649855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.651027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.651090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.651381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.653473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.654267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.654730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.655912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.655974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.656238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.658438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.659093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.660298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.660469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.660522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.660648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.662893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.663321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.665071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.665150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.665244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.665334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.667391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.667857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.669702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.669812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.669912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.670004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.671690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.672187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.674110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.674112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.679459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.679754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.682634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.683200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.684889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.685016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.685162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.685406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.686992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.687554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.689922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.689954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.690082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.691121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.691384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.691782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.728357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.728508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.733851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.736656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.736960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.737207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.739688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.739814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.740316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.741677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.741841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.742997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.745563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.747109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.748170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.748406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.748558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.750006: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.759895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.780055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.780322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.781808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.782516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.783052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.783378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.785720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.786023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.787731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.788123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.788961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.789152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.792277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.792570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.795139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.795300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.796141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.798035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.798548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.799885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.800681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.801062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.803602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.803840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.805961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.806107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.808060: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.808578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.808953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.810778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.810873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.814113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.814716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.816471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.816556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.817853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.818712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.819113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.821636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.821678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.823481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.824435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.824842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.827208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.828482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.830293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.830600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.830929: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.832865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.835482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.835630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.837660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.840227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.840441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.843576: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.844968: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.845073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.848444: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:18:29.853292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.854616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.857730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.859823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.861535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.861592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.862932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.866198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.866309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:29.867991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.077090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.077704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.078650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.079447: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.079510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:18:31.097432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.098080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.098587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.099399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.100332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.100805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:18:31.110577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.111213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.111739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.112211: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.112268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:18:31.131908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.132803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.133334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.133917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.134429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.135109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:18:31.148381: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.148610: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.203472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.204096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.204444: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:18:31.204798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.205279: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.205332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:18:31.222548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.223202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.223913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.224486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.225006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.225471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:18:31.225946: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.226138: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.228238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:18:31.279978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.280597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.281139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.282277: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.282361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:18:31.297262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.297262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.297262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.298846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.298902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.298930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.300055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.300254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.300442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.300487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.300604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.302452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.302539: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.302585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:18:31.302847: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.302908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:18:31.302988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.303001: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.303046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:18:31.304395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.304480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.305389: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:18:31.305438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:18:31.305435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.306162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.306630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:18:31.307979: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.308162: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.310161: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:18:31.320677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.321351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.321557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.321819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.322235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.322450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.323288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.323684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.324174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.324409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.325261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.325622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.326104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.326273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.327340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.327729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.327877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:18:31.328269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.328911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.329167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.329678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:18:31.330289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:18:31.330503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:18:31.330705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:18:31.352819: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.353018: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.355393: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 03:18:31.374067: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.374273: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.375148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.375319: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.375683: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.375817: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.376137: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 03:18:31.377022: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.377060: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 03:18:31.377155: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:18:31.378186: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 03:18:31.378996: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
[HCTR][03:18:32.619][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.619][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.625][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.631][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.632][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.640][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.640][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:18:32.640][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 100it [00:01, 83.75it/s]warmup run: 201it [00:01, 182.94it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 303it [00:01, 293.70it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 96it [00:01, 81.42it/s]warmup run: 101it [00:01, 88.19it/s]warmup run: 100it [00:01, 83.79it/s]warmup run: 94it [00:01, 79.36it/s]warmup run: 99it [00:01, 86.79it/s]warmup run: 402it [00:01, 404.15it/s]warmup run: 96it [00:01, 83.52it/s]warmup run: 94it [00:01, 82.40it/s]warmup run: 185it [00:01, 168.95it/s]warmup run: 202it [00:01, 190.64it/s]warmup run: 189it [00:01, 173.26it/s]warmup run: 195it [00:01, 176.53it/s]warmup run: 199it [00:01, 188.54it/s]warmup run: 501it [00:02, 512.61it/s]warmup run: 192it [00:01, 180.67it/s]warmup run: 187it [00:01, 176.92it/s]warmup run: 274it [00:01, 265.45it/s]warmup run: 303it [00:01, 302.85it/s]warmup run: 284it [00:01, 276.99it/s]warmup run: 296it [00:01, 286.78it/s]warmup run: 299it [00:01, 299.72it/s]warmup run: 603it [00:02, 617.85it/s]warmup run: 288it [00:01, 286.81it/s]warmup run: 280it [00:01, 280.22it/s]warmup run: 364it [00:01, 367.10it/s]warmup run: 404it [00:01, 418.16it/s]warmup run: 380it [00:01, 385.96it/s]warmup run: 395it [00:01, 398.39it/s]warmup run: 397it [00:01, 410.56it/s]warmup run: 704it [00:02, 707.63it/s]warmup run: 383it [00:01, 394.97it/s]warmup run: 372it [00:01, 384.99it/s]warmup run: 458it [00:02, 473.93it/s]warmup run: 504it [00:01, 528.21it/s]warmup run: 476it [00:02, 492.52it/s]warmup run: 495it [00:02, 509.59it/s]warmup run: 493it [00:01, 512.97it/s]warmup run: 807it [00:02, 785.53it/s]warmup run: 478it [00:01, 499.42it/s]warmup run: 465it [00:01, 487.88it/s]warmup run: 553it [00:02, 573.00it/s]warmup run: 606it [00:02, 632.60it/s]warmup run: 573it [00:02, 591.65it/s]warmup run: 597it [00:02, 615.89it/s]warmup run: 585it [00:02, 597.55it/s]warmup run: 909it [00:02, 844.79it/s]warmup run: 574it [00:02, 596.09it/s]warmup run: 559it [00:02, 582.93it/s]warmup run: 649it [00:02, 661.55it/s]warmup run: 703it [00:02, 709.25it/s]warmup run: 670it [00:02, 677.14it/s]warmup run: 699it [00:02, 707.50it/s]warmup run: 685it [00:02, 689.43it/s]warmup run: 1010it [00:02, 889.13it/s]warmup run: 670it [00:02, 679.79it/s]warmup run: 654it [00:02, 667.20it/s]warmup run: 745it [00:02, 735.45it/s]warmup run: 803it [00:02, 781.66it/s]warmup run: 766it [00:02, 745.97it/s]warmup run: 800it [00:02, 780.88it/s]warmup run: 786it [00:02, 767.52it/s]warmup run: 1112it [00:02, 924.62it/s]warmup run: 767it [00:02, 750.66it/s]warmup run: 748it [00:02, 734.05it/s]warmup run: 840it [00:02, 789.96it/s]warmup run: 904it [00:02, 840.62it/s]warmup run: 861it [00:02, 797.74it/s]warmup run: 899it [00:02, 829.78it/s]warmup run: 886it [00:02, 827.08it/s]warmup run: 1215it [00:02, 952.42it/s]warmup run: 862it [00:02, 802.70it/s]warmup run: 842it [00:02, 786.52it/s]warmup run: 934it [00:02, 830.22it/s]warmup run: 1005it [00:02, 884.98it/s]warmup run: 956it [00:02, 837.73it/s]warmup run: 1000it [00:02, 876.48it/s]warmup run: 987it [00:02, 874.98it/s]warmup run: 1317it [00:02, 968.50it/s]warmup run: 957it [00:02, 840.29it/s]warmup run: 937it [00:02, 830.73it/s]warmup run: 1028it [00:02, 858.74it/s]warmup run: 1106it [00:02, 918.95it/s]warmup run: 1054it [00:02, 875.57it/s]warmup run: 1101it [00:02, 911.96it/s]warmup run: 1088it [00:02, 910.23it/s]warmup run: 1418it [00:02, 979.63it/s]warmup run: 1052it [00:02, 870.59it/s]warmup run: 1033it [00:02, 864.84it/s]warmup run: 1123it [00:02, 884.51it/s]warmup run: 1208it [00:02, 946.79it/s]warmup run: 1152it [00:02, 902.92it/s]warmup run: 1205it [00:02, 946.07it/s]warmup run: 1188it [00:02, 933.69it/s]warmup run: 1520it [00:03, 990.43it/s]warmup run: 1147it [00:02, 890.34it/s]warmup run: 1129it [00:02, 890.43it/s]warmup run: 1218it [00:02, 901.14it/s]warmup run: 1309it [00:02, 963.51it/s]warmup run: 1249it [00:02, 919.98it/s]warmup run: 1308it [00:02, 968.03it/s]warmup run: 1287it [00:02, 943.34it/s]warmup run: 1622it [00:03, 998.68it/s]warmup run: 1243it [00:02, 908.66it/s]warmup run: 1225it [00:02, 910.49it/s]warmup run: 1312it [00:02, 910.92it/s]warmup run: 1411it [00:02, 979.18it/s]warmup run: 1346it [00:02, 934.45it/s]warmup run: 1412it [00:02, 986.68it/s]warmup run: 1386it [00:02, 953.33it/s]warmup run: 1725it [00:03, 1007.13it/s]warmup run: 1340it [00:02, 924.01it/s]warmup run: 1320it [00:02, 920.76it/s]warmup run: 1407it [00:03, 920.90it/s]warmup run: 1513it [00:02, 989.85it/s]warmup run: 1444it [00:03, 945.67it/s]warmup run: 1517it [00:03, 1002.59it/s]warmup run: 1484it [00:02, 960.64it/s]warmup run: 1827it [00:03, 1008.05it/s]warmup run: 1439it [00:03, 942.09it/s]warmup run: 1416it [00:02, 931.80it/s]warmup run: 1502it [00:03, 929.04it/s]warmup run: 1615it [00:03, 997.93it/s]warmup run: 1541it [00:03, 952.44it/s]warmup run: 1621it [00:03, 1010.77it/s]warmup run: 1583it [00:03, 967.10it/s]warmup run: 1929it [00:03, 1003.00it/s]warmup run: 1539it [00:03, 956.26it/s]warmup run: 1512it [00:03, 937.71it/s]warmup run: 1598it [00:03, 935.97it/s]warmup run: 1717it [00:03, 1002.91it/s]warmup run: 1638it [00:03, 956.88it/s]warmup run: 1725it [00:03, 1016.94it/s]warmup run: 1682it [00:03, 969.89it/s]warmup run: 2033it [00:03, 1011.54it/s]warmup run: 1636it [00:03, 959.63it/s]warmup run: 1609it [00:03, 945.24it/s]warmup run: 1693it [00:03, 938.42it/s]warmup run: 1819it [00:03, 1005.05it/s]warmup run: 1736it [00:03, 962.93it/s]warmup run: 1828it [00:03, 1010.32it/s]warmup run: 1781it [00:03, 973.20it/s]warmup run: 2152it [00:03, 1063.97it/s]warmup run: 1734it [00:03, 964.89it/s]warmup run: 1705it [00:03, 948.66it/s]warmup run: 1789it [00:03, 942.59it/s]warmup run: 1921it [00:03, 1009.48it/s]warmup run: 1834it [00:03, 967.72it/s]warmup run: 1930it [00:03, 999.58it/s] warmup run: 1879it [00:03, 974.46it/s]warmup run: 2271it [00:03, 1101.46it/s]warmup run: 1832it [00:03, 968.60it/s]warmup run: 1801it [00:03, 951.03it/s]warmup run: 1889it [00:03, 957.07it/s]warmup run: 2027it [00:03, 1022.91it/s]warmup run: 1932it [00:03, 971.03it/s]warmup run: 2033it [00:03, 1008.43it/s]warmup run: 1979it [00:03, 981.90it/s]warmup run: 2391it [00:03, 1128.65it/s]warmup run: 1930it [00:03, 968.02it/s]warmup run: 1897it [00:03, 946.96it/s]warmup run: 1989it [00:03, 969.41it/s]warmup run: 2146it [00:03, 1070.62it/s]warmup run: 2036it [00:03, 991.41it/s]warmup run: 2156it [00:03, 1072.42it/s]warmup run: 2094it [00:03, 1030.64it/s]warmup run: 2508it [00:03, 1140.07it/s]warmup run: 2033it [00:03, 984.62it/s]warmup run: 1993it [00:03, 945.55it/s]warmup run: 2106it [00:03, 1028.45it/s]warmup run: 2264it [00:03, 1103.20it/s]warmup run: 2156it [00:03, 1052.37it/s]warmup run: 2279it [00:03, 1117.98it/s]warmup run: 2215it [00:03, 1081.97it/s]warmup run: 2628it [00:04, 1155.92it/s]warmup run: 2148it [00:03, 1031.73it/s]warmup run: 2108it [00:03, 1004.97it/s]warmup run: 2226it [00:03, 1077.77it/s]warmup run: 2382it [00:03, 1125.21it/s]warmup run: 2276it [00:03, 1094.79it/s]warmup run: 2402it [00:03, 1150.67it/s]warmup run: 2338it [00:03, 1123.86it/s]warmup run: 2748it [00:04, 1167.03it/s]warmup run: 2265it [00:03, 1070.29it/s]warmup run: 2228it [00:03, 1060.54it/s]warmup run: 2346it [00:03, 1113.01it/s]warmup run: 2501it [00:03, 1143.76it/s]warmup run: 2396it [00:03, 1124.32it/s]warmup run: 2523it [00:03, 1165.74it/s]warmup run: 2462it [00:03, 1157.21it/s]warmup run: 2867it [00:04, 1171.39it/s]warmup run: 2380it [00:03, 1093.37it/s]warmup run: 2346it [00:03, 1095.47it/s]warmup run: 2466it [00:04, 1137.73it/s]warmup run: 2620it [00:03, 1155.85it/s]warmup run: 2516it [00:04, 1144.34it/s]warmup run: 2646it [00:04, 1183.75it/s]warmup run: 2586it [00:04, 1181.42it/s]warmup run: 2987it [00:04, 1179.21it/s]warmup run: 3000it [00:04, 683.14it/s] warmup run: 2495it [00:04, 1108.37it/s]warmup run: 2465it [00:03, 1122.13it/s]warmup run: 2586it [00:04, 1153.65it/s]warmup run: 2739it [00:04, 1164.70it/s]warmup run: 2636it [00:04, 1158.98it/s]warmup run: 2768it [00:04, 1193.24it/s]warmup run: 2710it [00:04, 1198.10it/s]warmup run: 2615it [00:04, 1133.15it/s]warmup run: 2589it [00:04, 1154.92it/s]warmup run: 2707it [00:04, 1168.58it/s]warmup run: 2856it [00:04, 1150.79it/s]warmup run: 2756it [00:04, 1171.05it/s]warmup run: 2891it [00:04, 1203.76it/s]warmup run: 2833it [00:04, 1206.10it/s]warmup run: 2735it [00:04, 1150.83it/s]warmup run: 2713it [00:04, 1177.75it/s]warmup run: 2827it [00:04, 1175.57it/s]warmup run: 2973it [00:04, 1155.42it/s]warmup run: 2875it [00:04, 1174.83it/s]warmup run: 3000it [00:04, 685.58it/s] warmup run: 3000it [00:04, 692.49it/s] warmup run: 2956it [00:04, 1212.29it/s]warmup run: 2853it [00:04, 1159.22it/s]warmup run: 3000it [00:04, 690.15it/s] warmup run: 2832it [00:04, 1179.59it/s]warmup run: 2945it [00:04, 1166.51it/s]warmup run: 2993it [00:04, 1175.45it/s]warmup run: 3000it [00:04, 671.03it/s] warmup run: 3000it [00:04, 665.63it/s] warmup run: 2976it [00:04, 1177.78it/s]warmup run: 2950it [00:04, 1178.88it/s]warmup run: 3000it [00:04, 676.02it/s] warmup run: 3000it [00:04, 674.43it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.56it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1588.89it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.40it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.53it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.09it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1577.49it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.91it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1620.84it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1654.33it/s]warmup should be done:  11%|█         | 322/3000 [00:00<00:01, 1611.26it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1637.57it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1674.29it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1660.92it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1592.09it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1651.89it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1579.88it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1637.55it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1615.26it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.54it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1648.29it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1670.44it/s]warmup should be done:  16%|█▌        | 479/3000 [00:00<00:01, 1580.70it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1648.51it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1560.80it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1615.73it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1647.57it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1663.24it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1630.96it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1668.24it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1645.80it/s]warmup should be done:  21%|██▏       | 638/3000 [00:00<00:01, 1569.96it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1575.19it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1663.53it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1644.10it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1610.90it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1666.21it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1645.19it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1618.86it/s]warmup should be done:  27%|██▋       | 796/3000 [00:00<00:01, 1566.64it/s]warmup should be done:  27%|██▋       | 809/3000 [00:00<00:01, 1591.34it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1662.91it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1610.31it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1640.57it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1664.07it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1644.10it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1611.67it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1596.03it/s]warmup should be done:  32%|███▏      | 953/3000 [00:00<00:01, 1546.42it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1658.92it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1608.60it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1637.25it/s]warmup should be done:  39%|███▉      | 1173/3000 [00:00<00:01, 1660.89it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1641.21it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1614.21it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1598.92it/s]warmup should be done:  37%|███▋      | 1114/3000 [00:00<00:01, 1564.75it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1658.47it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1610.27it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1637.79it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1662.06it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1641.84it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1618.62it/s]warmup should be done:  43%|████▎     | 1293/3000 [00:00<00:01, 1604.34it/s]warmup should be done:  42%|████▏     | 1272/3000 [00:00<00:01, 1567.02it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1655.53it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1638.18it/s]warmup should be done:  49%|████▊     | 1457/3000 [00:00<00:00, 1610.91it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1661.39it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1642.84it/s]warmup should be done:  49%|████▉     | 1470/3000 [00:00<00:00, 1621.92it/s]warmup should be done:  48%|████▊     | 1455/3000 [00:00<00:00, 1607.37it/s]warmup should be done:  48%|████▊     | 1432/3000 [00:00<00:00, 1577.05it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1638.31it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1656.94it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1612.06it/s]warmup should be done:  56%|█████▌    | 1674/3000 [00:01<00:00, 1662.10it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1624.15it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1643.05it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1609.96it/s]warmup should be done:  53%|█████▎    | 1594/3000 [00:01<00:00, 1588.84it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1638.90it/s]warmup should be done:  59%|█████▉    | 1781/3000 [00:01<00:00, 1613.26it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1658.15it/s]warmup should be done:  61%|██████▏   | 1841/3000 [00:01<00:00, 1662.53it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1644.11it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1626.00it/s]warmup should be done:  59%|█████▉    | 1779/3000 [00:01<00:00, 1612.59it/s]warmup should be done:  58%|█████▊    | 1753/3000 [00:01<00:00, 1584.73it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1638.37it/s]warmup should be done:  65%|██████▍   | 1943/3000 [00:01<00:00, 1613.77it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1658.84it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1663.44it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1627.00it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1644.63it/s]warmup should be done:  65%|██████▍   | 1943/3000 [00:01<00:00, 1619.64it/s]warmup should be done:  64%|██████▍   | 1915/3000 [00:01<00:00, 1592.91it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1638.19it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1615.27it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1662.92it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1644.72it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1628.22it/s]warmup should be done:  72%|███████▏  | 2167/3000 [00:01<00:00, 1646.44it/s]warmup should be done:  70%|███████   | 2107/3000 [00:01<00:00, 1623.68it/s]warmup should be done:  69%|██████▉   | 2075/3000 [00:01<00:00, 1580.20it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1637.83it/s]warmup should be done:  76%|███████▌  | 2269/3000 [00:01<00:00, 1620.14it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1661.99it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1627.47it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1650.49it/s]warmup should be done:  77%|███████▋  | 2315/3000 [00:01<00:00, 1624.78it/s]warmup should be done:  76%|███████▌  | 2270/3000 [00:01<00:00, 1624.83it/s]warmup should be done:  74%|███████▍  | 2235/3000 [00:01<00:00, 1584.17it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1635.20it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1621.88it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1625.64it/s]warmup should be done:  84%|████████▎ | 2509/3000 [00:01<00:00, 1657.58it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1650.11it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1627.17it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1624.42it/s]warmup should be done:  80%|███████▉  | 2394/3000 [00:01<00:00, 1567.75it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1635.50it/s]warmup should be done:  86%|████████▋ | 2595/3000 [00:01<00:00, 1619.34it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1626.05it/s]warmup should be done:  89%|████████▉ | 2675/3000 [00:01<00:00, 1657.41it/s]warmup should be done:  89%|████████▉ | 2666/3000 [00:01<00:00, 1652.95it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1631.89it/s]warmup should be done:  87%|████████▋ | 2596/3000 [00:01<00:00, 1623.37it/s]warmup should be done:  85%|████████▌ | 2551/3000 [00:01<00:00, 1554.84it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1634.67it/s]warmup should be done:  93%|█████████▎| 2776/3000 [00:01<00:00, 1625.00it/s]warmup should be done:  94%|█████████▍| 2832/3000 [00:01<00:00, 1653.91it/s]warmup should be done:  95%|█████████▍| 2841/3000 [00:01<00:00, 1652.50it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1601.73it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1634.08it/s]warmup should be done:  92%|█████████▏| 2759/3000 [00:01<00:00, 1625.18it/s]warmup should be done:  90%|█████████ | 2710/3000 [00:01<00:00, 1564.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.97it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1639.71it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1628.39it/s]warmup should be done: 100%|█████████▉| 2998/3000 [00:01<00:00, 1649.74it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1637.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.92it/s]warmup should be done:  97%|█████████▋| 2924/3000 [00:01<00:00, 1631.12it/s]warmup should be done:  97%|█████████▋| 2918/3000 [00:01<00:00, 1589.48it/s]warmup should be done:  96%|█████████▌| 2867/3000 [00:01<00:00, 1559.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.35it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.80it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1607.21it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1572.31it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.28it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1629.33it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.98it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1618.70it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.75it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.79it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.33it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.45it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1616.65it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1676.71it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1686.30it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1683.26it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1679.23it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1669.65it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1616.55it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1647.09it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1676.45it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1684.79it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1688.90it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1613.30it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1682.13it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1671.70it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.41it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1623.14it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1677.48it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1690.12it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1685.83it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1695.40it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1620.15it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1675.31it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1655.23it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1616.10it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1680.93it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1690.39it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1622.30it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1701.80it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1679.27it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1655.92it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1680.92it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1632.50it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1682.92it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1704.85it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1677.84it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1683.83it/s]warmup should be done:  33%|███▎      | 976/3000 [00:00<00:01, 1620.80it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1654.75it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1637.31it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1673.62it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1678.20it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1688.98it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1705.01it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1655.93it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1622.55it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1674.04it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1656.20it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1673.51it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1697.97it/s]warmup should be done:  46%|████▌     | 1366/3000 [00:00<00:00, 1708.95it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1654.26it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1672.04it/s]warmup should be done:  43%|████▎     | 1302/3000 [00:00<00:01, 1619.92it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1676.42it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1670.26it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1680.68it/s]warmup should be done:  51%|█████     | 1529/3000 [00:00<00:00, 1702.56it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1709.96it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1655.81it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1678.10it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1683.15it/s]warmup should be done:  50%|█████     | 1515/3000 [00:00<00:00, 1664.41it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1684.66it/s]warmup should be done:  49%|████▉     | 1464/3000 [00:00<00:00, 1600.10it/s]warmup should be done:  57%|█████▋    | 1701/3000 [00:01<00:00, 1705.79it/s]warmup should be done:  57%|█████▋    | 1710/3000 [00:01<00:00, 1710.39it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1657.15it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1679.04it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1692.09it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1688.55it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1659.11it/s]warmup should be done:  54%|█████▍    | 1626/3000 [00:01<00:00, 1603.86it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1709.16it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1657.75it/s]warmup should be done:  63%|██████▎   | 1882/3000 [00:01<00:00, 1711.11it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1681.29it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1698.99it/s]warmup should be done:  62%|██████▏   | 1865/3000 [00:01<00:00, 1691.44it/s]warmup should be done:  62%|██████▏   | 1848/3000 [00:01<00:00, 1656.20it/s]warmup should be done:  60%|█████▉    | 1787/3000 [00:01<00:00, 1603.68it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1711.79it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1656.73it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1711.90it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1682.09it/s]warmup should be done:  68%|██████▊   | 2035/3000 [00:01<00:00, 1693.60it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1702.63it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1652.48it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1603.94it/s]warmup should be done:  74%|███████▍  | 2217/3000 [00:01<00:00, 1711.12it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1655.10it/s]warmup should be done:  74%|███████▍  | 2226/3000 [00:01<00:00, 1710.34it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1681.01it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1693.09it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1704.31it/s]warmup should be done:  70%|███████   | 2111/3000 [00:01<00:00, 1610.03it/s]warmup should be done:  73%|███████▎  | 2180/3000 [00:01<00:00, 1647.41it/s]warmup should be done:  78%|███████▊  | 2326/3000 [00:01<00:00, 1655.30it/s]warmup should be done:  80%|███████▉  | 2389/3000 [00:01<00:00, 1710.00it/s]warmup should be done:  80%|███████▉  | 2398/3000 [00:01<00:00, 1709.56it/s]warmup should be done:  79%|███████▊  | 2361/3000 [00:01<00:00, 1682.16it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1693.13it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1706.73it/s]warmup should be done:  76%|███████▌  | 2274/3000 [00:01<00:00, 1615.10it/s]warmup should be done:  78%|███████▊  | 2346/3000 [00:01<00:00, 1649.61it/s]warmup should be done:  85%|████████▌ | 2561/3000 [00:01<00:00, 1712.81it/s]warmup should be done:  83%|████████▎ | 2492/3000 [00:01<00:00, 1655.28it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1684.58it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1694.59it/s]warmup should be done:  84%|████████▍ | 2529/3000 [00:01<00:00, 1709.98it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1701.87it/s]warmup should be done:  81%|████████▏ | 2438/3000 [00:01<00:00, 1621.57it/s]warmup should be done:  84%|████████▍ | 2513/3000 [00:01<00:00, 1654.42it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1653.28it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1709.61it/s]warmup should be done:  90%|█████████ | 2700/3000 [00:01<00:00, 1684.90it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1712.33it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1693.51it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1694.89it/s]warmup should be done:  87%|████████▋ | 2603/3000 [00:01<00:00, 1627.25it/s]warmup should be done:  89%|████████▉ | 2679/3000 [00:01<00:00, 1655.99it/s]warmup should be done:  94%|█████████▍| 2824/3000 [00:01<00:00, 1652.83it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1702.78it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1684.23it/s]warmup should be done:  96%|█████████▌| 2873/3000 [00:01<00:00, 1712.68it/s]warmup should be done:  96%|█████████▌| 2885/3000 [00:01<00:00, 1691.11it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1632.29it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1656.60it/s]warmup should be done:  97%|█████████▋| 2910/3000 [00:01<00:00, 1687.58it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1680.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.64it/s]warmup should be done: 100%|█████████▉| 2992/3000 [00:01<00:00, 1660.82it/s]warmup should be done:  98%|█████████▊| 2933/3000 [00:01<00:00, 1635.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.48it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5995d520d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5995d522b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5995d54190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5996099d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5995d541f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f59960967f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5996097730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f5995d621c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 03:19:58.208034: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54c68349c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:19:58.208092: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:19:58.217757: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:02.465835: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54ca82c5c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:02.465902: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:02.467276: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54ce830360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:02.467320: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:02.474044: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:02.474641: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:02.697308: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54cb0317a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:02.697377: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:02.702184: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54c2f934b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:02.702243: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:02.711497: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:02.712594: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:03.191874: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54c6830e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:03.191940: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:03.202443: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:03.210369: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54cf028a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:03.210433: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:03.218302: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:03.227369: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f54c7035070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:20:03.227426: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:20:03.236598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:20:05.648060: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:09.570438: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:09.579316: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:09.860324: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:09.886247: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:09.940933: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:10.010930: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:20:10.168740: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:21:03.095][ERROR][RK0][tid #140002302355200]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:21:03.095][ERROR][RK0][tid #140002302355200]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.103][ERROR][RK0][tid #140002302355200]: coll ps creation done
[HCTR][03:21:03.104][ERROR][RK0][tid #140002302355200]: replica 7 waits for coll ps creation barrier
[HCTR][03:21:03.120][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:21:03.120][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.127][ERROR][RK0][main]: coll ps creation done
[HCTR][03:21:03.127][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][03:21:03.352][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:21:03.352][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.359][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:21:03.359][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.361][ERROR][RK0][main]: coll ps creation done
[HCTR][03:21:03.361][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][03:21:03.366][ERROR][RK0][main]: coll ps creation done
[HCTR][03:21:03.366][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][03:21:03.370][ERROR][RK0][tid #140003376097024]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:21:03.370][ERROR][RK0][tid #140003376097024]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.378][ERROR][RK0][tid #140003376097024]: coll ps creation done
[HCTR][03:21:03.378][ERROR][RK0][tid #140003376097024]: replica 1 waits for coll ps creation barrier
[HCTR][03:21:03.390][ERROR][RK0][tid #140002428180224]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:21:03.390][ERROR][RK0][tid #140002428180224]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.394][ERROR][RK0][tid #140002428180224]: coll ps creation done
[HCTR][03:21:03.394][ERROR][RK0][tid #140002428180224]: replica 3 waits for coll ps creation barrier
[HCTR][03:21:03.430][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:21:03.430][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.434][ERROR][RK0][main]: coll ps creation done
[HCTR][03:21:03.434][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][03:21:03.471][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:21:03.471][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:21:03.475][ERROR][RK0][main]: coll ps creation done
[HCTR][03:21:03.475][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][03:21:03.475][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][03:21:04.326][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][03:21:04.365][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][tid #140003376097024]: replica 1 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][tid #140002302355200]: replica 7 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][tid #140002428180224]: replica 3 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][03:21:04.365][ERROR][RK0][main]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][main]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][main]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][tid #140003376097024]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][tid #140002302355200]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][main]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][tid #140002428180224]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][main]: Calling build_v2
[HCTR][03:21:04.365][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][tid #140003376097024]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][tid #140002302355200]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][tid #140002428180224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:21:04.365][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 03:21:04[2022-12-12 03:21:04[2022-12-12 03:21:042022-12-12 03:21:04.2022-12-12 03:21:042022-12-12 03:21:04...2022-12-12 03:21:04365261..3652563652553652562022-12-12 03:21:04.: 365255365270: : : .365278E: : EEE365289:  EE   : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136::136136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] 136136] ] ] :136using concurrent impl MPSPhase] ] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase136] 
using concurrent impl MPSPhaseusing concurrent impl MPSPhase


] using concurrent impl MPSPhase

using concurrent impl MPSPhase

[2022-12-12 03:21:04.369563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:21:04.369601: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:21:04:.196369607] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 03:21:042022-12-12 03:21:04..369657369654: : EE [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:21:04/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.:196369680178] : ] [assigning 8 to cpuEv100x8, slow pcie2022-12-12 03:21:04
 
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc369699:: [212E2022-12-12 03:21:04]  [[.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:21:042022-12-12 03:21:04369731
:..: 178369741369741E] : :  v100x8, slow pcie[E[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 03:21:04 2022-12-12 03:21:04 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196369791:3697882022-12-12 03:21:04:] : 212: .178assigning 8 to cpuE[] E369815] 
 2022-12-12 03:21:04build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 : v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
:369838: [213: [[[178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:21:04] E2022-12-12 03:21:042022-12-12 03:21:042022-12-12 03:21:04] :.remote time is 8.68421 ...v100x8, slow pcie196369890
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc369910369909369911
] : [:: [: : assigning 8 to cpuE2022-12-12 03:21:04178E2022-12-12 03:21:04EE
 .]  .  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc370031v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc370019/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:21:04:: 
:: ::.178E196E212[213370100]  ]  ] 2022-12-12 03:21:04] : v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.remote time is 8.68421E
:
:
370149
 196214[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [[] 2022-12-12 03:21:04E:assigning 8 to cpu2022-12-12 03:21:042022-12-12 03:21:04cpu time is 97.0588[. 212
..
2022-12-12 03:21:04370242/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 370256370258.: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : 370282E196
EE: [ ]   E2022-12-12 03:21:04[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .2022-12-12 03:21:04:
::/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc370359.196213214:: 370380] ] ] 212E: assigning 8 to cpu[remote time is 8.68421cpu time is 97.0588]  E
2022-12-12 03:21:04

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc .
:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc3704612122022-12-12 03:21:04:[: ] .2132022-12-12 03:21:04E[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8370508] . 2022-12-12 03:21:04
: remote time is 8.68421370527/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E
: :[370542 E[2122022-12-12 03:21:04: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 03:21:04] .E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8370596 214:370611
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 213: E:cpu time is 97.0588[] E 212
2022-12-12 03:21:04remote time is 8.68421 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8370715[:213
: 2022-12-12 03:21:04214] E.] [remote time is 8.68421 370783cpu time is 97.05882022-12-12 03:21:04
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
.:E[370831213 2022-12-12 03:21:04: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.Eremote time is 8.68421:370870 
214: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E[:cpu time is 97.0588 2022-12-12 03:21:04213
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :370946remote time is 8.68421214: 
] Ecpu time is 97.0588 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:21:04:.214371029] : cpu time is 97.0588E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 03:22:23.652361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:22:23.692324: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:22:23.692416: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:22:23.693473: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 03:22:23.764841: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 03:22:24.158437: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 03:22:24.158536: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 03:22:31.911509: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 03:22:31.911607: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 03:22:33.660045: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 03:22:33.660151: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 03:22:33.662882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 03:22:33.662943: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 03:22:34. 15555: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 03:22:34. 44064: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 03:22:34. 45531: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 03:22:34. 66765: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 03:22:34.584564: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 03:22:34.586798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 03:22:34.589784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 03:22:34.592694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 03:22:34.595574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 03:22:34.598503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 03:22:34.601384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 03:22:34.604278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 03:22:34.607149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 03:25:55.710134: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 03:25:55.718962: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 03:25:55.725116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 03:25:55.775544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:25:55.775644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:25:55.775677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:25:55.775709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:25:55.776318: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:25:55.776373: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.777495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.778221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.791121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 03:25:55.791202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 03:25:55.791370: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 03:25:55.791432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 03:25:55.791638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:25:55.791695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.791845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:25:55.791896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.793166: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.793215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.793496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 03:25:55.793551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 03:25:55.793585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 03:25:55.793661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 03:25:55.793950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 03:25:55.793965[: [2022-12-12 03:25:55E2022-12-12 03:25:55. .793981/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc794015: :: E202E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
:202[1980] [2022-12-12 03:25:55] 6 solved2022-12-12 03:25:55.eager alloc mem 381.47 MB
.794112
794121: [: E2022-12-12 03:25:55E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu794174/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:: :1815E205]  ] Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 2 initing device 2
:
205] worker 0 thread 6 initing device 6
[2022-12-12 03:25:55.794300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.794527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 03:25:55.794579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[[2022-12-12 03:25:552022-12-12 03:25:55..794688794691: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151815] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8

[2022-12-12 03:25:55.[7948002022-12-12 03:25:55: .E794806 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :[eager alloc mem 381.47 MB19802022-12-12 03:25:55
] .eager alloc mem 381.47 MB794860
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 03:25:55.794914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.794994: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:25:55.795052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.798381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.798799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.798855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.799428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.799546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.801780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.801960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.802020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.802077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.802577: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:25:55.856170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 03:25:55.861432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 03:25:55.861554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:25:55.862357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.862970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:55.864036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:55.864084: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.14 MB
[[2022-12-12 03:25:552022-12-12 03:25:55..875110875110: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[2022-12-12 03:25:55.880229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes[[[
[2022-12-12 03:25:552022-12-12 03:25:552022-12-12 03:25:552022-12-12 03:25:55....880286880291880286880289: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::[:1980198019802022-12-12 03:25:551980] ] ] .] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes880465eager alloc mem 1023.00 Bytes


: 
[E2022-12-12 03:25:55 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc880544:: 638E]  eager release cuda mem 1023/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1023
[2022-12-12 03:25:55.880635: E[ 2022-12-12 03:25:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:880645638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:25:55.881532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.882064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.882685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:55.882718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:55.883716: [E2022-12-12 03:25:55 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc883726:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 03:25:55.883779: [W2022-12-12 03:25:55 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc883788:: 43W]  WORKER[0] alloc host memory 38.11 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 38.12 MB
[2022-12-12 03:25:55.886276: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 03:25:55.886358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:25:55.886450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 03:25:55.886529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:25:55.886561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 03:25:552022-12-12 03:25:55..886638886652: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 03:25:55.[8867232022-12-12 03:25:55: .E886744 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 1023638
] eager release cuda mem 400000000
[2022-12-12 03:25:55.886829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:25:55.887204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.888073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.889042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.889546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.890052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 03:25:55.890600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:55.890955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 03:25:55[2022-12-12 03:25:55.2022-12-12 03:25:55.891241.891240: 891258: E: E [E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:25:55 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980891290:1980] : 1980] eager alloc mem 611.00 KBE] eager alloc mem 25.25 KB
 eager alloc mem 611.00 KB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:55.891604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:55.891652: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.14 MB
[2022-12-12 03:25:55.891926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:25:55.[8919622022-12-12 03:25:55: .E 891971/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 4.77 GB
[2022-12-12 03:25:55.892057: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.14 MB
[2022-12-12 03:25:55.892326: E[ 2022-12-12 03:25:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:892338638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 03:25:55638.] 892358eager release cuda mem 625663[: 
2022-12-12 03:25:55E. 892386/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :W638 [] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 03:25:55eager release cuda mem 625663:.
43892412] : WORKER[0] alloc host memory 38.15 MBW
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.12 MB[
2022-12-12 03:25:55.892453: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.12 MB
[2022-12-12 03:25:55.909695: [E2022-12-12 03:25:55 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu909718:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-12 03:25:55[.2022-12-12 03:25:55910334.: 910340E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 25855] 
eager release cuda mem 25855
[2022-12-12 03:25:55[.2022-12-12 03:25:55910402.: 910406E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 4.77 GB] 
eager alloc mem 4.77 GB
[2022-12-12 03:25:55.917244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:25:55.917599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:25:55.917783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:25:55.917852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:25:55.917895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:25:55.918037: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:25:55.918081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:25:55.918192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:25:55.918236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:25:55.918377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:25:55.918421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 03:25:55.918625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:25:55.918666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 03:25:551980.] 918677eager alloc mem 4.77 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:25:55.918729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[2022-12-12 03:25:57[.555352[2022-12-12 03:25:57[: 2022-12-12 03:25:57[.E[2022-12-12 03:25:57.[2022-12-12 03:25:575553522022-12-12 03:25:57 2022-12-12 03:25:57.5553552022-12-12 03:25:57.: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.555353: .555352E555352:555360: E555352:  : 1926: E : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : Device 4 init p2p of link 5 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ::1926] :1926Device 3 init p2p of link 219261926] Device 0 init p2p of link 31926] 
] ] Device 7 init p2p of link 4
] Device 5 init p2p of link 6Device 6 init p2p of link 0Device 2 init p2p of link 1Device 1 init p2p of link 7




[2022-12-12 03:25:57.555930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.555987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.556016: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[2022-12-12 03:25:57:[2022-12-12 03:25:57.[19802022-12-12 03:25:57[.5560272022-12-12 03:25:57] .2022-12-12 03:25:57556028: .eager alloc mem 611.00 KB556038.: E556040
: 556048E : E:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] :1980:] eager alloc mem 611.00 KB1980] 1980eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-12 03:25:57.556817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.556863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.556956: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57[.2022-12-12 03:25:57557120.: 557125E:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:25:57 :./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638557154:[] : [6382022-12-12 03:25:57eager release cuda mem 625663E2022-12-12 03:25:57] .
 .eager release cuda mem 625663557187/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc557192
: :: E638E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
:638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 03:25:57.572461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 03:25:57.572518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 03:25:57.572618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.572672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.573488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 03:25:572022-12-12 03:25:57..573529573542: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 4 init p2p of link 7eager release cuda mem 625663

[2022-12-12 03:25:57.573733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.573901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 03:25:57.574074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.574324: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 03:25:57.574465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.574559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 03:25:57.574601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.574712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.574753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[[2022-12-12 03:25:572022-12-12 03:25:57..574942574942: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 03:25:57.575059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 03:25:57.575246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.575307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.575567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.575879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.576101: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.588096: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 03:25:57.588212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.589070: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.589307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 03:25:57.589424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.589632: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:25:57:.1926589662] : Device 7 init p2p of link 6E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 03:25:57.589707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 03:25:57.589789[: 2022-12-12 03:25:57E. 589800/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980[] 2022-12-12 03:25:57eager alloc mem 611.00 KB.
589834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.590134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 03:25:57.[5902582022-12-12 03:25:57: .E590261 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-12 03:25:57.590448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 03:25:57.590561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.590662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 03:25:57.590681[: 2022-12-12 03:25:57E. 590695/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 03:25:57.590788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 03:25:57.590908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.591077: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.591399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.591744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.606044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 03:25:57.606155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.607003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.607212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 03:25:57.607327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.608165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.608552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 03:25:57.608677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.609524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.610259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 03:25:57.610371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.611008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 03:25:57.611164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.611227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.611679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 03:25:57.611819: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.612067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.612156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 03:25:57.612266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.612434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 03:25:57.612546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:25:57.612703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.613065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.613342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:25:57.617750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.620760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9993382 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29953669 / 100000000 nodes ( 29.95 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.82597 secs 
[2022-12-12 03:25:57.622580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.624844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9993549 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29953502 / 100000000 nodes ( 29.95 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.83317 secs 
[2022-12-12 03:25:57.625352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.626098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9999557 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29947494 / 100000000 nodes ( 29.95 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.83181 secs 
[2022-12-12 03:25:57.627251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.627402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.629065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.629397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9991370 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29955681 / 100000000 nodes ( 29.96 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.83751 secs 
[2022-12-12 03:25:57.629565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9997715 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29949336 / 100000000 nodes ( 29.95 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.83478 secs 
[2022-12-12 03:25:57.629646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.630365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 03:25:57.631404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9997813 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29949238 / 100000000 nodes ( 29.95 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.83741 secs 
[2022-12-12 03:25:57.632596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9991657 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29955394 / 100000000 nodes ( 29.96 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.83756 secs 
[2022-12-12 03:25:57.633469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9998743 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29948308 / 100000000 nodes ( 29.95 %) | cpu 60052949 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.85711 secs 
[2022-12-12 03:25:57.634636: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 13.09 GB
[2022-12-12 03:25:59.234891: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 13.36 GB
[2022-12-12 03:25:59.235986: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 13.36 GB
[2022-12-12 03:25:59.237575: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 13.36 GB
[2022-12-12 03:26:00.681880: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 13.62 GB
[2022-12-12 03:26:00.682761: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 13.62 GB
[2022-12-12 03:26:00.683350: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 13.62 GB
[2022-12-12 03:26:01.963053: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 13.83 GB
[2022-12-12 03:26:01.963724: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 13.83 GB
[2022-12-12 03:26:01.965957: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 13.83 GB
[2022-12-12 03:26:03.530655: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.05 GB
[2022-12-12 03:26:03.531216: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.05 GB
[2022-12-12 03:26:03.531559: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 14.05 GB
[2022-12-12 03:26:03.532132: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 14.05 GB
[2022-12-12 03:26:03.532371: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 14.05 GB
[2022-12-12 03:26:04.887658: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 14.25 GB
[2022-12-12 03:26:04.887853: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 14.25 GB
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][tid #140002428180224]: replica 3 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][tid #140003376097024]: replica 1 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][tid #140002302355200]: replica 7 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][03:26:05.055][ERROR][RK0][tid #140003376097024]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][tid #140002302355200]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][tid #140002428180224]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][tid #140003376097024]: init per replica done
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:26:05.055][ERROR][RK0][tid #140002302355200]: init per replica done
[HCTR][03:26:05.055][ERROR][RK0][main]: init per replica done
[HCTR][03:26:05.055][ERROR][RK0][tid #140002428180224]: init per replica done
[HCTR][03:26:05.055][ERROR][RK0][main]: init per replica done
[HCTR][03:26:05.055][ERROR][RK0][main]: init per replica done
[HCTR][03:26:05.055][ERROR][RK0][main]: init per replica done
[HCTR][03:26:05.058][ERROR][RK0][main]: init per replica done
[HCTR][03:26:05.094][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3794238400
[HCTR][03:26:05.094][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3794558400
[HCTR][03:26:05.094][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f3794b98400
[HCTR][03:26:05.094][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f3794eb8400
[HCTR][03:26:05.094][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3814238400
[HCTR][03:26:05.094][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f3814558400
[HCTR][03:26:05.094][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f3814b98400
[HCTR][03:26:05.094][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f3814eb8400
[HCTR][03:26:05.094][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f37fc238400
[HCTR][03:26:05.094][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f37fc558400
[HCTR][03:26:05.094][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f37fcb98400
[HCTR][03:26:05.094][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f37fceb8400
[HCTR][03:26:05.094][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f3864238400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002377856768]: 5 allocated 3276800 at 0x7f386c238400
[HCTR][03:26:05.094][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f3864558400
[HCTR][03:26:05.094][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f3864b98400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002377856768]: 5 allocated 6553600 at 0x7f386c558400
[HCTR][03:26:05.094][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f3864eb8400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002302355200]: 7 allocated 3276800 at 0x7f386e238400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002377856768]: 5 allocated 3276800 at 0x7f386cb98400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002302355200]: 7 allocated 6553600 at 0x7f386e558400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002377856768]: 5 allocated 6553600 at 0x7f386ceb8400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002302355200]: 7 allocated 3276800 at 0x7f386eb98400
[HCTR][03:26:05.094][ERROR][RK0][tid #140002302355200]: 7 allocated 6553600 at 0x7f386eeb8400
[HCTR][03:26:05.095][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f3824238400
[HCTR][03:26:05.095][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f3824558400
[HCTR][03:26:05.095][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f3824b98400
[HCTR][03:26:05.095][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f3824eb8400
[HCTR][03:26:05.097][ERROR][RK0][tid #140002377856768]: 0 allocated 3276800 at 0x7f37d8320000
[HCTR][03:26:05.098][ERROR][RK0][tid #140002377856768]: 0 allocated 6553600 at 0x7f37d8640000
[HCTR][03:26:05.098][ERROR][RK0][tid #140002377856768]: 0 allocated 3276800 at 0x7f37d8c80000
[HCTR][03:26:05.098][ERROR][RK0][tid #140002377856768]: 0 allocated 6553600 at 0x7f37d8fa0000
