2022-12-12 02:39:14.114742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.126734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.137995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.144126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.154899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.166542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.171471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.172894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.191552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.192403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.192914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.194080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.194434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.195570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.196046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.197205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.197581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.198819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.199005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.200391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.200597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.202051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.202251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.203778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.209908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.210676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.211630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.212803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.213797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.214835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.214847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.215958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.217279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.217545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.218073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.218588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.220051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.220216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.220506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.220942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.221769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.223411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.223596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.223834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.223882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.224054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.225081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.227782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.227983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.228018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.228061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.228518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.229460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.232182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.232288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.232377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.232643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.233119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.235325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.235759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.236030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.237018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.239185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.239792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.240023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.240455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.241751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.242428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.243174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.244073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.244687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.245419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.246109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.247480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.247910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.249100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.249508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.250682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.251072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.251474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.252430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.253017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.253526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.254406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.255073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.255508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.256457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.257129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.257502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.258517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.259462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.259981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.260148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.261247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.262096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.262151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.263245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.264163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.264214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.265306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.266192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.266315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.267193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.268262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.268314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.269164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.270288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.270387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.271154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.272346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.272402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.273136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.274405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.274453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.275121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.276539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.276890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.277750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.278081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.278486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.278677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.280160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.280792: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.280913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.281124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.281356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.282486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.283588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.283851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.284654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.284793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.285756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.285893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.286441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.287765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.287783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.288870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.289064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.289622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.289984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.290045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.290944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.291038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.292515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.292740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.292761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.293779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.294402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.294580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.295439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.295626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.297290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.297429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.297508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.298572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.299146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.299220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.300227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.300310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.302377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.302417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.302569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.303916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.304323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.305347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.305524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.307412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.307457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.307705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.309076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.309976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.311093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.311201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.313574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.313626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.313888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.315441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.316279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.316385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.317874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.317964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.318100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.319422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.319528: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.320472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.321747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.321791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.322005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.323238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.324021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.324119: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.325413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.325556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.325706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.326955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.327697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.328723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.329067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.329381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.329405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.330962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.332099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.332670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.333415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.333804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.334076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.334171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.335960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.336915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.337677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.338388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.338707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.338879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.338962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.340825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.342540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.343236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.343465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.343571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.344391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.345353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.347790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.347859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.348056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.348961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.355396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.358990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.359102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.359207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.360173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.361452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.365569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.365711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.366386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.367014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.368012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.404006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.404103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.404803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.405211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.406873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.408926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.409684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.409725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.410010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.412631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.414427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.416303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.416425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.417318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.419668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.421296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.421965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.422222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.423144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.425053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.427265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.427499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.430265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.430300: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.431967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.432163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.432641: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.434274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.439235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.441418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.441550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.441668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.445482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.451501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.453480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.453693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.456590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.457114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.457125: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.458306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.458566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.465817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.494444: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.495848: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:39:14.503874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.504742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.523464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.529004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.529018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.529390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.537226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:14.537437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.737579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.738210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.738741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.739217: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.739272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:39:15.756201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.757328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.757838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.758413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.758924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.759728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:39:15.792136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.792774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.793371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.794275: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.794330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:39:15.806505: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.806731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.814236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.815086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.815621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.816203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.816719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.817797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:39:15.860463: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:39:15.878612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.879251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.879777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.880241: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.880295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:39:15.898962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.899633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.900141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.900727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.901263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.901735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:39:15.905144: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.905333: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.907100: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:39:15.912364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.912965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.913506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.913690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.914369: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.914448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:39:15.914751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.915329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.915794: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.915839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:39:15.927748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.928373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.929267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.929756: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.929813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:39:15.932795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.932850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.934235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.934338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.934660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.935374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.935674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.936129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.936902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.937127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.937516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.938276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.938526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.938844: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.938897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:39:15.939604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:39:15.939724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:39:15.947728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.948363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.948460: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.948609: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.948936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.949545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.950055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.950320: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:39:15.950528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:39:15.951073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.951701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.952312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.952784: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:39:15.952831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:39:15.957201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.957803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.958317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.958876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.959408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.959875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:39:15.970942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.971628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.972143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.972733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.973272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:39:15.973742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:39:15.984563: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.984739: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.986483: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:39:15.996073: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.996248: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:15.998139: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 02:39:16.004998: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:16.005173: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:16.006997: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:39:16.018537: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:16.018702: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:16.020644: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:39:16.031407: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:16.031570: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:39:16.033365: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:39:17.303][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 99it [00:01, 81.29it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 92it [00:01, 78.31it/s]warmup run: 198it [00:01, 176.85it/s]warmup run: 101it [00:01, 87.37it/s]warmup run: 96it [00:01, 83.06it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 190it [00:01, 176.17it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 299it [00:01, 285.46it/s]warmup run: 199it [00:01, 185.71it/s]warmup run: 196it [00:01, 184.17it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 98it [00:01, 85.47it/s]warmup run: 99it [00:01, 85.83it/s]warmup run: 288it [00:01, 283.92it/s]warmup run: 99it [00:01, 85.97it/s]warmup run: 396it [00:01, 392.69it/s]warmup run: 294it [00:01, 289.29it/s]warmup run: 298it [00:01, 297.79it/s]warmup run: 91it [00:01, 79.99it/s]warmup run: 200it [00:01, 189.10it/s]warmup run: 198it [00:01, 185.63it/s]warmup run: 386it [00:01, 395.88it/s]warmup run: 200it [00:01, 188.06it/s]warmup run: 493it [00:02, 497.64it/s]warmup run: 392it [00:01, 400.97it/s]warmup run: 401it [00:01, 416.72it/s]warmup run: 181it [00:01, 171.72it/s]warmup run: 302it [00:01, 302.68it/s]warmup run: 298it [00:01, 296.56it/s]warmup run: 484it [00:02, 503.97it/s]warmup run: 301it [00:01, 300.00it/s]warmup run: 590it [00:02, 595.08it/s]warmup run: 493it [00:02, 514.80it/s]warmup run: 504it [00:02, 532.24it/s]warmup run: 271it [00:01, 272.00it/s]warmup run: 398it [00:01, 410.99it/s]warmup run: 405it [00:01, 421.04it/s]warmup run: 583it [00:02, 605.32it/s]warmup run: 403it [00:01, 417.02it/s]warmup run: 688it [00:02, 681.48it/s]warmup run: 596it [00:02, 623.31it/s]warmup run: 608it [00:02, 639.41it/s]warmup run: 363it [00:01, 377.94it/s]warmup run: 497it [00:02, 519.35it/s]warmup run: 506it [00:01, 531.66it/s]warmup run: 682it [00:02, 692.99it/s]warmup run: 504it [00:01, 527.97it/s]warmup run: 783it [00:02, 745.07it/s]warmup run: 698it [00:02, 715.13it/s]warmup run: 713it [00:02, 733.27it/s]warmup run: 455it [00:01, 480.38it/s]warmup run: 598it [00:02, 622.63it/s]warmup run: 609it [00:02, 636.73it/s]warmup run: 781it [00:02, 765.79it/s]warmup run: 606it [00:02, 632.10it/s]warmup run: 878it [00:02, 793.33it/s]warmup run: 799it [00:02, 788.17it/s]warmup run: 817it [00:02, 809.43it/s]warmup run: 548it [00:02, 575.99it/s]warmup run: 699it [00:02, 711.26it/s]warmup run: 713it [00:02, 729.25it/s]warmup run: 881it [00:02, 824.99it/s]warmup run: 708it [00:02, 721.12it/s]warmup run: 972it [00:02, 828.43it/s]warmup run: 901it [00:02, 847.35it/s]warmup run: 920it [00:02, 866.53it/s]warmup run: 642it [00:02, 660.09it/s]warmup run: 798it [00:02, 779.29it/s]warmup run: 815it [00:02, 799.94it/s]warmup run: 982it [00:02, 873.70it/s]warmup run: 809it [00:02, 792.63it/s]warmup run: 1066it [00:02, 858.17it/s]warmup run: 1004it [00:02, 895.92it/s]warmup run: 1023it [00:02, 909.15it/s]warmup run: 736it [00:02, 728.78it/s]warmup run: 896it [00:02, 830.71it/s]warmup run: 917it [00:02, 855.63it/s]warmup run: 1082it [00:02, 906.92it/s]warmup run: 911it [00:02, 850.24it/s]warmup run: 1160it [00:02, 874.74it/s]warmup run: 1107it [00:02, 932.61it/s]warmup run: 1125it [00:02, 937.58it/s]warmup run: 830it [00:02, 783.38it/s]warmup run: 1018it [00:02, 893.31it/s]warmup run: 994it [00:02, 862.09it/s]warmup run: 1181it [00:02, 930.25it/s]warmup run: 1012it [00:02, 891.79it/s]warmup run: 1257it [00:02, 901.00it/s]warmup run: 1209it [00:02, 947.35it/s]warmup run: 1227it [00:02, 958.96it/s]warmup run: 924it [00:02, 824.12it/s]warmup run: 1120it [00:02, 927.36it/s]warmup run: 1091it [00:02, 888.20it/s]warmup run: 1281it [00:02, 948.22it/s]warmup run: 1113it [00:02, 923.97it/s]warmup run: 1357it [00:03, 928.99it/s]warmup run: 1310it [00:02, 960.96it/s]warmup run: 1330it [00:02, 979.19it/s]warmup run: 1017it [00:02, 850.95it/s]warmup run: 1222it [00:02, 953.67it/s]warmup run: 1188it [00:02, 895.47it/s]warmup run: 1381it [00:02, 962.60it/s]warmup run: 1215it [00:02, 950.23it/s]warmup run: 1460it [00:03, 955.65it/s]warmup run: 1412it [00:02, 975.53it/s]warmup run: 1434it [00:02, 995.71it/s]warmup run: 1110it [00:02, 870.72it/s]warmup run: 1324it [00:02, 969.99it/s]warmup run: 1288it [00:02, 923.44it/s]warmup run: 1481it [00:03, 972.41it/s]warmup run: 1316it [00:02, 966.69it/s]warmup run: 1563it [00:03, 974.64it/s]warmup run: 1516it [00:03, 993.29it/s]warmup run: 1538it [00:03, 1008.50it/s]warmup run: 1203it [00:02, 885.61it/s]warmup run: 1425it [00:02, 978.57it/s]warmup run: 1389it [00:02, 946.01it/s]warmup run: 1582it [00:03, 980.68it/s]warmup run: 1417it [00:02, 978.56it/s]warmup run: 1666it [00:03, 988.21it/s]warmup run: 1619it [00:03, 1003.91it/s]warmup run: 1643it [00:03, 1019.29it/s]warmup run: 1296it [00:02, 892.33it/s]warmup run: 1527it [00:03, 988.76it/s]warmup run: 1489it [00:03, 958.98it/s]warmup run: 1682it [00:03, 985.93it/s]warmup run: 1519it [00:03, 988.18it/s]warmup run: 1768it [00:03, 995.97it/s]warmup run: 1723it [00:03, 1011.65it/s]warmup run: 1747it [00:03, 1022.99it/s]warmup run: 1389it [00:02, 902.11it/s]warmup run: 1628it [00:03, 993.08it/s]warmup run: 1590it [00:03, 973.04it/s]warmup run: 1782it [00:03, 988.82it/s]warmup run: 1621it [00:03, 996.29it/s]warmup run: 1870it [00:03, 1002.66it/s]warmup run: 1826it [00:03, 1016.81it/s]warmup run: 1851it [00:03, 1025.46it/s]warmup run: 1482it [00:03, 909.40it/s]warmup run: 1730it [00:03, 999.42it/s]warmup run: 1691it [00:03, 983.25it/s]warmup run: 1882it [00:03, 991.49it/s]warmup run: 1723it [00:03, 999.29it/s]warmup run: 1974it [00:03, 1011.04it/s]warmup run: 1930it [00:03, 1021.25it/s]warmup run: 1955it [00:03, 1027.80it/s]warmup run: 1575it [00:03, 913.74it/s]warmup run: 1831it [00:03, 1002.49it/s]warmup run: 1794it [00:03, 996.96it/s]warmup run: 1982it [00:03, 993.08it/s]warmup run: 1825it [00:03, 1004.62it/s]warmup run: 2087it [00:03, 1044.98it/s]warmup run: 2039it [00:03, 1039.72it/s]warmup run: 2067it [00:03, 1054.85it/s]warmup run: 1669it [00:03, 919.85it/s]warmup run: 1934it [00:03, 1008.10it/s]warmup run: 1897it [00:03, 1006.43it/s]warmup run: 2098it [00:03, 1041.36it/s]warmup run: 1927it [00:03, 1003.90it/s]warmup run: 2205it [00:03, 1085.22it/s]warmup run: 2162it [00:03, 1093.82it/s]warmup run: 2189it [00:03, 1102.47it/s]warmup run: 1763it [00:03, 923.17it/s]warmup run: 2043it [00:03, 1029.94it/s]warmup run: 2000it [00:03, 1010.81it/s]warmup run: 2218it [00:03, 1088.06it/s]warmup run: 2031it [00:03, 1013.36it/s]warmup run: 2324it [00:03, 1113.82it/s]warmup run: 2284it [00:03, 1131.07it/s]warmup run: 2311it [00:03, 1135.19it/s]warmup run: 1857it [00:03, 927.39it/s]warmup run: 2163it [00:03, 1080.49it/s]warmup run: 2121it [00:03, 1067.91it/s]warmup run: 2339it [00:03, 1121.74it/s]warmup run: 2147it [00:03, 1054.99it/s]warmup run: 2407it [00:03, 1158.19it/s]warmup run: 2433it [00:03, 1157.67it/s]warmup run: 1951it [00:03, 929.53it/s]warmup run: 2436it [00:04, 1043.36it/s]warmup run: 2284it [00:03, 1116.64it/s]warmup run: 2242it [00:03, 1109.36it/s]warmup run: 2460it [00:03, 1145.66it/s]warmup run: 2264it [00:03, 1088.58it/s]warmup run: 2529it [00:03, 1176.29it/s]warmup run: 2555it [00:03, 1174.16it/s]warmup run: 2056it [00:03, 964.99it/s]warmup run: 2405it [00:03, 1142.36it/s]warmup run: 2363it [00:03, 1138.63it/s]warmup run: 2542it [00:04, 1000.12it/s]warmup run: 2581it [00:04, 1162.67it/s]warmup run: 2381it [00:03, 1112.47it/s]warmup run: 2650it [00:04, 1184.36it/s]warmup run: 2675it [00:04, 1180.52it/s]warmup run: 2174it [00:03, 1028.63it/s]warmup run: 2525it [00:03, 1159.29it/s]warmup run: 2484it [00:03, 1157.94it/s]warmup run: 2659it [00:04, 1045.59it/s]warmup run: 2701it [00:04, 1171.00it/s]warmup run: 2498it [00:03, 1127.66it/s]warmup run: 2773it [00:04, 1195.23it/s]warmup run: 2797it [00:04, 1189.57it/s]warmup run: 2293it [00:03, 1075.16it/s]warmup run: 2644it [00:04, 1165.96it/s]warmup run: 2605it [00:04, 1171.79it/s]warmup run: 2777it [00:04, 1083.58it/s]warmup run: 2821it [00:04, 1179.55it/s]warmup run: 2614it [00:04, 1134.73it/s]warmup run: 2895it [00:04, 1201.31it/s]warmup run: 2919it [00:04, 1195.96it/s]warmup run: 2412it [00:03, 1108.10it/s]warmup run: 2764it [00:04, 1173.73it/s]warmup run: 2725it [00:04, 1179.43it/s]warmup run: 2895it [00:04, 1110.93it/s]warmup run: 2940it [00:04, 1181.69it/s]warmup run: 2730it [00:04, 1140.38it/s]warmup run: 3000it [00:04, 699.26it/s] warmup run: 3000it [00:04, 681.50it/s] warmup run: 3000it [00:04, 695.00it/s] warmup run: 2529it [00:04, 1125.89it/s]warmup run: 2882it [00:04, 1174.00it/s]warmup run: 2846it [00:04, 1186.02it/s]warmup run: 3000it [00:04, 659.37it/s] warmup run: 2848it [00:04, 1150.46it/s]warmup run: 2647it [00:04, 1141.81it/s]warmup run: 3000it [00:04, 695.61it/s] warmup run: 2968it [00:04, 1195.07it/s]warmup run: 2967it [00:04, 1159.79it/s]warmup run: 3000it [00:04, 688.50it/s] warmup run: 3000it [00:04, 690.09it/s] warmup run: 2765it [00:04, 1147.27it/s]warmup run: 2884it [00:04, 1158.75it/s]warmup run: 3000it [00:04, 666.55it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1589.22it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.16it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.78it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.98it/s]warmup should be done:   5%|▌         | 153/3000 [00:00<00:01, 1524.46it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.20it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1640.95it/s]warmup should be done:   5%|▌         | 151/3000 [00:00<00:01, 1500.31it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1600.21it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1638.18it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.61it/s]warmup should be done:  10%|█         | 307/3000 [00:00<00:01, 1531.02it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1628.54it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1650.34it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.14it/s]warmup should be done:  10%|█         | 303/3000 [00:00<00:01, 1509.44it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1645.39it/s]warmup should be done:  16%|█▌        | 466/3000 [00:00<00:01, 1555.59it/s]warmup should be done:  15%|█▌        | 464/3000 [00:00<00:01, 1553.85it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1642.08it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1624.30it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1658.68it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1648.69it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1593.62it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1645.08it/s]warmup should be done:  21%|██        | 624/3000 [00:00<00:01, 1564.50it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1644.26it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1633.07it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1656.89it/s]warmup should be done:  21%|██        | 625/3000 [00:00<00:01, 1572.12it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1648.13it/s]warmup should be done:  21%|██▏       | 641/3000 [00:00<00:01, 1573.18it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1646.02it/s]warmup should be done:  26%|██▌       | 782/3000 [00:00<00:01, 1567.62it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1634.50it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1655.89it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1646.84it/s]warmup should be done:  26%|██▌       | 785/3000 [00:00<00:01, 1578.81it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1637.99it/s]warmup should be done:  27%|██▋       | 799/3000 [00:00<00:01, 1571.54it/s]warmup should be done:  31%|███▏      | 940/3000 [00:00<00:01, 1571.74it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1640.78it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1631.35it/s]warmup should be done:  32%|███▏      | 946/3000 [00:00<00:01, 1587.35it/s]warmup should be done:  33%|███▎      | 998/3000 [00:00<00:01, 1650.24it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1640.97it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1633.21it/s]warmup should be done:  32%|███▏      | 957/3000 [00:00<00:01, 1568.85it/s]warmup should be done:  37%|███▋      | 1098/3000 [00:00<00:01, 1571.05it/s]warmup should be done:  38%|███▊      | 1153/3000 [00:00<00:01, 1642.55it/s]warmup should be done:  37%|███▋      | 1106/3000 [00:00<00:01, 1588.42it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1650.94it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1640.06it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1625.54it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1626.72it/s]warmup should be done:  37%|███▋      | 1115/3000 [00:00<00:01, 1570.88it/s]warmup should be done:  44%|████▍     | 1318/3000 [00:00<00:01, 1642.97it/s]warmup should be done:  42%|████▏     | 1256/3000 [00:00<00:01, 1569.23it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1651.48it/s]warmup should be done:  42%|████▏     | 1265/3000 [00:00<00:01, 1583.06it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1638.70it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1623.19it/s]warmup should be done:  42%|████▏     | 1274/3000 [00:00<00:01, 1576.13it/s]warmup should be done:  44%|████▍     | 1315/3000 [00:00<00:01, 1616.44it/s]warmup should be done:  49%|████▉     | 1483/3000 [00:00<00:00, 1643.23it/s]warmup should be done:  50%|████▉     | 1496/3000 [00:00<00:00, 1650.19it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1638.20it/s]warmup should be done:  47%|████▋     | 1413/3000 [00:00<00:01, 1559.45it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1621.82it/s]warmup should be done:  47%|████▋     | 1424/3000 [00:00<00:01, 1571.93it/s]warmup should be done:  48%|████▊     | 1432/3000 [00:00<00:00, 1576.72it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1610.30it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1642.17it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1638.39it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1621.07it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1644.28it/s]warmup should be done:  53%|█████▎    | 1590/3000 [00:01<00:00, 1577.00it/s]warmup should be done:  52%|█████▏    | 1569/3000 [00:01<00:00, 1542.22it/s]warmup should be done:  55%|█████▍    | 1639/3000 [00:01<00:00, 1609.65it/s]warmup should be done:  53%|█████▎    | 1582/3000 [00:01<00:00, 1550.68it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1641.79it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1638.16it/s]warmup should be done:  61%|██████    | 1827/3000 [00:01<00:00, 1644.08it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1617.65it/s]warmup should be done:  58%|█████▊    | 1748/3000 [00:01<00:00, 1577.60it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1610.55it/s]warmup should be done:  57%|█████▋    | 1724/3000 [00:01<00:00, 1530.40it/s]warmup should be done:  58%|█████▊    | 1738/3000 [00:01<00:00, 1536.21it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1641.41it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1637.97it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1645.01it/s]warmup should be done:  64%|██████▎   | 1907/3000 [00:01<00:00, 1579.44it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1614.17it/s]warmup should be done:  65%|██████▌   | 1963/3000 [00:01<00:00, 1606.17it/s]warmup should be done:  63%|██████▎   | 1878/3000 [00:01<00:00, 1522.19it/s]warmup should be done:  63%|██████▎   | 1892/3000 [00:01<00:00, 1526.35it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1641.13it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1637.76it/s]warmup should be done:  72%|███████▏  | 2157/3000 [00:01<00:00, 1645.91it/s]warmup should be done:  69%|██████▉   | 2066/3000 [00:01<00:00, 1579.83it/s]warmup should be done:  71%|███████   | 2123/3000 [00:01<00:00, 1612.33it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1602.42it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1515.18it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1517.93it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1635.31it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1632.63it/s]warmup should be done:  77%|███████▋  | 2322/3000 [00:01<00:00, 1643.82it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1579.74it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1591.11it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1510.77it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1512.79it/s]warmup should be done:  76%|███████▌  | 2285/3000 [00:01<00:00, 1482.04it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1635.25it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1644.58it/s]warmup should be done:  82%|████████▏ | 2472/3000 [00:01<00:00, 1625.84it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1576.61it/s]warmup should be done:  82%|████████▏ | 2446/3000 [00:01<00:00, 1593.85it/s]warmup should be done:  78%|███████▊  | 2335/3000 [00:01<00:00, 1505.60it/s]warmup should be done:  78%|███████▊  | 2349/3000 [00:01<00:00, 1506.60it/s]warmup should be done:  81%|████████  | 2436/3000 [00:01<00:00, 1437.33it/s]warmup should be done:  88%|████████▊ | 2634/3000 [00:01<00:00, 1635.32it/s]warmup should be done:  88%|████████▊ | 2653/3000 [00:01<00:00, 1647.17it/s]warmup should be done:  88%|████████▊ | 2635/3000 [00:01<00:00, 1622.67it/s]warmup should be done:  85%|████████▍ | 2540/3000 [00:01<00:00, 1563.06it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1604.07it/s]warmup should be done:  83%|████████▎ | 2486/3000 [00:01<00:00, 1502.19it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1502.14it/s]warmup should be done:  86%|████████▋ | 2593/3000 [00:01<00:00, 1472.97it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1634.33it/s]warmup should be done:  94%|█████████▍| 2819/3000 [00:01<00:00, 1649.51it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1619.74it/s]warmup should be done:  90%|████████▉ | 2698/3000 [00:01<00:00, 1567.55it/s]warmup should be done:  92%|█████████▏| 2773/3000 [00:01<00:00, 1613.29it/s]warmup should be done:  88%|████████▊ | 2637/3000 [00:01<00:00, 1499.58it/s]warmup should be done:  88%|████████▊ | 2651/3000 [00:01<00:00, 1499.64it/s]warmup should be done:  92%|█████████▏| 2750/3000 [00:01<00:00, 1500.43it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1640.19it/s]warmup should be done: 100%|█████████▉| 2986/3000 [00:01<00:00, 1652.75it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1621.95it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1567.85it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1625.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1649.84it/s]warmup should be done:  93%|█████████▎| 2788/3000 [00:01<00:00, 1499.99it/s]warmup should be done:  93%|█████████▎| 2801/3000 [00:01<00:00, 1498.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.95it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1531.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1618.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1574.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1571.94it/s]warmup should be done:  98%|█████████▊| 2945/3000 [00:01<00:00, 1520.35it/s]warmup should be done:  99%|█████████▊| 2956/3000 [00:01<00:00, 1512.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1535.24it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1532.99it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.42it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.11it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.44it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.35it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1549.05it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.08it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1594.22it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.47it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1674.65it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1649.99it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.89it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1692.49it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1596.86it/s]warmup should be done:  11%|█         | 320/3000 [00:00<00:01, 1605.27it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.41it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1630.02it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1680.92it/s]warmup should be done:  16%|█▌        | 481/3000 [00:00<00:01, 1602.65it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1651.53it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1667.40it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1696.54it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1625.80it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1649.07it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1631.81it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1679.25it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1666.39it/s]warmup should be done:  23%|██▎       | 680/3000 [00:00<00:01, 1696.69it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1651.22it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1632.75it/s]warmup should be done:  21%|██▏       | 642/3000 [00:00<00:01, 1594.52it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1629.96it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1640.17it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1654.26it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1695.62it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1634.85it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1663.04it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1672.06it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1632.25it/s]warmup should be done:  27%|██▋       | 803/3000 [00:00<00:01, 1596.68it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1637.99it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1657.15it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1696.26it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1639.70it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1663.15it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1673.39it/s]warmup should be done:  32%|███▏      | 964/3000 [00:00<00:01, 1600.12it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1632.10it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1638.74it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1657.43it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1645.58it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1697.49it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1663.12it/s]warmup should be done:  39%|███▉      | 1178/3000 [00:00<00:01, 1676.00it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1635.39it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1643.08it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1477.00it/s]warmup should be done:  44%|████▎     | 1311/3000 [00:00<00:01, 1644.83it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1699.63it/s]warmup should be done:  44%|████▍     | 1327/3000 [00:00<00:01, 1653.86it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1662.35it/s]warmup should be done:  45%|████▍     | 1347/3000 [00:00<00:00, 1678.95it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1632.91it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1607.59it/s]warmup should be done:  42%|████▎     | 1275/3000 [00:00<00:01, 1481.14it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1647.66it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1698.58it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1653.35it/s]warmup should be done:  51%|█████     | 1516/3000 [00:00<00:00, 1681.03it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1660.79it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1635.96it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1610.15it/s]warmup should be done:  48%|████▊     | 1439/3000 [00:00<00:01, 1526.17it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1649.34it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1699.44it/s]warmup should be done:  55%|█████▌    | 1660/3000 [00:01<00:00, 1656.06it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1681.87it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1665.47it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1637.93it/s]warmup should be done:  55%|█████▌    | 1650/3000 [00:01<00:00, 1623.30it/s]warmup should be done:  53%|█████▎    | 1602/3000 [00:01<00:00, 1556.40it/s]warmup should be done:  60%|██████    | 1808/3000 [00:01<00:00, 1649.41it/s]warmup should be done:  62%|██████▏   | 1874/3000 [00:01<00:00, 1700.65it/s]warmup should be done:  61%|██████    | 1826/3000 [00:01<00:00, 1654.42it/s]warmup should be done:  61%|██████▏   | 1840/3000 [00:01<00:00, 1672.40it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1679.24it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1632.04it/s]warmup should be done:  60%|██████    | 1813/3000 [00:01<00:00, 1622.62it/s]warmup should be done:  59%|█████▉    | 1763/3000 [00:01<00:00, 1570.00it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1647.27it/s]warmup should be done:  68%|██████▊   | 2045/3000 [00:01<00:00, 1701.70it/s]warmup should be done:  67%|██████▋   | 2008/3000 [00:01<00:00, 1673.05it/s]warmup should be done:  66%|██████▋   | 1992/3000 [00:01<00:00, 1651.13it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1678.69it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1621.67it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1628.32it/s]warmup should be done:  64%|██████▍   | 1926/3000 [00:01<00:00, 1586.73it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1646.21it/s]warmup should be done:  74%|███████▍  | 2216/3000 [00:01<00:00, 1700.79it/s]warmup should be done:  73%|███████▎  | 2190/3000 [00:01<00:00, 1678.06it/s]warmup should be done:  73%|███████▎  | 2176/3000 [00:01<00:00, 1671.30it/s]warmup should be done:  72%|███████▏  | 2158/3000 [00:01<00:00, 1649.47it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1625.24it/s]warmup should be done:  71%|███████▏  | 2142/3000 [00:01<00:00, 1630.64it/s]warmup should be done:  70%|██████▉   | 2090/3000 [00:01<00:00, 1601.05it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1644.93it/s]warmup should be done:  79%|███████▊  | 2358/3000 [00:01<00:00, 1678.30it/s]warmup should be done:  80%|███████▉  | 2387/3000 [00:01<00:00, 1697.26it/s]warmup should be done:  78%|███████▊  | 2344/3000 [00:01<00:00, 1669.79it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1652.95it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1629.77it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1633.93it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1610.57it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1643.04it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1680.16it/s]warmup should be done:  85%|████████▌ | 2558/3000 [00:01<00:00, 1699.07it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1668.77it/s]warmup should be done:  83%|████████▎ | 2492/3000 [00:01<00:00, 1655.45it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1631.52it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1634.43it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1616.00it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1641.04it/s]warmup should be done:  90%|████████▉ | 2696/3000 [00:01<00:00, 1681.19it/s]warmup should be done:  91%|█████████ | 2729/3000 [00:01<00:00, 1701.29it/s]warmup should be done:  89%|████████▉ | 2678/3000 [00:01<00:00, 1667.82it/s]warmup should be done:  89%|████████▊ | 2658/3000 [00:01<00:00, 1655.98it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1631.64it/s]warmup should be done:  88%|████████▊ | 2635/3000 [00:01<00:00, 1633.29it/s]warmup should be done:  86%|████████▌ | 2581/3000 [00:01<00:00, 1621.68it/s]warmup should be done:  93%|█████████▎| 2798/3000 [00:01<00:00, 1643.41it/s]warmup should be done:  96%|█████████▌| 2865/3000 [00:01<00:00, 1680.29it/s]warmup should be done:  94%|█████████▍| 2824/3000 [00:01<00:00, 1657.13it/s]warmup should be done:  95%|█████████▍| 2845/3000 [00:01<00:00, 1665.05it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1694.57it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1634.53it/s]warmup should be done:  93%|█████████▎| 2799/3000 [00:01<00:00, 1635.11it/s]warmup should be done:  92%|█████████▏| 2745/3000 [00:01<00:00, 1626.96it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.76it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.79it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.37it/s]warmup should be done:  99%|█████████▉| 2963/3000 [00:01<00:00, 1645.00it/s]warmup should be done: 100%|█████████▉| 2991/3000 [00:01<00:00, 1659.42it/s]warmup should be done:  99%|█████████▊| 2958/3000 [00:01<00:00, 1637.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.54it/s]warmup should be done:  99%|█████████▉| 2964/3000 [00:01<00:00, 1637.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.54it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1628.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1588.02it/s]2022-12-12 02:40:52.489106: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f55d802d9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:52.489172: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.025649: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f55440299a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.025709: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.067618: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f726ff93090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.067685: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.426694: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7283830da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.426756: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.648628: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7283834020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.648700: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.689186: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f727f82c7d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.689259: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.728418: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f727f833b00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.728487: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:53.750494: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7277831610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:40:53.750581: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:40:54.738559: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:55.352008: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:55.410152: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:55.677546: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:55.953242: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:56.022954: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:56.033035: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:56.143587: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:40:57.621391: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.275578: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.384906: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.569161: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.882000: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.937882: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.939168: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:40:58.994269: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:41:26.882][ERROR][RK0][tid #140130211849984]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:41:26.882][ERROR][RK0][tid #140130211849984]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:26.889][ERROR][RK0][tid #140130211849984]: coll ps creation done
[HCTR][02:41:26.889][ERROR][RK0][tid #140130211849984]: replica 5 waits for coll ps creation barrier
[HCTR][02:41:26.975][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:41:26.975][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:26.983][ERROR][RK0][main]: coll ps creation done
[HCTR][02:41:26.983][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][02:41:27.009][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:41:27.009][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:27.019][ERROR][RK0][main]: coll ps creation done
[HCTR][02:41:27.019][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:41:27.019][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][02:41:27.019][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:27.024][ERROR][RK0][main]: coll ps creation done
[HCTR][02:41:27.024][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][02:41:27.027][ERROR][RK0][tid #140130069239552]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:41:27.028][ERROR][RK0][tid #140130069239552]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:27.032][ERROR][RK0][tid #140130069239552]: coll ps creation done
[HCTR][02:41:27.032][ERROR][RK0][tid #140130069239552]: replica 1 waits for coll ps creation barrier
[HCTR][02:41:27.072][ERROR][RK0][tid #140130010523392]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:41:27.073][ERROR][RK0][tid #140130010523392]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:27.076][ERROR][RK0][tid #140129935021824]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:41:27.076][ERROR][RK0][tid #140129935021824]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:27.077][ERROR][RK0][tid #140130010523392]: coll ps creation done
[HCTR][02:41:27.077][ERROR][RK0][tid #140130010523392]: replica 2 waits for coll ps creation barrier
[HCTR][02:41:27.083][ERROR][RK0][tid #140129935021824]: coll ps creation done
[HCTR][02:41:27.083][ERROR][RK0][tid #140129935021824]: replica 7 waits for coll ps creation barrier
[HCTR][02:41:27.089][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:41:27.089][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:41:27.093][ERROR][RK0][main]: coll ps creation done
[HCTR][02:41:27.093][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][02:41:27.093][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][02:41:27.956][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][02:41:27.991][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][tid #140130069239552]: replica 1 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][tid #140129935021824]: replica 7 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][tid #140130211849984]: replica 5 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][tid #140130010523392]: replica 2 calling init per replica
[HCTR][02:41:27.991][ERROR][RK0][main]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][main]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][main]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][tid #140130069239552]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][tid #140129935021824]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][tid #140130211849984]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][main]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.991][ERROR][RK0][tid #140130010523392]: Calling build_v2
[HCTR][02:41:27.991][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.991][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.991][ERROR][RK0][tid #140130069239552]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.992][ERROR][RK0][tid #140129935021824]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.992][ERROR][RK0][tid #140130211849984]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.992][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:41:27.992][ERROR][RK0][tid #140130010523392]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 02:41:27[2022-12-12 02:41:272022-12-12 02:41:272022-12-12 02:41:27.[2022-12-12 02:41:272022-12-12 02:41:27.2022-12-12 02:41:27..992076..992075.992075992094: 2022-12-12 02:41:27992090992094: 992075: : E.: : E: EE 992117EE E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136 ::136:136136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136] 136] using concurrent impl MPSPhase] using concurrent impl MPSPhase:] ] using concurrent impl MPSPhase] 
using concurrent impl MPSPhase
136using concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase
] 


using concurrent impl MPSPhase
[2022-12-12 02:41:27.996680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:41:27.996718: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 02:41:27196.] 996726assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:41:27[.2022-12-12 02:41:27996772.: [996770E2022-12-12 02:41:27:  .E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc996786 :: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196E:]  178assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 
:2022-12-12 02:41:27v100x8, slow pcie212.
] 996819build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [
E2022-12-12 02:41:27[ .2022-12-12 02:41:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc996866[.:: 2022-12-12 02:41:27996878178E.: ]  996897E[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  2022-12-12 02:41:27
:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[196 :[9969202022-12-12 02:41:27] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2122022-12-12 02:41:27: .assigning 8 to cpu:] [.E996965
213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 02:41:27996979 : ] 
.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[Eremote time is 8.68421997011E:2022-12-12 02:41:27[2022-12-12 02:41:27 
:  178.2022-12-12 02:41:27./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 997065v100x8, slow pcie.997072: :2022-12-12 02:41:27: 
997093: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196.E: E] [:] 997131 E v100x8, slow pcie2022-12-12 02:41:27178assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.] 
v100x8, slow pcieE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:997182[
 178:212: 2022-12-12 02:41:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 213[] E.:v100x8, slow pcie] [2022-12-12 02:41:27build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 997256214
remote time is 8.684212022-12-12 02:41:27.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 
.997305[997285:Ecpu time is 97.0588[: [2022-12-12 02:41:27: 196 
2022-12-12 02:41:27E2022-12-12 02:41:27.E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. .997353 assigning 8 to cpu:997370/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc997382: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
196: :: E:] E212E 196assigning 8 to cpu ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:assigning 8 to cpu:2022-12-12 02:41:27
:196
213.214] [] 997563[] assigning 8 to cpu2022-12-12 02:41:27remote time is 8.68421: 2022-12-12 02:41:27cpu time is 97.0588
.
E.[
997614 997623[2022-12-12 02:41:27: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [2022-12-12 02:41:27.E:E2022-12-12 02:41:27.997664 212 .997682: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc997702: E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:: E 212
213E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 02:41:27212

:214.] 212] [997829[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] cpu time is 97.05882022-12-12 02:41:27: 2022-12-12 02:41:27
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
.E.
997873[ 997877: 2022-12-12 02:41:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: E.:2022-12-12 02:41:27E 997928213. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 997948/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:Eremote time is 8.68421: :214 
E213] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] [cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212022-12-12 02:41:27
213:
.] 213998035[remote time is 8.68421] : 2022-12-12 02:41:27
remote time is 8.68421E.
[ 9980922022-12-12 02:41:27[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .2022-12-12 02:41:27:E998124.214 : 998138] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: cpu time is 97.0588: E
214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588214:
] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 02:42:45.849908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:42:45.890029: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:42:45.890117: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:42:45.891095: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 02:42:45.964944: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 02:42:46.348144: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 02:42:46.348233: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 02:42:53.135674: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 02:42:53.135780: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 02:42:54.802495: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 02:42:54.802591: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 02:42:54.805575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 02:42:54.805632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 02:42:55. 62713: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 02:42:55. 91467: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 02:42:55. 92898: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 02:42:55.113159: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 02:42:55.632281: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 02:42:55.634543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 02:42:55.637555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 02:42:55.640480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 02:42:55.643364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 02:42:55.646264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 02:42:55.649119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 02:42:55.652013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 02:42:55.654856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 02:44:25.650060: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 02:44:25.658902: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 02:44:25.662411: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 02:44:25.710639: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:44:25.710742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:44:25.710776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:44:25.710807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:44:25.711374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:44:25.711430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.712345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.712995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.726115: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 02:44:25.726191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 02:44:25.726347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 02:44:25.726422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 02:44:25.726521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 02:44:25.726583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 02:44:25.726638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:44:25.726693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.726743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 02:44:25.726802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 02:44:25.726869: E[ 2022-12-12 02:44:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7268681815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 02:44:25.726960[: 2022-12-12 02:44:25E. 726965/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuworker 0 thread 2 initing device 2[:
2022-12-12 02:44:251980.] 727014eager alloc mem 381.47 MB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:44:25.727084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.727244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:44:25.[7272742022-12-12 02:44:25: .E727299 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[E:2022-12-12 02:44:25 202./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 7273166 solved:: 
1980E]  eager alloc mem 381.47 MB[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
2022-12-12 02:44:25:.202727404] : E5 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 02:44:25[205.2022-12-12 02:44:25] 727455.worker 0 thread 6 initing device 6: 727462
E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1815:] 205Building Coll Cache with ... num gpu device is 8] 
worker 0 thread 5 initing device 5
[2022-12-12 02:44:25.727572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.727888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:44:25.727934: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.727964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:44:25.728011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.731052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.731306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.731525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.732027: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.732080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.732137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.732653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.735444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.735698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.735807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.735861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.736357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.736403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.736464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:44:25.788403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 02:44:25.793998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:44:25.794134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:44:25.794958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.795615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:25.796665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:25.796713: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.03 MB
[2022-12-12 02:44:25.816664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[2022-12-12 02:44:252022-12-12 02:44:25[[[2022-12-12 02:44:25..2022-12-12 02:44:252022-12-12 02:44:252022-12-12 02:44:25.816728816728...816731: : 816738816738816738: EE: : : E  EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:19801980:::1980] ] 198019801980] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes] ] ] eager alloc mem 1024.00 Bytes

eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 02:44:25.823525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:44:25.823623[: 2022-12-12 02:44:25E. 823616/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1024
[2022-12-12 02:44:25.823690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:44:25.823743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:44:25.823772[: 2022-12-12 02:44:25E. 823777/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1024
[2022-12-12 02:44:25.823874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 02:44:25] .eager release cuda mem 400000000823873
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 02:44:25.823952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 02:44:25638.] 823983eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:44:25.824041[: 2022-12-12 02:44:25E. 824033/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 1024
[2022-12-12 02:44:25.824133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:44:25.824723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.825394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.825447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.826017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.826061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.76 GB
[2022-12-12 02:44:25.826295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.827085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.827668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.833878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.834455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 38.53 MB
[2022-12-12 02:44:25.835198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:25.835580: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:25.835846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:25.836140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:25.836183: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 02:44:25.836216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:25.836235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 02:44:251980.] 836270eager alloc mem 611.00 KB: 
W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 02:44:25WORKER[0] alloc host memory 38.15 MB.
836288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:25.836593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:25.836639: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.10 MB
[2022-12-12 02:44:25.836859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:25.836904: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.09 MB
[2022-12-12 02:44:25.837155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:25.837202: W[ 2022-12-12 02:44:25/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.:43] 837208: WORKER[0] alloc host memory 38.10 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 02:44:25
.837262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:25.837295: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] [[WORKER[0] alloc host memory 38.13 MB2022-12-12 02:44:252022-12-12 02:44:25
..837311837314: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 625663WORKER[0] alloc host memory 38.14 MB

[2022-12-12 02:44:25.837387: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 38.15 MB
[2022-12-12 02:44:25.861447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.861576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.861795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.862053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.862097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:44:25.862168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.862209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:44:25.862411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.862456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:44:25.863146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.863190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.863453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.863745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.863790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 02:44:25eager alloc mem 4.77 GB.
863802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.863866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:44:25.863968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:44:25.864041: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.864081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[2022-12-12 02:44:25.864569: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:44:25.864614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.77 GB
[[[[[[[[2022-12-12 02:44:272022-12-12 02:44:272022-12-12 02:44:272022-12-12 02:44:272022-12-12 02:44:272022-12-12 02:44:272022-12-12 02:44:27.2022-12-12 02:44:27......520287.520287520287520287520288520287520293: 520289: : : : : : E: EEEEEE E      /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::] :192619261926192619261926Device 5 init p2p of link 61926] ] ] ] ] ] 
] Device 1 init p2p of link 7Device 3 init p2p of link 2Device 4 init p2p of link 5Device 0 init p2p of link 3Device 2 init p2p of link 1Device 6 init p2p of link 0
Device 7 init p2p of link 4





[2022-12-12 02:44:27.520905[: 2022-12-12 02:44:27E. 520913/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: [[[:E2022-12-12 02:44:272022-12-12 02:44:272022-12-12 02:44:27[1980[ ...2022-12-12 02:44:27] 2022-12-12 02:44:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu520927520928520925.eager alloc mem 611.00 KB.:: : [: 520938
5209421980EE2022-12-12 02:44:27E: : ]   . EEeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu520983/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  
::: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980E1980::] ]  ] 19801980eager alloc mem 611.00 KBeager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] ] 

:
eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980

] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.521893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.521925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.[[5220522022-12-12 02:44:272022-12-12 02:44:27: ..[[E5220555220562022-12-12 02:44:272022-12-12 02:44:27 : : ../hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccEE522076: 522075:  [E: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:44:27 E] ::./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc eager release cuda mem 625663638638522142:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
] ] : 638:eager release cuda mem 625663eager release cuda mem 625663E] 638

 eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
eager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 02:44:27.537536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:44:27.537699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.537737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:44:27.537901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.538575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27[.2022-12-12 02:44:27538751.: 538770E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1926:] 638Device 5 init p2p of link 4] 
eager release cuda mem 625663
[2022-12-12 02:44:27.538969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.539226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 02:44:27.539392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.539555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 02:44:27.539656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 02:44:27.539716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 02:44:272022-12-12 02:44:27..[5398005398152022-12-12 02:44:27: : .EE539826  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:: 19261980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] :Device 0 init p2p of link 6eager alloc mem 611.00 KB638

] eager release cuda mem 625663
[2022-12-12 02:44:27.540059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.540263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.540460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 02:44:27.540566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.540613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.540755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.540880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.541432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.554385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:44:27.554502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.554769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:44:27.554891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.555163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:44:27.555285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.555367[: 2022-12-12 02:44:27E. 555367/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1926] Device 0 init p2p of link 1
[2022-12-12 02:44:27.555456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:44:27.555513: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.555588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.555749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 02:44:27] .eager release cuda mem 625663555759
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 02:44:27.555810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 02:44:27.555894: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 02:44:27eager alloc mem 611.00 KB.[
5558992022-12-12 02:44:27: .E555930 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :Device 3 init p2p of link 51980
] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.556122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 02:44:27.1980556138] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.556365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.556445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.556727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.556829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.556980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.575432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:44:27.575552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.576349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 02:44:27.576398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.576471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.576595: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:44:27.576707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.577313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.577557: [E2022-12-12 02:44:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc577557:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1926] Device 7 init p2p of link 5
[2022-12-12 02:44:27.577710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.577784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:44:27.577851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 02:44:27.577898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.577976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.578171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:44:27.578288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 02:44:27
.578296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 02:44:27.578420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:44:27.578556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.578736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.578822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.579099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.579242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:44:27.596346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.596483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.596846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9995863 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29956911 / 100000000 nodes ( 29.96 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.86977 secs 
[2022-12-12 02:44:27.597125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9999875 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29952899 / 100000000 nodes ( 29.95 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.86912 secs 
[2022-12-12 02:44:27.597740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.598211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.598724: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.598892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.598947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.599451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9999367 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29953407 / 100000000 nodes ( 29.95 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.87216 secs 
[2022-12-12 02:44:27.599879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 40400000
[2022-12-12 02:44:27.600340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9999700 / 100000000 nodes ( 10.00 %~10.00 %) | remote 29953074 / 100000000 nodes ( 29.95 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.87339 secs 
[2022-12-12 02:44:27.600713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9988331 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29964443 / 100000000 nodes ( 29.96 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.87279 secs 
[2022-12-12 02:44:27.600831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9986296 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29966478 / 100000000 nodes ( 29.97 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.87416 secs 
[2022-12-12 02:44:27.600951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9987086 / 100000000 nodes ( 9.99 %~10.00 %) | remote 29965688 / 100000000 nodes ( 29.97 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.77 GB | 1.87339 secs 
[2022-12-12 02:44:27.603395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 9968707 / 100000000 nodes ( 9.97 %~10.00 %) | remote 29984067 / 100000000 nodes ( 29.98 %) | cpu 60047226 / 100000000 nodes ( 60.05 %) | 4.76 GB | 1.89198 secs 
[2022-12-12 02:44:27.604574: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.40 GB
[2022-12-12 02:44:29. 73367: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.66 GB
[2022-12-12 02:44:29. 74014: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.66 GB
[2022-12-12 02:44:29. 76402: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.66 GB
[2022-12-12 02:44:30.715098: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.92 GB
[2022-12-12 02:44:30.716271: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.92 GB
[2022-12-12 02:44:30.717059: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.92 GB
[2022-12-12 02:44:31.916626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 12.14 GB
[2022-12-12 02:44:31.916911: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 12.14 GB
[2022-12-12 02:44:31.917277: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 12.14 GB
[2022-12-12 02:44:33.414593: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 12.35 GB
[2022-12-12 02:44:33.416846: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 12.35 GB
[2022-12-12 02:44:33.422490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 12.35 GB
[2022-12-12 02:44:33.423042: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 12.35 GB
[2022-12-12 02:44:33.425054: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 12.35 GB
[2022-12-12 02:44:34.870249: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 12.55 GB
[2022-12-12 02:44:34.870396: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 12.55 GB
[HCTR][02:44:34.870][ERROR][RK0][tid #140130010523392]: replica 2 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][tid #140129935021824]: replica 7 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][tid #140130069239552]: replica 1 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][tid #140130211849984]: replica 5 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][tid #140129935021824]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][tid #140130010523392]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][tid #140130069239552]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][tid #140130211849984]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:44:34.870][ERROR][RK0][main]: init per replica done
[HCTR][02:44:34.870][ERROR][RK0][tid #140129935021824]: init per replica done
[HCTR][02:44:34.870][ERROR][RK0][tid #140130010523392]: init per replica done
[HCTR][02:44:34.870][ERROR][RK0][tid #140130069239552]: init per replica done
[HCTR][02:44:34.870][ERROR][RK0][main]: init per replica done
[HCTR][02:44:34.870][ERROR][RK0][tid #140130211849984]: init per replica done
[HCTR][02:44:34.870][ERROR][RK0][main]: init per replica done
[HCTR][02:44:34.873][ERROR][RK0][main]: init per replica done
[HCTR][02:44:34.876][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7471d20000
[HCTR][02:44:34.876][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7471d20000
[HCTR][02:44:34.876][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7472200000
[HCTR][02:44:34.876][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7472200000
[HCTR][02:44:34.876][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7472840000
[HCTR][02:44:34.876][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7472840000
[HCTR][02:44:34.876][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f7472b60000
[HCTR][02:44:34.876][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7472b60000
[HCTR][02:44:34.876][ERROR][RK0][tid #140129943414528]: 3 allocated 3276800 at 0x7f7471d20000
[HCTR][02:44:34.876][ERROR][RK0][tid #140130069239552]: 6 allocated 3276800 at 0x7f746fd20000
[HCTR][02:44:34.876][ERROR][RK0][tid #140129943414528]: 3 allocated 6553600 at 0x7f7472200000
[HCTR][02:44:34.876][ERROR][RK0][tid #140129943414528]: 3 allocated 3276800 at 0x7f7472840000
[HCTR][02:44:34.876][ERROR][RK0][tid #140130069239552]: 6 allocated 6553600 at 0x7f7470200000
[HCTR][02:44:34.876][ERROR][RK0][tid #140129943414528]: 3 allocated 6553600 at 0x7f7472b60000
[HCTR][02:44:34.876][ERROR][RK0][tid #140130069239552]: 6 allocated 3276800 at 0x7f7470840000
[HCTR][02:44:34.877][ERROR][RK0][tid #140130069239552]: 6 allocated 6553600 at 0x7f7470b60000
[HCTR][02:44:34.877][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f746fd20000
[HCTR][02:44:34.877][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f7471d20000
[HCTR][02:44:34.877][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f7470200000
[HCTR][02:44:34.877][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f7472200000
[HCTR][02:44:34.877][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f7470840000
[HCTR][02:44:34.877][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f7472840000
[HCTR][02:44:34.877][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f7470b60000
[HCTR][02:44:34.877][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f7472b60000
[HCTR][02:44:34.877][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f7471d20000
[HCTR][02:44:34.877][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f7472200000
[HCTR][02:44:34.877][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f7472840000
[HCTR][02:44:34.877][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f7472b60000
[HCTR][02:44:34.880][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f7474520000
[HCTR][02:44:34.880][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f7474a00000
[HCTR][02:44:34.880][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f747570e800
[HCTR][02:44:34.880][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f7475a2e800








