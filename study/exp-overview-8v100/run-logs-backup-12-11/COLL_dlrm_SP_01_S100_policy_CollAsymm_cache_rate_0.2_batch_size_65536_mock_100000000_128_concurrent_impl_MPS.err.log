2022-12-12 05:07:48.450302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.463224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.472613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.476804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.489131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.494519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.506550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.519438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.552172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.553266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.554260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.555319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.556273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.557258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.558250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.559338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.568876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.569143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.570345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.570698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.571955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.572383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.574099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.574285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.575982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.576006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.576021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.577958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.578058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.578178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.580146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.580293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.580553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.582136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.582694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.583185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.584733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.586025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.586744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.586996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.588052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.589007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.589555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.591094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.592068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.593046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.593432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.594127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.594806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.596190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.596778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.597159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.597608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.598990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.599665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.600092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.600130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.600656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.601507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.602365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.602453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.602616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.603524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.604228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.605118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.605179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.605309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.605864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.606361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.607245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.608244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.608263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.608495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.609092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.609768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.611111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.611498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.611651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.611967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.612727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.613723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.614330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.614475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.614606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.615567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.616324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.617262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.617359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.618766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.619197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.620050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.620845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.621075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.621668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.622606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.622949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.623574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.624298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.624821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.625315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.625828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.626323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.626819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.627321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.628341: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.628653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.629245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.629794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.630314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.630512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.631188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.631532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.632244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.632469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.633541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.633543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.634646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.634722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.635576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.636112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.636626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.637121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.637530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.637643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.638480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.638572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.639433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.639507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.639950: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.640432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.641363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.641856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.642635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.643583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.644135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.645214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.646423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.646459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.648121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.648131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.648962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.649928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.649927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.651025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.651908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.652097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.653018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.653616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.653900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.654799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.655453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.655899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.658636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.659208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.659912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.661206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.661359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.661761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.662630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.664382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.664663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.664918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.665190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.666733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.701701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.701996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.702227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.702696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.702928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.704312: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.705553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.706547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.706786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.707307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.707570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.710452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.711231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.711626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.712949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.713134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.713618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.715365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.716159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.717012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.717482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.717794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.718307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.720715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.721557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.722614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.722887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.723234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.723529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.725688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.726626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.727599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.727781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.729055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.730468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.732473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.733743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.733977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.735064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.736256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.737158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.738307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.738353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.740356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.741114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.742097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.771780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.772069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.773498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.774145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.776591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.777811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.778156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.780264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.780686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.781966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.782894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.783944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.785621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.785961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.788255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.789082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.789626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.791241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.792211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.792904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.793826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.794303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.797140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.797443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.798270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.798626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.799681: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.801789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.802663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.802831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.803230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.806374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.807453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.808704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.808997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.809038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.824555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.825481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.826490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.826743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.826799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.829423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.830395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.831780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.832006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.833711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.836558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.865527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.866591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.867525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.869933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.870977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.873035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.874741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.878660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.879738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.880604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.880717: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.884184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.885167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.886308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.889535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.890093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.891003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.892254: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.894499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.898739: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.899949: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:07:48.901162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.901291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.905972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.907723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.908688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.910333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.912985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.914363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.919375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:48.921028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.032610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.033232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.034376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.034862: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.034916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:07:50.053794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.055030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.055741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.056318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.056841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.057314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:07:50.089248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.090132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.091096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.091680: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.091748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:07:50.103919: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.104117: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.110397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.111058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.111593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.112400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.113295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.113785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:07:50.140374: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:07:50.158457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.159142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.159905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.160791: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.160848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:07:50.178644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.180094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.180957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.181708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.182232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.182714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:07:50.202173: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.202383: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.204121: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:07:50.247153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.247777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.248309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.248785: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.248835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:07:50.265959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.267080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.267628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.268203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.268727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.269440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:07:50.273393: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.273566: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.274513: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:07:50.282074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.282693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.283226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.283703: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.283754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:07:50.300478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.301100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.301615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.302175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.303106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.303856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:07:50.309656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.309674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.311058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.311101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.312207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.312269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.313208: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.313266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:07:50.313298: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.313350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:07:50.318115: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.318287: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.319206: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:07:50.323337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.323935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.324460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.324928: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:07:50.324974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:07:50.330921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.331037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.332150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.332234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.333258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.333426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.334218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.334517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.335148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.336027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.336570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:07:50.336802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:07:50.341968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.342722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.343306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.343909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.344436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:07:50.344904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:07:50.349429: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.349605: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.351283: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 05:07:50.381598: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.381792: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.383472: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:07:50.383510: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.383707: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.385516: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:07:50.394760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.394923: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:07:50.396649: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][05:07:51.658][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.658][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.659][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.663][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.663][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.663][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.664][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:07:51.664][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 99it [00:01, 83.08it/s]warmup run: 98it [00:01, 82.71it/s]warmup run: 101it [00:01, 85.68it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 199it [00:01, 181.37it/s]warmup run: 1it [00:01,  1.57s/it]warmup run: 196it [00:01, 179.35it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 201it [00:01, 184.70it/s]warmup run: 101it [00:01, 87.67it/s]warmup run: 299it [00:01, 290.01it/s]warmup run: 99it [00:01, 82.34it/s]warmup run: 293it [00:01, 284.81it/s]warmup run: 96it [00:01, 83.54it/s]warmup run: 302it [00:01, 294.96it/s]warmup run: 203it [00:01, 190.65it/s]warmup run: 399it [00:01, 402.91it/s]warmup run: 200it [00:01, 181.10it/s]warmup run: 1it [00:01,  1.45s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 391it [00:01, 395.65it/s]warmup run: 192it [00:01, 180.72it/s]warmup run: 402it [00:01, 408.09it/s]warmup run: 303it [00:01, 301.00it/s]warmup run: 498it [00:02, 511.00it/s]warmup run: 302it [00:01, 291.53it/s]warmup run: 98it [00:01, 87.50it/s]warmup run: 100it [00:01, 87.58it/s]warmup run: 489it [00:02, 504.14it/s]warmup run: 288it [00:01, 287.13it/s]warmup run: 503it [00:02, 519.88it/s]warmup run: 397it [00:01, 404.94it/s]warmup run: 598it [00:02, 612.54it/s]warmup run: 404it [00:01, 406.42it/s]warmup run: 194it [00:01, 186.47it/s]warmup run: 201it [00:01, 190.26it/s]warmup run: 589it [00:02, 607.04it/s]warmup run: 384it [00:01, 396.60it/s]warmup run: 606it [00:02, 625.86it/s]warmup run: 490it [00:01, 503.55it/s]warmup run: 699it [00:02, 702.66it/s]warmup run: 506it [00:02, 519.52it/s]warmup run: 291it [00:01, 295.66it/s]warmup run: 303it [00:01, 304.09it/s]warmup run: 689it [00:02, 696.42it/s]warmup run: 479it [00:01, 501.18it/s]warmup run: 709it [00:02, 717.95it/s]warmup run: 584it [00:02, 596.17it/s]warmup run: 798it [00:02, 771.18it/s]warmup run: 610it [00:02, 627.02it/s]warmup run: 391it [00:01, 412.01it/s]warmup run: 405it [00:01, 420.89it/s]warmup run: 789it [00:02, 770.91it/s]warmup run: 575it [00:02, 597.98it/s]warmup run: 811it [00:02, 792.16it/s]warmup run: 679it [00:02, 677.24it/s]warmup run: 896it [00:02, 824.14it/s]warmup run: 714it [00:02, 720.36it/s]warmup run: 491it [00:01, 523.79it/s]warmup run: 502it [00:01, 524.43it/s]warmup run: 889it [00:02, 829.01it/s]warmup run: 669it [00:02, 677.45it/s]warmup run: 912it [00:02, 848.35it/s]warmup run: 774it [00:02, 744.45it/s]warmup run: 994it [00:02, 862.44it/s]warmup run: 814it [00:02, 786.77it/s]warmup run: 592it [00:02, 627.42it/s]warmup run: 605it [00:02, 630.96it/s]warmup run: 988it [00:02, 871.68it/s]warmup run: 771it [00:02, 762.58it/s]warmup run: 1014it [00:02, 893.12it/s]warmup run: 869it [00:02, 796.13it/s]warmup run: 1094it [00:02, 900.10it/s]warmup run: 914it [00:02, 833.51it/s]warmup run: 692it [00:02, 714.60it/s]warmup run: 708it [00:02, 723.43it/s]warmup run: 1087it [00:02, 902.97it/s]warmup run: 872it [00:02, 825.51it/s]warmup run: 1115it [00:02, 924.27it/s]warmup run: 963it [00:02, 832.87it/s]warmup run: 1195it [00:02, 929.35it/s]warmup run: 1013it [00:02, 872.61it/s]warmup run: 791it [00:02, 782.99it/s]warmup run: 810it [00:02, 795.59it/s]warmup run: 1186it [00:02, 919.55it/s]warmup run: 971it [00:02, 869.95it/s]warmup run: 1218it [00:02, 952.49it/s]warmup run: 1058it [00:02, 863.27it/s]warmup run: 1296it [00:02, 950.41it/s]warmup run: 1113it [00:02, 907.78it/s]warmup run: 889it [00:02, 834.48it/s]warmup run: 912it [00:02, 852.93it/s]warmup run: 1284it [00:02, 926.97it/s]warmup run: 1070it [00:02, 901.06it/s]warmup run: 1320it [00:02, 970.94it/s]warmup run: 1153it [00:02, 886.54it/s]warmup run: 1397it [00:02, 966.33it/s]warmup run: 1212it [00:02, 930.85it/s]warmup run: 987it [00:02, 867.97it/s]warmup run: 1014it [00:02, 896.80it/s]warmup run: 1381it [00:02, 939.09it/s]warmup run: 1169it [00:02, 925.04it/s]warmup run: 1422it [00:02, 981.06it/s]warmup run: 1249it [00:02, 906.08it/s]warmup run: 1497it [00:03, 968.77it/s]warmup run: 1313it [00:02, 953.00it/s]warmup run: 1085it [00:02, 897.06it/s]warmup run: 1115it [00:02, 919.17it/s]warmup run: 1478it [00:03, 935.29it/s]warmup run: 1268it [00:02, 942.44it/s]warmup run: 1523it [00:03, 984.13it/s]warmup run: 1348it [00:02, 930.43it/s]warmup run: 1597it [00:03, 976.03it/s]warmup run: 1414it [00:02, 967.59it/s]warmup run: 1182it [00:02, 914.13it/s]warmup run: 1215it [00:02, 937.79it/s]warmup run: 1574it [00:03, 932.40it/s]warmup run: 1367it [00:02, 955.46it/s]warmup run: 1447it [00:03, 943.56it/s]warmup run: 1624it [00:03, 981.29it/s]warmup run: 1697it [00:03, 980.75it/s]warmup run: 1515it [00:03, 977.24it/s]warmup run: 1279it [00:02, 924.15it/s]warmup run: 1316it [00:02, 958.39it/s]warmup run: 1669it [00:03, 928.80it/s]warmup run: 1466it [00:02, 965.33it/s]warmup run: 1544it [00:03, 945.59it/s]warmup run: 1724it [00:03, 974.92it/s]warmup run: 1799it [00:03, 989.84it/s]warmup run: 1616it [00:03, 984.50it/s]warmup run: 1377it [00:02, 937.77it/s]warmup run: 1768it [00:03, 944.32it/s]warmup run: 1416it [00:02, 923.33it/s]warmup run: 1565it [00:03, 970.00it/s]warmup run: 1640it [00:03, 949.68it/s]warmup run: 1823it [00:03, 969.72it/s]warmup run: 1900it [00:03, 995.20it/s]warmup run: 1717it [00:03, 991.16it/s]warmup run: 1479it [00:02, 961.24it/s]warmup run: 1871it [00:03, 968.11it/s]warmup run: 1514it [00:03, 939.19it/s]warmup run: 1665it [00:03, 977.98it/s]warmup run: 1736it [00:03, 951.41it/s]warmup run: 1921it [00:03, 965.53it/s]warmup run: 2001it [00:03, 993.48it/s]warmup run: 1818it [00:03, 996.16it/s]warmup run: 1582it [00:03, 979.09it/s]warmup run: 1971it [00:03, 976.59it/s]warmup run: 1615it [00:03, 959.15it/s]warmup run: 1764it [00:03, 980.70it/s]warmup run: 1832it [00:03, 950.85it/s]warmup run: 2021it [00:03, 975.58it/s]warmup run: 2122it [00:03, 1057.09it/s]warmup run: 1919it [00:03, 997.51it/s]warmup run: 1685it [00:03, 991.65it/s]warmup run: 2084it [00:03, 1021.92it/s]warmup run: 1719it [00:03, 980.60it/s]warmup run: 1865it [00:03, 987.47it/s]warmup run: 1928it [00:03, 948.37it/s]warmup run: 2142it [00:03, 1044.51it/s]warmup run: 2244it [00:03, 1104.22it/s]warmup run: 2023it [00:03, 1008.96it/s]warmup run: 1788it [00:03, 1002.40it/s]warmup run: 2205it [00:03, 1077.59it/s]warmup run: 1819it [00:03, 985.28it/s]warmup run: 1966it [00:03, 991.67it/s]warmup run: 2028it [00:03, 961.68it/s]warmup run: 2263it [00:03, 1093.24it/s]warmup run: 2366it [00:03, 1137.35it/s]warmup run: 2142it [00:03, 1060.37it/s]warmup run: 1891it [00:03, 1007.62it/s]warmup run: 2327it [00:03, 1117.43it/s]warmup run: 1919it [00:03, 975.24it/s]warmup run: 2076it [00:03, 1022.03it/s]warmup run: 2146it [00:03, 1026.03it/s]warmup run: 2384it [00:03, 1127.24it/s]warmup run: 2487it [00:03, 1158.47it/s]warmup run: 2258it [00:03, 1088.28it/s]warmup run: 1994it [00:03, 1013.35it/s]warmup run: 2449it [00:03, 1145.45it/s]warmup run: 2018it [00:03, 978.68it/s]warmup run: 2197it [00:03, 1077.05it/s]warmup run: 2264it [00:03, 1070.24it/s]warmup run: 2505it [00:03, 1151.86it/s]warmup run: 2609it [00:04, 1174.32it/s]warmup run: 2377it [00:03, 1117.66it/s]warmup run: 2113it [00:03, 1065.67it/s]warmup run: 2569it [00:04, 1159.71it/s]warmup run: 2134it [00:03, 1031.74it/s]warmup run: 2318it [00:03, 1116.18it/s]warmup run: 2382it [00:03, 1101.10it/s]warmup run: 2627it [00:04, 1169.69it/s]warmup run: 2727it [00:04, 1173.17it/s]warmup run: 2497it [00:03, 1139.44it/s]warmup run: 2234it [00:03, 1108.34it/s]warmup run: 2688it [00:04, 1166.62it/s]warmup run: 2250it [00:03, 1069.35it/s]warmup run: 2439it [00:03, 1143.61it/s]warmup run: 2500it [00:04, 1124.01it/s]warmup run: 2748it [00:04, 1178.96it/s]warmup run: 2849it [00:04, 1184.45it/s]warmup run: 2617it [00:04, 1155.57it/s]warmup run: 2355it [00:03, 1138.70it/s]warmup run: 2808it [00:04, 1175.35it/s]warmup run: 2367it [00:03, 1097.67it/s]warmup run: 2560it [00:04, 1163.31it/s]warmup run: 2619it [00:04, 1140.99it/s]warmup run: 2870it [00:04, 1188.58it/s]warmup run: 2970it [00:04, 1191.57it/s]warmup run: 2736it [00:04, 1164.36it/s]warmup run: 2477it [00:03, 1160.79it/s]warmup run: 3000it [00:04, 680.52it/s] warmup run: 2928it [00:04, 1181.21it/s]warmup run: 2484it [00:03, 1117.74it/s]warmup run: 2680it [00:04, 1172.75it/s]warmup run: 2736it [00:04, 1148.22it/s]warmup run: 2991it [00:04, 1193.37it/s]warmup run: 3000it [00:04, 685.45it/s] warmup run: 2857it [00:04, 1175.59it/s]warmup run: 3000it [00:04, 675.44it/s] warmup run: 2595it [00:03, 1165.70it/s]warmup run: 2600it [00:04, 1129.80it/s]warmup run: 2800it [00:04, 1180.71it/s]warmup run: 2854it [00:04, 1156.46it/s]warmup run: 2978it [00:04, 1184.28it/s]warmup run: 3000it [00:04, 679.59it/s] warmup run: 2712it [00:04, 1139.38it/s]warmup run: 2716it [00:04, 1137.01it/s]warmup run: 2921it [00:04, 1188.05it/s]warmup run: 2973it [00:04, 1164.70it/s]warmup run: 3000it [00:04, 674.73it/s] warmup run: 3000it [00:04, 685.47it/s] warmup run: 2827it [00:04, 1130.18it/s]warmup run: 2836it [00:04, 1155.04it/s]warmup run: 2941it [00:04, 1127.13it/s]warmup run: 2957it [00:04, 1169.43it/s]warmup run: 3000it [00:04, 687.49it/s] warmup run: 3000it [00:04, 690.94it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1647.24it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.42it/s]warmup should be done:   5%|▌         | 155/3000 [00:00<00:01, 1544.80it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.25it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.48it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.51it/s]warmup should be done:   4%|▎         | 108/3000 [00:00<00:02, 1071.36it/s]warmup should be done:   4%|▎         | 108/3000 [00:00<00:02, 1070.75it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.83it/s]warmup should be done:  10%|█         | 311/3000 [00:00<00:01, 1550.49it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1640.67it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1662.65it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.57it/s]warmup should be done:   8%|▊         | 228/3000 [00:00<00:02, 1145.67it/s]warmup should be done:   8%|▊         | 228/3000 [00:00<00:02, 1144.20it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1636.09it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.05it/s]warmup should be done:  13%|█▎        | 394/3000 [00:00<00:01, 1379.71it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1648.15it/s]warmup should be done:  13%|█▎        | 393/3000 [00:00<00:01, 1374.37it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1639.89it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.99it/s]warmup should be done:  16%|█▌        | 467/3000 [00:00<00:01, 1548.11it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1633.67it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1666.80it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1650.62it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1639.17it/s]warmup should be done:  19%|█▊        | 559/3000 [00:00<00:01, 1483.05it/s]warmup should be done:  19%|█▊        | 558/3000 [00:00<00:01, 1479.52it/s]warmup should be done:  21%|██        | 622/3000 [00:00<00:01, 1544.55it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1657.91it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1631.48it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1664.72it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1636.60it/s]warmup should be done:  24%|██▍       | 725/3000 [00:00<00:01, 1543.61it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1649.58it/s]warmup should be done:  24%|██▍       | 724/3000 [00:00<00:01, 1542.17it/s]warmup should be done:  26%|██▌       | 777/3000 [00:00<00:01, 1544.47it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1655.32it/s]warmup should be done:  27%|██▋       | 822/3000 [00:00<00:01, 1626.08it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1659.86it/s]warmup should be done:  30%|██▉       | 886/3000 [00:00<00:01, 1562.97it/s]warmup should be done:  30%|██▉       | 889/3000 [00:00<00:01, 1575.46it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1644.23it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1630.30it/s]warmup should be done:  31%|███       | 932/3000 [00:00<00:01, 1542.72it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1622.45it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1635.39it/s]warmup should be done:  35%|███▍      | 1046/3000 [00:00<00:01, 1574.06it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1660.05it/s]warmup should be done:  35%|███▌      | 1053/3000 [00:00<00:01, 1595.24it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1644.58it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1630.84it/s]warmup should be done:  36%|███▌      | 1087/3000 [00:00<00:01, 1538.67it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1632.80it/s]warmup should be done:  39%|███▉      | 1164/3000 [00:00<00:01, 1638.13it/s]warmup should be done:  40%|████      | 1207/3000 [00:00<00:01, 1583.89it/s]warmup should be done:  41%|████      | 1218/3000 [00:00<00:01, 1611.10it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1644.31it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1658.76it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1630.59it/s]warmup should be done:  41%|████▏     | 1241/3000 [00:00<00:01, 1538.98it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1638.55it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1639.67it/s]warmup should be done:  46%|████▌     | 1368/3000 [00:00<00:01, 1590.96it/s]warmup should be done:  46%|████▌     | 1383/3000 [00:00<00:00, 1621.56it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1658.54it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1644.84it/s]warmup should be done:  46%|████▋     | 1395/3000 [00:00<00:01, 1537.79it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1629.90it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1641.93it/s]warmup should be done:  50%|████▉     | 1493/3000 [00:00<00:00, 1639.41it/s]warmup should be done:  51%|█████     | 1529/3000 [00:01<00:00, 1595.15it/s]warmup should be done:  56%|█████▌    | 1668/3000 [00:01<00:00, 1658.78it/s]warmup should be done:  52%|█████▏    | 1548/3000 [00:01<00:00, 1627.50it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1644.72it/s]warmup should be done:  52%|█████▏    | 1549/3000 [00:01<00:00, 1537.19it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1630.06it/s]warmup should be done:  55%|█████▍    | 1648/3000 [00:01<00:00, 1645.00it/s]warmup should be done:  55%|█████▌    | 1658/3000 [00:01<00:00, 1640.42it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1598.83it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1628.99it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1643.64it/s]warmup should be done:  61%|██████    | 1834/3000 [00:01<00:00, 1654.62it/s]warmup should be done:  57%|█████▋    | 1704/3000 [00:01<00:00, 1538.10it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1629.99it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1647.66it/s]warmup should be done:  61%|██████    | 1823/3000 [00:01<00:00, 1640.46it/s]warmup should be done:  62%|██████▏   | 1851/3000 [00:01<00:00, 1601.72it/s]warmup should be done:  62%|██████▎   | 1875/3000 [00:01<00:00, 1627.35it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1643.14it/s]warmup should be done:  62%|██████▏   | 1858/3000 [00:01<00:00, 1538.65it/s]warmup should be done:  67%|██████▋   | 2000/3000 [00:01<00:00, 1648.47it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1630.27it/s]warmup should be done:  66%|██████▋   | 1988/3000 [00:01<00:00, 1640.42it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1634.80it/s]warmup should be done:  67%|██████▋   | 2013/3000 [00:01<00:00, 1604.83it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1538.74it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1642.58it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1623.91it/s]warmup should be done:  72%|███████▏  | 2165/3000 [00:01<00:00, 1645.13it/s]warmup should be done:  71%|███████   | 2134/3000 [00:01<00:00, 1629.85it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1639.94it/s]warmup should be done:  71%|███████▏  | 2143/3000 [00:01<00:00, 1631.98it/s]warmup should be done:  73%|███████▎  | 2178/3000 [00:01<00:00, 1615.82it/s]warmup should be done:  72%|███████▏  | 2166/3000 [00:01<00:00, 1538.24it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1622.60it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1640.44it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1642.21it/s]warmup should be done:  77%|███████▋  | 2297/3000 [00:01<00:00, 1624.24it/s]warmup should be done:  77%|███████▋  | 2317/3000 [00:01<00:00, 1637.66it/s]warmup should be done:  77%|███████▋  | 2307/3000 [00:01<00:00, 1626.10it/s]warmup should be done:  78%|███████▊  | 2342/3000 [00:01<00:00, 1621.53it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1537.59it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1641.41it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1620.79it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1641.64it/s]warmup should be done:  82%|████████▏ | 2460/3000 [00:01<00:00, 1620.60it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1638.18it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1625.34it/s]warmup should be done:  84%|████████▎ | 2507/3000 [00:01<00:00, 1627.93it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1535.47it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1642.50it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1620.14it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1639.99it/s]warmup should be done:  87%|████████▋ | 2623/3000 [00:01<00:00, 1622.58it/s]warmup should be done:  88%|████████▊ | 2633/3000 [00:01<00:00, 1625.27it/s]warmup should be done:  88%|████████▊ | 2645/3000 [00:01<00:00, 1619.82it/s]warmup should be done:  89%|████████▉ | 2671/3000 [00:01<00:00, 1631.23it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1535.89it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1645.50it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1618.98it/s]warmup should be done:  94%|█████████▍| 2825/3000 [00:01<00:00, 1639.70it/s]warmup should be done:  93%|█████████▎| 2786/3000 [00:01<00:00, 1622.28it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1626.39it/s]warmup should be done:  94%|█████████▎| 2808/3000 [00:01<00:00, 1613.85it/s]warmup should be done:  94%|█████████▍| 2835/3000 [00:01<00:00, 1633.66it/s]warmup should be done:  93%|█████████▎| 2782/3000 [00:01<00:00, 1532.05it/s]warmup should be done:  99%|█████████▉| 2975/3000 [00:01<00:00, 1651.14it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1621.21it/s]warmup should be done: 100%|█████████▉| 2990/3000 [00:01<00:00, 1641.45it/s]warmup should be done:  98%|█████████▊| 2951/3000 [00:01<00:00, 1628.46it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1633.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1651.07it/s]warmup should be done:  99%|█████████▉| 2970/3000 [00:01<00:00, 1609.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1645.29it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.78it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1629.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1579.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1638.50it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1573.79it/s]warmup should be done:  98%|█████████▊| 2936/3000 [00:01<00:00, 1533.54it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1537.80it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1669.50it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1577.65it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1678.64it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1657.01it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1635.63it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.77it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.02it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.50it/s]warmup should be done:  11%|█         | 316/3000 [00:00<00:01, 1577.59it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.66it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1676.15it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1685.58it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1681.67it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1691.11it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.95it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.30it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1693.97it/s]warmup should be done:  16%|█▌        | 475/3000 [00:00<00:01, 1580.66it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1685.00it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1686.13it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1674.11it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1696.22it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1664.82it/s]warmup should be done:  17%|█▋        | 502/3000 [00:00<00:01, 1668.54it/s]warmup should be done:  21%|██        | 634/3000 [00:00<00:01, 1583.43it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1697.00it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1684.33it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1688.24it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1668.71it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1698.25it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1671.01it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1667.47it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1700.75it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1668.45it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1683.63it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1690.77it/s]warmup should be done:  28%|██▊       | 851/3000 [00:00<00:01, 1697.50it/s]warmup should be done:  26%|██▋       | 793/3000 [00:00<00:01, 1579.20it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1675.35it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1663.26it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1685.41it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1699.88it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1669.93it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1673.96it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1684.65it/s]warmup should be done:  32%|███▏      | 951/3000 [00:00<00:01, 1571.86it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1672.89it/s]warmup should be done:  34%|███▍      | 1021/3000 [00:00<00:01, 1675.42it/s]warmup should be done:  40%|███▉      | 1191/3000 [00:00<00:01, 1695.72it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1678.58it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1662.02it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1671.12it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1672.13it/s]warmup should be done:  40%|███▉      | 1186/3000 [00:00<00:01, 1681.30it/s]warmup should be done:  37%|███▋      | 1109/3000 [00:00<00:01, 1569.36it/s]warmup should be done:  40%|███▉      | 1189/3000 [00:00<00:01, 1668.92it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1697.35it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1685.22it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1671.44it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1683.39it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1671.85it/s]warmup should be done:  45%|████▍     | 1338/3000 [00:00<00:01, 1656.88it/s]warmup should be done:  42%|████▏     | 1266/3000 [00:00<00:01, 1562.33it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1666.95it/s]warmup should be done:  51%|█████     | 1532/3000 [00:00<00:00, 1698.07it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1693.37it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1684.78it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1674.87it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1653.68it/s]warmup should be done:  47%|████▋     | 1423/3000 [00:00<00:01, 1564.54it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1665.48it/s]warmup should be done:  51%|█████     | 1523/3000 [00:00<00:00, 1664.29it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1698.56it/s]warmup should be done:  57%|█████▋    | 1696/3000 [00:01<00:00, 1698.94it/s]warmup should be done:  56%|█████▋    | 1693/3000 [00:01<00:00, 1685.99it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1677.27it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1653.54it/s]warmup should be done:  53%|█████▎    | 1580/3000 [00:01<00:00, 1564.26it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1663.76it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1647.85it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1701.17it/s]warmup should be done:  62%|██████▏   | 1868/3000 [00:01<00:00, 1704.12it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1686.58it/s]warmup should be done:  62%|██████▏   | 1849/3000 [00:01<00:00, 1678.38it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1654.34it/s]warmup should be done:  58%|█████▊    | 1737/3000 [00:01<00:00, 1562.55it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1664.90it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1650.85it/s]warmup should be done:  68%|██████▊   | 2044/3000 [00:01<00:00, 1702.11it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1687.49it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1706.73it/s]warmup should be done:  67%|██████▋   | 2017/3000 [00:01<00:00, 1674.77it/s]warmup should be done:  67%|██████▋   | 2002/3000 [00:01<00:00, 1652.67it/s]warmup should be done:  63%|██████▎   | 1894/3000 [00:01<00:00, 1564.37it/s]warmup should be done:  67%|██████▋   | 2024/3000 [00:01<00:00, 1665.42it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1651.32it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1701.10it/s]warmup should be done:  73%|███████▎  | 2200/3000 [00:01<00:00, 1686.55it/s]warmup should be done:  74%|███████▎  | 2211/3000 [00:01<00:00, 1706.25it/s]warmup should be done:  68%|██████▊   | 2051/3000 [00:01<00:00, 1565.15it/s]warmup should be done:  73%|███████▎  | 2185/3000 [00:01<00:00, 1668.38it/s]warmup should be done:  72%|███████▏  | 2168/3000 [00:01<00:00, 1649.92it/s]warmup should be done:  73%|███████▎  | 2192/3000 [00:01<00:00, 1667.19it/s]warmup should be done:  73%|███████▎  | 2177/3000 [00:01<00:00, 1652.49it/s]warmup should be done:  80%|███████▉  | 2386/3000 [00:01<00:00, 1700.91it/s]warmup should be done:  79%|███████▉  | 2369/3000 [00:01<00:00, 1686.92it/s]warmup should be done:  79%|███████▉  | 2382/3000 [00:01<00:00, 1707.28it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1562.47it/s]warmup should be done:  78%|███████▊  | 2334/3000 [00:01<00:00, 1651.21it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1666.28it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1669.92it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1654.60it/s]warmup should be done:  85%|████████▌ | 2557/3000 [00:01<00:00, 1702.45it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1689.28it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1709.70it/s]warmup should be done:  79%|███████▉  | 2365/3000 [00:01<00:00, 1563.00it/s]warmup should be done:  83%|████████▎ | 2501/3000 [00:01<00:00, 1655.38it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1666.19it/s]warmup should be done:  84%|████████▍ | 2528/3000 [00:01<00:00, 1672.84it/s]warmup should be done:  84%|████████▎ | 2510/3000 [00:01<00:00, 1656.27it/s]warmup should be done:  91%|█████████ | 2728/3000 [00:01<00:00, 1703.47it/s]warmup should be done:  91%|█████████ | 2726/3000 [00:01<00:00, 1711.58it/s]warmup should be done:  90%|█████████ | 2709/3000 [00:01<00:00, 1689.87it/s]warmup should be done:  89%|████████▉ | 2668/3000 [00:01<00:00, 1658.24it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1565.06it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1664.33it/s]warmup should be done:  90%|████████▉ | 2696/3000 [00:01<00:00, 1674.40it/s]warmup should be done:  89%|████████▉ | 2676/3000 [00:01<00:00, 1655.46it/s]warmup should be done:  97%|█████████▋| 2899/3000 [00:01<00:00, 1703.56it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1711.34it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1686.88it/s]warmup should be done:  94%|█████████▍| 2834/3000 [00:01<00:00, 1658.42it/s]warmup should be done:  89%|████████▉ | 2680/3000 [00:01<00:00, 1564.89it/s]warmup should be done:  95%|█████████▌| 2853/3000 [00:01<00:00, 1661.23it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1672.71it/s]warmup should be done:  95%|█████████▍| 2842/3000 [00:01<00:00, 1653.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1699.70it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1698.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1686.77it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1667.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1657.89it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1562.79it/s]warmup should be done: 100%|█████████▉| 2994/3000 [00:01<00:00, 1564.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1566.87it/s]2022-12-12 05:09:28.478519: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f723402cd20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:28.478584: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:28.481653: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f71e8031290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:28.481691: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:28.948305: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f72d802da60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:28.948376: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:28.957081: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f90a77972a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:28.957131: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:29.129138: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f90a382f770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:29.129205: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:29.155125: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f72c40280b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:29.155210: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:29.156149: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f90ab82b340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:29.156198: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:29.198300: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f90a3833720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:09:29.198390: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:09:30.755683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:30.757491: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:31.217600: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:31.225365: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:31.421153: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:31.466666: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:31.482040: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:31.496495: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:09:33.613233: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:33.661831: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:34.084692: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:34.133400: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:34.331880: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:34.360468: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:34.402370: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:09:34.404226: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:09:58.917][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:09:58.917][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:58.928][ERROR][RK0][main]: coll ps creation done
[HCTR][05:09:58.928][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][05:09:58.942][ERROR][RK0][tid #140259698386688]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:09:58.943][ERROR][RK0][tid #140259698386688]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:58.951][ERROR][RK0][tid #140259698386688]: coll ps creation done
[HCTR][05:09:58.951][ERROR][RK0][tid #140259698386688]: replica 6 waits for coll ps creation barrier
[HCTR][05:09:59.054][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:09:59.054][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:59.060][ERROR][RK0][main]: coll ps creation done
[HCTR][05:09:59.060][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:09:59.069][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:09:59.069][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:59.073][ERROR][RK0][main]: coll ps creation done
[HCTR][05:09:59.073][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][05:09:59.093][ERROR][RK0][tid #140259689993984]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:09:59.093][ERROR][RK0][tid #140259689993984]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:59.100][ERROR][RK0][tid #140259689993984]: coll ps creation done
[HCTR][05:09:59.100][ERROR][RK0][tid #140259689993984]: replica 1 waits for coll ps creation barrier
[HCTR][05:09:59.129][ERROR][RK0][tid #140259639670528]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:09:59.129][ERROR][RK0][tid #140259639670528]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:59.134][ERROR][RK0][tid #140259639670528]: coll ps creation done
[HCTR][05:09:59.134][ERROR][RK0][tid #140259639670528]: replica 4 waits for coll ps creation barrier
[HCTR][05:09:59.135][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:09:59.136][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:59.136][ERROR][RK0][tid #140259706779392]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:09:59.136][ERROR][RK0][tid #140259706779392]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:09:59.140][ERROR][RK0][tid #140259706779392]: coll ps creation done
[HCTR][05:09:59.140][ERROR][RK0][tid #140259706779392]: replica 2 waits for coll ps creation barrier
[HCTR][05:09:59.143][ERROR][RK0][main]: coll ps creation done
[HCTR][05:09:59.143][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][05:09:59.143][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][05:10:00.014][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][05:10:00.057][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][tid #140259689993984]: replica 1 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][tid #140259706779392]: replica 2 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][tid #140259698386688]: replica 6 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][tid #140259639670528]: replica 4 calling init per replica
[HCTR][05:10:00.057][ERROR][RK0][main]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][tid #140259689993984]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][main]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][main]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][main]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][tid #140259706779392]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][tid #140259698386688]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][tid #140259689993984]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][tid #140259639670528]: Calling build_v2
[HCTR][05:10:00.057][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][tid #140259706779392]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][tid #140259698386688]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:10:00.057][ERROR][RK0][tid #140259639670528]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 05:10:002022-12-12 05:10:00[[[2022-12-12 05:10:00..[.2022-12-12 05:10:002022-12-12 05:10:00 572082022-12-12 05:10:002022-12-12 05:10:00 57204 57205..2022-12-12 05:10:00: ..: :  57225 57225.E 57230 57222EE: :  57243 : :   EE: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  E:  ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] ::] ] 136136:using concurrent impl MPS136136using concurrent impl MPSusing concurrent impl MPS] ] 136
] ] 

using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS

using concurrent impl MPS


[2022-12-12 05:10:00. 61559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:10:00. 61599: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:10:00:.196 61605] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 05:10:002022-12-12 05:10:00.. 61655 61650: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::2022-12-12 05:10:00196178.] []  61683assigning 8 to cpu2022-12-12 05:10:00v100x8, slow pcie: 
.
E 61693 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E:2022-12-12 05:10:00 212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  61729:[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [2022-12-12 05:10:001782022-12-12 05:10:00
E.] .  61745v100x8, slow pcie 61747/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [
[: :E2022-12-12 05:10:002022-12-12 05:10:00E[[196 .. 2022-12-12 05:10:002022-12-12 05:10:00] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 61799 61796/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..assigning 8 to cpu:: : : 61839 61826
[178EE212: : 2022-12-12 05:10:00]   ] EE.[v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8   619002022-12-12 05:10:00
::
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .213[178::E[ 61970] 2022-12-12 05:10:00] 178196 2022-12-12 05:10:00: remote time is 8.68421.v100x8, slow pcie] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E
 62044
v100x8, slow pcie[assigning 8 to cpu: 62063 : 
2022-12-12 05:10:00[[
178: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.2022-12-12 05:10:002022-12-12 05:10:00] E:  62154..[v100x8, slow pcie 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  62158 621882022-12-12 05:10:00
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :E: : .:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196[ EE 62222213
] 2022-12-12 05:10:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  : ] assigning 8 to cpu.:[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEremote time is 8.68421
 623012142022-12-12 05:10:00:: 
: ] .196196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E[cpu time is 97.0588 62361] ] :2022-12-12 05:10:00 2022-12-12 05:10:00
: assigning 8 to cpuassigning 8 to cpu212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E

]  62426: 62458 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: E196: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 ] E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[assigning 8 to cpu [213[:2022-12-12 05:10:00
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:10:00] 2022-12-12 05:10:00212.:.remote time is 8.68421.]  62587214 62596
 62600build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: ] : : 
[Ecpu time is 97.0588[EE2022-12-12 05:10:00 
[2022-12-12 05:10:00  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:10:00./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 62677:. 62681::: 212 62708: 213212E] : E] ]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[214[:212[2022-12-12 05:10:00] 2022-12-12 05:10:00213] 2022-12-12 05:10:00.cpu time is 97.0588.] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. 62873
 62878remote time is 8.68421
 62885: : 
: EEE[[   2022-12-12 05:10:002022-12-12 05:10:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..::: 62966 62973213214213: : ] ] ] EEremote time is 8.68421cpu time is 97.0588remote time is 8.68421  


/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[::213[2022-12-12 05:10:00214] ] 2022-12-12 05:10:00.cpu time is 97.0588remote time is 8.68421. 63087

 63094: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:10:00::.214214 63148] ] : cpu time is 97.0588cpu time is 97.0588E

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 05:11:17.126400: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:11:17.166850: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 05:11:17.166944: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 05:11:17.167981: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 05:11:17.241034: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 05:11:17.637248: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 05:11:17.637340: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 05:11:24.531855: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 05:11:24.531947: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 05:11:26.211191: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 05:11:26.211287: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 05:11:26.214128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:11:26.214183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 05:11:26.494613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 05:11:26.526020: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 05:11:26.527426: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 05:11:26.548352: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 05:11:27. 85757: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 05:14:48.171289: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 05:14:48.180492: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 05:14:48.260498: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 05:14:48.307681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:14:48.307778: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:14:48.307814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:14:48.307848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:14:48.308365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:14:48.308422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.309321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.309991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.323238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 05:14:48.323317: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 05:14:48.323467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 05:14:48.323530: [E[2022-12-12 05:14:48 2022-12-12 05:14:48./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.323525:323536: 205: E] E worker 0 thread 5 initing device 5 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved6 solved
[
[2022-12-12 05:14:482022-12-12 05:14:48.[.3236092022-12-12 05:14:48323631: .: E[323642E 2022-12-12 05:14:48:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:323640 :202: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205] E:] [3 solved 205worker 0 thread 4 initing device 42022-12-12 05:14:48
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 
.2022-12-12 05:14:48[:worker 0 thread 6 initing device 6323729.2022-12-12 05:14:48202
: 323779.] E: 3238061 solved E: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[ 202:2022-12-12 05:14:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] 1815.] 7 solved] 323920worker 0 thread 3 initing device 3
Building Coll Cache with ... num gpu device is 8: 

E[ 2022-12-12 05:14:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.[:3240072022-12-12 05:14:48205[: .] 2022-12-12 05:14:48E324022worker 0 thread 1 initing device 1. : 
324038/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: : E205/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuworker 0 thread 7 initing device 71815:
] 1980Building Coll Cache with ... num gpu device is 8] 
eager alloc mem 381.47 MB
[2022-12-12 05:14:48.324188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.324232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:14:48.324259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 05:14:481815.] 324277Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.324311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.324432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:14:48.324477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.324533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:14:48.324569: E[ 2022-12-12 05:14:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:3245831815: ] EBuilding Coll Cache with ... num gpu device is 8 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.324630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.328773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.328832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.328883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.328942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.329010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.329081: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.329131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.333348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:14:48.390496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:14:48.395855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:14:48.395998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:14:48.396847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.397521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.398631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.398680: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 75.96 MB
[2022-12-12 05:14:48.413528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:14:48.413627: E [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:14:482022-12-12 05:14:48:..1980413665413665] : : eager alloc mem 1023.00 BytesE
E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[2022-12-12 05:14:48.418408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:14:48.419555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:14:48.419645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:14:48.419649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 05:14:482022-12-12 05:14:48..419730419746: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 05:14:48.419808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 05:14:48638.] 419841eager release cuda mem 1023: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:14:48.419899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:14:48.420559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.421208: [E2022-12-12 05:14:48 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu421224:: 1980E]  eager alloc mem 1023.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 05:14:48.421590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.422105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.422671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.423788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.424177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.424565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.424620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.424901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.424951: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.25 MB
[2022-12-12 05:14:48.425223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.425271: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 75.85 MB
[2022-12-12 05:14:48.425390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 05:14:48.425467: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:14:48.425610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.425655: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:14:48.425684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.425729: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.28 MB
[2022-12-12 05:14:48.426245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.426349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[[2022-12-12 05:14:482022-12-12 05:14:48..426410426426: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1023eager release cuda mem 400000000

[2022-12-12 05:14:48.426506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:14:48.426892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.427466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.427980: [E2022-12-12 05:14:48 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc427989:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 76.68 MB
[2022-12-12 05:14:48.428071: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:14:48.428737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.428820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:48.429797: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.429844: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.21 MB
[2022-12-12 05:14:48.429868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:48.429914: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:14:48.451001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.451636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.451682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.50 GB
[2022-12-12 05:14:48.474720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.475067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.475202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.475359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.475403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.49 GB
[2022-12-12 05:14:48.475688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.475732: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:14:48.475806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.475848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:14:48.476942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.477574: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.477619: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:14:48.478510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.479113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.479170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:14:48.479966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.480568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.480608: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:14:48:.1980480605] : eager alloc mem 9.54 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:14:48.481228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:14:48.481272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.53 GB
[[[[[[[[2022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:51........676485676485676486676487676485676485676489676490: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] Device 0 init p2p of link 3] ] ] Device 3 init p2p of link 2Device 6 init p2p of link 0Device 5 init p2p of link 6Device 4 init p2p of link 5
Device 2 init p2p of link 1Device 7 init p2p of link 4Device 1 init p2p of link 7






[[[2022-12-12 05:14:51[2022-12-12 05:14:51[2022-12-12 05:14:51[.2022-12-12 05:14:51.[2022-12-12 05:14:51.2022-12-12 05:14:51[677034.6770352022-12-12 05:14:51.677036.2022-12-12 05:14:51: 677039: .677043: 677045.E: E677061: E: 677055 E : E E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1980] :1980] 1980:eager alloc mem 611.00 KB] eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB] 1980
eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[2022-12-12 05:14:51.678156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.678213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-12 05:14:51
2022-12-12 05:14:51[..2022-12-12 05:14:51678232678236[[[.: : 2022-12-12 05:14:512022-12-12 05:14:512022-12-12 05:14:51678247EE...:   678265678266678267E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : :  ::EEE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638638   :] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663eager release cuda mem 625663:::] 

638638638eager release cuda mem 625663] ] ] 
eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-12 05:14:51.694465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 05:14:51.694621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.695558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.696521: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 05:14:51.696687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.697229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 05:14:51.697382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.697604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.697810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 05:14:51.697962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:14:512022-12-12 05:14:51..698280698293: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 2 init p2p of link 3eager release cuda mem 625663

[2022-12-12 05:14:51.698458: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.698786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 05:14:51.698891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.698949: [E[2022-12-12 05:14:51 2022-12-12 05:14:51./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.698948:698957: 1980: E] E eager alloc mem 611.00 KB /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 7 init p2p of link 1Device 0 init p2p of link 6

[2022-12-12 05:14:51.699212: [E2022-12-12 05:14:51 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu699223:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.699323: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.699919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.700084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.700143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.703882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 05:14:51.703999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.704881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.713964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 05:14:51.714091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.714113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 05:14:51.714228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.714978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.715164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.715538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 05:14:51.715630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] [2022-12-12 05:14:51Device 5 init p2p of link 72022-12-12 05:14:51.
.715661715655: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801926] ] eager alloc mem 611.00 KBDevice 2 init p2p of link 0

[2022-12-12 05:14:51.715767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.715814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.716339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 05:14:51.716456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.716536: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-12 05:14:51] .Device 0 init p2p of link 1716565
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.716665[: 2022-12-12 05:14:51E. 716671/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB[:
2022-12-12 05:14:51638.] 716703eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.717297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.717558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.724013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 05:14:51.724137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.725021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.737064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 05:14:51.737177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.738058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.738395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 05:14:51.738512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.738791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 05:14:51.738907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.739407: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.739798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.740108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 05:14:51.740226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.740673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 05:14:51.740807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.741157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.741651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 05:14:51.741707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.741776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.742028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 05:14:51.742149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:14:51.742643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.743012: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:14:51.744331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.747824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19997990 / 100000000 nodes ( 20.00 %~20.00 %) | remote 58871687 / 100000000 nodes ( 58.87 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.54 GB | 3.42352 secs 
[2022-12-12 05:14:51.756908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.761219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.761686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.763409: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.764181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.764641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.765064: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:14:51.781468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19884848 / 100000000 nodes ( 19.88 %~20.00 %) | remote 58984829 / 100000000 nodes ( 58.98 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.49 GB | 3.4572 secs 
[2022-12-12 05:14:51.782246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19978539 / 100000000 nodes ( 19.98 %~20.00 %) | remote 58891138 / 100000000 nodes ( 58.89 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.53 GB | 3.45778 secs 
[2022-12-12 05:14:51.782417: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19999511 / 100000000 nodes ( 20.00 %~20.00 %) | remote 58870166 / 100000000 nodes ( 58.87 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.54 GB | 3.45785 secs 
[2022-12-12 05:14:51.782589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19997426 / 100000000 nodes ( 20.00 %~20.00 %) | remote 58872251 / 100000000 nodes ( 58.87 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.54 GB | 3.45841 secs 
[2022-12-12 05:14:51.782755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19999391 / 100000000 nodes ( 20.00 %~20.00 %) | remote 58870286 / 100000000 nodes ( 58.87 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.54 GB | 3.45873 secs 
[2022-12-12 05:14:51.782938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19911308 / 100000000 nodes ( 19.91 %~20.00 %) | remote 58958369 / 100000000 nodes ( 58.96 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.50 GB | 3.47453 secs 
[2022-12-12 05:14:51.783164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19989413 / 100000000 nodes ( 19.99 %~20.00 %) | remote 58880264 / 100000000 nodes ( 58.88 %) | cpu 21130323 / 100000000 nodes ( 21.13 %) | 9.54 GB | 3.45854 secs 
[2022-12-12 05:14:51.783882: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.14 GB
[2022-12-12 05:14:53.278217: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.40 GB
[2022-12-12 05:14:53.301443: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.40 GB
[2022-12-12 05:14:53.302878: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.40 GB
[2022-12-12 05:14:54.451495: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.66 GB
[2022-12-12 05:14:54.451666: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.66 GB
[2022-12-12 05:14:54.452751: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.66 GB
[2022-12-12 05:14:55.707031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.88 GB
[2022-12-12 05:14:55.707195: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.88 GB
[2022-12-12 05:14:55.707517: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.88 GB
[2022-12-12 05:14:56.863530: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.09 GB
[2022-12-12 05:14:56.863991: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.09 GB
[2022-12-12 05:14:56.864429: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.09 GB
[2022-12-12 05:14:57.988765: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.55 GB
[2022-12-12 05:14:57.993931: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.55 GB
[2022-12-12 05:14:57.996033: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.55 GB
[2022-12-12 05:14:59.445224: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.75 GB
[2022-12-12 05:14:59.445655: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.75 GB
[HCTR][05:15:00.946][ERROR][RK0][tid #140259706779392]: replica 2 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][tid #140259639670528]: replica 4 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][tid #140259698386688]: replica 6 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][tid #140259689993984]: replica 1 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][05:15:00.946][ERROR][RK0][tid #140259689993984]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259698386688]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259706779392]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259639670528]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259689993984]: init per replica done
[HCTR][05:15:00.946][ERROR][RK0][main]: init per replica done
[HCTR][05:15:00.946][ERROR][RK0][main]: init per replica done
[HCTR][05:15:00.946][ERROR][RK0][main]: init per replica done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259698386688]: init per replica done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259706779392]: init per replica done
[HCTR][05:15:00.946][ERROR][RK0][tid #140259639670528]: init per replica done
[HCTR][05:15:00.950][ERROR][RK0][main]: init per replica done
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 7 allocated 3276800 at 0x7f7dbf920000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 7 allocated 6553600 at 0x7f929c800000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 7 allocated 3276800 at 0x7f929ce40000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 7 allocated 6553600 at 0x7f929d160000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259706779392]: 2 allocated 3276800 at 0x7f7dbb920000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259706779392]: 2 allocated 6553600 at 0x7f929e800000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259706779392]: 2 allocated 3276800 at 0x7f929ee40000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259706779392]: 2 allocated 6553600 at 0x7f929f160000
[HCTR][05:15:00.953][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f7dbb920000
[HCTR][05:15:00.953][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f929a800000
[HCTR][05:15:00.953][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f929ae40000
[HCTR][05:15:00.953][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f7dbf920000
[HCTR][05:15:00.953][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f929b160000
[HCTR][05:15:00.953][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f929e800000
[HCTR][05:15:00.953][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f929ee40000
[HCTR][05:15:00.953][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f929f160000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 6 allocated 3276800 at 0x7f7dbb920000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 6 allocated 6553600 at 0x7f929c800000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 6 allocated 3276800 at 0x7f929ce40000
[HCTR][05:15:00.953][ERROR][RK0][tid #140259698386688]: 6 allocated 6553600 at 0x7f929d160000
[HCTR][05:15:00.953][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f7dc3920000
[HCTR][05:15:00.953][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f929e800000
[HCTR][05:15:00.953][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f929ee40000
[HCTR][05:15:00.953][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f929f160000
[HCTR][05:15:00.954][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f7db7920000
[HCTR][05:15:00.954][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f929e800000
[HCTR][05:15:00.954][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f929ee40000
[HCTR][05:15:00.954][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f929f160000
[HCTR][05:15:00.956][ERROR][RK0][tid #140259891320576]: 0 allocated 3276800 at 0x7f792cf20000
[HCTR][05:15:00.956][ERROR][RK0][tid #140259891320576]: 0 allocated 6553600 at 0x7f792d400000
[HCTR][05:15:00.956][ERROR][RK0][tid #140259891320576]: 0 allocated 3276800 at 0x7f759a50e800
[HCTR][05:15:00.956][ERROR][RK0][tid #140259891320576]: 0 allocated 6553600 at 0x7f759a82e800








