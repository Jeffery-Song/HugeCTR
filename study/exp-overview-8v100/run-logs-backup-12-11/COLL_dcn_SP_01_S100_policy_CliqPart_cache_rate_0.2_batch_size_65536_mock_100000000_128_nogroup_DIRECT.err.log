2022-12-12 05:24:26.760618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.766127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.772123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.775677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.789133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.798784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.803901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.811963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.860029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.867106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.876075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.879250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.883025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.897954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.904545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.905516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.906782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.908631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.909345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.909847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.911158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.911329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.912873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.912903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.914433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.914487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.915786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.916127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.917489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.917876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.919337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.919708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.920734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.921180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.922553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.923498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.924524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.925501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.926438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.927372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.932701: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:26.938263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.940393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.941745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.941930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.942132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.943749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.944222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.944584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.946180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.946890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.947038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.947291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.949548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.949984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.950206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.952701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.953150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.953420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.954010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.955688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.956130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.956396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.957256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.959161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.959454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.960244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.961633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.962099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.962826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.964884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.965624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.966644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.967636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.967838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.968430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.969927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.971516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.971789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.971991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.972232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.973902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.974281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.974639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.974818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.986755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.987540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.987625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.989073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.989689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:26.990070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.002629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.007799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.014225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.025069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.025625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.026183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.028559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.028602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.028641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.029514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.029797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.030427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.030436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.033196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.033434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.033479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.034537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.034611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.036068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.036302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.039113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.039402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.039447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.040413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.041192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.041674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.043413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.043561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.043605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.044476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.045158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.047734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.047909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.047997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.048596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.049570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.051821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.051937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.051979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.052705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.053380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.055479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.055603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.055741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.056273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.056905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.059044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.059094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.059280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.059792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.060574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.062537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.062568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.062748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.063469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.064126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.067188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.067235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.067377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.068842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.069125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.070600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.070702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.070739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.072280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.072853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.074404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.074472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.074529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.076049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.076617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.077914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.078126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.078168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.079316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.080549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.081367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.081527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.081610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.082848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.084001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.084422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.084935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.085060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.085146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.086664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.088441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.089173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.089608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.090916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.090917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.091719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.091951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.093397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.093884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.094487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.095281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.095322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.096371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.097088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.098103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.098378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.098728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.099774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.099905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.100758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.101561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.102722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.103280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.104012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.104096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.104677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.105358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.106589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.106916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.107008: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.107588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.108198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.108696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.110187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.111515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.111907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.112671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.112767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.113288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.114336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.114999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.115754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.116529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.116788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.117144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.117238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.118248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.119253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.119974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.121144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.121164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.121706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.121745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.122708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.123663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.124310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.125576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.126916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.127790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.128659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.129264: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.129504: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.129510: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.131664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.132287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.134833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.135083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.135101: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.137264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.137591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.138270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.138344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.138769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.140609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.140935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.142309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.143186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.143537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.143742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.146658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.147239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.148654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.148918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.149318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.149497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.151894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.152044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.154128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.156857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.158041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.161210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.192057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.195491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.196771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.214289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.217147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.220526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.223336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.226918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.228503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.262346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.263442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.269870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.272514: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.278041: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:24:27.281784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.287397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.304864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.305061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.310974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:27.311403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.349550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.352111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.354115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.355109: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.355181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:24:28.372600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.373239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.374354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.374931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.375627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.376096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:24:28.423149: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.423366: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.471927: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:24:28.600672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.601504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.602028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.602846: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.602903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:24:28.620265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.621427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.622385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.623518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.624504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.625450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:24:28.652067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.652517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.654035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.654554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.655694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.656144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.657791: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.657850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:24:28.658162: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.658217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:24:28.658986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.661186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.662197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.663146: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.663200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:24:28.675071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.675267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.676782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.676859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.676899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.678242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.678601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.678636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.679767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.680229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.680253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.680343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.683740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.685462: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.685507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.685515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:24:28.685635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.687017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:24:28.687665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:24:28.687684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.688689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.689568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.690388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:24:28.703505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.704133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.704649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.705228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.705743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.706215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:24:28.707841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.708480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.708557: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.708705: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.709001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.709469: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.709515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:24:28.709582: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:24:28.719314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.719939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.720469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.720927: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:24:28.720979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:24:28.727676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.728349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.728861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.729458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.729973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.730457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:24:28.734277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.734446: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.736082: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.736264: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.736319: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:24:28.737986: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:24:28.738362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.739720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.740911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.742182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.743296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:24:28.744353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:24:28.752830: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.752993: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.754629: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 05:24:28.755662: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.755821: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.757587: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:24:28.776657: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.776834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.778622: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:24:28.790371: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.790537: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:24:28.792261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][05:24:30.059][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.060][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.060][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.061][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.061][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.061][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.061][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:24:30.062][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 93it [00:01, 78.10it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 187it [00:01, 170.52it/s]warmup run: 102it [00:01, 87.07it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 101it [00:01, 86.77it/s]warmup run: 280it [00:01, 271.41it/s]warmup run: 199it [00:01, 183.15it/s]warmup run: 94it [00:01, 80.04it/s]warmup run: 95it [00:01, 80.55it/s]warmup run: 94it [00:01, 79.66it/s]warmup run: 98it [00:01, 85.27it/s]warmup run: 101it [00:01, 86.89it/s]warmup run: 201it [00:01, 186.73it/s]warmup run: 373it [00:01, 376.07it/s]warmup run: 293it [00:01, 285.04it/s]warmup run: 188it [00:01, 173.47it/s]warmup run: 192it [00:01, 176.81it/s]warmup run: 188it [00:01, 172.67it/s]warmup run: 196it [00:01, 184.32it/s]warmup run: 203it [00:01, 189.19it/s]warmup run: 302it [00:01, 297.80it/s]warmup run: 467it [00:02, 480.22it/s]warmup run: 389it [00:01, 393.53it/s]warmup run: 284it [00:01, 279.10it/s]warmup run: 289it [00:01, 282.81it/s]warmup run: 281it [00:01, 274.00it/s]warmup run: 294it [00:01, 292.70it/s]warmup run: 304it [00:01, 300.31it/s]warmup run: 402it [00:01, 411.38it/s]warmup run: 563it [00:02, 579.88it/s]warmup run: 487it [00:02, 502.97it/s]warmup run: 380it [00:01, 388.35it/s]warmup run: 387it [00:01, 394.25it/s]warmup run: 375it [00:01, 380.40it/s]warmup run: 393it [00:01, 405.85it/s]warmup run: 406it [00:01, 416.68it/s]warmup run: 504it [00:02, 525.00it/s]warmup run: 660it [00:02, 668.09it/s]warmup run: 586it [00:02, 605.03it/s]warmup run: 476it [00:02, 495.02it/s]warmup run: 483it [00:02, 498.76it/s]warmup run: 470it [00:02, 486.00it/s]warmup run: 492it [00:01, 515.52it/s]warmup run: 508it [00:02, 529.39it/s]warmup run: 607it [00:02, 630.81it/s]warmup run: 756it [00:02, 738.37it/s]warmup run: 685it [00:02, 692.91it/s]warmup run: 573it [00:02, 594.67it/s]warmup run: 583it [00:02, 602.98it/s]warmup run: 566it [00:02, 584.62it/s]warmup run: 592it [00:02, 617.64it/s]warmup run: 612it [00:02, 637.28it/s]warmup run: 710it [00:02, 722.77it/s]warmup run: 850it [00:02, 789.18it/s]warmup run: 783it [00:02, 763.63it/s]warmup run: 669it [00:02, 678.53it/s]warmup run: 684it [00:02, 696.10it/s]warmup run: 661it [00:02, 668.78it/s]warmup run: 692it [00:02, 705.18it/s]warmup run: 716it [00:02, 729.55it/s]warmup run: 813it [00:02, 798.53it/s]warmup run: 946it [00:02, 833.76it/s]warmup run: 882it [00:02, 820.65it/s]warmup run: 763it [00:02, 742.01it/s]warmup run: 784it [00:02, 770.22it/s]warmup run: 757it [00:02, 739.67it/s]warmup run: 791it [00:02, 775.28it/s]warmup run: 819it [00:02, 802.44it/s]warmup run: 916it [00:02, 857.06it/s]warmup run: 1043it [00:02, 871.04it/s]warmup run: 979it [00:02, 861.03it/s]warmup run: 857it [00:02, 791.34it/s]warmup run: 883it [00:02, 825.54it/s]warmup run: 852it [00:02, 793.55it/s]warmup run: 890it [00:02, 830.51it/s]warmup run: 922it [00:02, 861.50it/s]warmup run: 1017it [00:02, 894.10it/s]warmup run: 1140it [00:02, 898.40it/s]warmup run: 1076it [00:02, 891.34it/s]warmup run: 954it [00:02, 837.87it/s]warmup run: 981it [00:02, 864.99it/s]warmup run: 947it [00:02, 834.57it/s]warmup run: 990it [00:02, 875.39it/s]warmup run: 1024it [00:02, 903.79it/s]warmup run: 1120it [00:02, 929.77it/s]warmup run: 1238it [00:02, 920.06it/s]warmup run: 1173it [00:02, 912.89it/s]warmup run: 1051it [00:02, 873.75it/s]warmup run: 1079it [00:02, 896.83it/s]warmup run: 1042it [00:02, 864.75it/s]warmup run: 1091it [00:02, 911.64it/s]warmup run: 1127it [00:02, 938.20it/s]warmup run: 1223it [00:02, 957.16it/s]warmup run: 1336it [00:02, 936.30it/s]warmup run: 1271it [00:02, 930.00it/s]warmup run: 1146it [00:02, 880.60it/s]warmup run: 1177it [00:02, 916.38it/s]warmup run: 1137it [00:02, 886.71it/s]warmup run: 1194it [00:02, 942.87it/s]warmup run: 1230it [00:02, 963.77it/s]warmup run: 1325it [00:02, 975.20it/s]warmup run: 1438it [00:03, 959.16it/s]warmup run: 1372it [00:02, 951.56it/s]warmup run: 1244it [00:02, 907.52it/s]warmup run: 1276it [00:02, 936.14it/s]warmup run: 1233it [00:02, 905.72it/s]warmup run: 1294it [00:02, 955.59it/s]warmup run: 1333it [00:02, 981.11it/s]warmup run: 1427it [00:02, 987.19it/s]warmup run: 1539it [00:03, 973.65it/s]warmup run: 1475it [00:03, 972.70it/s]warmup run: 1339it [00:02, 904.83it/s]warmup run: 1376it [00:02, 952.88it/s]warmup run: 1328it [00:02, 915.33it/s]warmup run: 1395it [00:02, 969.87it/s]warmup run: 1436it [00:02, 993.26it/s]warmup run: 1531it [00:03, 999.67it/s]warmup run: 1641it [00:03, 985.67it/s]warmup run: 1578it [00:03, 989.47it/s]warmup run: 1433it [00:03, 903.54it/s]warmup run: 1476it [00:03, 966.28it/s]warmup run: 1423it [00:03, 922.60it/s]warmup run: 1495it [00:03, 973.82it/s]warmup run: 1539it [00:03, 1000.93it/s]warmup run: 1634it [00:03, 1005.68it/s]warmup run: 1742it [00:03, 991.39it/s]warmup run: 1681it [00:03, 1000.38it/s]warmup run: 1528it [00:03, 915.19it/s]warmup run: 1575it [00:03, 973.16it/s]warmup run: 1518it [00:03, 926.36it/s]warmup run: 1595it [00:03, 981.37it/s]warmup run: 1642it [00:03, 1000.41it/s]warmup run: 1737it [00:03, 1011.74it/s]warmup run: 1844it [00:03, 997.56it/s]warmup run: 1784it [00:03, 1008.17it/s]warmup run: 1621it [00:03, 895.10it/s]warmup run: 1675it [00:03, 979.43it/s]warmup run: 1613it [00:03, 931.67it/s]warmup run: 1695it [00:03, 983.31it/s]warmup run: 1744it [00:03, 991.11it/s] warmup run: 1840it [00:03, 1014.55it/s]warmup run: 1945it [00:03, 999.90it/s]warmup run: 1887it [00:03, 1013.67it/s]warmup run: 1718it [00:03, 915.08it/s]warmup run: 1775it [00:03, 984.71it/s]warmup run: 1708it [00:03, 935.70it/s]warmup run: 1795it [00:03, 981.52it/s]warmup run: 1845it [00:03, 975.62it/s]warmup run: 1943it [00:03, 1015.91it/s]warmup run: 2054it [00:03, 1025.00it/s]warmup run: 1991it [00:03, 1019.58it/s]warmup run: 1815it [00:03, 928.89it/s]warmup run: 1875it [00:03, 981.51it/s]warmup run: 1803it [00:03, 937.78it/s]warmup run: 1894it [00:03, 977.31it/s]warmup run: 1944it [00:03, 977.05it/s]warmup run: 2052it [00:03, 1037.06it/s]warmup run: 2173it [00:03, 1074.07it/s]warmup run: 2110it [00:03, 1070.15it/s]warmup run: 1910it [00:03, 933.81it/s]warmup run: 1898it [00:03, 941.09it/s]warmup run: 1974it [00:03, 960.14it/s]warmup run: 1993it [00:03, 976.94it/s]warmup run: 2052it [00:03, 1005.74it/s]warmup run: 2171it [00:03, 1081.32it/s]warmup run: 2292it [00:03, 1107.67it/s]warmup run: 2232it [00:03, 1113.90it/s]warmup run: 2008it [00:03, 945.13it/s]warmup run: 1994it [00:03, 944.34it/s]warmup run: 2080it [00:03, 988.90it/s]warmup run: 2112it [00:03, 1039.78it/s]warmup run: 2173it [00:03, 1064.92it/s]warmup run: 2290it [00:03, 1113.66it/s]warmup run: 2412it [00:03, 1135.17it/s]warmup run: 2354it [00:03, 1143.54it/s]warmup run: 2122it [00:03, 1001.63it/s]warmup run: 2112it [00:03, 1012.15it/s]warmup run: 2192it [00:03, 1025.28it/s]warmup run: 2234it [00:03, 1091.87it/s]warmup run: 2294it [00:03, 1107.75it/s]warmup run: 2410it [00:03, 1136.63it/s]warmup run: 2533it [00:04, 1156.30it/s]warmup run: 2476it [00:03, 1165.89it/s]warmup run: 2236it [00:03, 1041.83it/s]warmup run: 2232it [00:03, 1067.26it/s]warmup run: 2298it [00:03, 1033.78it/s]warmup run: 2356it [00:03, 1128.52it/s]warmup run: 2415it [00:03, 1137.77it/s]warmup run: 2529it [00:03, 1151.74it/s]warmup run: 2654it [00:04, 1171.17it/s]warmup run: 2598it [00:04, 1181.11it/s]warmup run: 2341it [00:03, 1039.02it/s]warmup run: 2352it [00:03, 1104.19it/s]warmup run: 2404it [00:03, 1041.38it/s]warmup run: 2478it [00:03, 1154.45it/s]warmup run: 2536it [00:03, 1158.31it/s]warmup run: 2649it [00:04, 1163.47it/s]warmup run: 2774it [00:04, 1178.02it/s]warmup run: 2720it [00:04, 1191.39it/s]warmup run: 2455it [00:04, 1066.62it/s]warmup run: 2472it [00:04, 1130.20it/s]warmup run: 2516it [00:04, 1062.17it/s]warmup run: 2600it [00:04, 1172.10it/s]warmup run: 2654it [00:04, 1162.78it/s]warmup run: 2766it [00:04, 1159.31it/s]warmup run: 2895it [00:04, 1185.75it/s]warmup run: 2840it [00:04, 1191.87it/s]warmup run: 2570it [00:04, 1089.18it/s]warmup run: 2592it [00:04, 1148.50it/s]warmup run: 2628it [00:04, 1078.37it/s]warmup run: 2722it [00:04, 1184.69it/s]warmup run: 2774it [00:04, 1171.37it/s]warmup run: 2885it [00:04, 1167.88it/s]warmup run: 3000it [00:04, 672.12it/s] warmup run: 2962it [00:04, 1198.12it/s]warmup run: 3000it [00:04, 685.67it/s] warmup run: 2680it [00:04, 1057.77it/s]warmup run: 2711it [00:04, 1160.77it/s]warmup run: 2739it [00:04, 1086.39it/s]warmup run: 2841it [00:04, 1183.36it/s]warmup run: 2894it [00:04, 1179.81it/s]warmup run: 3000it [00:04, 692.17it/s] warmup run: 2789it [00:04, 1066.57it/s]warmup run: 2830it [00:04, 1167.92it/s]warmup run: 2849it [00:04, 1089.12it/s]warmup run: 2961it [00:04, 1187.61it/s]warmup run: 3000it [00:04, 692.27it/s] warmup run: 3000it [00:04, 688.92it/s] warmup run: 2900it [00:04, 1077.54it/s]warmup run: 2949it [00:04, 1172.32it/s]warmup run: 2965it [00:04, 1109.88it/s]warmup run: 3000it [00:04, 667.00it/s] warmup run: 3000it [00:04, 665.71it/s] warmup run: 3000it [00:04, 655.32it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.89it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1577.92it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1639.66it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.37it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.38it/s]warmup should be done:   5%|▌         | 153/3000 [00:00<00:01, 1526.27it/s]warmup should be done:   5%|▌         | 152/3000 [00:00<00:01, 1511.29it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.50it/s]warmup should be done:  10%|█         | 307/3000 [00:00<00:01, 1534.27it/s]warmup should be done:  11%|█         | 319/3000 [00:00<00:01, 1594.26it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1625.60it/s]warmup should be done:  10%|█         | 310/3000 [00:00<00:01, 1551.25it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1630.78it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1641.08it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1620.89it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1634.02it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1624.00it/s]warmup should be done:  16%|█▌        | 467/3000 [00:00<00:01, 1560.53it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1610.79it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1630.91it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1632.34it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1633.22it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1613.71it/s]warmup should be done:  16%|█▌        | 466/3000 [00:00<00:01, 1487.76it/s]warmup should be done:  21%|██        | 628/3000 [00:00<00:01, 1579.11it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1616.49it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1626.91it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1630.77it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1630.73it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1615.84it/s]warmup should be done:  21%|██        | 619/3000 [00:00<00:01, 1501.71it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1499.80it/s]warmup should be done:  26%|██▋       | 790/3000 [00:00<00:01, 1591.87it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1633.04it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1614.00it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1629.46it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1614.49it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1628.19it/s]warmup should be done:  26%|██▌       | 772/3000 [00:00<00:01, 1508.69it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1536.86it/s]warmup should be done:  32%|███▏      | 951/3000 [00:00<00:01, 1597.19it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1634.81it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1628.37it/s]warmup should be done:  32%|███▏      | 974/3000 [00:00<00:01, 1614.85it/s]warmup should be done:  32%|███▏      | 970/3000 [00:00<00:01, 1607.80it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1624.64it/s]warmup should be done:  31%|███       | 925/3000 [00:00<00:01, 1515.26it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1558.97it/s]warmup should be done:  37%|███▋      | 1112/3000 [00:00<00:01, 1598.44it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1633.95it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1625.12it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1612.57it/s]warmup should be done:  38%|███▊      | 1131/3000 [00:00<00:01, 1603.20it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1620.60it/s]warmup should be done:  36%|███▌      | 1077/3000 [00:00<00:01, 1508.85it/s]warmup should be done:  38%|███▊      | 1140/3000 [00:00<00:01, 1570.45it/s]warmup should be done:  42%|████▎     | 1275/3000 [00:00<00:01, 1606.34it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1636.16it/s]warmup should be done:  44%|████▎     | 1308/3000 [00:00<00:01, 1626.54it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1613.38it/s]warmup should be done:  43%|████▎     | 1292/3000 [00:00<00:01, 1603.40it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1620.14it/s]warmup should be done:  41%|████      | 1230/3000 [00:00<00:01, 1515.10it/s]warmup should be done:  43%|████▎     | 1301/3000 [00:00<00:01, 1580.63it/s]warmup should be done:  48%|████▊     | 1437/3000 [00:00<00:00, 1609.67it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1638.87it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1625.85it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1612.69it/s]warmup should be done:  48%|████▊     | 1453/3000 [00:00<00:00, 1597.68it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1620.16it/s]warmup should be done:  46%|████▌     | 1385/3000 [00:00<00:01, 1524.49it/s]warmup should be done:  49%|████▊     | 1462/3000 [00:00<00:00, 1588.16it/s]warmup should be done:  53%|█████▎    | 1599/3000 [00:01<00:00, 1612.68it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1641.56it/s]warmup should be done:  54%|█████▍    | 1634/3000 [00:01<00:00, 1624.23it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1611.87it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1621.77it/s]warmup should be done:  54%|█████▍    | 1613/3000 [00:01<00:00, 1592.15it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:01<00:00, 1509.33it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1594.37it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1616.16it/s]warmup should be done:  60%|██████    | 1805/3000 [00:01<00:00, 1643.72it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1624.53it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1611.87it/s]warmup should be done:  60%|█████▉    | 1798/3000 [00:01<00:00, 1622.84it/s]warmup should be done:  59%|█████▉    | 1773/3000 [00:01<00:00, 1590.04it/s]warmup should be done:  56%|█████▋    | 1694/3000 [00:01<00:00, 1523.68it/s]warmup should be done:  59%|█████▉    | 1784/3000 [00:01<00:00, 1597.55it/s]warmup should be done:  64%|██████▍   | 1927/3000 [00:01<00:00, 1623.99it/s]warmup should be done:  66%|██████▌   | 1970/3000 [00:01<00:00, 1644.21it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1623.72it/s]warmup should be done:  65%|██████▍   | 1946/3000 [00:01<00:00, 1611.44it/s]warmup should be done:  65%|██████▌   | 1961/3000 [00:01<00:00, 1622.40it/s]warmup should be done:  64%|██████▍   | 1933/3000 [00:01<00:00, 1587.88it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1522.92it/s]warmup should be done:  65%|██████▍   | 1945/3000 [00:01<00:00, 1599.32it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1644.82it/s]warmup should be done:  70%|██████▉   | 2091/3000 [00:01<00:00, 1627.05it/s]warmup should be done:  71%|███████   | 2123/3000 [00:01<00:00, 1623.10it/s]warmup should be done:  70%|███████   | 2108/3000 [00:01<00:00, 1610.93it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1622.46it/s]warmup should be done:  70%|██████▉   | 2092/3000 [00:01<00:00, 1585.11it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1527.71it/s]warmup should be done:  70%|███████   | 2106/3000 [00:01<00:00, 1602.08it/s]warmup should be done:  77%|███████▋  | 2300/3000 [00:01<00:00, 1645.03it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1620.64it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1622.44it/s]warmup should be done:  76%|███████▌  | 2270/3000 [00:01<00:00, 1610.01it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1623.16it/s]warmup should be done:  75%|███████▌  | 2251/3000 [00:01<00:00, 1583.94it/s]warmup should be done:  72%|███████▏  | 2156/3000 [00:01<00:00, 1533.92it/s]warmup should be done:  76%|███████▌  | 2269/3000 [00:01<00:00, 1609.61it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1643.20it/s]warmup should be done:  82%|████████▏ | 2449/3000 [00:01<00:00, 1620.85it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1615.58it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1608.44it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1620.42it/s]warmup should be done:  80%|████████  | 2410/3000 [00:01<00:00, 1582.88it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1507.55it/s]warmup should be done:  81%|████████  | 2432/3000 [00:01<00:00, 1615.56it/s]warmup should be done:  88%|████████▊ | 2630/3000 [00:01<00:00, 1642.44it/s]warmup should be done:  87%|████████▋ | 2612/3000 [00:01<00:00, 1622.68it/s]warmup should be done:  86%|████████▌ | 2579/3000 [00:01<00:00, 1614.87it/s]warmup should be done:  86%|████████▋ | 2594/3000 [00:01<00:00, 1609.94it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1619.72it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1583.41it/s]warmup should be done:  82%|████████▏ | 2461/3000 [00:01<00:00, 1501.80it/s]warmup should be done:  87%|████████▋ | 2597/3000 [00:01<00:00, 1623.67it/s]warmup should be done:  93%|█████████▎| 2795/3000 [00:01<00:00, 1642.34it/s]warmup should be done:  92%|█████████▎| 2775/3000 [00:01<00:00, 1623.88it/s]warmup should be done:  91%|█████████▏| 2741/3000 [00:01<00:00, 1614.42it/s]warmup should be done:  92%|█████████▏| 2756/3000 [00:01<00:00, 1610.07it/s]warmup should be done:  92%|█████████▎| 2775/3000 [00:01<00:00, 1618.25it/s]warmup should be done:  91%|█████████ | 2728/3000 [00:01<00:00, 1584.41it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1509.33it/s]warmup should be done:  92%|█████████▏| 2761/3000 [00:01<00:00, 1627.78it/s]warmup should be done:  99%|█████████▊| 2961/3000 [00:01<00:00, 1646.60it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1629.87it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1617.63it/s]warmup should be done:  97%|█████████▋| 2921/3000 [00:01<00:00, 1619.25it/s]warmup should be done:  98%|█████████▊| 2939/3000 [00:01<00:00, 1624.24it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1591.01it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1513.26it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1639.16it/s]warmup should be done:  97%|█████████▋| 2924/3000 [00:01<00:00, 1617.46it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1626.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1613.81it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1607.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1598.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1594.16it/s]warmup should be done:  97%|█████████▋| 2922/3000 [00:01<00:00, 1522.05it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1517.95it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.18it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1667.59it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1666.23it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.79it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1675.44it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1655.23it/s]warmup should be done:   5%|▍         | 144/3000 [00:00<00:01, 1432.00it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1631.43it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1648.02it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1673.69it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1666.92it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1683.50it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1656.22it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1679.18it/s]warmup should be done:  10%|▉         | 294/3000 [00:00<00:01, 1468.11it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1628.51it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1652.25it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1689.90it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1660.62it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1677.17it/s]warmup should be done:  15%|█▌        | 452/3000 [00:00<00:01, 1517.19it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1674.77it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1629.49it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1651.99it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1693.18it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1650.85it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1676.40it/s]warmup should be done:  20%|██        | 611/3000 [00:00<00:01, 1545.58it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1664.35it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1672.82it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1628.60it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1655.17it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1676.58it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1691.19it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1654.61it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1666.87it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1633.72it/s]warmup should be done:  28%|██▊       | 841/3000 [00:00<00:01, 1667.09it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1664.46it/s]warmup should be done:  26%|██▌       | 766/3000 [00:00<00:01, 1517.14it/s]warmup should be done:  34%|███▎      | 1009/3000 [00:00<00:01, 1679.26it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1655.04it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1690.56it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1665.61it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1632.99it/s]warmup should be done:  34%|███▎      | 1010/3000 [00:00<00:01, 1672.21it/s]warmup should be done:  34%|███▎      | 1005/3000 [00:00<00:01, 1667.94it/s]warmup should be done:  31%|███       | 923/3000 [00:00<00:01, 1531.99it/s]warmup should be done:  39%|███▉      | 1177/3000 [00:00<00:01, 1679.29it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1691.36it/s]warmup should be done:  39%|███▊      | 1162/3000 [00:00<00:01, 1656.76it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1664.67it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1634.43it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1667.47it/s]warmup should be done:  39%|███▉      | 1179/3000 [00:00<00:01, 1675.54it/s]warmup should be done:  36%|███▌      | 1078/3000 [00:00<00:01, 1536.93it/s]warmup should be done:  45%|████▍     | 1346/3000 [00:00<00:00, 1679.99it/s]warmup should be done:  45%|████▌     | 1357/3000 [00:00<00:00, 1693.45it/s]warmup should be done:  44%|████▍     | 1328/3000 [00:00<00:01, 1656.02it/s]warmup should be done:  45%|████▍     | 1337/3000 [00:00<00:00, 1666.80it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1632.06it/s]warmup should be done:  45%|████▍     | 1340/3000 [00:00<00:00, 1670.88it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1680.40it/s]warmup should be done:  41%|████      | 1236/3000 [00:00<00:01, 1547.43it/s]warmup should be done:  50%|█████     | 1514/3000 [00:00<00:00, 1679.87it/s]warmup should be done:  51%|█████     | 1527/3000 [00:00<00:00, 1692.62it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1657.23it/s]warmup should be done:  50%|█████     | 1504/3000 [00:00<00:00, 1666.71it/s]warmup should be done:  50%|█████     | 1508/3000 [00:00<00:00, 1672.22it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1634.69it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1676.99it/s]warmup should be done:  47%|████▋     | 1396/3000 [00:00<00:01, 1563.68it/s]warmup should be done:  56%|█████▌    | 1683/3000 [00:01<00:00, 1680.95it/s]warmup should be done:  57%|█████▋    | 1697/3000 [00:01<00:00, 1693.13it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1660.43it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1665.28it/s]warmup should be done:  55%|█████▍    | 1642/3000 [00:01<00:00, 1637.93it/s]warmup should be done:  56%|█████▌    | 1677/3000 [00:01<00:00, 1674.90it/s]warmup should be done:  56%|█████▌    | 1686/3000 [00:01<00:00, 1671.99it/s]warmup should be done:  52%|█████▏    | 1553/3000 [00:01<00:00, 1551.80it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1682.30it/s]warmup should be done:  62%|██████▏   | 1867/3000 [00:01<00:00, 1694.56it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1662.52it/s]warmup should be done:  61%|██████▏   | 1838/3000 [00:01<00:00, 1664.49it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1638.87it/s]warmup should be done:  62%|██████▏   | 1846/3000 [00:01<00:00, 1676.87it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1669.14it/s]warmup should be done:  57%|█████▋    | 1714/3000 [00:01<00:00, 1567.81it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1681.18it/s]warmup should be done:  68%|██████▊   | 2037/3000 [00:01<00:00, 1695.30it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1660.69it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1663.45it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1636.63it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1675.62it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1664.67it/s]warmup should be done:  62%|██████▏   | 1871/3000 [00:01<00:00, 1548.85it/s]warmup should be done:  74%|███████▎  | 2207/3000 [00:01<00:00, 1693.76it/s]warmup should be done:  73%|███████▎  | 2190/3000 [00:01<00:00, 1679.17it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1659.47it/s]warmup should be done:  73%|███████▎  | 2182/3000 [00:01<00:00, 1674.43it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1655.50it/s]warmup should be done:  71%|███████   | 2135/3000 [00:01<00:00, 1632.06it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1657.06it/s]warmup should be done:  68%|██████▊   | 2030/3000 [00:01<00:00, 1558.67it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1693.38it/s]warmup should be done:  79%|███████▊  | 2359/3000 [00:01<00:00, 1679.83it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1661.27it/s]warmup should be done:  78%|███████▊  | 2350/3000 [00:01<00:00, 1675.28it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1634.41it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1654.00it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1657.03it/s]warmup should be done:  73%|███████▎  | 2187/3000 [00:01<00:00, 1558.40it/s]warmup should be done:  85%|████████▍ | 2547/3000 [00:01<00:00, 1693.45it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1676.23it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1662.43it/s]warmup should be done:  84%|████████▍ | 2518/3000 [00:01<00:00, 1676.36it/s]warmup should be done:  82%|████████▏ | 2463/3000 [00:01<00:00, 1634.92it/s]warmup should be done:  83%|████████▎ | 2504/3000 [00:01<00:00, 1649.60it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1657.76it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1557.85it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1694.79it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1673.35it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1660.52it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1674.66it/s]warmup should be done:  88%|████████▊ | 2627/3000 [00:01<00:00, 1634.16it/s]warmup should be done:  89%|████████▉ | 2669/3000 [00:01<00:00, 1644.08it/s]warmup should be done:  90%|████████▉ | 2686/3000 [00:01<00:00, 1657.51it/s]warmup should be done:  83%|████████▎ | 2500/3000 [00:01<00:00, 1559.38it/s]warmup should be done:  96%|█████████▌| 2887/3000 [00:01<00:00, 1693.07it/s]warmup should be done:  94%|█████████▍| 2831/3000 [00:01<00:00, 1661.44it/s]warmup should be done:  95%|█████████▌| 2863/3000 [00:01<00:00, 1668.84it/s]warmup should be done:  93%|█████████▎| 2791/3000 [00:01<00:00, 1634.32it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1669.13it/s]warmup should be done:  94%|█████████▍| 2834/3000 [00:01<00:00, 1640.53it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1656.25it/s]warmup should be done:  89%|████████▊ | 2656/3000 [00:01<00:00, 1542.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1676.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1669.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1665.81it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1666.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.31it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1634.56it/s]warmup should be done: 100%|█████████▉| 2999/3000 [00:01<00:00, 1640.99it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.37it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1554.86it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.70it/s]warmup should be done:  99%|█████████▉| 2973/3000 [00:01<00:00, 1560.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1547.14it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3459365190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34593661c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34593651f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34593732b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34596a8730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34596aad30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34593680d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f34596a7e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 05:26:01.434065: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f8702d2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:01.434129: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:01.443770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:01.482471: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f9282c210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:01.482537: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:01.492498: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:01.692846: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f8e8303d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:01.692907: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:01.697193: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f9a833fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:01.697236: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:01.703309: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:01.705818: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:02.166094: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f928378a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:02.166158: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:02.166545: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f8ef91c20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:02.166592: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:02.174105: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:02.174799: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:02.220261: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f9682be00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:02.220327: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:02.227901: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:02.280038: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f2f8a833b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:26:02.280109: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:26:02.288904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:26:08.716571: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:08.749711: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:08.751841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:08.927767: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:08.995953: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:09.182713: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:09.209709: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:26:09.240328: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:26:59.380][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:26:59.381][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:26:59.389][ERROR][RK0][main]: coll ps creation done
[HCTR][05:26:59.389][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][05:26:59.525][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:26:59.525][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:26:59.533][ERROR][RK0][main]: coll ps creation done
[HCTR][05:26:59.533][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][05:26:59.689][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:26:59.689][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:26:59.695][ERROR][RK0][main]: coll ps creation done
[HCTR][05:26:59.695][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][05:26:59.703][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:26:59.703][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:26:59.710][ERROR][RK0][main]: coll ps creation done
[HCTR][05:26:59.710][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:26:59.878][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:26:59.878][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:26:59.885][ERROR][RK0][main]: coll ps creation done
[HCTR][05:26:59.885][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:26:59.926][ERROR][RK0][tid #139842759407360]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:26:59.926][ERROR][RK0][tid #139842759407360]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:26:59.930][ERROR][RK0][tid #139842759407360]: coll ps creation done
[HCTR][05:26:59.931][ERROR][RK0][tid #139842759407360]: replica 2 waits for coll ps creation barrier
[HCTR][05:27:00.029][ERROR][RK0][tid #139842885232384]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:27:00.029][ERROR][RK0][tid #139842885232384]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:27:00.034][ERROR][RK0][tid #139842885232384]: coll ps creation done
[HCTR][05:27:00.034][ERROR][RK0][tid #139842885232384]: replica 1 waits for coll ps creation barrier
[HCTR][05:27:00.062][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:27:00.062][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][05:27:00.066][ERROR][RK0][main]: coll ps creation done
[HCTR][05:27:00.066][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][05:27:00.066][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][05:27:00.904][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][05:27:00.956][ERROR][RK0][tid #139842759407360]: replica 2 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][tid #139842885232384]: replica 1 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][05:27:00.956][ERROR][RK0][tid #139842759407360]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][tid #139842885232384]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][main]: Calling build_v2
[HCTR][05:27:00.956][ERROR][RK0][tid #139842759407360]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][tid #139842885232384]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:27:00.956][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 05:27:00.960346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178[] v100x8, slow pcie
2022-12-12 05:27:00.[9603862022-12-12 05:27:00: .E960423 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :2022-12-12 05:27:00v100x8, slow pcie196.
] 960430assigning 0 to cpu: 
E[ 2022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:960479178: ] Ev100x8, slow pcie [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 0 to cpu[
2022-12-12 05:27:00[2022-12-12 05:27:00.2022-12-12 05:27:00[.960486.960525: [9605302022-12-12 05:27:00: E2022-12-12 05:27:00: [.E .E960527 2022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc960569[ : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 05:27:00:960573178E: .196: ] [ 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc960616] Ev100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :: assigning 0 to cpu 
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8178E2022-12-12 05:27:00
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212
] [ .:] v100x8, slow pcie2022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:960682178[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[
.178: ] 2022-12-12 05:27:00
2022-12-12 05:27:00960785] [Ev100x8, slow pcie..: v100x8, slow pcie2022-12-12 05:27:00[ 
960830960836E
.2022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :  [960875.[:EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:27:00: 9609112022-12-12 05:27:00178  :.E: .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196960957 E960967v100x8, slow pcie::] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc : 
213212assigning 0 to cpuE:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] ] 
 [196: remote time is 8.68421build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:27:00] 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

:[.[assigning 0 to cpu] :196[2022-12-12 05:27:009611152022-12-12 05:27:00
remote time is 8.68421196] 2022-12-12 05:27:00.: .
] assigning 0 to cpu.961176E961208assigning 0 to cpu
[
961200[:  : 2022-12-12 05:27:00: 2022-12-12 05:27:00E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE.E.[ : [961282 9612892022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:27:00: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .:] :.E:E961343212assigning 0 to cpu213961354 214 : ] 
] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421E:cpu time is 97.0588: 

 214
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] ] [:2022-12-12 05:27:00:2022-12-12 05:27:00cpu time is 97.0588build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 05:27:00212.212.

.] 961528] 961537961541build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: : 
E2022-12-12 05:27:00
EE .[  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc9616272022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:27:00:: .::.212E961674214213961688]  : ] ] : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588remote time is 8.68421E
: 

 213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [:[:remote time is 8.684212022-12-12 05:27:002132022-12-12 05:27:00213
.] .] 961841[remote time is 8.68421961848remote time is 8.68421: 2022-12-12 05:27:00
: 
E.E[ [961892 2022-12-12 05:27:00/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:27:00: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:.E:961933213961938 214: ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] Eremote time is 8.68421E:cpu time is 97.0588 
 214
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ::cpu time is 97.0588[214214
2022-12-12 05:27:00] ] .cpu time is 97.0588cpu time is 97.0588962055

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 05:28:17.654542: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:28:17.694772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 05:28:17.826970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:28:17.827031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:28:17.872872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:28:17.872909: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:28:17.873385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:28:17.873433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.874387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.875229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.888057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 05:28:17.888113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 05:28:17.888391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[[2022-12-12 05:28:172022-12-12 05:28:17..888447888434: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 05:28:17::[.205202[2022-12-12 05:28:17888469] ] 2022-12-12 05:28:17.: [worker 0 thread 3 initing device 34 solved.888489E[
2022-12-12 05:28:17
888511:  2022-12-12 05:28:17.: E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.888515E 2022-12-12 05:28:17:888528:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.202: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:888580] E :202: 1 solved /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc1815] E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:] 6 solved :202[Building Coll Cache with ... num gpu device is 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc202] 2022-12-12 05:28:17
:] 5 solved[.2052 solved
2022-12-12 05:28:17888662] 
[.: [worker 0 thread 4 initing device 42022-12-12 05:28:17888685E[2022-12-12 05:28:17
.:  2022-12-12 05:28:17.888705E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.888712:  :888721: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205: E :] E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu205worker 0 thread 1 initing device 1 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:1980worker 0 thread 6 initing device 6:205] 
205] eager alloc mem 381.47 MB] worker 0 thread 5 initing device 5
worker 0 thread 2 initing device 2

[2022-12-12 05:28:17.888937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:28:17.888981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.889120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:28:17.889164: E[ 2022-12-12 05:28:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[:8891722022-12-12 05:28:171980: .] E889192[[eager alloc mem 381.47 MB : 2022-12-12 05:28:172022-12-12 05:28:17
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE..: 8892118892131815/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : ] :EEBuilding Coll Cache with ... num gpu device is 81815  
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuBuilding Coll Cache with ... num gpu device is 8::
1815[1815] 2022-12-12 05:28:17] Building Coll Cache with ... num gpu device is 8.Building Coll Cache with ... num gpu device is 8
[889289
2022-12-12 05:28:17: .E889303 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 05:28:17[E:.2022-12-12 05:28:17 1980889322./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] : 889328:eager alloc mem 381.47 MBE: 1980
 E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu eager alloc mem 381.47 MB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[2022-12-12 05:28:17.893082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.893483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.893541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.893604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.894084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.894190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.894246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.897637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.897936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 05:28:17] .eager alloc mem 381.47 MB897969
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.898036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.898565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.898617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.898665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:28:17.956456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 05:28:17.962505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:28:17.962627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:28:17.963453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:17.964299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:17.965410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:17.965459: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:28:17.982771: E [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[2022-12-12 05:28:172022-12-12 05:28:17:2022-12-12 05:28:172022-12-12 05:28:17..1980..982818982818] 982831982831: : eager alloc mem 5.00 Bytes: : EE
EE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[[2022-12-12 05:28:172022-12-12 05:28:17..989088989088: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-12 05:28:17.989441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:28:17.989526: E[ 2022-12-12 05:28:17/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:989523638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:28:17.989618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:28:17.989660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:28:17.989739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 05:28:17
.989743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:28:17.989827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 05:28:17eager release cuda mem 400000000.
989828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 05:28:17.989917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:28:17.991391: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:17.991908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:17.992422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:17.992949: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:17.993475: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:18.   236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-12 05:28:18eager alloc mem 611.00 KB.
   260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:18.   308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:18.   366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:18.   461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:18.  1249: [E2022-12-12 05:28:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  1250:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 5
[2022-12-12 05:28:18.[  13042022-12-12 05:28:18: .E  1313 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 05:28:18] [:.eager release cuda mem 625663[2022-12-12 05:28:1843  1335
2022-12-12 05:28:18.] : .  1332WORKER[0] alloc host memory 76.29 MBE  1355: 
 : [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE2022-12-12 05:28:18 [: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:28:18638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc  1393:.] :: 638  1405eager release cuda mem 400000000638W] : 
]  eager release cuda mem 5Eeager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43:] 638WORKER[0] alloc host memory 76.29 MB] 
eager release cuda mem 625663[
2022-12-12 05:28:18.  1502: [[W2022-12-12 05:28:182022-12-12 05:28:18 .[./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc  15102022-12-12 05:28:18  1513:: .: 43E  1524E]  :  WORKER[0] alloc host memory 76.29 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccW/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
: :638/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc638] :] eager release cuda mem 62566343eager release cuda mem 400000000
] 
WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:28:18.  1615: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:28:18.  2477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:18.  3264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:28:18.  3996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:18.  4205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:18.  5047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:18.  5091: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:28:18.  5256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:18.  5299: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:28:18. 18850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:28:18. 19499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:28:18. 19567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:28:18. 52047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:28:18. 52691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:28:18. 52734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:28:18. 52825: E[ 2022-12-12 05:28:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: 528461980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:28:18. 53455: E[ 2022-12-12 05:28:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 53466638: ] Eeager release cuda mem 25855 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:28:18. 53513: E[ 2022-12-12 05:28:18/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: 535231980: ] Eeager alloc mem 9.54 GB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:28:18. 53664: [E2022-12-12 05:28:18 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 53679:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-12 05:28:18. 54282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:28:18. 54308: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:28:18:.638 54323] : eager release cuda mem 25855E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:28:18. 54372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:28:18. 54833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:28:18. 55241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:28:18. 55431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:28:18. 55473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:28:18. 55837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:28:18. 55879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:21........661066661066661066661066661074661066661066661069: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 5 init p2p of link 6Device 4 init p2p of link 5Device 7 init p2p of link 4Device 0 init p2p of link 3Device 6 init p2p of link 0Device 3 init p2p of link 2Device 1 init p2p of link 7







[[[[[2022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:21[2022-12-12 05:28:212022-12-12 05:28:21...2022-12-12 05:28:21.[.661624661624661624.661624[2022-12-12 05:28:21661624: : : 661636: 2022-12-12 05:28:21.: EEE: E.661649E   E 661663:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:E :198019801980:1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] ] ] 1980] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB:1980eager alloc mem 611.00 KB


eager alloc mem 611.00 KB
1980] 

] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 05:28:21.662630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.662666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[[2022-12-12 05:28:212022-12-12 05:28:212022-12-12 05:28:21..[.6627556627572022-12-12 05:28:21662758: : .: EE[662771E [ 2022-12-12 05:28:21:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:28:21/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.:662791 :638662797638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638] : ] E:] eager release cuda mem 625663Eeager release cuda mem 625663 638eager release cuda mem 625663
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663:638
638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 05:28:21.676201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 05:28:21.676355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.676490: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 05:28:21.676640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.677267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.677573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.685033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 05:28:21.685189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.685441: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 05:28:21.685587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.685783: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 05:28:21.685848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 05:28:21.685945[: 2022-12-12 05:28:21E. 685954/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 05:28:21.685995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.686036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 05:28:21.686202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.686220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 05:28:21.686377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.686491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 05:28:212022-12-12 05:28:21..686902686903: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663[

2022-12-12 05:28:21.686978: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.687311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.690043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 05:28:21.690154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.690192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 05:28:21.690308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.691057: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.691223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.698013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 05:28:21.698129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.699059: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.699248: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 05:28:21.699372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.700315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.706898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 05:28:21.707018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.707142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 05:28:21.707267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.707311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 05:28:21.707436: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.707803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.708042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.708124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 05:28:21.708251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.708338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.709153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.715225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 05:28:21.715336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.715566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 05:28:21.715677: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.716258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.716601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.718790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 05:28:21.718900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.719074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 05:28:21.719202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.719813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.720121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.721420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 05:28:21.721532: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.721768: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 05:28:21.721887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.722450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.722801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.733006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 05:28:21.733121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.733372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 05:28:21.733487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:28:21.733897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.734257: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:28:21.736773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:28:21.737750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84859 secs 
[2022-12-12 05:28:21.738021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:28:21.738399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:28:21.738478: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:28:21.738761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84944 secs 
[2022-12-12 05:28:21.739170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.84985 secs 
[2022-12-12 05:28:21.739342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85037 secs 
[2022-12-12 05:28:21.743651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[[2022-12-12 05:28:212022-12-12 05:28:21..744053744065: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381955] ] eager release cuda mem 80400000
Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85477 secs 
[2022-12-12 05:28:21.744549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85527 secs 
[2022-12-12 05:28:21.745845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:28:21.746091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:28:21.746271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.85757 secs 
[2022-12-12 05:28:21.746597: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.87317 secs 
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][tid #139842759407360]: replica 2 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][tid #139842885232384]: replica 1 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][tid #139842759407360]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][tid #139842885232384]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:28:21.746][ERROR][RK0][main]: init per replica done
[HCTR][05:28:21.746][ERROR][RK0][tid #139842759407360]: init per replica done
[HCTR][05:28:21.746][ERROR][RK0][tid #139842885232384]: init per replica done
[HCTR][05:28:21.746][ERROR][RK0][main]: init per replica done
[HCTR][05:28:21.746][ERROR][RK0][main]: init per replica done
[HCTR][05:28:21.746][ERROR][RK0][main]: init per replica done
[HCTR][05:28:21.746][ERROR][RK0][main]: init per replica done
[HCTR][05:28:21.749][ERROR][RK0][main]: init per replica done
